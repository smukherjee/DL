1
00:00:00,360 --> 00:00:04,530
В останньому відео ти бачив/ла 
як обчислити передбачення нейронної мережі (НМ),

2
00:00:04,530 --> 00:00:06,610
маючи один тренувальний зразок.

3
00:00:06,610 --> 00:00:11,520
В цьому відео ти побачиш 
як провести векторизацію для багатьох тренувальних зразків.

4
00:00:11,520 --> 00:00:15,350
Результати будуть дуже схожі до тих, що ти бачив/ла в ЛР,

5
00:00:15,350 --> 00:00:19,050
де складанням різних тренувальних зразків в стовпці

6
00:00:19,050 --> 00:00:23,630
матриці ми зможемо застосувати рівняння з попереднього відео

7
00:00:23,630 --> 00:00:27,860
і, трішки змінивши рівняння, 
дозволити НМ обчислити

8
00:00:27,860 --> 00:00:32,340
результати для всіх зразків за, практично, той же час.

9
00:00:32,340 --> 00:00:35,080
Давай детальніше розглянемо як це зробити.

10
00:00:35,080 --> 00:00:40,192
Ось 4 рівняння, які ми отримали в попередньому відео. 
Вони обчислюють z[1],

11
00:00:40,192 --> 00:00:41,348
a[1], z[2] і a[2].

12
00:00:41,348 --> 00:00:46,867
Вони показують як, маючи вектор вхідних ознак x,

13
00:00:46,867 --> 00:00:53,810
можна отримати a[2], що =ŷ для окремого тренувального зразка.

14
00:00:54,920 --> 00:01:00,050
Тепер, коли ми маємо m тренувальних зразків, 
цей процес потрібно повторити

15
00:01:00,050 --> 00:01:01,870
для першого тренувального зразка -

16
00:01:01,870 --> 00:01:06,600
x з верхнім індексом 1 в круглих дужках - щоб обчислити

17
00:01:06,600 --> 00:01:11,062
ŷ⁽¹⁾ - передбачення для 1-ого тренувального зразка.

18
00:01:11,062 --> 00:01:16,537
Потім x⁽²⁾, щоб згенерувати передбачення ŷ⁽²⁾.

19
00:01:16,537 --> 00:01:23,050
І т.д. до x⁽ᵐ⁾, щоб згенерувати передбачення ŷ⁽ᵐ⁾.

20
00:01:23,050 --> 00:01:28,349
Щоб позначити всі ці функції активації,

21
00:01:28,349 --> 00:01:31,669
я буду використовувати позначення a[2]⁽¹⁾.

22
00:01:31,669 --> 00:01:36,676
Це буде a[2]⁽²⁾.

23
00:01:36,676 --> 00:01:40,640
і a[2]⁽ᵐ⁾. Тож

24
00:01:40,640 --> 00:01:46,830
це позначення - a[2]ŷ⁽ⁱ⁾.

25
00:01:46,830 --> 00:01:52,520
"i" в круглих дужках позначає i-ий тренувальний зразок,

26
00:01:52,520 --> 00:01:57,220
а 2 в квадратних дужках позначає 2-ий шар.

27
00:01:58,530 --> 00:02:02,460
Ось що означають індекси в круглих і квадратних дужках.

28
00:02:04,170 --> 00:02:07,920
Якщо подумати, то при невекторизованій реалізації, щоб

29
00:02:07,920 --> 00:02:11,000
обчислити передбачення для всіх тренувальних зразків

30
00:02:11,000 --> 00:02:15,630
потрібно застосувати цикл for по i від 1 до m.

31
00:02:15,630 --> 00:02:18,260
Тоді реалізувати оці 4 рівняння, правильно?

32
00:02:18,260 --> 00:02:24,162
Потрібно обчислити

33
00:02:24,162 --> 00:02:30,064
z[1]⁽ⁱ⁾=w[1]x⁽ⁱ⁾+b[1],

34
00:02:30,064 --> 00:02:38,253
a[1]⁽ⁱ⁾=σ(z[1]⁽ⁱ⁾),

35
00:02:38,253 --> 00:02:43,683
z[2]⁽ⁱ⁾=w[2]a[1]⁽ⁱ⁾+b[2] і

36
00:02:43,683 --> 00:02:50,099
z[2]⁽ⁱ⁾=w[2]a[1]⁽ⁱ⁾+b[2] і

37
00:02:50,099 --> 00:02:56,686
a[2]⁽ⁱ⁾=σ(z[2]⁽ⁱ⁾).

38
00:02:56,686 --> 00:03:03,172
Тож, по суті, до цих рівнянь вгорі ми додаємо верхній індекс i

39
00:03:03,172 --> 00:03:08,788
в круглих дужках. 
До всіх змінних, що залежать від тренувального зразка.

40
00:03:08,788 --> 00:03:12,612
Тобто, додаємо верхній індекс i в круглих дужках до x, z та a,

41
00:03:12,612 --> 00:03:18,570
якщо хочемо обчислити вихідні дані для m тренувальних зразків.

42
00:03:18,570 --> 00:03:23,930
Але ми б хотіли векторизувати ці обчислення і позбутися циклу for.

43
00:03:23,930 --> 00:03:27,680
І, до речі, якщо здається, що я використовую багато базової

44
00:03:27,680 --> 00:03:31,170
лінійної алгебри, то, виявляється, 
що це дуже важливо в еру глибокого навчання,

45
00:03:31,170 --> 00:03:34,580
це дає змогу правильно реалізувати такі обчислення.

46
00:03:34,580 --> 00:03:38,160
І ми ретельно підбирали позначення для цього курсу,

47
00:03:38,160 --> 00:03:41,460
щоб зробити кроки векторизації якомога простішими.

48
00:03:41,460 --> 00:03:46,140
Тож, я надіюсь, що ця база допоможе тобі

49
00:03:46,140 --> 00:03:49,750
швидше і правильно реалізувати ці алгоритми.

50
00:03:51,060 --> 00:03:56,210
Отож. Давай скопіюємо це блок на наступний слайд і

51
00:03:56,210 --> 00:03:57,880
подивимось як їх векторизувати.

52
00:03:59,130 --> 00:04:02,154
Тож ось що ми маємо з попереднього слайду в

53
00:04:02,154 --> 00:04:04,324
циклі for для всіх наших m тренувальних зразків.

54
00:04:04,324 --> 00:04:09,769
Тож згадаймо, що ми визначили матрицю X як

55
00:04:09,769 --> 00:04:16,860
складені ось так в стовпці наші тренувальні зразки.

56
00:04:16,860 --> 00:04:20,180
Тож беремо тренувальні зразки і складаємо їх в стовпці.

57
00:04:20,180 --> 00:04:23,220
Тож це стає n або

58
00:04:23,220 --> 00:04:27,860
nₓ*m розміру матрицю.

59
00:04:29,198 --> 00:04:32,630
Одразу скажу що потрібно зробити щоб

60
00:04:32,630 --> 00:04:35,760
отримати векторизовану реалізацію цього циклу for.

61
00:04:35,760 --> 00:04:41,394
Виявляється, що потрібно обчислити

62
00:04:41,394 --> 00:04:46,035
Z[1]=W[1]X+b[1],

63
00:04:46,035 --> 00:04:50,692
A[1]=σ(Z[1]),

64
00:04:50,692 --> 00:04:56,157
Z[2]=W[2]A[1]+b[2] і

65
00:04:56,157 --> 00:05:01,348
Z[2]=W[2]A[1]+b[2] і

66
00:05:01,348 --> 00:05:10,100
потім A[2]=σ(Z[2]).

67
00:05:10,100 --> 00:05:16,440
Тож це дуже схоже. Ми замінили маленькі x векторів

68
00:05:16,440 --> 00:05:23,480
на великі X матриць, склавши маленькі x в стовпці.

69
00:05:23,480 --> 00:05:28,494
Якщо зробити те саме для z. Наприклад,

70
00:05:28,494 --> 00:05:33,509
якщо взяти z[1]⁽¹⁾, z[1]⁽²⁾ і т.д.

71
00:05:33,509 --> 00:05:40,290
(а вони є векторами-стовпцями) до z[1]⁽ᵐ⁾, так?

72
00:05:40,290 --> 00:05:46,270
Тож, якщо оцю величину взяти і скласти в стовпці,

73
00:05:46,270 --> 00:05:50,045
то ми отримаємо матрицю Z[1].

74
00:05:50,045 --> 00:05:55,299
І, аналогічно, для оцієї величини.

75
00:05:55,299 --> 00:06:00,957
Візьмемо a[1]⁽¹⁾, a[1]⁽²⁾ і т.д. до

76
00:06:00,957 --> 00:06:06,980
a[1]⁽ᵐ⁾ і складемо їх в стовпці.

77
00:06:06,980 --> 00:06:11,610
І тоді (так само як ми перейшли від маленьких x до великих і

78
00:06:11,610 --> 00:06:13,280
від маленьких z до великих)

79
00:06:13,280 --> 00:06:20,920
перейдемо від маленьких a, векторів, до великої A[1]

80
00:06:20,920 --> 00:06:26,685
ось тут і, аналогічно, до Z[2] і A[2].

81
00:06:26,685 --> 00:06:30,141
Вони так само отримуються оцими векторами

82
00:06:30,141 --> 00:06:32,016
складеними по горизонталі

83
00:06:32,016 --> 00:06:37,326
і оцими векторами складеними по горизонталі,

84
00:06:37,326 --> 00:06:40,840
щоб отримати велику Z[2] і велику A[2].

85
00:06:40,840 --> 00:06:44,042
Одна з властивостей таких позначень, яка, можливо, допоможе

86
00:06:44,042 --> 00:06:47,391
краще це уявити, - це те, що ці матриці Z та A

87
00:06:47,391 --> 00:06:51,420
горизонтально індексують тренувальні зразки.

88
00:06:51,420 --> 00:06:55,631
Ось чому горизонтальний [стовпцевий] індекс 
відповідає тренувальним зразкам,

89
00:06:55,631 --> 00:06:59,730
коли ми переміщуємось вліво-вправо, 
то ми переміщуємось по тренувальних комірках.

90
00:06:59,730 --> 00:07:04,617
А вертикально, вертикальний [стовпцевий] індекс відповідає

91
00:07:04,617 --> 00:07:06,130
вузлам НМ.

92
00:07:06,130 --> 00:07:11,077
Тож, наприклад, оцей вузол, оця величина вгорі

93
00:07:11,077 --> 00:07:16,554
зліва відповідає активаторам

94
00:07:16,554 --> 00:07:21,633
першого прихованого вузла першого тренувального зразка.

95
00:07:21,633 --> 00:07:25,812
Величина на одну позицію нижче 
відповідає активатору в 2-ому прихованому вузлі

96
00:07:25,812 --> 00:07:27,525
1-ого тренувального зразка.

97
00:07:27,525 --> 00:07:31,505
Далі 3-ій прихований вузол 1-ого тренувального зразка і т.д.

98
00:07:31,505 --> 00:07:37,540
Тож, переміщаючись вниз, 
індекс відображає позицію прихованого вузла.

99
00:07:39,670 --> 00:07:42,564
В той же час, переміщуючись горизонтально, 
від 1-ого прихованого вузла

100
00:07:42,564 --> 00:07:45,450
1-ого тренувального зразка, до 1-ого прихованого вузла, але

101
00:07:45,450 --> 00:07:48,240
вже 2-ого тренувального зразка, 3-ого тренувального зразка

102
00:07:48,240 --> 00:07:53,718
і т.д. до оцього вузла, що відповідає активатору 1-ого

103
00:07:53,718 --> 00:07:59,030
прихованого вузла останнього тренувального зразка -
 n-ого тренувального зразка.

104
00:08:00,760 --> 00:08:07,663
Зрозуміло? Тож по матриці A 
горизонтально ми рухаємось по тренувальних зразках,

105
00:08:10,150 --> 00:08:14,195
а вертикально - індекси матриці

106
00:08:14,195 --> 00:08:17,589
відповідають прихованим вузлам.

107
00:08:22,342 --> 00:08:26,870
Подібним чином побудовані і матриці Z та X,

108
00:08:26,870 --> 00:08:31,840
в яких горизонтально індекси відповідають тренувальним зразкам,

109
00:08:31,840 --> 00:08:36,227
а вертикально - вхідним ознакам, які,

110
00:08:36,227 --> 00:08:41,180
насправді, є вузлами вхідного шару НМ.

111
00:08:42,750 --> 00:08:46,600
Тож, маючи ці рівняння, ти тепер знаєш як реалізувати НМ

112
00:08:46,600 --> 00:08:51,320
з векторизацією, векторизацією для багатьох зразків.

113
00:08:51,320 --> 00:08:55,130
В наступному відео я постараюсь обґрунтувати чому

114
00:08:55,130 --> 00:08:59,070
це правильна реалізація цього типу векторизації.

115
00:08:59,070 --> 00:09:03,468
Виявляється, що обґрунтування подібне до того, 
яке ми вже бачили для ЛР.

116
00:09:03,468 --> 00:09:05,300
Давай перейдемо до наступного відео.