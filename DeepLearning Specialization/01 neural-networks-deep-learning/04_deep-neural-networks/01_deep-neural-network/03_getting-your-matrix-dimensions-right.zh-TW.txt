當建置深度神經網路時
一個我常用的除錯工具來 檢查我的程式
是拿一張紙 一一檢查我用的
矩陣的維度 讓我向您展示如何做到
因為我希望這會讓您 建置您的深度網路容易一點 大寫 L 等於 5, 很快的算
不算輸入層 這個有五層
四層隱藏層跟一層輸出層 所以
當您建置正向傳播時 第一步會是 z1 = w1x + b1 讓我們先忽略偏差值 b
先將注意力放在參數 w 第一個隱藏層有三個
隱藏單元, 這是第 0 層 第一層, 第二層, 第三層
第四層，第五層 用我們之前的記號
我們有 n1 也就是第一層
的隱藏單元數目, 等於 3 這裡我們有
 n2 等於 5 n3 等於 4, n4 等於 2
n5 等於 1 目前為止我們看過的神經網路
只有一個輸出單元, 但在以後 課程裡, 我們會談到神經網路
也有多個輸出單元 最後對於輸入層 我們有 n0 = nx = 2 現在, 讓我們想想看
z, w, 跟 x 的維度 z 是啟動向量對於 第一個隱層層, 所以
z 會是 3 乘 1 這會是 維度 3 向量 我將它寫成 n1
乘 1 維度向量 n1 乘 1 維度矩陣
所以是 3 乘 1 在這個例子 那有關輸入特徵 x 呢
我們有兩個輸入特徵 所以 x 在這個例子是 2 乘 1
一般而言會是 n0 乘 1 我們需要
矩陣 w1 是某些當我們 乘上 n0 乘 1 向量時
我們會得到 n1 乘 1 向量, 是吧? 您會有 3 
維向量等於 某項乘上一個 2 維向量 所以
用矩陣相乘規則 這應該會是 3 乘 2 矩陣 因為 3 乘 2 矩陣
乘上 2 乘 1 矩陣, 或者說 乘上 2 乘 1 向量
會給您 3 乘 1 向量 一般化以後, 這會是
一個 n1 乘 n0 維度的矩陣 所以我們發現 w1 的維度會是 n1 乘 n0 而更一般化, 
wl 的維度會是 nl 乘 nl-1 舉個例子, w2 的維度 會是 5 乘 3 或者說 n2 乘 n1 因為我們將計算 z2 為 w2 乘 a1, 再一次 讓我們先忽略偏差值 所以這會是 3 乘 1 而我們需要這個為 5 乘 1, 所以 這個應該是 5 乘 3 同樣的, w3
是下一層的維度 逗號, 
前一層的維度 所以這會是 4 乘 5, w4 會是 2 乘 4, 而 w5 會是 1 乘 2 對吧? 所以一般公式
來檢查當 您建置
l 層矩陣時 這個矩陣
維度會是 nl 乘 nl - 1 現在讓我們看看
向量 b 的維度 這個會是 3 乘 1 向量
所以您需要加另一個 3 乘 1 向量，為了要
得到 3 乘 1 向量的結果 或者在這個例子, 我們需要這個
會是 5 乘 1 所以這會是
另一個 5 乘 1 向量 為了要
將這兩項我用框框畫起來相加 它也是 5 乘 1 向量 所以一般化的規則
在左邊這個例子 b1 是 n1 乘 1, 也就是 3 乘 1 在第二個例子
這是 n2 乘 1 所以更一般化後 bl 應該是 nl 乘 1 維度 希望這兩個方程式幫助
您再次檢查 您個矩陣 w 維度跟您的
向量 b 是正確的維度 當然
如果您建置反向傳播 dw 的維度應該
跟 w 維度一樣 所以 dw 跟 w 維度一樣 db 應該跟 b 維度一樣 其他重要的變數
也需要檢查維度的是 z, x, 跟 a of l
我們並沒多談 因為 z of l 等於 
g of l 用逐元素運算 所以 z 跟 a 應該
是同一維度在這樣的網路 現在我們來看看當您
向量化建置同時在 多重例子上 即使是向量化建置 當然 w b
dw 跟 db 的維度會是一樣 但是 z, 
a, x 的維度 會在向量化建置時改變 前面 我們用 z1 = w1x + b1 而這是 n1 乘 1 也就是 n1 乘 n0 x 是 n0 乘 1, b 是 n1 乘 1 在向量化 建置您 有 Z1 = W1X + b1 現在 Z1 是
拿 每一個單獨的例子
所以 z11, z12 直到 z1m, 疊起來
這給您 Z1 所以 Z1 的維度
不再是 n1 乘 1 它會是 n1 乘 m
m 是您訓練集的大小 W1 的維度不變
所以還是 n1 乘 n0 而 X, 不再是 n0 乘 1 所有您的訓練例子
水平疊起來 現在是 n0 乘 m
您注意到當您 用 n1 乘 n0 矩陣
乘上一個 n0 乘 m 矩陣 它們一起會給您一個
預期中的 n1 乘 m 維度矩陣 最後一個細節是
 b1 還是 n1 乘 1, 但是 當您用這個加上 b
應用 Python 傳播 這會複製成為 n1 乘
 m 矩陣, 然後逐元素相加 在前面的影片中, 我們談到
 w, b, dw 跟 db維度 這裡我們看到 zl 跟 al 的維度是 nl 乘 1 我們現在的
Zl 跟 Al 是 nl 乘 m 而特別情況是
當 l 等於 0 這種情況下 A0, 也就是等於 您的訓練集輸入特徵 X 是等於
期望中的 n0 乘 m 當然當您
建置這個在反向傳播時 我們晚點會看到
計算 dZ 跟 dA 這些當然是 跟 Z 跟 A 同維度 我希望這段小小的練習
幫助您釐清 您使用矩陣的維度 當您建置反向傳播對於
一個神經網路時，記得當您 寫您的程式時請確認所有
這些矩陣的維度是一致的 這通常會有幫助 這會某種程度消除
一些造成錯誤的原因 我希望這個練習來找出
不同矩陣的維度 您會覺得有幫助 當您建置一個深度學習網路
如果您確保了 這些您使用的矩陣
跟向量的維度 希望會幫助您去除
一些可能的錯誤形成 這當然幫助我使得我的程式正確 下一段, 我們已經看過一些
正向傳播如何作用的機制 在一個神經網路 但為什麼深度學習網路
如此有效率 為什麼它們比
淺層做得更好 讓我們花幾分鐘在
下段影片來討論它