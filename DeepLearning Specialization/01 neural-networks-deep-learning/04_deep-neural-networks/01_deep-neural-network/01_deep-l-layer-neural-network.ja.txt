このコースの４週目に
ようこそ これまでに ニューラルネットワークでの
隠れ層が１つの時の 順誤差伝播法と逆誤差伝播法と
ロジステック回帰を見てきました ベクトル化についてや
重みを乱数的に初期化することが 重要な時について学びました これまでの数週の宿題を
行って実装も行っていたら 自分自身でこれらの考えが
動作することを見てきました 今では 深層ニューラルネットワークを実装するのに必要な
考えはほとんど見てきました 今週行うのは これらの考えを
つなげてみて 自分自身で深層ニューラルネットワークを
実装できるようにすることです 今週のコーディング演習は
長なっているので 作業が多くなっています
このため今週のビデオは 短くしておきました
ビデオは素早く見終わって より多くの時間をプログラム演習に
避けるようにしたいです これによって誇りに思えるような
深層ニューラルネットワークを作った経験を残したいです 深層ニューラルネットワークとは
何でしょうか ロジステック回帰のこの絵は
見てきましたね 隠れ層１つのニューラルネットワークも
見てきました これは 隠れ層２つの
ニューラルネットワークの例と 隠れ層５つのニューラルネットワークの例です これを見ると ロジステック回帰は
非常に浅いモデルで 一方で ここのモデルは
それよりかなり深いモデルです 浅いか深いかは
程度の問題です 右上は隠れ層１つの
ニューラルネットワークで ２層のニューラルネットワークです 層の数え方を覚えていますか
入力層は数えずに 隠れ層と出力層だけを
数えます これは２層ニューラルネットワークですが
まだ浅いものですが ロジステック回帰ほどは
浅くありません 技術的に言えば ロジステック回帰は
層１つのニューラルネットワークです ここ数年にわたって
AIまたは 機械学習のコミュニティでは
非常に深い深層ニューラルネットワークでは学習できて 浅いようなモデルでは
学習できないことが多いと実感してきています ただどんな問題であっても
事前にどの程度の深さのネットワークが良いかを予測するのは 難しいでしょう このため ロジステック回帰を試して
隠れ層１つを試し ２つを試した後に
隠れ層の数を様々な値を試せるような ハイパーパラメーターとして
見なしても良いでしょう ホールドアウト交差検証を行ったり
Dev セットの評価に使ってもよいでしょう 後ほど
詳しく見ていきましょう 今は 深層ニューラルネットワークを表すための
表記方法を見ていきましょう ここには１、２、３、４層の
ニューラルネットワークがあります ３つの隠れ層で
隠れ層のユニットの数は ５、５、３で
１つの出力ユニットがあります 使っていく表記方法は 大文字のLで
ネットワークの層の数を表します この場合にはL=4で
これが層の数です n[l]を使って
小文字のl で その層での ノードやユニットの数を表します これに索引を付ければ
入力層を０とし これを１ これを２
これを３ これを４層目とします こうすると
n[1]はこれです 最初の隠れ層でn[1]=5です
５つの隠れユニットがあるためです これについては n[2]=5で ２つ目の隠れ層のユニットの数です n[3]=3 です n[4]=n[L]で
出力ユニットの数は１です 大文字のLは４だからです この入力層の部分では n[0]=nx=3です これがそれぞれの層にあるノードの数を
表わす表記方法 です それぞれの層Lに対して a[l]と書いて
l層の活性化を表します 後に順誤差伝播法で
これを見ることになります a[l]=活性化のg[l](Z[l])で 恐らく活性化は
層lで索引を付けます W[l]を使って重みを表します 重みでz[l]として計算します b[l]もz[l]を計算するのに使われます 最後に表記方法を纏めると
入力特徴量はX です 層０の活性化は
a[0]=Xです 最後の層の活性化は
a[L]=yハットです a[L]はニューラルネットワークの
予測値のｙハットと出力予測と同じです これで深層ニューラルネットワークが
どんなものか分かりましたし 深層ネットワークで記述し計算する
今後使っていく表記も学びました このビデオで多くの表記を
紹介しましたが もし単純な表記について忘れてしまったら
コースのサイトにも表記のシートを載せましたので見てください 探して様々な表記を探すこともできます
表記方法のシートやガイドもあります 次回は 順誤差伝播法では
この種のネットワークを記述するとどうなるのかお見せしようと 思っています 次のビデオを見てみましょう