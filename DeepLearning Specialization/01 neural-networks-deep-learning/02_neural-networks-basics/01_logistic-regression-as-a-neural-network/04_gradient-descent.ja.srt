1
00:00:00,590 --> 00:00:03,210
ロジスティック回帰モデルを見てきました。

2
00:00:03,210 --> 00:00:06,560
また、単一の教師サンプルに対しての予測が
どれだけうまくいっているかを測るという

3
00:00:06,560 --> 00:00:08,780
損失関数についても学習しました。

4
00:00:08,780 --> 00:00:13,530
さらに、教師サンプル全体に対して
パラメータwとbがどれくらいうまくいっているかを測る

5
00:00:13,530 --> 00:00:16,590
コスト関数についても見てきました。

6
00:00:16,590 --> 00:00:21,600
では、教師サンプルにおけるパラメータwとbを
学習するために

7
00:00:21,600 --> 00:00:25,730
最急降下法をどのように使えるかをお話ししましょう。

8
00:00:25,730 --> 00:00:30,030
おさらいすると、
これが見慣れたロジスティック回帰のアルゴリズムです。

9
00:00:31,130 --> 00:00:34,700
そして二行目にはコスト関数Jがあり、

10
00:00:34,700 --> 00:00:37,879
これはパラメータwとbの関数です。

11
00:00:37,879 --> 00:00:39,960
この関数は、損失関数の平均として定義されています。

12
00:00:39,960 --> 00:00:44,140
よって、損失関数の総和のm分の1となっています。

13
00:00:44,140 --> 00:00:48,470
つまり、損失関数はアルゴリズムが

14
00:00:48,470 --> 00:00:53,170
それぞれの教師サンプルに対する
実際にラベル付けされたy(i)と比べて

15
00:00:53,170 --> 00:00:58,000
どれだけうまくyハット(i)を予測できているかを測ります。

16
00:00:58,000 --> 00:01:00,886
右側に展開された式が書かれています。

17
00:01:00,886 --> 00:01:04,130
コスト関数は、訓練データにおいて

18
00:01:04,130 --> 00:01:06,760
パラメータwとbがうまくいっているかを測ります。

19
00:01:06,760 --> 00:01:11,510
パラメータwとbを学習するために、

20
00:01:11,510 --> 00:01:17,930
コスト関数J(w,b)をできる限り最小にする
wとbを見つけたいと考えるのが自然ですね。

21
00:01:17,930 --> 00:01:21,320
これが、最急降下法の図解です。

22
00:01:21,320 --> 00:01:25,320
この図において、水平方向の軸は

23
00:01:25,320 --> 00:01:28,510
パラメータwとbを表しています。

24
00:01:28,510 --> 00:01:32,350
実際に使うときにはwはもっと高次元になりますが、
グラフ化するにあたって、

25
00:01:32,350 --> 00:01:38,190
wを一つの実数とし、
bも一つの実数としておきましょう。

26
00:01:38,190 --> 00:01:40,770
コスト関数J(w,b)は、

27
00:01:40,770 --> 00:01:45,130
水平方向の軸wとbの上にある面となります。

28
00:01:45,130 --> 00:01:50,720
面の高さは、ある一点におけるJ(w,b)の値を示します。

29
00:01:50,720 --> 00:01:55,070
ここで私たちがやりたいことは、

30
00:01:55,070 --> 00:01:59,730
Jが最小になっている部分に対応する
wとbの値を見つけることです。

31
00:02:00,830 --> 00:02:06,050
コスト関数Jは凸の形であることがわかります。

32
00:02:06,050 --> 00:02:10,327
なので、これはただの大きなお椀のようなものです。

33
00:02:10,327 --> 00:02:13,717
こういった凸状の関数は、

34
00:02:13,717 --> 00:02:18,120
このように凸状でないため
いくつもの局所最適がある関数とは対極のものです。

35
00:02:18,120 --> 00:02:22,240
ここで定義されたコスト関数J(w,b)が
凸状であるというのは、

36
00:02:22,240 --> 00:02:27,020
私たちがこのコスト関数Jを
ロジスティック回帰に使うことの

37
00:02:27,020 --> 00:02:29,610
最大の理由の一つです。

38
00:02:29,610 --> 00:02:33,810
パラメータとして良い値を見つけるために、

39
00:02:33,810 --> 00:02:39,160
まずwとbを何らかの値に初期化します。

40
00:02:39,160 --> 00:02:43,360
その初期値をこの小さな赤い点で示します。

41
00:02:43,360 --> 00:02:47,562
ロジスティック回帰では
ほとんどどんな初期化方法もうまくいきますが、

42
00:02:47,562 --> 00:02:50,690
たいていは0に初期化するでしょう。

43
00:02:50,690 --> 00:02:52,910
ランダムに初期化する方法でもいけますが、

44
00:02:52,910 --> 00:02:55,630
ロジスティック回帰においてはあまり使われません。

45
00:02:55,630 --> 00:02:59,310
しかし、この関数が凸状であるため、
どんな初期値に設定したとしても

46
00:02:59,310 --> 00:03:02,180
だいたい同じ点に辿り着くでしょう。

47
00:03:02,180 --> 00:03:06,450
最急降下法は、この初期値の点から出発し、

48
00:03:06,450 --> 00:03:10,310
最も下りの勾配が急になる方向に一歩進みます。

49
00:03:10,310 --> 00:03:15,290
そして、ロジスティック回帰で一歩進んだ後は
このあたりにいるでしょう。

50
00:03:15,290 --> 00:03:19,320
なぜなら下りの勾配が最も激しくなる方向、

51
00:03:19,320 --> 00:03:21,250
つまり最速で坂を下れる方向に進もうとするからです。

52
00:03:21,250 --> 00:03:23,600
これが、
最急降下法が繰り返しの中の一回でやることです。

53
00:03:23,600 --> 00:03:27,084
二回繰り返すとここにきて、

54
00:03:27,084 --> 00:03:28,830
三回目はここ…という感じです。

55
00:03:28,830 --> 00:03:32,640
たぶんここではグラフの裏側に隠れてしまっていますが、

56
00:03:32,640 --> 00:03:38,880
最終的には大域解かそれに近い場所に収束します。

57
00:03:38,880 --> 00:03:42,300
この絵は最急降下法の図解です。

58
00:03:42,300 --> 00:03:44,310
もう少し詳しく描きましょう。

59
00:03:44,310 --> 00:03:47,750
図解するために、
J(w)という関数を最小化したいと仮定します。

60
00:03:47,750 --> 00:03:51,700
その関数はこのような形をしているでしょう。

61
00:03:51,700 --> 00:03:54,650
描きやすくするために、今はbを無視しています。

62
00:03:54,650 --> 00:03:59,210
そうすることで、
高次元ではなく一次元で表すことができます。

63
00:03:59,210 --> 00:04:01,240
最急降下法が行うのは、

64
00:04:01,240 --> 00:04:06,740
次のようなアップデートを繰り返し行うということです。

65
00:04:06,740 --> 00:04:09,467
wの値を取ってアップデートし、

66
00:04:09,467 --> 00:04:12,508
コロンイコールという記号を使って
wをアップデートすることを表しますね、

67
00:04:12,508 --> 00:04:17,426
wをw引くαかける…

68
00:04:17,426 --> 00:04:22,200
dJ(w)/dwというのは微分です。

69
00:04:22,200 --> 00:04:26,230
これを、アルゴリズムが収束するまで繰り返します。

70
00:04:26,230 --> 00:04:30,666
表記法についていくつか説明すると、
ここのαは学習率で、

71
00:04:30,666 --> 00:04:36,820
これは最急降下法の繰り返しの都度
どれだけ一歩を大きくするかを決めるものです。

72
00:04:36,820 --> 00:04:41,200
後で学習率αを決めるいくつかの方法をお話しします。

73
00:04:41,200 --> 00:04:44,490
二つ目はこれです。これは微分で、

74
00:04:44,490 --> 00:04:48,010
基本的にはパラメータwについて行いたい変更です。

75
00:04:48,010 --> 00:04:52,700
最急降下法を実装するコードを書き始めるときには、

76
00:04:52,700 --> 00:04:57,380
コード中の変数名dwが
この微分の項を表すという慣習を

77
00:04:58,620 --> 00:05:02,300
使うことにします。

78
00:05:02,300 --> 00:05:06,551
そのため、コードを書くときには、

79
00:05:06,551 --> 00:05:10,046
w:=w-α×dwという感じで書くことになるでしょう。

80
00:05:10,046 --> 00:05:14,750
つまり、dwという変数名でこの微分の項を表す
ということです。

81
00:05:14,750 --> 00:05:19,330
では、この最急降下法のアップデートの意味が
わかるように説明しましょう。

82
00:05:19,330 --> 00:05:21,880
wがここにあるとします。

83
00:05:21,880 --> 00:05:26,060
すると、コスト関数J(w)では
ここにいることになります。

84
00:05:26,060 --> 00:05:29,270
微分の定義は

85
00:05:29,270 --> 00:05:31,420
関数のある一点での傾きであることを
思い出してください。

86
00:05:31,420 --> 00:05:36,190
関数の傾きは、

87
00:05:36,190 --> 00:05:40,290
この点でJ(w)のタンジェントとなる三角の高さを
幅で割ったものとなります。

88
00:05:40,290 --> 00:05:43,900
ここでは、微分した結果は正の値です。

89
00:05:43,900 --> 00:05:48,830
wはw引く学習率かける微分にアップデートされます。

90
00:05:48,830 --> 00:05:53,310
微分が正ということは、wから引くことになるので、

91
00:05:53,310 --> 00:05:55,260
左側に進むことになります。

92
00:05:55,260 --> 00:05:59,380
このように、もしwを大きい数字から始めた場合は、

93
00:05:59,380 --> 00:06:04,450
最急降下法はアルゴリズムが
ゆっくりとパラメータを小さくするように働くのです。

94
00:06:04,450 --> 00:06:08,545
もう一つの例として、もしwがここにあれば、

95
00:06:08,545 --> 00:06:15,050
この点での傾きdJ/dwは負であるので、

96
00:06:15,050 --> 00:06:22,771
最急降下法はwをw引くαかける負の数値に
アップデートします。

97
00:06:22,771 --> 00:06:27,122
すると、繰り返しを続けることで

98
00:06:27,122 --> 00:06:31,530
wはだんだんと大きくなっていきます。

99
00:06:31,530 --> 00:06:34,387
つまり、右から出発しようと左から出発しようと、

100
00:06:34,387 --> 00:06:39,000
最急降下法が大域解に導いてくれるのです。

101
00:06:39,000 --> 00:06:43,100
もしあなたが微分や微積分学に詳しくなくて、

102
00:06:43,100 --> 00:06:49,710
dJ(w)/dwという項の意味がわからなくても、
気にしないでください。

103
00:06:49,710 --> 00:06:53,770
次のビデオで微分についてもう少しお話しします。

104
00:06:53,770 --> 00:06:56,761
微積分学について深い知識があれば、

105
00:06:56,761 --> 00:07:02,321
ニューラルネットワークの働きについて
より深い直感を得ることができるでしょう。

106
00:07:02,321 --> 00:07:05,471
でも、もしそれほど微積分学に詳しくなくても、

107
00:07:05,471 --> 00:07:10,091
次のいくつかの動画で微分について
十分な直感をお伝えするので、

108
00:07:10,091 --> 00:07:14,980
あなたもニューラルネットワークを
効果的に使えるようになるでしょう。

109
00:07:14,980 --> 00:07:16,410
しかし今の時点では、

110
00:07:16,410 --> 00:07:21,520
この項が関数の傾きを表していて、

111
00:07:21,520 --> 00:07:26,760
勾配を最も急に下る方向に行くために、

112
00:07:26,760 --> 00:07:31,140
今のパラメータにおいての関数の傾きを知りたい

113
00:07:31,140 --> 00:07:35,450
という直感があれば大丈夫です。

114
00:07:36,660 --> 00:07:42,520
wというパラメータしかなかった場合の
J(w)の最急降下法を描きました。

115
00:07:42,520 --> 00:07:47,150
ロジスティック回帰では、
関数にはwとbの両方があります。

116
00:07:47,150 --> 00:07:50,894
その場合、この最急降下法の内側のループが、

117
00:07:50,894 --> 00:07:53,302
次のように繰り返されなければなりません。

118
00:07:53,302 --> 00:07:57,970
wをw-α×wについてのJ(w,b)の微分に

119
00:07:57,970 --> 00:08:02,030
更新することになります。

120
00:08:02,030 --> 00:08:07,460
そして、bはb-α×bについてのコスト関数の微分に

121
00:08:07,460 --> 00:08:12,270
更新されます。

122
00:08:12,270 --> 00:08:17,300
下に書いたこれらの二つの式が、実際に実装するアップデートです。

123
00:08:17,300 --> 00:08:22,320
余談ですが、微積分学での表記法の慣習について

124
00:08:22,320 --> 00:08:24,560
少しややこしいところがあるので言及しておきます。

125
00:08:24,560 --> 00:08:28,387
微積分学を理解するのがそれほど重要だとは思いませんが、

126
00:08:28,387 --> 00:08:32,411
あなたがこれを目にしたときに悩まないようにしておきたいのです。

127
00:08:32,411 --> 00:08:35,519
微積分学では、この項は、

128
00:08:35,519 --> 00:08:40,730
実際にはこのように面白いくねっとした記号で書きます。

129
00:08:40,730 --> 00:08:46,160
これはちょっと変わったフォントでの
dの小文字に過ぎませんが、

130
00:08:46,160 --> 00:08:51,070
これを見たときには

131
00:08:51,070 --> 00:08:56,145
関数J(w,b)がどれだけ

132
00:08:56,145 --> 00:09:01,580
wの方向に傾いているのかを示しているだけなんだと思ってください。

133
00:09:01,580 --> 00:09:06,640
微積分学における表記法では、

134
00:09:06,640 --> 00:09:11,780
私は論理的だと思いませんし

135
00:09:11,780 --> 00:09:16,940
無駄に物事を複雑にするだけだと思いますが、

136
00:09:16,940 --> 00:09:21,550
もしJに二つ以上の変数があれば、小文字のdの代わりにこの面白い記号を使うのです。

137
00:09:21,550 --> 00:09:24,380
これは偏微分の記号と呼ばれます。

138
00:09:24,380 --> 00:09:26,120
でもそんなに気にしないでください。

139
00:09:26,120 --> 00:09:31,090
もしJが一つの変数についての関数なら、dの小文字を使います。

140
00:09:31,090 --> 00:09:33,960
つまり、この面白い偏微分の記号を使うか

141
00:09:33,960 --> 00:09:38,040
それとも上記のように小文字のbを使うかの違いは、

142
00:09:38,040 --> 00:09:41,570
Jが複数の変数についての関数であるかどうかということです。

143
00:09:41,570 --> 00:09:45,900
複数の変数があるならこの偏微分の記号を使うし、

144
00:09:45,900 --> 00:09:51,480
変数が一つだけなら小文字のdを使います。

145
00:09:51,480 --> 00:09:55,410
これは私にとっては無駄に複雑なだけに見える

146
00:09:55,410 --> 00:09:58,540
微積分学における面白い表記ルールの一つです。

147
00:09:58,540 --> 00:10:03,300
もしこの偏微分の記号を目にしたら、

148
00:10:03,300 --> 00:10:07,290
ある一つの変数について
関数の傾きを測ったものだと思ってください。

149
00:10:07,290 --> 00:10:12,530
微積分学の形式的に正しい様式に準拠すると、

150
00:10:12,530 --> 00:10:18,070
このJには一つではなく二つの変数があるので、

151
00:10:18,070 --> 00:10:22,540
下に書かれたこれは
偏微分の記号で表されるべきですね。

152
00:10:22,540 --> 00:10:28,290
でも、小文字のdとそれほど意味は変わりません。

153
00:10:28,290 --> 00:10:31,360
最後に、これをコードで実装するときには、

154
00:10:31,360 --> 00:10:36,220
wをどれくらいアップデートするかというこの量を

155
00:10:36,220 --> 00:10:41,980
変数dwとして表すという慣習を使います。

156
00:10:41,980 --> 00:10:44,220
bをどれくらいアップデートするかという量も

157
00:10:44,220 --> 00:10:47,230
同様に、

158
00:10:47,230 --> 00:10:50,740
コードではdbと表します。

159
00:10:50,740 --> 00:10:55,580
ここまでが、最急降下法を実装する方法です。

160
00:10:55,580 --> 00:10:59,830
もし微積分に何年も触れていないなら、

161
00:10:59,830 --> 00:11:03,770
ここまでで出てきた微分が多すぎると
感じるかもしれません。

162
00:11:03,770 --> 00:11:06,330
でも、そう感じていても大丈夫です。

163
00:11:06,330 --> 00:11:10,150
次の動画で、微分についてもっと直感を養います。

164
00:11:10,150 --> 00:11:13,560
微積分について
数学的にそんなに深く理解していなくとも、

165
00:11:13,560 --> 00:11:16,310
直感的理解があるだけで、

166
00:11:16,310 --> 00:11:19,130
ニューラルネットワークを
効率的に使うことができますよ。

167
00:11:19,130 --> 00:11:22,743
さあ、次の動画に進んで、

168
00:11:22,743 --> 00:11:23,470
もう少し微分についてお話ししましょう。