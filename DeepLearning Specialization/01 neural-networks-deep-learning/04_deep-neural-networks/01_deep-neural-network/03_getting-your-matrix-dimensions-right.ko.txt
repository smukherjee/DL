심층신경망을 도입하는 경우에 디버깅을 
찾기 위한 도구로, 저는 코딩을 correctness를 확인하기 위해 종이를 한 장 꺼내서 제가 작업하고 있는 매트릭스의 다이멘션을 공부합니다. 어떻게 하는지 보여드리겠습니다.
이 방법이 여러분이 심층 네트워크를 도입하는데 도움을 줄 수 있을 것이기 때문이죠. 대문자 L은 5입니다. 
재빨리 계산하면, 입력값 층은 제외하면, 여기는 5개의 층이 있는데요, 
4개의 숨겨진 레이어와 1개의 결과값 층입니다. 전 방향전파을 도입하면, 첫번째 단계는 z1=w1x + b1입니다. b라는 bias 항은 일단 무시하겠습니다.
그리고 w 매개변수에 집중하겠습니다. 첫번째 숨겨진 레이어는 3개의 숨겨진 유닛이 있는데요, 
이것은 층 0이구요, 층 1, 층 2, 층 3, 
층 4, 그리고 층 5입니다. 이전 비디오에서의 표기를 그대로 쓰면, n1, n1은 층1에 있는 숨겨진 유닛의 개수입니다. 이 값은 3이죠. 그리고 n2는 그러면 5가 되구요, n3는 4이구요, 
n4는 2, n5는 1이 될 것입니다. 현재까지는 오로지 하나의 결과값 유닛만 있는 신경망에 대해서만
다루었는데요, 나중 코스에서는 복수의 결과값 유닛을 갖는 신경망 네트워크에 대해서도 
이야기하겠습니다. 마지막으로, 입력값 층으로, n0= nx=2 입니다. 그러면 이제, z, w, 그리고 x의 
다이멘션도 한번 생각해보겠습니다. z는 첫번째 숨겨진 레이어의 activation 벡터인데요
그러므로, z는 3 x 1일 것입니다. 즉, 3차원의 벡터일 것입니다. 그럼 저는 이것을 n1, 1차원이 벡터로 표기할 것입니다. n1 x 1차원이 매트릭스인 것이죠. 
이 경우에는 3 x 1 이 것입니다. 입력 특성 x 는 어떨까요? 
2개의 입력 특성이 있습니다. x는 이 경우, 2 x 1이고, 
더 일반적으로는 n0 x 1일 것입니다. w1 매트릭스로 필요한 것은, n0 x 1 벡터를 곱할때, 
n1 x 1 벡터가 나오는 값이 필요합니다. 그럼 이렇게 3차원의 벡터는 어떤 거 곱하기 2차원 벡터인 경우를 찾습니다. 그러면 매트릭스의 곱하기 규칙에 따라, 이 값은 3 x 2 매트릭스여야 하겠죠. 3 x 2 매트릭스 곱하기
2 x 1 매트릭스 또는, 곱하기 2 x 1 벡터는
3 x 1 벡터를 줍니다. 더 일반적인 경우, n1 x n0 차원이 매트릭스가 
될 것입니다. 여기서 알아낸 것은, w1 의 다이메션이 n1 x n0 이여야 한다는 것입니다. 일반적인 경우에는, 
wL의 다이멘션은 nL x nL -1 이여야 한다는 것입니다. 예를 들어, w2 의 다이멘션의 경우 이 경우에는 5 x 3일 것입니다. 또는 n2 x n1입니다. 저희는 z2를 w2 곱하기 a1으로 계산할 것이기 때문에, 그리고 일당은 bias는 무시하겠습니다. 그러면 이것은 3 x 1 이 될 것이고, 이것은 5 x 1가 되어야 합니다. 이것은 그러면 5 x 3이 되어야 하구요. 비슷하게 w3는 다음 층의 다이멘션 컴마 
다름 층의 다이멘션 인데요 그러면 이것은 4, 5이고, w4는 2, 4, 그리고 w5는 1, 2 가 됩니다. 아시겠죠? 그러므로 확인하실 공식은 L층에 대해 매트릭스를 도입할 때 말이죠, 그 매트릭스의 다이멘션은 
nL x nL-1입니다. 다음으로 벡터b의 다이멘션을 생객해보겠습니다. 이것은 3 x 1 벡터일 것입니다.
그러므로 3 x 1 벡터를 더해야 
3 x 1 벡터를 결과값으로 가질 수 있는데요, 이 경우에는 이 값을 더해야 하는데요, 
이 값은 5 x 1입니다. 5 x 1벡터가 하나 더 있는 것인데요, 여기 박스 안에 있는 것들의 합이 5 x 1벡터이여야 합니다. 더 일반적인 규칙은, 
왼쪽에서 보면, b1은 n1 x 1 이죠, 이것은 3 x 1입니다. 2번째는 이것이 n2 x 1입니다. 그러므로 일반적인 케이스에서는 bL이 nL x 1 차원인 경우입니다. 이런 공식들이 여러분의 w 매트릭스와 벡터 b에 대해서 올바른 다이멘션 값을
갖는지 더블체크할 때 쓰일 수 있을 것입니다. 그리고 당연하지만, 
후 방향전파을 도입하는 경우, dw의 다이멘션은 w 의 다이멘션과 
동일해야 합니다. 즉, dw 과 w의 똑같은 다이멘션 이여 야합니다. 그리고 db는 b와 똑같은 다이멘션 이여야 합니다. z의 다이멘션도 필수적으로 체크해 야합니다. x 와 a의 l도 마찬가지입니다.
a(l)은 여기서 많이 이야기 하진 않았습니다만, 중요합니다. z(l)=g(l) a(l)이기 때문에, applied element wise 경우 그러면 z 와 a 는 이런 네트워크 경우
똑같은 다이멘션 일 것입니다. 여러 복수 샘플에 대해 벡터화 도입을 보는 경우 어떻게 되는지 살펴보겠습니다. 벡터화 도입을 하는 경우에도, 당연히, wb, dw 그리고 db의 다이멘션
동일할 것입니다. 벡터화 도입에서 그러나 z, a, x의 다이멘션은 약간 변할 것입니다. 이전에는 z1=w1x+b1가 있었습니다. 여기는 n1 x 1이였는데요, 이것은 n1 x n0 입니다. x는 n0 x 1, b는 n1 x 1 입니다. 그러면 벡터화 도입에는 z1=w1x + b1 가 될 것입니다. z1는 개인 샘플들에 대한 값을 통해 어구요
그러면 z11, z12, z1m까지 말이죠. 이렇게 쌓도록 합니다.
그러면 이 값이 z1이 됩니다. z1의 다이멘션은 n1 x 1대신에 n1 x m이 됩니다.
그리고 m은 트레이닝 세트의 크기입니다. w1의 다이멘션은 똑같습니다.
그러므로 n1 x n0입니다. x는 n0 x 1대신에 모든 트레이닝 예시들이 가로로 쌓인 값입니다. 그러므로 이제 n0 x m 입니다. n1 x n0 매트릭스으 ㅣ경우
이 값을 n0 x m 매크릭스로 곱합니다. 예상대로 그러면, 이것들이 같이 
n1 x m 차원의 매트릭스 값을 줍니다. 마지막으로 b1은 똑같이
n1 x 1입니다. 하지만 이 값을 갖고 b에 더하면, 
파이썬 broadcasting에서는 이것이 n1 x m 매트릭스 에 중복될 것입니다. 이전 슬라이드에서는 wb, dw, db의
다이멘션 관련하여 이야기했는데요, 여기서 보는 것은 zL과 aL은 nL x 1 의 다이멘션을 갖습니다. 반면에 ZL 과 AL은 nL x m입니다. 특별한 케이스는 
L이 0인 경우인데요, 이 경우, A0는 트레이닝세트 입력 특성 x 와 동일합니다. 이 값은 n0 x m 이 됩니다. 이것을 후 방향전파에 도입하는 경우, 나중에 보겠지만, 
dZ오 dA를 계산하게 될텐데요, 이 값들은 Z 와 A와 그 다이멘션이 동일할 것입니다. 이 연습이 여러분이 작업하는 매트릭스가 어떤 다이멘션인지 분명하게 해주도록
도움이 됐으면 합니다. 심층신경망에서 후 방향전파을 도입하는 경우, 여러분의 코딩을 잘하고 
매트릭스의 다이멘션이 일관되게 유지되면 보통 도움이 됩니다. 일반적으로 생기는 버그의 원인을 
제거해줄 수 있을 것입니다. 이렇게 여러 매트릭스의 다이멘션을 
알 수 있는 연습이 도움이 됐기를 바랍니다. 심층신경망을 도입하는 경우에, 작업하는 여러 매트릭스와 벡터 다이멘션을
일직선으로 유지하면 버그의 원인을 일부 제거할 수 있을 것입니다. 코딩이 올바르도록 확실히 도와주기는 합니다. 전 방향전파을 신경망에서 하는 방법에 대한 메카닉을 보았는데요, 왜 심층신경망이 효과적인 것일까요? 또 shallow representations보다 왜 더 잘 작동하는 것일까요? 다음 비디오에서 이와 관련된 내용을 다루겠습니다.