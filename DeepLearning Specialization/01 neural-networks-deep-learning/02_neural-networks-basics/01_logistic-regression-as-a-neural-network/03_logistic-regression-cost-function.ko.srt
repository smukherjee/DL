1
00:00:00,000 --> 00:00:01,530
이번 비디오에서는, 

2
00:00:01,530 --> 00:00:04,227
로지스틱 회귀분석법을 보았는데요,

3
00:00:04,227 --> 00:00:07,526
회귀분석 모델에서 W와 B 파라미터를 트레이닝 시키기 위해서는 먼저

4
00:00:07,526 --> 00:00:10,570
비용함수를 정의해야 합니다.

5
00:00:10,570 --> 00:00:14,430
로지스틱 회귀 분석에 사용할 수 있는 비용 함수에 대해 살펴보도록 하죠.

6
00:00:14,430 --> 00:00:18,195
복습하자면, 이전 슬라이드에서 찾았던 내용인데요. 

7
00:00:18,195 --> 00:00:20,792
결과값 ŷ은 w의 시그모이드 

8
00:00:20,792 --> 00:00:24,690
transpose x 더하기 b 이고, Z의 시그모이드는 이렇게 정의됩니다.

9
00:00:24,690 --> 00:00:27,600
모델에 대한 파라미터를 학습하기 위해 

10
00:00:27,600 --> 00:00:31,200
m개의 트레이닝 example로 이루어진 트레이닝 세트가 주어지고, 

11
00:00:31,200 --> 00:00:34,060
트레이닝세트에서 매개 변수 W와 

12
00:00:34,060 --> 00:00:37,781
B를 찾는 것이 자연스러운 것 같습니다.

13
00:00:37,781 --> 00:00:40,225
여러분의 트레이닝세트에 대한 예상치는

14
00:00:40,225 --> 00:00:43,260
ŷ (i) 이라고 표기하는데요 

15
00:00:43,260 --> 00:00:47,720
트레이닝세트에서 나온 ground truth label y_i값과 비슷할 것입니다.

16
00:00:47,720 --> 00:00:52,110
위에 있는 공식에대해 조금더 상세히 알려드리자면, 

17
00:00:52,110 --> 00:00:56,205
앞서 ŷ은 위처럼 정의된다고 했었죠, 

18
00:00:56,205 --> 00:01:00,930
x 트레이닝샘플에 대해서 말이죠. 

19
00:01:00,930 --> 00:01:03,240
그리고 각각의 크레이닝 샘플에 대해서요. 밑에 이 아래첨자에

20
00:01:03,240 --> 00:01:07,710
괄호를 사용해서 인덱싱하고 example들을 차별화하시켰습니다.

21
00:01:07,710 --> 00:01:12,870
트레이닝 샘플 (i)의 예상치는 ŷ(i)이며, 

22
00:01:12,870 --> 00:01:18,835
기 값은 시그모이드 함수를 W transpose X값에 적용하여, 

23
00:01:18,835 --> 00:01:25,905
(i) 트레이닝 example 입력값 더하기 V를 통해 Z(i)도 정의할 수 있습니다.

24
00:01:25,905 --> 00:01:30,110
Z(i)는 W transpose x (i) 플러스 b입니다.

25
00:01:30,110 --> 00:01:31,350
이번 코스를 통해

26
00:01:31,350 --> 00:01:33,966
우린 규칙화된 표기법을 쓸 것인데요,

27
00:01:33,966 --> 00:01:41,605
위 첨자의 괄호 i는 데이터를 뜻합니다.

28
00:01:41,605 --> 00:01:47,615
X 또는 Y 또는 Z 또는 i번째 트레이닝 example과 연관된

29
00:01:47,615 --> 00:01:50,885
i번째 example인거죠. 

30
00:01:50,885 --> 00:01:54,840
이것이 위 첨자 i가 의미하는 것입니다.

31
00:01:54,840 --> 00:01:57,630
자 그럼 이제 알고리즘이 얼마나 잘 작동하고 있는지를 알아내기 위해 

32
00:01:57,630 --> 00:02:01,315
사용할 수 있는 loss 함수 또는 오류함수를 보도록 하겠습니다.

33
00:02:01,315 --> 00:02:06,015
한가지 방법은 loss를 알고리즘의 결과값이 ŷ이고 true label인 Y가 제곱 오류 또는 

34
00:02:06,015 --> 00:02:12,320
0.5제곱 오류가 되게하는 것입니다.

35
00:02:12,320 --> 00:02:14,975
이렇게 할 수 있는데요, 

36
00:02:14,975 --> 00:02:17,670
로지스틱 회귀분석법에서는 사람들이 이렇게 잘 하지 않습니다.

37
00:02:17,670 --> 00:02:21,000
파라미터를 배우면서 알게되면, 

38
00:02:21,000 --> 00:02:25,682
최적한 문제에서 나중에 배우겠지만 비볼록하게 됩니다.

39
00:02:25,682 --> 00:02:30,105
결과적으로 복수 국부 최적의 optimization 문제를 갖게 됩니다.

40
00:02:30,105 --> 00:02:33,285
그렇게해서 gradient descent가 전역 최적값을 못 찾을 수 있습니다. 

41
00:02:33,285 --> 00:02:35,580
방금 이야기한 것들이 잘 이해가 안 가셨다고 하면

42
00:02:35,580 --> 00:02:38,320
너무 걱정하지 마십시요. 추후 강의에서 더 자세히 다루도록 하겠습니다.

43
00:02:38,320 --> 00:02:40,990
지금 이해햐셔야할 직관적인 부분은

44
00:02:40,990 --> 00:02:44,620
L이라는 loss 함수는 true label y를 갖는 경우, 

45
00:02:44,620 --> 00:02:51,265
얼마나 정확히 ŷ 결과값을 산출하는지 정의할 때 사용합니다.

46
00:02:51,265 --> 00:02:54,345
제곱 오류 값이 오히려 더 합리적인 방법이라고 생각할 수 있지만

47
00:02:54,345 --> 00:02:58,160
gradient descent가 잘 안나온다는 단점이 있습니다.

48
00:02:58,160 --> 00:03:00,500
그러므로 로지스틱 회귀분석법에서는, 

49
00:03:00,500 --> 00:03:05,695
여러가지 제곱 오류와 비슷한 역할을하는 loss함수를 정의해서 

50
00:03:05,695 --> 00:03:08,910
볼록한 최적화 문제를 주도록 할 것입니다. 

51
00:03:08,910 --> 00:03:13,530
그렇게되면 나중에 비디오 강의를 통해 보시겠지만, 최적화를 하기가 훨씬 더 쉬워집니다.

52
00:03:13,530 --> 00:03:17,310
로지스틱 회귀분석법에서 사용하는 것은 

53
00:03:17,310 --> 00:03:21,795
여기 위에 있는 loss함수입니다. 

54
00:03:21,795 --> 00:03:31,740
여기서는 마이너스 y 로그 ŷ 더하기 1, 이 선은 y log

55
00:03:31,740 --> 00:03:34,600
이 선은 ŷ 입니다.

56
00:03:34,600 --> 00:03:38,785
직관적으로 이 loss 함수가 왜 말이 되는지 설명해드리겠습니다.

57
00:03:38,785 --> 00:03:41,285
아셔야 할 것은, 

58
00:03:41,285 --> 00:03:45,820
제곱오류를 사용하는 경우, 이 값이 최대한 작아야 좋습니다.

59
00:03:45,820 --> 00:03:48,680
regression loss함수를 이용하면, 

60
00:03:48,680 --> 00:03:51,495
이 것 또한 작은 값일 수록 좋죠.

61
00:03:51,495 --> 00:03:53,508
이 것이 왜 말이 되는지, 

62
00:03:53,508 --> 00:03:55,260
2가지의 케이스를 통해서 보도록 하겠습니다.

63
00:03:55,260 --> 00:03:56,570
첫번째 경우에는 

64
00:03:56,570 --> 00:03:59,430
Y가 1, 그리소 

65
00:03:59,430 --> 00:04:05,415
loss 함수 ŷ 컴마 y는 이 마이너스 부호를 쓸 수 있도록 해주죠. 

66
00:04:05,415 --> 00:04:08,735
그러므로 마이너스 로그 ŷ

67
00:04:08,735 --> 00:04:10,770
만약 y가 1인 경우 말이죠. 그 이유는 y가 1이면,

68
00:04:10,770 --> 00:04:14,070
두번째 항인 1-Y 는 0이 됩니다. 

69
00:04:14,070 --> 00:04:19,880
그럼 y가 1이면, 마이너스 로그 ŷ은 큰 값이되게 하는게 좋습니다. 

70
00:04:19,880 --> 00:04:26,040
그 뜻은 즉 log ŷ을 큰 값으로 만드는게 좋은것 인데요

71
00:04:26,040 --> 00:04:32,935
최대한 크게 만드는 것이 좋기 때문에 ŷ이 큰 값을 갖게 하는 것이 좋겠죠. 

72
00:04:32,935 --> 00:04:35,170
그렇지만 ŷ은 아시다시피, 

73
00:04:35,170 --> 00:04:38,440
시그모이드 함수이기 때문에 1보다 큰 값을 가질 수 없습니다.

74
00:04:38,440 --> 00:04:41,850
즉, y가 1인 경우

75
00:04:41,850 --> 00:04:44,050
ŷ 값이 최대한 큰 값을 갖도록 하는 것이 좋다는 것을 알게 됩니다.

76
00:04:44,050 --> 00:04:48,220
그렇지만 이 값은 절대로 1보다 큰 값이 될 수 없기 때문에, ŷ을 1과 최대한 가깝게 
하는 것이 목표라고 해석할 수도 있습니다.

77
00:04:48,220 --> 00:04:50,740
2번째 경우는 y가 0인 경우인데요, 

78
00:04:50,740 --> 00:04:55,375
만약 y가 0인 경우, loss 함수의 첫번째 항이 0이 됩니다.

79
00:04:55,375 --> 00:05:01,290
y 가 0, 그러면 2번째 항이 loss함수를 정의하게 되죠. 

80
00:05:01,290 --> 00:05:07,210
이렇게 되면 loss는 마이너스 로그 1 마이너스 ŷ이 됩니다.

81
00:05:07,210 --> 00:05:11,480
여러분의 러닝 단계에서 loss 함수를 작게 만들려고 한다면, 

82
00:05:11,480 --> 00:05:19,450
로그 1 마이너스 ŷ이 최대한 값을 갖길 원한다는 뜻입니다. 

83
00:05:19,450 --> 00:05:22,050
그리고 여기가 마이너스 부호이기 때문에

84
00:05:22,050 --> 00:05:24,660
이 작은 이유 하나로 

85
00:05:24,660 --> 00:05:30,870
해당 loss 함수가 ŷ의 값을 최대한 작은 값으로 하려고 하는 것을 알 수 있습니다.

86
00:05:30,870 --> 00:05:34,320
또, ŷ이 0에서 1사이 값을 가져야 하기 때문에

87
00:05:34,320 --> 00:05:38,155
만약 y가 0이라면 

88
00:05:38,155 --> 00:05:43,790
loss 함수는 파라미터가 일을해서 ŷ이 최대한 0에 가까운 값이되도록 할 것입니다.

89
00:05:43,790 --> 00:05:48,305
Rafidah's effect가 작용하는 여러가지 함수가 있는데요, 

90
00:05:48,305 --> 00:05:52,950
만약 y가 1인 경우, ŷ을 최대한 크게하고, Y가 0인 경우 ŷ을 작게하는 것입니다. 

91
00:05:52,950 --> 00:05:55,150
여기 초록색으로 된 부분을 

92
00:05:55,150 --> 00:05:59,920
아주 편안하게 비용함수에 대해 정의했는데요, 

93
00:05:59,920 --> 00:06:03,970
이 부분은 선택적인 비디오 강의 시청을 통해 조금 더 

94
00:06:03,970 --> 00:06:08,500
공식적인 배경을 설명하겠습니다. 왜 로지스틱 회귀분석법에서 이런 형식의 loss 함수를 
사용하는지 말이죠. 

95
00:06:08,500 --> 00:06:13,630
마지막으로, loss 함수는 single training example을 바탕으로 정의되었었는데요,

96
00:06:13,630 --> 00:06:16,760
single training example에서 얼마나 잘 작동하는지 여부를 측정합니다.

97
00:06:16,760 --> 00:06:21,148
비용함수를 정의해볼텐데요, 

98
00:06:21,148 --> 00:06:24,690
이것은 전체적인 트레이닝세트에서 얼마나 잘 작동하는지 여부를 측정해 줄 것입니다.

99
00:06:24,690 --> 00:06:28,660
비용함수 J는 

100
00:06:28,660 --> 00:06:33,130
W 파라미터에 적용될텐데요, B 파라미터는 각각의 트레이닝 샘플에 
적용된 m개의 loss 함수의 합과

101
00:06:33,130 --> 00:06:43,270
함께 평균치가 될 것입니다.

102
00:06:43,270 --> 00:06:45,435
그리고 여기 ŷ은 당연히, 

103
00:06:45,435 --> 00:06:49,570
로지스텍 회귀분석법 알고리즘에 의거한 결과값의 예상치입니다,

104
00:06:49,570 --> 00:06:52,430
W와 B 파라미터세트를 사용해서 나온 값 말이죠. 

105
00:06:52,430 --> 00:06:54,480
부가 설명을 드리자면, 

106
00:06:54,480 --> 00:06:58,010
이 것은 1 나누기 

107
00:06:58,010 --> 00:07:03,550
m이 비용함수가 1에서 m까지의 합인 경우입니다. 

108
00:07:03,550 --> 00:07:07,530
그러므로 이값은 y (i) Log ŷ

109
00:07:07,530 --> 00:07:14,530
(i) 더하기 1 선을 적용하면 y (i) log one 선을 적용하여 ŷ (i)가 됩니다..

110
00:07:14,530 --> 00:07:17,880
여기에 대괄호를 기입할 수도 있겠죠. 

111
00:07:17,880 --> 00:07:20,945
여기 마이너스 부호는 밖에 위치시킵니다.

112
00:07:20,945 --> 00:07:23,665
여기서는 이런 표현를 쓸텐데요.

113
00:07:23,665 --> 00:07:29,120
loss 함수가 single training example에 적용되었다고 말이죠.

114
00:07:29,120 --> 00:07:33,010
비용함수는 파라미터의 비용을 나타낸 것이구요, 

115
00:07:33,010 --> 00:07:36,115
그러므로 로지스틱 회귀분석법 모델을 트레이닝 시키는데 있어, 

116
00:07:36,115 --> 00:07:38,980
W와 B라는 파라미터를 찾고

117
00:07:38,980 --> 00:07:43,475
밑에 있는 전체적인 기계의 비용 J를 줄일 것입니다.

118
00:07:43,475 --> 00:07:48,040
이제까지 로지스틱 회귀분석법 알고리즘의 세팅,

119
00:07:48,040 --> 00:07:50,770
트레이닝 example의 비용함수, 

120
00:07:50,770 --> 00:07:54,190
그리고 알고리즘의 전체 비용함수의 파라미터

121
00:07:54,190 --> 00:07:59,485
알고보니 로지스틱 회귀분석법은 아주 아주 작은 신경망 네트워크로 볼 수 있겠습니다.

122
00:07:59,485 --> 00:08:01,905
다음 비디오에서는 이 내용에 대해 다룰텐데요

123
00:08:01,905 --> 00:08:04,965
신경망의 직관적인 부분은 미리 생각해 놓으실 수 있습니다. 

124
00:08:04,965 --> 00:08:08,230
자 그럼 다름 비디오로 넘어가서

125
00:08:08,230 --> 00:08:11,630
어떻게 로지스틱 회귀분석법을 아주 작은 신경망으로 볼 수 있는지 알아보겠습니다.