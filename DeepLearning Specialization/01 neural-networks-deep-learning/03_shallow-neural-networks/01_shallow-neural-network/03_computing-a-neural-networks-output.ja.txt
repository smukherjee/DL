前回の動画では 隠れ層が一つの
ニューラルネットワークがどういうものか見ました 今回は 実際にその
ニューラルネットワークが どのように出力を計算しているのか
細かく見てみましょう ロジスティック回帰と似たようなものが
何度も繰り返されているという感じです 次の例をみてみましょう。 これが二層のニューラルネットワークです これが正確には何を計算するのか
詳細に学びましょう 以前 ロジスティック回帰での丸は 二つの計算のステップを表していると
お話ししました まずこのようにzを計算し 次にzのシグモイド関数である
活性化を計算します ニューラルネットワークは
これを何度も繰り返すだけです 隠れ層の中のノード一つのみに
焦点を当ててみましょう 隠れ層の一つ目のノードを見てください とりあえず他のノードは
薄く表示しておきます 左のロジスティック回帰と同じように この隠れ層のノードも
二つの計算のステップを行います 一つ目のステップが
この左半分にあたると考えてもらうと この部分はz=wT・x+bを計算します 表記法についてですが これらの値は
全て一つ目の隠れ層についてのものなので このようにいっぱい角括弧を書きます そして隠れ層の一つ目のノードなので
下付き文字で１と書きます 一つ目はこの計算を行って 二つ目のステップでは
a[1]1=σ(z[1]1)を計算します zとaのどちらについても
表記法の慣例としては aに[l]とiをつけると 上に角括弧で書かれた
lは層の番号を表し 下に書かれたiは その層の中での
ノードの番号を表します 今見ているノードは一つ目の層
つまり隠れ層にあって その中の一つ目のノードなので
二つの数字がどちらも１なのです このニューラルネットワークでの
一つ目のノードである小さな丸が これらの二つのステップの計算を
行うことを表しています 次に隠れ層の二つ目のノードを
見てみましょう 左のロジスティック回帰のユニットと
似たような感じで この小さな丸は
二つの計算のステップを表します まずzを計算します
まだ層は１つ目ですが 今回は二つ目のノードですね
これがw[1]2T・x+b[1]2です 次にa[1]2=σ(z[1]2)です 必要なら動画を止めてもらって ここに紫で書いた表記法と
ちゃんと合っているか確認してみてください ここまででニューラルネットワークの
最初の二つの隠れユニットについて見てきました ３、４つ目の隠れユニットも
似たような計算を表します では この二つの等式を取って 次のスライドにコピーします これがニューラルネットワークで
これが一つ目 そしてこれが二つ目の式です 一つ目と二つ目の隠れ層について
さっき書いたものですね 更に進んで 三つ目と四つ目の隠れ層に
対応する式を書くと このようになります 表記法について再度確認すると
これはベクトルw[1]1で それを転置したものにxを掛けてあります
いいですか？ ここの上付き文字のTは
ベクトルの転置を表しているのです 想像できるかもしれませんが
実際ニューラルネットワークを実装するとき これらにforループを使うのは
非常に非効率です なので これら四つの式を取って
ベクトル化します まずzをベクトルとして計算したら
どうなるかを考えましょう これは次のようにできます これらのwたちを取って
行列に詰め込みます するとw[1]1の転置
つまり行ベクトルができます もともと列ベクトルだったものを
転置したので 行ベクトルになるのです そしてw[1]2の転置 w[1]3の転置
w[1]4の転置と続きます これら四つのベクトルを積み上げることで
行列が得られるのです もう一つの考え方としては
ロジスティック回帰のユニットが四つあって それぞれのユニットが
対応するパラメータベクトルwを持っていて それら四つのベクトルをまとめることで 4×3の行列ができあがる
と考えることもできます この行列に入力の特徴量
x1、x2、x3をかけると 行列の掛け算のやり方で計算すると w[1]1T・x、w[1]2T・x、
w[1]3T・x、w[1]4T・x ができます bも忘れてはいけませんね なのでここにベクトルb[1]1、b[1]2、 b[1]3、b[1]4 を足します
この部分にあたりますね そしてこちらもb[1]1、b[1]2、
b[1]3、b[1]4です ここで得られる四行のそれぞれが 上で計算した四つの値に
ぴったり一致しているのがわかります つまり これがz[1]1となって z[1]2、z[1]3、z[1]4となるのです
ここで定義されている通りですね そして 当たり前に思えるかもしれませんが
これら全体をz[1]と呼ぶことにします 個々のzを積み上げて
列ベクトルとしたものですね ベクトル化をするときの
大まかなルールは 一つの層に異なるノードがあるとき
縦に並べるということです そのため z[1]1からz[1]4までがあって それらが隠れ層の四つのノードに
対応しているとき それらを縦に並べてベクトルz[1]を作ったのです もう一つ表記法を使ってみます さっき小文字のw[1]1、w[1]2などを
並べて作った この4×3の行列ですが これを大文字でW[1]と呼ぶことにします 同様に これはb[1]と呼びます
4×1のベクトルですね これで このベクトルや行列の表記法を使って
zを計算することができました 最後にやりたいことは
このaの値も計算することです 当然 a[1]を これらの活性化
a[1]1からa[1]4を 縦に並べたものとして定義します この四つの値を取ってきて
a[1]と呼ばれるベクトルに詰め込んだだけです そしてこれはσ(z[1])になります このシグモイド関数は
四つのzの要素を取り込んで 要素ごとにシグモイド関数を
適用するというものです 復習ですが z[1]=W[1]x+b[1]で a[1]=σ(z[1])であることがわかりました 次のスライドにこれらをコピーします 入力xがあるとき
ニューラルネットワークの一層目では z[1]=W[1]x+b[1]で a[1]=σ(z[1])となっています その次元は(4,1)=(4,3)×(3,1)+(4,1)
となっています aはzと同じ次元数なので(4,1)ですね そしてx=a[0]でyハットはa[2]
だったと思い出してください なので 実際にxを取ってa[0]と
置き換えることもできるのです a[0]は入力xのただの別名だからです 似たような方法で
次の層の計算も導くことができます 出力層はパラメータW[2]とb[2]に 紐づけられています この場合 W[2]は1×4の行列で b[2]は1×1のただの実数です z[2]は実数になるので
1×1の行列と書けます これは(1,4) aは(4,1)で
b[2]は(1,1)です なので答えとして実数が得られます 最後の出力ユニットを
ロジスティック回帰のように捉えると ロジスティック回帰でのwは
W[2]Tの役割をしていて つまりW[2]はwの転置ですね
そしてbはb[2]と同じものです とりあえずネットワークの
左側を無視すると この最後の出力ユニットは
ロジスティック回帰によく似ています 違うところは
パラメータをwとbと書く代わりに 今回は(1,4)次元のW[2]と
(1,1)次元のb[2]と書くところです おさらいすると
ロジスティック回帰では 出力 つまり予測を実装するには z=wT・x+bと
yハット=a=σ(z)を計算します 隠れ層が一つのニューラルネットワークを
実装するためには この四つの式を実装すればいいだけです 隠れ層の四つのロジスティック回帰ユニット
からの出力を ベクトル化して実装したのが
この部分がやっていることで 出力層のロジスティック回帰の計算は
この部分が担っています この説明でわかってもらえればいいのですが
ここで学んでほしいのは このニューラルネットワークの出力の計算には
この四行のコードが必要ということです ここまでで
入力の特徴量ベクトルxが一つあるとき このニューラルネットワークの出力を
四行のコードで計算できるとわかりました ロジスティック回帰と似たような感じで 複数の教師サンプルにまたがって
ベクトル化したいです 教師サンプルを行列の列として並べると
少し修正するだけで実現できます また ロジスティック回帰と同じように ニューラルネットワークの出力を
一回に一つの教師サンプルではなく 全てのサンプルに対して
一気に計算できるようになります 次の動画で
その詳細を見てみましょう