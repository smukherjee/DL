この動画は楽しい動画になりそうです。 この動画では、 隠れ層が一つのニューラルネットワークのための 最急降下法の実装方法を説明します。 ここでは、 最急降下法のバックプロパゲーションが動くようにするための 実装に必要な等式をお伝えします。 そしてこの後の動画で、 どうしてこれらの等式が ニューラルネットワークのための 勾配の計算に必要な等式として 正しいのかということについて もう少し直感的な説明をしたいと思います。 - 隠れ層が一つの ニューラルネットワークは、 パラメータw1,b1,w2,b2などを持っています。 思い出してください nx個の...代わりに あー n[0]個の入力特徴量とn[1]個の隠れ層ユニット そして n[2]個の出力ユニットが 我々の例にはあり 今のところ n[2]は１に等しい よって 行列w[1]は n[1]×n[0]行列に b[1]は n[1]次元ベクトルになり つまり n[1]×1次元行列に書くことができて 実際は 列ベクトルだけど W[2]の次元は n[2]×n[1]で b[2]の次元は n[2]×1になる 再び得たこの例が ここまで 見てきたn[2]が1に等しい場合のもので それは 単一の隠れ層のみを持つ ニューラルネットワークにも コスト関数を持つ そして 今のところ 以下の仮定をする 2項分類を行うと。よって この場合 ここに挙げたパラメータ群のコストは ロス関数の平均のm分の1になる そして ここで Lは ニューラルネットワークが y^ を予測した時のロスで これ(y^)は 本当は a[2]で ラベルは y に等しい そして 2項分類を行っているのなら ロス関数は 正に 以前 ロジスティック・リグレッションに使ったものと同じになり得る よって アルゴリズムのパラメータを学習するのに 勾配降下法を実行する必要がある ニューラルネットワークを学習させるのに 重要なのは パラメータを ランダムに初期化し 全てを0にしないことだ 0 なぜ そうなのかは 後で見るが パラメータを何かに初期化した後 勾配降下法の 各ループで 予測値が計算される つまり まずは y^(i)を iが1からmまでについて 計算して そして 微分値を計算する必要がある つまり dw[1] それは コスト関数の微分値で パラメータw[1]に対するものだ もう一つの変数も計算する必要がある それは db[1]で コスト関数の微分 もしくは 傾きで 変数b[1]に対するものだ 同様のことを パラメータw[2]とb[2]についても行う 最終的に 勾配降下法の更新は w[1]を　w[1]から アルファ(つまり学習率)×dw[1] を引いた物にし b[1]を b[1]から 学習率×db[1] を引いた物にする 同じことを w[2]とb[2]にも行う 私は 時々:= を使ったり、時々=を使ったりするが どちらの表記でもいいだろう そして、これが 勾配降下法の1反復だ そして パラメータが収束していることが分かるまで これを何回か繰り返すのだ 前のビデオで 予測値の計算方法や出力の計算方法について 話した ベクトル化して計算する方法も見た つまり要点は 偏微分値の計算方法を理解することだ dw[1], db[1]についてと 同じく dw[2]とdb[2]についての微分値を 私が行いたいのは、 あなたに これらの微分値を計算するために 必要となる等式を与えることだ 次のビデオ オプションビデオ で説明するけども それは どのように それらの等式を導出するかを 説明する素晴らしい回だよ さて 再びまとめると 順伝播の等式 Z[1] = w[1] X + b[1]　そして A[1]は z[1]の各要素を適用させた その層の活性化関数に等しい Z[2] = w[2] A[1] + b[2] そして 最終的に これが 全ての学習セットを通じてベクトル化したものだ A[2]は Z[2]をg[2]に入れた物だ 2項分類を行うことを前提とするならば この活性化関数は シグモイド関数にすべきだ この領域ではそうなる これが ニューラルネットワークの 順伝播 もしくは 左から右へ進む計算だ 次に 微分を計算しよう これは 誤差逆伝播ステップだ では計算しよう dz[2] = A[2] - 測定値Y ちょっと思い出して欲しいんだけど これら全ては サンプルに跨がってベクトル化したもので 行列Yは このように1×M行列で 全てのm個のサンプルを横方向にリストしたものだ そして dw[2]はこうなり 実際には これらの最初の3つの等式は ロジスティックリグレッションの勾配降下法のと とても 良く似ている カンマ axis=1 カンマ keepdims=True 些細なことだけど この np.sum は Python numpy コマンドで 行列の一つの次元に跨がって合計を取る この場合は 水平方向に合計を取る keepdims が行うのは Pythonが 奇妙なランク1配列を出力するのを 防ぐことだ ランク1配列とは (知っているだろうが)その次元が(n, )のものだ keepdims=True とすることで Pythonの db[2]ベクトルへの出力が (n, 1) になることを保証する 技術的には(n[2], 1) この場合は 単に(1, 1) なので 問題にならないが 後で それが 本当に問題になる場合を見るだろう ここまで やってきたことは ロジスティックリグレッションに 非常に良く似ている　しかし 誤差逆伝播を走らせ続けると これを計算するわけだが　dZ[2]×g[1]'(Z[1]) この g[1]'の値 は 隠れ層に どのような活性化関数を使ったとしても その微分値である 出力層については 2項分類を想定し、 シグモイド関数を使うが それは、既にdZ[2]の式に織り込み済みだ そして 今度は 要素に跨がった積で ここは (n[1], m) 行列だ そして ここは 要素に跨がった 微分値で また (n[1], m) 行列だ そして そこにこれを掛けて 2つの行列の要素に跨がったかけ算を行う そして 最終的には dw[1]はこれに等しくなり db[1]はこれに等しくなる np.sum(dZ[1], axis=1, keepdims=True) 以前に keepdims は これまで n[2]が 1に等しい場合、つまり1ｘ1の場合、一つの実数値の場合は あまり問題にならないとしたが ここでは、db[1] は (n[1], 1) ベクトルであり Pythonのnp.sumには このような次元の出力をしてほしい 奇妙なライク1配列よりも それは 後の計算を めちゃくちゃにしてしまうから 別に keepdimsパラメータを使わずに 明示的に reshapeを呼んで np.sumの出力を dbがそうなるように この次元に整形する方法もある これが 順伝播であり 4つの等式が想定され 誤差逆伝播では 6つの等式が想定される 私は 今 単に これらの等式を書き下しただけだ 次のオプションビデオでは どのように 誤差逆伝播アルゴリズム対する この6つの等式が導かれるのか 見てみる それを見るかどうかは あなたの自由ですが いずれにせよ もし あなたが これらのアルゴリズムを実装するのなら あなたは 順伝播と逆伝播の正しい実装をすることになるし 必要な微分値を計算できるようになるでしょう ニューラルネットワークの パラメータを学習するための 勾配降下法を適用するため アルゴリズムを実装し、それを動作させることが可能だ 微積分の深い理解無くても 多くのディープラーニングの 成功した実践者は そうしている しかし あなたが 次のビデオも見ることができると思うなら これらの等式の 微分に関して もう少し多くの直感を得ることが できるだろう