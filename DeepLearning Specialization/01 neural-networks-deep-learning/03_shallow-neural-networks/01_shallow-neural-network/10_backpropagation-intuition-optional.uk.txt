В останньому відео ти бачив/ла рівняння зворотного поширення. В цьому відео давай спробуємо зрозуміти з допомогою Діаграми обчислень як ці рівняння були виведені. Це відео абсолютно необов'язкове, тож дивитимешся ти його чи ні, ти, все одно, зможеш виконати домашнє завдання. Тож згадаймо, що коли ми говорили про логістичну регресію (ЛР), ми повинні були здійснити пряме поширення, де обчислювали z, потім a, а потім - Втрату. А потім, щоб взяти похідні, ми повинні були здійснити зворотне поширення, 
де спочатку могли обчислити da, а потім - dz, а потім - dw і db. Тож визначенням Втрати було L(a, y)=-y*log(a)- (1-y)*log(1-a). Тож, якщо ти знайомий/а з диференціальним численням (диф.численням), 
то можеш взяти похідні по a, що дасть тобі формулу da. Тож da= оцьому. І, якщо ми це розв'яжемо згідно диф.числення, 
то зможемо показати, що це =-y/a+(1-y)(1-a). Це виводиться диф.численням, якщо просто взяти похідні. Виявляється, якщо пройти ще назад і обчислити dz, то ми отримаємо, що dz=a-y. Я не пояснював цього раніше, проте, виявляється, що, згідно ланцюгового правила диф.числення, dz=da*g'(z), де g(z)=σ(z) і є нашою функцією активації вихідного вузла ЛР. Правильно? Тож просто запам'ятай, при ЛР ми маємо x₁, x₂, x₃ і далі лише один сигмоїдальний вузол, який дає нам a, що дає нам ŷ. Тож тут функцією активації була сигмоїда. Трохи не по темі, тільки для тих, хто знайомий з ланцюговим правилом диф.числення: підставою для цього є те, що a=σ(z), тож частинна похідна L по z (∂L/∂z) = частинній похідній L по a (∂L/∂a) помножене на da/dz. Це - a, що =σ(z), тобто це = d/dz*g(z), що =g'(z). Ось чому оцей вираз, що в нашому коді буде dz, = оцьому виразу, що в нашому коді буде da, помноженому на g'(z). Тож оце - це оце. Останнє виведення матиме сенс, якщо ти знайомий/а з диф.численням, 
а, точніше, - з ланцюговим правилом. Але якщо ти незнайомий/а - не хвилюйся, я намагаюсь трохи пояснювати де це потрібно. І, нарешті, обчисливши dz для цієї регресії, ми обчислюватимемо dw, яке, виявляється, було dw*x, і db, яке просто =dz, 
якщо ми маємо лише один тренувальний зразок. Тож це була ЛР. Обчислюючи зворотне обчислення ми будемо проводити дуже схожі обчислення, але робити це будемо двічі, 
бо тепер x буде входити не в вихідний вузол, а у прихований шар, а вже потім в вихідний вузол. Тож замість цих обчислень, складених в 1 крок отут, ми матимемо вже 2 кроки. Це вже буде НМ з двома шарами. І в цій двошаровій НМ ми маємо вхідний шар, прихований шар і вихідний шар. Запам'ятай кроки обчислень. Спочатку ми обчислюємо z[1] з допомогою оцих рівнянь, потім обчислюємо a[1], потім - z[2] (пам'ятай, що z[2] теж залежить від параметрів w[2] і b[2]), а потім, базуючись на z[2], обчислюємо a[2] і вкінці отримуємо Втрату. А зворотне поширення піде назад (справа-наліво), 
щоб обчислити da[2], потім dz[2], потім ще назад, щоб обчислити dw[2] і db[2] і ще назад, щоб обчислити da[1], dz[1] і т.д. Немає потреби брати похідні по вхідних x, бо вхідні x для контрольованого навчання є сталими. Тож ми не намагаємось оптимізувати x 
і не будемо надокучати похідними, принаймі, для контрольованого навчання, по x. Я пропущу явне обчислення da[2]. Якщо хочеш, можеш обчислити da[2], а потім використати його, щоб обчислити dz[2], але на практиці ці кроки можна об'єднати в один і отримати dz[2]=a[2]-y (як і раніше). Ми також (запишу dw[2] і db[2] внизу) маємо dw[2]=dz[2]*a[1]ᵀ, а db[2]=dz[2]. Тож цей крок дуже схожий на той, що ми мали в ЛР. Ми мали dw=dz*x. А зараз маємо a[1] замість x і додатково маємо транспонування, бо зв'язок між матрицею велике W і окремими параметрами w тут є транспонованим. Правильно? Тому що w[2] є вектором-рядком 
(у випадку ЛР з одним вихідним вузлом), в той час як тут w було вектором-стовпцем. 
Ось чому ми додатково транспонуємо a[1], хоча не транспонували x в ЛР. Тож це - половина зворотного поширення. Знову ж, ти можеш обчислити da[1], якщо хочеш. Хоча на практиці обчислення da[1] і dz[1], як правило, об'єднують в один крок і реалізувати доводиться лише dz[1]= w[2]ᵀ помножене на dz[2] і потім поелементно помножити на g'(z[1]). Давай зробимо перевірку розмірів. Якщо ми маємо ось таку НМ, що видає ŷ. Якщо ми маємо nₓ=n[0] вхідних ознак, n[1] прихованих вузлів, і n[2], що в нашому випадку має лише 1 вихідний вузол, то матриця w[2] матиме розмір n[2]*n[1], z[2] і, отже, dz[2] матимуть розмір n[2]*1 (насправді розмір буде 1*1 для двійкової класифікації), і z[2] і, отже, dz[1] матимуть розмір n[1]*1. Варто запам'ятати, що змінні foo і dfoo 
(матриця величини і матриця похідних цієї величини) 
завжди будуть однакового розміру. Тож тому w і dw завжди матимуть однакові розміри і, аналогічно, b та db, z та dz і т.п. Щоб пересвідчитись, що всі розміри співпадають, ми маємо dz[1]=w[2]ᵀ*dz[2] і далі поелементне множення на g[1]'(z[1]). Тож, підставивши розміри згори, матимемо n[1]*1= w[2]ᵀ. Після транспонування матимемо розмір n[1]*n[2]. dz[2] матиме розмір n[2]*1. А ось це матиме той же розмір, що й z[1]. Тож це також має розмір n[1]*1 (тут поелементне множення). 
Тож розміри мають значення, правильно? Вектор n[1]*1 може бути отриманий множенням матриці розміру n[1]*n[2] на n[2]*n[1], 
тому що їх добуток дасть матрицю розміру n[1]*1. Це зробить можливим поелементний добуток двох векторів розміру n[1]*1. Значить розміри співпадають. Дам одну пораду для реалізації зворотного поширення. Якщо ми хочемо пересвідчитись, що розміри співпадають, то треба подивитись які розміри всіх матриць: w[1], w[2], z[1], z[2], a[1], a[2] і т.д. і пересвідчитись, що розміри матриць підходять для математичних операцій. Інколи це дозволяє позбутись багатьох помилок 
при реалізації зворотного поширення. Добре. Тож це дає нам dz[1] і, нарешті (вже допишемо), dw[1] і db[1]. Думаю, їх краще записати ось тут. Але так як я вже не маю місця, то запишу їх справа. dw[1] і db[1] отримуються наступними формулами: оце буде =dz[1]xᵀ, а це - =dz. Можна помітити подібність між цими і оцими рівняннями. Що не є випадковістю, бо x грає роль a[0]. Тож xᵀ - це a[0]ᵀ. Тож ці рівняння дійсно дуже схожі. Тож ми отримали певне розуміння виведення зворотного поширення. Тепер ми маємо 6 головних рівнянь для dz[2], dw[2], db[2], dz[1], dw[1] і db[1]. Давай скопіюємо ці 6 рівнянь на наступний слайд. Ось вони. До цього моменту ми виводили зворотне поширення для тренування 1-ого тренувального зразка. Тож не буде дивним, що замість роботи над одним тренувальним зразком нам захочеться векторизувати це для всіх тренувальних зразків. Як пам'ятаємо, для прямого поширення, коли ми працювали над 1-им зразком за ітерацію, у нас були наступні рівняння: z[1]=w[1]x+b[1], a[1]=g[1](z[1]). Щоб векторизувати, ми брали z і складали їх ось так в стовпці і називали це великою Z. І подібним складанням в стовпці визначали також решту великобуквенних величин. Тож отримували Z[1]=W[1]X+b[1] і A[1]=g[1](Z[1]). Правильно? Ми використовуємо позначення дуже уважно 
в цьому курсі, щоб пересвідчитись, що складення зразків в стовпці матриці правильно працюватиме. Виявляється, якщо уважно виконувати математичні обчислення, то подібні дії підходять і для зворотного поширення. 
То векторизовані рівняння виглядають наступним чином. По-перше, якщо взяти dz різних тренувальних зразків і скласти їх в стовпці матриці (і так само для A і для Y), то отримаємо векторизовану реалізацію. А ось визначення або формула обчислення dW[2]. Тут також буде 1/m, тому що функція Витрат J є 1/m від суми від 1 до m Втрат. І при обчисленні похідних маємо вираз 1/m, як було і при обчисленні коригування w для ЛР. А ось коригування db[2]. Тут теж сума dZ від 1 до m. 
І потім dZ[1] обчислюється отаким чином. Знову ж, це - поелементний добуток (на відміну від випадку, що ми бачили на попередньому слайді, 
де це було вектором розміру n[1]*1). І тепер це - матриця розміру n[1]*m. І ці обидві матриці теж мають розмір n[1]*m. Ось чому тут зірочка. Це поелементне множення. 
І, нарешті, останні 2 коригування. Сподіваюсь - не несподівані. Надіюсь, ти отримав/ла певне розуміння 
як виводиться алгоритм зворотного поширення. Всюди в машинному навчанні (МН), на мою думку, виведення алгоритму зворотного поширення є, насправді, однією з найскладніших математичних частин 
з тих, що я бачив. Воно вимагає знання і лінійної алгебри, і матричних похідних, які треба виводити з нуля, 
керуючись базовими принципами. Якщо ти експерт матричного диф.числення, то використовуючи базові принципи, ти, 
можливо виведеш алгоритм самостійно, але, на мою думку, насправді, 
більшість практиків глибокого навчання, що бачили виведення на тому ж рівні, що й ти в цьому відео, вже отримали достатнє розуміння і можуть реалізувати цей алгоритм 
дуже ефективно. Тож, якщо ти експерт з диф.числення, то спробуй вивести весь алгоритм з нуля. Це одна з найважчих його математичних частин. Одне з найважчих виведень, які я зустрічав в МН. Проте, незважаючи на те, чи вдасться тобі це вивести, алгоритм працюватиме і я маю надію, 
що ти маєш достатнє розуміння, щоб змусити його працювати. Отже, залишається ще одна деталь, якою я хочу з тобою поділитись перед тим, як ти почнеш реалізувати свою НМ, і це - як ініціалізувати коефіцієнти НМ. Виявляється, що ініціалізація параметрів не в 0, а у випадкове значення, буде грати значну роль в тренуванні твоєї НМ. В наступному відео ти побачиш чому.