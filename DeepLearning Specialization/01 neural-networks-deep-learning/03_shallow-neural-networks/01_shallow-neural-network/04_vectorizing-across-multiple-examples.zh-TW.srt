1
00:00:00,360 --> 00:00:04,530
在上段影片中您看到如何計算
神經網路的預估

2
00:00:04,530 --> 00:00:06,610
給予一個訓練例子

3
00:00:06,610 --> 00:00:11,520
在這段影片，您看到如何用向量化
跨越多重訓練例子

4
00:00:11,520 --> 00:00:15,350
而結果會是很類似
您在羅吉斯迴歸分析看到的

5
00:00:15,350 --> 00:00:19,050
將不同訓練例子
疊在矩陣不同的列上

6
00:00:19,050 --> 00:00:23,630
您可以用
從前面影片得到方程式

7
00:00:23,630 --> 00:00:27,860
只要小小修改
就可以用在神經網路

8
00:00:27,860 --> 00:00:32,340
同時計算所有的例子

9
00:00:32,340 --> 00:00:35,080
讓我們來看看
如何運作的細節

10
00:00:35,080 --> 00:00:40,192
這是我們從前面影片得到四個方程式
您如何來計算 z[1]

11
00:00:40,192 --> 00:00:41,348
a[1], z[2] 跟 a[2]

12
00:00:41,348 --> 00:00:46,867
而這告訴您
給予一個輸入特徵 x

13
00:00:46,867 --> 00:00:53,810
您可以用它來產生 a[2] = y-hat 
對於單一訓練例子

14
00:00:54,920 --> 00:01:00,050
如果您有 m 個訓練例子
您需要重複這個程序

15
00:01:00,050 --> 00:01:01,870
第一個訓練例子

16
00:01:01,870 --> 00:01:06,600
x上標 (1) 來計算

17
00:01:06,600 --> 00:01:11,062
y-hat (1), 您
第一個訓練例子的預估

18
00:01:11,062 --> 00:01:16,537
x(2) 用它來產生
預估 y-hat(2)

19
00:01:16,537 --> 00:01:23,050
等等直到 x(m) 
產生預估 y-hat(m)

20
00:01:23,050 --> 00:01:28,349
用啟動
函數的記號

21
00:01:28,349 --> 00:01:31,669
我將它寫成 a[2](1)

22
00:01:31,669 --> 00:01:36,676
這是 a[2](2)

23
00:01:36,676 --> 00:01:40,640
跟 a[2](m)

24
00:01:40,640 --> 00:01:46,830
這個記號 a[2](i)

25
00:01:46,830 --> 00:01:52,520
這圓括號我指的是
訓練例子 i

26
00:01:52,520 --> 00:01:57,220
這方括號 2
指的是第二層

27
00:01:58,530 --> 00:02:02,460
所以這是方括號
跟圓括號的作用

28
00:02:04,170 --> 00:02:07,920
建議如果您有
非向量化的建置

29
00:02:07,920 --> 00:02:11,000
想要計算
所有訓練例子的預估

30
00:02:11,000 --> 00:02:15,630
您需要做 for i = 1 to m

31
00:02:15,630 --> 00:02:18,260
然後基本上建置
這四個方程式

32
00:02:18,260 --> 00:02:24,162
您需要使 z[1](i)

33
00:02:24,162 --> 00:02:30,064
= W[1] x(i) + b[1]

34
00:02:30,064 --> 00:02:38,253
a[1](i) = S形函數 of z[1](i)

35
00:02:38,253 --> 00:02:43,683
z[2](i) = w[2]a[1](i) 

36
00:02:43,683 --> 00:02:50,099
+ b[2], z[2](i) 等於 w[2]a[1](i) + b[2}

37
00:02:50,099 --> 00:02:56,686
a[2](i) 等於S形函數 of z[2](i)

38
00:02:56,686 --> 00:03:03,172
基本上將上面這四個方程式
加上上標圓括號

39
00:03:03,172 --> 00:03:08,788
到所有變數
根據相對的訓練例子

40
00:03:08,788 --> 00:03:12,612
所以加這個上標圓括號
 i 到 x, z 跟 a

41
00:03:12,612 --> 00:03:18,570
如果您想要計算所有的
您 m 個訓練例子的輸出

42
00:03:18,570 --> 00:03:23,930
我們想做的是向量化這整個
計算，來去掉 for 迴圈

43
00:03:23,930 --> 00:03:27,680
順便說一下, 如果我用了很多基本的

44
00:03:27,680 --> 00:03:31,170
線性代數, 是因為
能夠

45
00:03:31,170 --> 00:03:34,580
正確的建置它是
在深度學習時很重要的事

46
00:03:34,580 --> 00:03:38,160
我們實際上
在這門課程很小心選擇符號

47
00:03:38,160 --> 00:03:41,460
讓向量化
步驟盡可能容易些

48
00:03:41,460 --> 00:03:46,140
我希望透過這些
本質會真正幫助您

49
00:03:46,140 --> 00:03:49,750
更快的正確建置
的這些可行的演算法

50
00:03:51,060 --> 00:03:56,210
讓我複製這整個
程式到下一張投影片

51
00:03:56,210 --> 00:03:57,880
然後我們來看如何做向量化

52
00:03:59,130 --> 00:04:02,154
這是我們從
前面投影片複製來的用

53
00:04:02,154 --> 00:04:04,324
迴圈經過所有 m 個訓練例子

54
00:04:04,324 --> 00:04:09,769
記得我們定義
矩陣 X 等於

55
00:04:09,769 --> 00:04:16,860
將我們訓練例子
用列疊起來

56
00:04:16,860 --> 00:04:20,180
拿這些訓練例子
用列疊起來

57
00:04:20,180 --> 00:04:23,220
這變成 n 或者說

58
00:04:23,220 --> 00:04:27,860
nx 乘 m 維度矩陣

59
00:04:29,198 --> 00:04:32,630
我將直接給您答案
告訴您需要建置什麼

60
00:04:32,630 --> 00:04:35,760
來完成取代這個迴圈的向量化

61
00:04:35,760 --> 00:04:41,394
實際上您需要做的是計算

62
00:04:41,394 --> 00:04:46,035
Z[1] = W[1] X + b[1]

63
00:04:46,035 --> 00:04:50,692
A[1] = S形函數 of Z[1]

64
00:04:50,692 --> 00:04:56,157
Z[2] = W[2]乘

65
00:04:56,157 --> 00:05:01,348
A[1] + b[2]

66
00:05:01,348 --> 00:05:10,100
然後 A[2] = S形函數 of Z[2]

67
00:05:10,100 --> 00:05:16,440
如果您要比喻的話
我們從小寫向量 x

68
00:05:16,440 --> 00:05:23,480
到大寫 X 矩陣
用小寫 x 疊在不同的列上

69
00:05:23,480 --> 00:05:28,494
如果您對 z 
做同樣的事

70
00:05:28,494 --> 00:05:33,509
如果您拿 z[1](1), z[1](2), 等等

71
00:05:33,509 --> 00:05:40,290
這些都是列向量
直到 z[1](m)

72
00:05:40,290 --> 00:05:46,270
這個是
將第一個跟所有這些 m 個用列疊起來

73
00:05:46,270 --> 00:05:50,045
這會給您矩陣 Z[1]

74
00:05:50,045 --> 00:05:55,299
同樣的您看
這個量

75
00:05:55,299 --> 00:06:00,957
拿 a[1](1), a[1](2) 等等到

76
00:06:00,957 --> 00:06:06,980
a[1](m) 然後用列疊起來

77
00:06:06,980 --> 00:06:11,610
這個就像我們從
小寫 x 到大寫 X,

78
00:06:11,610 --> 00:06:13,280
小寫 z 到大寫 Z

79
00:06:13,280 --> 00:06:20,920
這個從小寫 a 
一個向量到這個大寫 A[1]

80
00:06:20,920 --> 00:06:26,685
就在那裡
同樣的 Z[2] 跟 A[2]

81
00:06:26,685 --> 00:06:30,141
它們也是
用這些向量

82
00:06:30,141 --> 00:06:32,016
水平疊以來

83
00:06:32,016 --> 00:06:37,326
拿這些向量
水平疊起來

84
00:06:37,326 --> 00:06:40,840
為了得到 Z[2] 跟 A[2]

85
00:06:40,840 --> 00:06:44,042
這個記號的一個屬性
可能可以幫助

86
00:06:44,042 --> 00:06:47,391
您的是想像
這個矩陣 Z 跟 A

87
00:06:47,391 --> 00:06:51,420
我們水平的
索引整個訓練例子

88
00:06:51,420 --> 00:06:55,631
所以這是為什麼水平索引
相對於不同的訓練例子

89
00:06:55,631 --> 00:06:59,730
當您從左邊掃到右邊您
正好也掃過所有的訓練集

90
00:06:59,730 --> 00:07:04,617
而垂直的這個垂直索引
相對於

91
00:07:04,617 --> 00:07:06,130
神經網路不同節點

92
00:07:06,130 --> 00:07:11,077
舉個例子，這個點
這個在最左上的點

93
00:07:11,077 --> 00:07:16,554
在矩陣上是
相對於

94
00:07:16,554 --> 00:07:21,633
第一個訓練例子的第一個隱藏單元的啟動值

95
00:07:21,633 --> 00:07:25,812
下面一項是相對於

96
00:07:25,812 --> 00:07:27,525
第一個訓練例子第二個隱藏單元的啟動值

97
00:07:27,525 --> 00:07:31,505
然後是第一個訓練例子第三個隱藏單元等等

98
00:07:31,505 --> 00:07:37,540
所以當您向下掃這是您
索引的隱藏單元數字

99
00:07:39,670 --> 00:07:42,564
而如果您往水平移動
您將從第一個隱藏單元

100
00:07:42,564 --> 00:07:45,450
第一個訓練例子第一個隱藏單元

101
00:07:45,450 --> 00:07:48,240
第二個訓練例子
第三個訓練例子

102
00:07:48,240 --> 00:07:53,718
等等直到這一個節點相對於

103
00:07:53,718 --> 00:07:59,030
最後訓練例子的的第一個隱藏單元
第 m 個訓練例子

104
00:08:00,760 --> 00:08:07,663
矩陣水平
掃過每一個訓練例子

105
00:08:10,150 --> 00:08:14,195
垂直方向不同
的矩陣索引

106
00:08:14,195 --> 00:08:17,589
相對於不同的隱藏單元

107
00:08:22,342 --> 00:08:26,870
類似的直觀
對於矩陣 Z 跟 X 也是一樣

108
00:08:26,870 --> 00:08:31,840
水平相對於
不同的訓練例子

109
00:08:31,840 --> 00:08:36,227
垂直相對於
不同的輸入特徵

110
00:08:36,227 --> 00:08:41,180
其實是不同的
輸入層節點在神經網路上

111
00:08:42,750 --> 00:08:46,600
所以用這些方程式, 您知道
如何建置您的網路

112
00:08:46,600 --> 00:08:51,320
向量化, 也就是
用向量化來橫跨多重例子

113
00:08:51,320 --> 00:08:55,130
在下一段影片, 我想要
給一點理由為什麼

114
00:08:55,130 --> 00:08:59,070
這種向量化
是正確的建置

115
00:08:59,070 --> 00:09:03,468
實際上會是
類似於您看過的羅吉斯迴歸分析的理由

116
00:09:03,468 --> 00:09:05,300
讓我們進入下一段影片