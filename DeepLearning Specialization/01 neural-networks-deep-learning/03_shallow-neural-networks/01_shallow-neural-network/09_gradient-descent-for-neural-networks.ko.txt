이번 비디오가 매우 흥미로웠던 것 같은데요, 이번 비디오에서는 여러분의 신경망에서 기울기 강하를 도입하는 방법에 대해 배우겠습니다. 한개의 숨겨진 레이어의 경우에 말이죠. 이번 비디오에서는 여러분에게 도입을 위해 필요한 공식을 드릴것인데요. 기울기 강하의 후 방향전파가 작동하게끔 말입니다. 그리고 이 비디오 이후로 더 직관적인 부분에 대해서 다룰텐데요 왜 이런 특정 공식이 정확한 공식인지, 또는 기울기를 계산하는데 올바른 공식인지 신경망과 관련하여 살펴보겠습니다. 1개의 숨겨진 래이어로 이루어진 여러분의 신경망은 일단 w1, v1, w2, b2등의 파라미터값을 가질 것입니다. 만약 nx 또는 n0 입력 특성을 갖는 경우, n1의 숨겨진 유닛과 n2 결과값 유닛이 있는 경우, 여기 예제에서는 이 값들이 1이 되도록 합니다. 그러면 w1 매트릭스는 (n1, n0), 그리고 b1은 (n, 1)차원 벡터인데요 그러므로 (n1, 1)로 표기해주면 됩니다. 칼럼 벡터이죠. w2의 디이멘션은 n2 x n1 인데요 그리고 b2의 다이멘션은 (n1, 1)입니다. 아직까지는 n2가 1인 경우만 봤습니다. 오직 1개의 숨겨진 유닛이 있는 경우 말이죠. 또한, 신경망에서는 비용함수도 있는데요, 현재까지는 일단 이진분류법으로만 진행한다고 가정할 것입니다. 이러한 경우, 파라미터들의 비용은 이렇게 생길 텐데요, 1 나누기 m 그리고 loss 함수의 평균값, 여기 L은 신경망이 예측하는 ŷ의 값인데요, 이것은 a2와 마찬가지 인데요, ground true label 이 Y인 경우 말이죠, 그리고 만약 이진분류법을 진행하면, loss함수는 이전에 이용했던 로지스틱 회귀 함수와 동일할 수 있습니다. 여러분의 알고리즘의 파라미터를 트레이닝 시키기 위해서는 기울기 강하를 진행하여야 합니다. 신경망을 트레이닝 시킬때는 파라미터를 임의로 초기화하는데 굉장히 중요합니다. 0으로 초기화하는 대신에 말이죠. 이것이 왜 그런지는 나중에 보겠습니다. 하지만 각각의 기울기 강하 loop은 예측수치를 산출할 텐데요, 기본적으로 i가 1에서 m 사이인 경우의 ŷ i 를 구하는 것입니다. 그 다음으로는 derivative를 구해야 하는데요, dw1을 구하고, 파라미터 w1에 대해서 비용함수의 derivative를 구하는데요, 또 하나의 변수를 구해야 하는데요, 이것을 db1이라고 할 텐데요, 이것은 비용함수의 기울기를 변수 B1에 대해서 구한 값인데요. 이렇게하여 비슷하게 파라미터 w1, b2를 구할 수도 있습니다. 그리고 마지막으로 기울기 강하 업데이트는 w1을 w1 빼기 알파, 러닝속도죠, 곱하기 dw1 입니다. b1은 b1 빼기 러닝속도 곱하기 db1이 됩니다. w2 b2도 비슷하게 됩니다. 가끔 저는 콜론을 사용하는데요, =을 뜻하는 것으로 2개의 표기법 모두 괜찮습니다. 이것이 그러면 기울기 강하의 iteration인데요, 이런 절차를 몇번 반복하여 파라미터가 수렴하는 것처럼 보일때까지 진행합니다. 이전 비디오에서는 예측값을 산출하는 방법에 대해 이야기 했는데요, 이것을 어떻게 하는지 봤습니다. 벡터화 방식으로 말이죠. 핵심은 바로 이런 partial derivative 항을 산출하는 방법입니다. dw1, db1, 그리고 derivative인 bw2, dp2 같이 말이죠. 제가 하고 싶은 것은 여러분에게 공식을 제공해서 derivative를 산출할 수 있도록 하고, 다음 비디오로 넘어가서, 선택적인 비디오인데요, 이러한 공식이 어떻게 나왔는지에 대해 설명하도록 하겠습니다. 다시 요약하자면, 전 방향전파 공식을 말이죠, 이 공식에서는 z1=w1 x 더하기 b1 그리고 여기 a1은 activation 함수인데요 g1(z1)입니다. 그리고 z2=w2 a1 더하기 b2입니다. 그리고 마지막으로 이것은 트레이닝세트에 걸쳐 벡터화된 것인데요, a2=g2(z2)입니다. 다시 한번 여기서는 이진분류법을 이용한다는 가정이 있습니다. 그러면 여기 activation 함수는 사실 시그모이드 함수기 되야하겠죠. 여기다가 적겠습니다. 이것이 전 방향전파 또는 왼쪽에서 오른쪽으로 전개되는 신경망 산출법인데요. 다음으로, derivative를 계산해보겠습니다. 이것은 후 방향전파 인데요, dz2를 계산할 것인데요, a2 - ground truth Y인데요, 다시 말씀 드리자면 이것은 예시에대해 vectorize된 것입니다. 매트릭스 Y는 1 x m 매트릭스 인데요 모든 M 예시에 대해 가로로 정리합니다. dw2는 이거와 같은데요, 여기 첫번째 3개의 공식은 로지스틱 회귀 의 기울기 강하와 매우 유사합니다. x는 1 컴마, keepdimms= true인데요, 여기 상세 내용인데요, NP.sum 은 파이썬 넘파이 명령어 인데요 1차원의 매트릭스에서의 명령어입니다. 이번 예제에서의 경우, 가로로 더하는 것을 뜻합니다. 그리고 keepdimms는 파이썬이 웃기게 생긴 rank 1 arra가 뜨지 않도록 방지해줍니다. 이것은 (n, ) 이렇게 생겼었죠. 여기 이 keepdimms=true를 진행함으로써 파이썬이 db2의 결과값으로 벡터 (n, 1)의 값을 출력하게 해줍니다. 엄밀히 이야기하면 이것은 (n2, 1)이 될것입니다. 이 경우에는 어차피 (1, 1) 이여서 상관은 없으나 나중에 이것이 중요한 때 다시 이야기하겠습니다. 지금까지 한 것은 로지스틱 회귀와 매우 흡사한데요, 산출하면서 계속 후 방향전파을 실행합니다. 이것은 dz2 곱하기 z1 의 g1 프라임 여기 g1 프라임은 여러분이 결과값 층 L에 대한 숨겨진 레이어를 사용한 activation 함수의 derivative인데요, 결과값 층 L에 대해서는 이진분류법을 가정합니다. 이 가정에는 시그모이드 함수가 함께 내포하고 있습니다. 여기 dz2 와 같이 말이죠, 그리고 여기 곱하기는 element-wise product 입니다. 그러므로 여기 이 부분은 (n1, m) 매트릭스가 될 것인데요, 그리고 여기 element wise derivative 또한 (n1, m) 매트릭스 일 것입니다. 여기 곱하기는 element wise product인데요 여기 2개의 매트릭스에 대해서 말이죠. 그리고 dw1은 이거와 같고, db1은 이것과 동일합니다. 여기 np 점 sum dz 1, 1과 같도, keepdimms=true 입니다. 이전에는 keepdimss가 n=2 였으므로 별로 중요하지 않았지만, (1, 1)이 그냥 실수였기 때문이었습니다. 하지만 여기서는 db1이 (n1, 1) 벡터가 됨에 따라, 파이썬 명령이 np.sum이 이러한 디이멘션을 결과값으로 갖도록 해야할 것입니다. 웃기게 생긴 rank 1 array 의 디아멘션 이 나오지 않게 말이죠. 이것이 나오면 나중에 계산하는데서 망가트릴 수 있습니다. keepdimms parameter 대신에 사용할 수 있는 방법은 reshape를 불러오는 것입니다. NP.sum 결과값을 다시 reshape 하는 것입니다 db가 원하는 디아멘션으로 말이죠. 전 방향전파에 대해 4가지 공식을 통해 알아봤는데요, 후 방향전파은 6개의 공식을 통해 알아봤습니다. 이번 강의에서는 이런 공식을 단순히 적었지만 다음 선택적 비디오에서는 이런 6개의 후 방향전파 알고리즘 공식이 어떻게 파생되었는지 직관적인 부분을 다루겠습니다. 자유롭게 선택적으로 보시면 됩니다. 이런 알고리즘을 도입하면 전 방향전파 과 후 방향전파이 올바르게 나올 수 있습니다. 또한 신경망의 파라미터를 배우기 위해 적용해야 하는 기울기 강하에서도 derivative를 올바르게 산출할 수 있게 해줄 것입니다. 미적분학에 대한 부분을 깊이 이해하지 않더라도 성공적으로 도입하여 잘 작동시킬 수 있는데요, 많은 성공한 사람들은 이전부터 그렇게 하였습니다. 원하시면 다음 비디오를 시청하셔도 좋습니다. 조금더 이 공식들이 파생되었던 직관적인 부분을 다룰 테니까요. 방정식