ロジスティック回帰モデルを見てきました。 また、単一の教師サンプルに対しての予測が
どれだけうまくいっているかを測るという 損失関数についても学習しました。 さらに、教師サンプル全体に対して
パラメータwとbがどれくらいうまくいっているかを測る コスト関数についても見てきました。 では、教師サンプルにおけるパラメータwとbを
学習するために 最急降下法をどのように使えるかをお話ししましょう。 おさらいすると、
これが見慣れたロジスティック回帰のアルゴリズムです。 そして二行目にはコスト関数Jがあり、 これはパラメータwとbの関数です。 この関数は、損失関数の平均として定義されています。 よって、損失関数の総和のm分の1となっています。 つまり、損失関数はアルゴリズムが それぞれの教師サンプルに対する
実際にラベル付けされたy(i)と比べて どれだけうまくyハット(i)を予測できているかを測ります。 右側に展開された式が書かれています。 コスト関数は、訓練データにおいて パラメータwとbがうまくいっているかを測ります。 パラメータwとbを学習するために、 コスト関数J(w,b)をできる限り最小にする
wとbを見つけたいと考えるのが自然ですね。 これが、最急降下法の図解です。 この図において、水平方向の軸は パラメータwとbを表しています。 実際に使うときにはwはもっと高次元になりますが、
グラフ化するにあたって、 wを一つの実数とし、
bも一つの実数としておきましょう。 コスト関数J(w,b)は、 水平方向の軸wとbの上にある面となります。 面の高さは、ある一点におけるJ(w,b)の値を示します。 ここで私たちがやりたいことは、 Jが最小になっている部分に対応する
wとbの値を見つけることです。 コスト関数Jは凸の形であることがわかります。 なので、これはただの大きなお椀のようなものです。 こういった凸状の関数は、 このように凸状でないため
いくつもの局所最適がある関数とは対極のものです。 ここで定義されたコスト関数J(w,b)が
凸状であるというのは、 私たちがこのコスト関数Jを
ロジスティック回帰に使うことの 最大の理由の一つです。 パラメータとして良い値を見つけるために、 まずwとbを何らかの値に初期化します。 その初期値をこの小さな赤い点で示します。 ロジスティック回帰では
ほとんどどんな初期化方法もうまくいきますが、 たいていは0に初期化するでしょう。 ランダムに初期化する方法でもいけますが、 ロジスティック回帰においてはあまり使われません。 しかし、この関数が凸状であるため、
どんな初期値に設定したとしても だいたい同じ点に辿り着くでしょう。 最急降下法は、この初期値の点から出発し、 最も下りの勾配が急になる方向に一歩進みます。 そして、ロジスティック回帰で一歩進んだ後は
このあたりにいるでしょう。 なぜなら下りの勾配が最も激しくなる方向、 つまり最速で坂を下れる方向に進もうとするからです。 これが、
最急降下法が繰り返しの中の一回でやることです。 二回繰り返すとここにきて、 三回目はここ…という感じです。 たぶんここではグラフの裏側に隠れてしまっていますが、 最終的には大域解かそれに近い場所に収束します。 この絵は最急降下法の図解です。 もう少し詳しく描きましょう。 図解するために、
J(w)という関数を最小化したいと仮定します。 その関数はこのような形をしているでしょう。 描きやすくするために、今はbを無視しています。 そうすることで、
高次元ではなく一次元で表すことができます。 最急降下法が行うのは、 次のようなアップデートを繰り返し行うということです。 wの値を取ってアップデートし、 コロンイコールという記号を使って
wをアップデートすることを表しますね、 wをw引くαかける… dJ(w)/dwというのは微分です。 これを、アルゴリズムが収束するまで繰り返します。 表記法についていくつか説明すると、
ここのαは学習率で、 これは最急降下法の繰り返しの都度
どれだけ一歩を大きくするかを決めるものです。 後で学習率αを決めるいくつかの方法をお話しします。 二つ目はこれです。これは微分で、 基本的にはパラメータwについて行いたい変更です。 最急降下法を実装するコードを書き始めるときには、 コード中の変数名dwが
この微分の項を表すという慣習を 使うことにします。 そのため、コードを書くときには、 w:=w-α×dwという感じで書くことになるでしょう。 つまり、dwという変数名でこの微分の項を表す
ということです。 では、この最急降下法のアップデートの意味が
わかるように説明しましょう。 wがここにあるとします。 すると、コスト関数J(w)では
ここにいることになります。 微分の定義は 関数のある一点での傾きであることを
思い出してください。 関数の傾きは、 この点でJ(w)のタンジェントとなる三角の高さを
幅で割ったものとなります。 ここでは、微分した結果は正の値です。 wはw引く学習率かける微分にアップデートされます。 微分が正ということは、wから引くことになるので、 左側に進むことになります。 このように、もしwを大きい数字から始めた場合は、 最急降下法はアルゴリズムが
ゆっくりとパラメータを小さくするように働くのです。 もう一つの例として、もしwがここにあれば、 この点での傾きdJ/dwは負であるので、 最急降下法はwをw引くαかける負の数値に
アップデートします。 すると、繰り返しを続けることで wはだんだんと大きくなっていきます。 つまり、右から出発しようと左から出発しようと、 最急降下法が大域解に導いてくれるのです。 もしあなたが微分や微積分学に詳しくなくて、 dJ(w)/dwという項の意味がわからなくても、
気にしないでください。 次のビデオで微分についてもう少しお話しします。 微積分学について深い知識があれば、 ニューラルネットワークの働きについて
より深い直感を得ることができるでしょう。 でも、もしそれほど微積分学に詳しくなくても、 次のいくつかの動画で微分について
十分な直感をお伝えするので、 あなたもニューラルネットワークを
効果的に使えるようになるでしょう。 しかし今の時点では、 この項が関数の傾きを表していて、 勾配を最も急に下る方向に行くために、 今のパラメータにおいての関数の傾きを知りたい という直感があれば大丈夫です。 wというパラメータしかなかった場合の
J(w)の最急降下法を描きました。 ロジスティック回帰では、
関数にはwとbの両方があります。 その場合、この最急降下法の内側のループが、 次のように繰り返されなければなりません。 wをw-α×wについてのJ(w,b)の微分に 更新することになります。 そして、bはb-α×bについてのコスト関数の微分に 更新されます。 下に書いたこれらの二つの式が、実際に実装するアップデートです。 余談ですが、微積分学での表記法の慣習について 少しややこしいところがあるので言及しておきます。 微積分学を理解するのがそれほど重要だとは思いませんが、 あなたがこれを目にしたときに悩まないようにしておきたいのです。 微積分学では、この項は、 実際にはこのように面白いくねっとした記号で書きます。 これはちょっと変わったフォントでの
dの小文字に過ぎませんが、 これを見たときには 関数J(w,b)がどれだけ wの方向に傾いているのかを示しているだけなんだと思ってください。 微積分学における表記法では、 私は論理的だと思いませんし 無駄に物事を複雑にするだけだと思いますが、 もしJに二つ以上の変数があれば、小文字のdの代わりにこの面白い記号を使うのです。 これは偏微分の記号と呼ばれます。 でもそんなに気にしないでください。 もしJが一つの変数についての関数なら、dの小文字を使います。 つまり、この面白い偏微分の記号を使うか それとも上記のように小文字のbを使うかの違いは、 Jが複数の変数についての関数であるかどうかということです。 複数の変数があるならこの偏微分の記号を使うし、 変数が一つだけなら小文字のdを使います。 これは私にとっては無駄に複雑なだけに見える 微積分学における面白い表記ルールの一つです。 もしこの偏微分の記号を目にしたら、 ある一つの変数について
関数の傾きを測ったものだと思ってください。 微積分学の形式的に正しい様式に準拠すると、 このJには一つではなく二つの変数があるので、 下に書かれたこれは
偏微分の記号で表されるべきですね。 でも、小文字のdとそれほど意味は変わりません。 最後に、これをコードで実装するときには、 wをどれくらいアップデートするかというこの量を 変数dwとして表すという慣習を使います。 bをどれくらいアップデートするかという量も 同様に、 コードではdbと表します。 ここまでが、最急降下法を実装する方法です。 もし微積分に何年も触れていないなら、 ここまでで出てきた微分が多すぎると
感じるかもしれません。 でも、そう感じていても大丈夫です。 次の動画で、微分についてもっと直感を養います。 微積分について
数学的にそんなに深く理解していなくとも、 直感的理解があるだけで、 ニューラルネットワークを
効率的に使うことができますよ。 さあ、次の動画に進んで、 もう少し微分についてお話ししましょう。