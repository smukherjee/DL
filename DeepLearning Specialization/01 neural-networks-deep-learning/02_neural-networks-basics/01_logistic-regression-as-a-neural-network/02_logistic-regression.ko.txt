In this video, we'll go over logistic regression. 이 러닝 알고리즘은 지도학습 문제에서 결과값 레이블 Y의 값이 0 이거나 1인 경우 사용되는 학습 알고리즘입니다 즉 이진 분류 문제의 경우이죠. 입력 특성 벡터 X가 고양이이거나 고양이가 아닌 사진을 분류하는 이미지에 해당하는 경우 결과값을 예측하는 알고리즘이 필요할 것입니다. 이것을 ŷ (y hat) 이라고 하겠습니다. 이것은 Y의 평균값이죠. 공식적으로는, ŷ 의 값이 Y가 주어진 입력 특성 X가 되는 확률로 만드는게 좋습니다. 다시 말해서, X 가 사진이면, 저번 비디오에서 본 것처럼 말이죠, ŷ 은 이 사진이 고양이일 확률은 얼마나 되는가? 를 말해주는 것이 좋습니다. 그러므로, 저번 비디오에서 본 것과 같이 X는 X 다이멘션얼 벡터입니다. 로지스틱 회귀분석법의 매개변수가 W이고, W가 동시에 X 다이멘션얼 벡터이며, 실수인 b도  x 다이멘션얼 벡터인 경우 말이죠. 그러므로, 입력값 X와, 매개변수가 W와 b인 경우, ŷ 결과값은 어떻게 생성할까요? 시도할 수 있는 방법은, 물론 안되겠지만, ŷ 이 w transpose값 곱하기 X + b입니다. 입력값 X의 일차함수인 것처럼 말이죠. 사실 말한 것 그대로, 이것을 선형회귀에 씁니다. 그렇지만 이항분류문제로는 그닥 좋지 않은 알고리즘인데요, ŷ 이 Y가 1일 확률이 되도록 만드는 것이 좋기 때문입니다. 결과적으로 ŷ 이0에서 1사이에 있어야 하는데, 이렇게 만들기가 쉽지 않죠, W transpose 곱하기 X 플러스 B값이 1보다 훨씬 더 크고 마이너스일 수도 있기 때문입니다. 확률적인 측면으로 비추어보았을때, 말이 안되죠. 이것이 0에서 1사이에 있도록 하는 것이요. 로지스틱 회귀분석법에서는 결과값이 ŷ = 이 만큼의 양이 적용된 시그모이드 함수가 될 것입니다. 시그모이드 함수는 이렇게 생겼습니다. 가로축에 Z를 넣으면, Z의 시그모이드 함수는 이렇게 생깁니다. 부드럽게 0에서 1로 가죠. 축을 이렇게 넣겠습니다. 이 값은 0이고 세로축을 0.5에서 교차합니다. Z의 시그모이드 함수는 이렇게 생겼구요, 이 W transpose 곱하기
X 더하기 B에 대한 양을 나타내기 위해 Z를 쓰겠습니다. 여기 시그모이드 함수에 대한 공식입니다. Z의 시그모이드에서 Z는 실수, 1 나누기 1+E의 의 마이어스 Z승입니다. 2가지 알만한 사항이 있는데요. 만약 Z의 값이 매우 크면 E의 -Z승은 0에 가까울 것입니다. Z의 시그모이드는 대략 1 나누기 1 플러스 0에 가까운 값일 것입니다. 그 이유는 E의 아주 큰 숫자의 승은 0에 가깝디 때문이죠. 그러므로 이 값은 1에 가깝죠. 그럼 보시다시피, 왼쪽을 보시면 Z의 값이 매우 큰 경우, Z의 시그모이드는 1과 매우 가깝운 값이 됩니다. 반대로 Z의 값이 작으면, 또는 아주 큰 음수인 경우, Z의 시그모이드는 1 나누기 1+E의 -Z승이 됩니다. 이 값은 아주 큰 값이 되겠죠. 그러면 이 것은, 1 나누기 1 더하기 아주 큰값이라고 생각해보십시요. 0에 가까운 값이 되겠죠. 그러면 Z의 값이 매우 큰 음수의 값이 되면서, Z의 시그모이드는 0과 가까운 값이 됩니다. 그러므로 로지스틱 회귀분석을 도입하는 경우에는, 여러분의 임무는 parameter W와 B를 배워서 ŷ 이 Y=1이 되는 확률을 잘 추정한 수치가 나올 수 있도록 하는 것입니다. 넘어가기에 앞서, 표기에 관한 내용을 이야기하겠습니다. 신경망을 프로그래밍할때, W와 B 매개변수를 따로 다룰텐데요, 여기서 B는 인터-스펙트럼의 끝을 나타냅니다. 다른 코스에서 이 표기방법을 다르게 하는 경우를 보셨을 수도 있는데요, 간혹, x0이라는 특성을 정의해서 이 값을 1로 만들어주는데요, 이 경우 x는 R의 nx+1승에 속하는데요, 그 다음에 ŷ 은 theta transpose x 곱하기 시그마가 되도록 합니다. 다른 방법의 표기방식에서는, 벡터 parameters 인 쎄타가 있는데요 쎄타0, 쎄타1, 쎄타2, 쎄타 NX까지 말이죠, 쎄타0은 쎄타 B줄이구요 이것은 단순 줄을 나타내는 것입니다. 쎄타1에서 쎄타 nx까지는 W줄가지 속합니다. 여러분이 신경망 네트워크를 도입하는 경우, B와 W 매개변수를 따로 취급하는 것이 훨씬 더 쉽습니다. 이번 수업에서는 여기 빨간색으로 적은 표기법은 쓰지 않도록 하겠습니다. 이전에 다른 코스에서 이런 표기법을 본 적이 없으시면, 신경 쓰실 필요 없습니다. 이미 이 표기방식을 접한 분들을 위해서 단순히 직접적으로 언급해서 이러한 표기법을 해당 코스에서느 쓰지 않는 것을
알려드리고 싶었습니다. 본 적이 없으시면 중요하지 않기 때문에 걱정하지 않으셔도 됩니다. 이제 여러분은 로지스틱 회귀분석 모델이 어떻게 생겼는지 보셨는데요, 다음은, W와 B 매개변수를 바꾸기 위해서는 비용함수를 정의해야 합니다. 다음 비디오에서 해보도록 하죠.