여러 가지 문제에서 심층신경망이 잘
작동한다고 많이 들었을텐데요, 단순히 신경망의 크기가 커서 되는 것이 아닙니다. 신경망 네트워크는 깊고, 숨겨진 레이어가 많아야 합니다. 왜 그런걸까요? 몇개의 예제를 통해
직관을 얻겠습니다. 왜 깊은 네트워크가 잘 작동하는지 말이죠. 첫째로,
깊은 네트워크는 무엇을 계산하죠? 여러분이 얼울 인식 또는 얼굴 감지 시스템을 만들때,
심층 신경망이 하는 일은 이렇습니다. 아마도 얼굴 사진을 넣었을때에,
첫번째 층의 신경망은 feature detector 또는 edge detector같은 것일수도 있겠죠. 이번 예제에서는, 이 이미지에서 아마 20개의 숨겨진 유닛을 통해 신경망이
계산을 하는 것을 그린 것인데요. 여기 사각형의 박스로 20개의 숨겨진 유닛이
시각화 시킵니다. 예를 들어서, 여기 이 작은 visualization은 숨겨진 유닛이 끝의 모서리가 어딘지
이 이미지에서 파악하려는 것입니다. 이 숨겨진 유닛은 가로 모서리가 어디인지 파악하려고
할 수 있게죠. 나중의 코스에서
컨볼루션 신경망 에 대해 이야기 할 텐데요, 이러한 시각화에 대한 내용이
이후에 더 이해가 빨리 되실 것입니다. 공식적으로 말씀드리자면,
신경망의 첫번째 층은 이 이미지를 보고
어디가 모서리인지 파악하려고 하는
것이라고 볼 수 있습니다. 이제 픽셀을 같이 그루핑하여 모서리를 구성할텐데요, 이를 통해
모서리를 파악하는 것입니다. 그 이후로, 파악된 모서리에 대해
모서리들을 그룹으로 만들고,
얼굴의 일부를 만들어 나갑니다. 예를 들어, 신경세포가
눈을 탖으려고 할 수 있습니다. 또는, 다른 신경세포가
여기 이부분의 코를 찾으려고 할 수도 있고요. 이렇게, 모서리들을 취합하는 과정을 통해 얼굴의 다른 부분들을 감지할 수 있는 것입니다. 그렇게하여 최종적으로는 여러 얼굴부위를
합치게 되는 것인데요, 눈, 코, 귀, 또는 턱 이렇게 말이죠.
그 이후로, 다른 얼굴들의 유형을 인식하거나 감지할 수 있게 됩니다. 직관적으로, 이런 초기의 층을 간단한 모서리와 같은 함수를 감지하는 것이라고
생각할 수 있습니다. 그리고나서, 다음 단계의 신경 층에서 이런
내용을 취합할 수 있습니다. 그렇게해서 더욱 복잡한 함수를
배울 수 있는것이죠. 컨볼루션얼 신경망에 대한 내용을 나중에 다룰 때
이런 시각화 내용을 더 빨리 이해하실 수 있을것입니다. 시각화의 기술적인 내용중 하나는, 이러한 모서리 감지 기능이
이미지의 조금한 부분을 볼텐데요, 이런 아주 조금한 이미지 범위 말이죠. 그리고나서, 얼굴 인식기능은
조금 더 큰 이미지 부분을 볼 수 있겠습니다. 이 부분에서 기억하실 것은
일단 먼저 이미지에서 간단한 것들을 찾은 뒤에, 모서리와 같은 것 말이죠, 그 다음에 취합하는 것입니다. 이러한 부분들을 합쳐서 조금 더 복잡한 것들을
감지할 수 있습니다. 눈과 코같은 부위 말이죠. 그리고 이런 부위를 또 합쳐서
더욱 복잡한 내용도 감지할 수 있는 것입니다. 이러한 복잡한 계층적인 representation이 또는 compositional representation이 이미지 또는 얼굴 인식 분야외에도
다른 분야의 데이터에도 적용됩니다. 예를 들어, 여러분이 음성 인식 시스템을
만드는 경우에, 음성을 시각화하기는 쉽지 않지만, 오디오 음성 파일을 삽입하면,
첫번째 층의 신경망이 낮은 레벨 음성 wave 형식을
감지하도록 배울 수 있습니다.
예를 들어, 이 톤이 올라가는 것인지 내려가는지 감지하는 것과 같이 말이죠. 백색소음인지, slithering 음성인지, 또는 피치는 어떤지 감지할 수 있습니다. 이렇게 low level wave form 특성들을
감지할 수 있도록 가르칠 수 있습니다. 기 이후에, low level wave forms를 취합하여 기본적인 음성유닛을 감지할 수 있도록 만들 수 있습니다. 음성학에서는 이것을 phonemes라고 하는데요. 예를 들어, cat이라는 단어에서 C가 phoneme이고
A가 phoneme입니다 T도 또 하나의 phoneme입니다. 이렇게 기본 음성 유닛을 감지하도록 배울 수 있고요, 이러한 phoneme을 취합하여
단어를 알아듣게 만들 수 있습니다. 그리고 단어를 합쳐서 전체 문구를 이해하거나 문장들을 이해할 수도 있겠죠. 그러므로 복수의 숨겨진 레이어가 있는 심층신경망 같은 경우, 초기의 층에서는 이런 낮은 레벨의
간단한 특성들을 배울 수 있게 만들고, 나중에 따라오는 층에서는 감지된
이런 간단한 것들을 합칠 수 있게 하고, 조금 더 복잡한 단어나 문구 또는 문장들, 발음을 감지할 수 있도록 만듭니다. 음성인식 기능을 수행하기 위해서 말이죠. 저희가 볼 수 있는 것은 첫번째 초기 단계에서는 입력값에 대한 간단한 함수를,
예를 들어 모서리 관한 정보를 다루겠지만 네트워크 깊숙히 진입하면
놀랍게도 복잡한 것들을 다루게 됩니다. 얼굴을 감지하거나, 단어를 감지하거나,
문구 또는 문장까지 말이죠. 어떤 사람들은 심승신경망을 인간의 두뇌로 비유하는데요, 저희가 믿는 내용과,
신경과학자들이 생각하는 것은 인간의 두뇌도 일단 먼저 간단한 것을
모서리나, 눈이 보는 어떠한 내용을 시작으로 점점 취합한다는 점입니다. 조금 더 복잡한 얼굴과도 같은 것 말이죠. 딥러닝과 인간 두놔의 비유는 간혹 위험하다고 생각은 드는데요, 인간의 두뇌가 어떤 원리로 사물을 받아들이는지와 간단한 것들을 먼저 시작으로 정보들을 취합하여 더 복잡한
물체를 만들어 간다는 점에서 딥러닝 분야에서 느슨한 영감을 준 것은
사실입니다. 인간의 두뇌에 대해서, 생물한적 뇌에 대한 내용은
다음주에 더 다루도록 하겠습니다. 심층 네트워크가 왜 더 잘 작동하는지에 대한 직관적인 부분이 또 한가지 더 있는데요, 이 결과는 circuit theory에서 유래되는데요,
다른 logic gates에 대해 어떤 유형의 함수를 쓸 수 있는지에 대한 생각을 담았습니다. 비공식적으로, 이 함수들은
작지만 깊은 네트워크로 계산하는데요, 여기서 작다고 하는 것은
숨겨진 유닛이 비교적 작다는 뜻입니다. 같은 함수를 얇은 네트워크로
계산하려고 하면, 즉, 숨겨진 레이어 가 충분하지 않으므로 계산을 하기 위해서는 기하급수적으로 많은
숨겨진 레이어의 수가 필요할 것입니다. 한가지 예제를 통해 한번 보여드리겠습니다. 여러분이 exclusive or를 계산하려고 한다 해보겠습니다. 입력특성 값의 차이를 구하는 것입니다. 즉 x1, xor, x2, xor x3, xor, xn까지 계산하려고 합니다.
n 또는 nx개의 특성이 있는 경우 말이죠. xor tree를 이렇게 그리면,
xor1에서 x1과 x2를 갖고, x3와 x4에 대해서 또
xor을 계산합니다. 엄밀히 이야기하면, AND 와 NOT 게이지를 이용하는 경우, XOR 함수를 계산하는데 2개정도의 층이 필요할 수 있습니다. 하지만 비교적 작은 circuit에서는
XOR를 계산할 수 있습니다. 그 다름에 XOR tree를 이와 같이 그릴 수 있습니다. 이렇게 Y라는 결과값을 주는 circuit을
만들 때까지 말이죠. y hat은 Y입니다. exclusive or 또는 parity of all these input입니다. XOR 를 계산하기 위해서는 네트워크의 깊이는
order of log N일 것입니다. 이런 XOR tree의 경우 말이죠. 그러므로 노드의 개수,
circuit components 개수, 또는 gates 숫자는 여기 네트워크에서는 그리 많지 않습니다. exclusive OR를 계산하기 위해서 그리 많은
gates를 요하지 않습니다. 그러나 만약 복수의 숨겨진 레이어가 있는 신경망을 사용할 수 없는 경우,
이 경우는 order log and 숨겨진 레이어s 인데요 하나의 숨겨진 레이어로 함수를 계산해야하는 경우, 여기 이 것들이 특정 숨겨진 유닛으로 가는 것인데요, 그러면 이 것들은 Y를 결과값으로 줍니다. XOR 함수의 parity를 계산하기 위해서는 여기 숨겨진 레이어가 기하급수적으로 커야할 것입니다. 2의 n승의 배열을 열거해하기 때문이죠 즉, 2의 n승입니다,
가능한 입력값의 배치인데요, exclusive or 가 1또는 0이 되는 경우입니다. 결과적으로 숨겨진 레이어가
기하급수적으로 큰 값이 필요하게 됩니다. bit의 단위로 말이죠 제가 생각하기엔,
엄밀히 맒하면 이것은 2의 n-1승으로 할 수 있을 것입니다. 이것은 order 2의 n승인데요,
기하급수적으로 큰 number of bits입니다. 앞서 다룬 내용을 통해
더 깊은 네트워크가 얇은 네트워크보다 더 쉽게 계산할 수 있는 수학적 함수가 있다는 것을
알게해준 계기가 됐길 바랍니다 저도 인정해야하는데요,
직관적으로 도움을 주는 부분에 대해서는
circuit theory가 덜 도움이 되었습니다. 하지만 이 방법이
여러 사람들이 인용하는 값들인데요,
깊은 네트워크를 대표해서 설명할 때 말이죠. 깊은 네트워크를 선호하는 것에 대한 이런 이유 말고도, 다른 이유는 사실 솔직히 말씀드리면
브랜딩의 효과인 것 같습니다. 이전에는 숨겨진 레이어가 많은 신경망이라고 했었는데요, "딥 러닝" 이라는 용어가 굉장히 훌륭한 브랜딩입니다.
굉장히 "deep"하게 다가오기 때문입니다. 이 단어가 익숙해지자
신경망이 rebranding 되었습니다. 숨겨진 레이어가 많은 신경망이라는 이름이
rebranding 된 것이죠. 인기가 많은 상상에 대한 부분도 사로잡았습니다. PR brading 을 감안하지 않더라도,
딥네트워크는 작 작동합니다. 가끔 사람들은 오버해서
굉장히 많은 양읜 숨겨진 레이어를 사용하기 권고하는데, 새로운 문제를 시작하는 경우에
저는 주로 심지어 로지스틱 회귀분석으로 시작합니다.
그 다음으로 한개 또는 2개의 숨겨진 레이어를 시도하고요
그 다음 그것을 하이퍼 파라미터로 이용하는 것이죠. 그것을 하이퍼 파라미터도 이용하거나,
튜닝을 하기 위한 파라미터로 사용해서 신경망에 적합한 깊이를 찾아내는 것입니다. 지난 몇 년간 특정 어플에서는 아주 굉장히 깊은 신경망 네트워크와
수 많은 숨겨진 레이어로 구성되어 있는 것이 가끔은 가장 좋은 모델로 쓰이는 것이
트렌드화 되었습니다. 이것으로 딥러닝이 왜 잘 작동하는지에 대한
직관적인 부분은 모두 설명해 드렸는데요, 이제는 전 방향전파에서 뿐만 아니라 후 방향전파에서도 도입할 수 있는 과정을 살펴보겠습니다.