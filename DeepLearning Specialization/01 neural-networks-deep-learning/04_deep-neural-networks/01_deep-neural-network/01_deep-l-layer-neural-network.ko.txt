4주차 강의에 오신 것을 환영합니다. 지금까지 여러분은 신경망과 관련된
4개의 promulgation과 back promulgation을 보았고, 한개의 숨겨진 유닛으로 이루어진 신경망, 
또한 로즈스틱 회귀분석에 대한 내용, 벡터화와 관련된 내용, 그리고 언제 무작위로 초기화를 하는 것이 좋은지도 
배웠습니다. 지난 2주간의 숙제를 모두 마치셨다면, 이러한 아이디어를 실제로 적용하고
도입해보셨을 텐데요. 지금쯤이면, 심층신경망을 도입하기 위해 필요한 아이디어를 살펴보았는데요, 이번주에는 이러한 아이디어를 바탕으로 본인이 직접 심층신경망을 도입할 수 있도록 그 내용을 
취합해보도록 하겠습니다. 이번주 연습학습이 조금 더 길고, 더 많은 작업을 요구하기 때문에, 
비디오 강의를 조금 짧게 진행하겠습니다. 비디오를 조금 더 빨리 마치고 나서 끝나고 나오는 연습 학습을 진행하는데 조금 더 시간을 가질 수 있도록
하겠습니다. 여러분이 이 학습을 통해 신경망에 대해 자부심을 갖고 
그 내용을 깊게 생각을 하실 수 있으면 좋겠습니다. 자 그러면 심층신경망은 무엇일까요? 이 로지스틱 회귀분석에 관한 사진을 보셨을텐데요. 또한, 하나의 숨겨진 층이 있는 신경망도 이과 같이 보셨을 것입니다. 여기서 보이는 그림은 2가지의 숨겨진 층으로 이루어진 신경망입니다. 이것은 5개의 숨겨진 층으로 이루어진 그림이구요. 우리는 로지스틱 회귀분석 모델을 굉장히 "shallow"한 모델이라고 표현합니다. 반면에 이 모델은 "deeper model"이라고 표현합니다. shallow와 depth의 차이는 결국 그 정도의 차이입니다. 이것은 싱글 숨겨진 레이어 네트워크는, 2 개의 층으로 이루어진 신경망입니다. 기억하실 것은, 신경망의 층을 세는 경우, 
우리는 입력값의 층을 세지 않습니다. 그저 결과값 그대로의 층의 수를 셉니다. 그러면 이것은 2개의 신경망 층으로 
조금 shallow 하지만 로지스틱 회귀분석 모델보다는 shallow하지 않습니다. 엄밀히 이야기하면 로지스틱 회귀분석 모델은
1개의 층으로 이루어진 신경망입니다. 하지만 지난 몇 년간 머신러닝 커뮤니티의 AI는 shallow한 모델과는 달리 심층신경망이 배울 수 있는 함수가 
많다는 것을 깨달았습니다. 어떠한 특정 문제에 대해서 미리 신경망의 적합한 깊이를 예측하기는 어렵겠지만 로지스틱 회귀분석 모델을 먼저 시도해보고 1개의 숨겨진 층, 2개의 층 순으로 시도해보고
층의 개수를 또 다른 매개변수로 지정하여 다른 값을 시도해보는 방법이 있습니다. 이렇게해서 hold-out cross validation data에 이르러 모두 시도해보고, 
아니면 development set에 시도해보는 것도 방법입니다. 나중에 더욱 자세히 보도록 하겠습니다. 이제 심층신경망을 나타내는 표기법에대해 알아보겠습니다. 여기는 하나, 둘, 셋, 네개의 층으로 이루어진 신경망인데요. 3개의 숨겨진 층으로 이루어져 있습니다. 이 층에서 숨겨진 유닛은 5, 5, 3
그리고 1 유닛입니다. 여기서 쓸 표기법은, 대문자 L을 쓸것이구요, 이 것은 네트워크에서의
층 개수를 나타낼 것입니다. 이 경우 L=4입니다. 이것은 
층의 개수이죠. 그리고 N의 위첨자 [l]을 사용해서 노드의 개수를 나타낼 것입니다. 또는 소문자 l로 나타낸 유닛의 개수입니다. 이것을 인덱스화하면, 
입력값을 "0" 층으로하고 이것을 1층, 이것은 2층, 이것은 3층, 이것은 4층 그렇게되면 이것처럼 
n[1] 이 있겠죠. 첫번째 층에서 나온 것이구요, 
이 값은 5입니다. 그 이유는 5개의 숨겨진 유닛이 있기 때문입니다. 이것은 n[2]인데요 2번째 숨겨진 숨겨진 층에서의 유닛 개수인데요 이 값은 마찬가지로 5입니다. 
n[3]는 3입니다. n[4]는 n[L]입니다.
upper unit의 개수는 1입니다. 대문자 L은 4와 같기 때문입니다. 그리고 여기에서는 입력 층으로 n[0]은 nx이고 이 값은 3이 됩니다. 이렇게해서 다른 층마다 있는 노드의 개수를 표기합니다. 각각의 L층에 대해서, a[l]을 이용해서 l 층에 대한 activation을 표기할 것입니다. 나중에 보겠지만, 방향전파를 위해 a[l]을 activation g(z[l]) 을 계산하게 될 것입니다. 아마도 activation은 l층에 대해서도 이렇게 인덱싱 될 것입니다. 그리고 W[l]를 이용해서 l 층에서의 z[l] 값을 계산하는 비중을 표기하도록 하겠습니다. 유사하게 b[l]은 z[l]을 계사한는데 쓰입니다. 표기에 대한 내용을 마무리하자면, 
입력 특성 값이 x라고 불립니다. x는 또한 0층의 activation인데요, 즉 a[0]은 x입니다. 그리고 마지막 층의 activation은 
a[L]=ŷ입니다. 결과적으로 a[L]은 신경망의 예상 결과값 ŷ이 되는거죠. 이제 여러분은 심층신경망이 어떻게 생겼는지 
보셨는데요, 신경망에서 계산할 때 쓰는 각종 표기법 또한 익히셨습니다. 이 비디오에서는 굉장히 많은 표기 방법에 대해 
다뤘는데요, 만약 공부하시면서 잘 기억이 나지 않는 심볼이 있는 경우, 
코스 홈페이지에 표기시트 또는 노테이션 가이드를 업로드 해놓았으니 확인하실 수 있습니다. 다름으로는 이런 네트워크에서 전 방향전파가 어떤지 보겠습니다. 다음 비디오로 넘어가겠습니다.