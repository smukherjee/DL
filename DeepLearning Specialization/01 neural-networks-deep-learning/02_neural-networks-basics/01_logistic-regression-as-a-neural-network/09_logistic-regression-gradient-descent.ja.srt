1
00:00:00,000 --> 00:00:02,250
おかえりなさい。 このビデオでは

2
00:00:02,250 --> 00:00:04,980
微分を計算する方法を話します

3
00:00:04,980 --> 00:00:08,330
ロジステック回帰の勾配降下法を
実装できるようにするためです

4
00:00:08,330 --> 00:00:11,040
覚えてほしいのは何を実装するかです

5
00:00:11,040 --> 00:00:13,230
つまり鍵となる数式で
ロジステック回帰での

6
00:00:13,230 --> 00:00:17,725
勾配降下法を
実装するのに必要な数式です

7
00:00:17,725 --> 00:00:22,185
このビデオでは計算グラフを使って
この計算を行おうと思います

8
00:00:22,185 --> 00:00:25,320
ロジステック回帰の勾配降下法を
算出する目的には

9
00:00:25,320 --> 00:00:29,342
多少労力を使い過ぎなことは認めますが

10
00:00:29,342 --> 00:00:31,183
この方法で説明を
徐々にしていき

11
00:00:31,183 --> 00:00:33,975
これらの考えを理解してもらえるようになり

12
00:00:33,975 --> 00:00:38,370
完全なニューラルネットワークについて話す頃には
より理解しやすくなるようにしたいのです

13
00:00:38,370 --> 00:00:44,235
それではロジステック回帰の勾配降下法に
飛び込んでいきましょう

14
00:00:44,235 --> 00:00:49,070
簡単に振り返ると
ロジステック回帰は次のようなものだと設定しました

15
00:00:49,070 --> 00:00:53,220
予測のyハットは
このように定義され

16
00:00:53,220 --> 00:00:56,490
zはこの上のようになり

17
00:00:56,490 --> 00:01:01,800
もし１データの場合に焦点を当てるか
１点のデータに関しては

18
00:01:01,800 --> 00:01:03,630
損失は次のように

19
00:01:03,630 --> 00:01:05,190
定義されます

20
00:01:05,190 --> 00:01:07,855
a はロジステック回帰の出力で

21
00:01:07,855 --> 00:01:10,535
y は正解のラベルです

22
00:01:10,535 --> 00:01:15,735
ではこのデータの時の
これの計算グラフを書きましょう

23
00:01:15,735 --> 00:01:20,520
特徴はX1 X2 の２つしかないと想定します

24
00:01:20,520 --> 00:01:22,860
z を計算するためには

25
00:01:22,860 --> 00:01:27,030
w1 w2 の入力が必要です

26
00:01:27,030 --> 00:01:31,130
そして特徴x1 x2 以外にも
b が必要です

27
00:01:31,130 --> 00:01:33,705
計算グラフでは
これらが

28
00:01:33,705 --> 00:01:36,910
z を計算するのに使われます

29
00:01:36,910 --> 00:01:41,588
w1x1+w2x2+b で

30
00:01:41,588 --> 00:01:45,380
これを四角で囲みます

31
00:01:45,380 --> 00:01:48,555
そしてyハットを計算します

32
00:01:48,555 --> 00:01:52,244
a=σ(z)で

33
00:01:52,244 --> 00:01:55,740
これが計算グラフの次のステップです

34
00:01:55,740 --> 00:01:58,725
そして最後に

35
00:01:58,725 --> 00:02:01,840
L(a,y) を計算しますが
式はここには書き移しません

36
00:02:01,840 --> 00:02:06,900
ロジステック回帰では
目指すのはパラメーターの

37
00:02:06,900 --> 00:02:12,830
wやbを変更して
損失を減らすことです

38
00:02:12,830 --> 00:02:15,870
１つのサンプルに対しての損失を
計算するような

39
00:02:15,870 --> 00:02:19,280
順誤差伝播法での計算方法を
記述していました

40
00:02:19,280 --> 00:02:23,940
今回は微分を逆方向に計算していく方法を
話していきましょう

41
00:02:23,940 --> 00:02:26,025
これは奇麗に書き直した図です

42
00:02:26,025 --> 00:02:30,690
この損失に関しての微分を計算するのが
目的なので

43
00:02:30,690 --> 00:02:33,570
逆方向に戻るときに
最初に行うのは

44
00:02:33,570 --> 00:02:38,010
このa に関する損失の微分を
計算します

45
00:02:38,010 --> 00:02:41,940
ここに少し書き加えます

46
00:02:41,940 --> 00:02:43,570
このコードでは

47
00:02:43,570 --> 00:02:49,000
da とこの変数を書きます

48
00:02:49,000 --> 00:02:52,725
微積分学を知っていれば

49
00:02:52,725 --> 00:03:02,004
これが-y/a+(1-y)/1-aと
書けるでしょう

50
00:03:02,004 --> 00:03:06,185
もし微積分学を知っていれば

51
00:03:06,185 --> 00:03:07,535
損失の式をつかって

52
00:03:07,535 --> 00:03:10,515
小文字a の変数に対する微分を
計算できて

53
00:03:10,515 --> 00:03:12,792
この式を作れます

54
00:03:12,792 --> 00:03:15,280
もし微積分学を知らなくても
気にしないでください

55
00:03:15,280 --> 00:03:17,960
このコースを通して
必要なときには

56
00:03:17,960 --> 00:03:20,100
微分の式を
お知らせします

57
00:03:20,100 --> 00:03:21,185
微積分学のエキスパートだったら

58
00:03:21,185 --> 00:03:24,590
微積分学を使って
前のスライドの損失の式を見て

59
00:03:24,590 --> 00:03:29,504
a に関する微分を
導いてみることをお勧めします

60
00:03:29,504 --> 00:03:32,635
十分に微積分学を知らなくても
気にしないでください

61
00:03:32,635 --> 00:03:35,491
da の量と
a に関する最終的な出力変数を

62
00:03:35,491 --> 00:03:38,825
計算し終わったら

63
00:03:38,825 --> 00:03:40,715
逆方向に行くことができます

64
00:03:40,715 --> 00:03:45,525
dz を導けます

65
00:03:45,525 --> 00:03:47,648
これはPython コードの変数名ですが

66
00:03:47,648 --> 00:03:51,200
このdz は損失の微分のdLと合わせて

67
00:03:51,200 --> 00:03:53,618
dL/dz になります

68
00:03:53,618 --> 00:03:59,850
損失についてはa y を
明示的に引数として書いても構いません

69
00:03:59,850 --> 00:04:04,230
どちらの表記方法も
同じように許容されます

70
00:04:04,230 --> 00:04:09,605
この式は
a-y と表現できます

71
00:04:09,605 --> 00:04:14,685
微積分学をよく知っている人たちのために
少し追加でお知らせしますが

72
00:04:14,685 --> 00:04:16,795
よく知らなくても
気にしないでください

73
00:04:16,795 --> 00:04:20,320
このdL dz は

74
00:04:20,320 --> 00:04:27,850
dL/da da/dz と
表現できます

75
00:04:27,850 --> 00:04:29,940
da/dz は

76
00:04:29,940 --> 00:04:33,755
a(1-a)とできます

77
00:04:33,755 --> 00:04:37,800
dL/da は先ほど
このようになると確認しました

78
00:04:37,800 --> 00:04:41,530
この２つの量を使うと

79
00:04:41,530 --> 00:04:43,846
dL/da の確認した項と
da/dz の確認した項を使って

80
00:04:43,846 --> 00:04:47,165
この２つを乗算すると

81
00:04:47,165 --> 00:04:51,915
a-y のように単純化させて
表現することができます

82
00:04:51,915 --> 00:04:53,220
こう算出しました

83
00:04:53,220 --> 00:04:57,390
これこそが連鎖律で
この形は避けられるものです

84
00:04:57,390 --> 00:05:02,770
微積分学に詳しければ
どうぞ全体の流れを計算してみてください

85
00:05:02,770 --> 00:05:05,345
でも詳しくない場合には
dz=a-y さえ知っておけばよいです

86
00:05:05,345 --> 00:05:09,365
あなたのために
この微積分の計算を行ってあります

87
00:05:09,365 --> 00:05:13,010
誤差逆伝播法の最終ステップは
逆方向に進んで

88
00:05:13,010 --> 00:05:17,480
w とb を
いくら変化する必要があるかを計算します

89
00:05:17,480 --> 00:05:24,610
w1 に関する微分は
"dw1"と呼んで表現できます

90
00:05:24,610 --> 00:05:31,810
"dw1" =x1 dz です

91
00:05:31,810 --> 00:05:36,485
同様にw2 についてのdw2は

92
00:05:36,485 --> 00:05:39,455
x2 dz です

93
00:05:39,455 --> 00:05:42,585
db=dz です

94
00:05:42,585 --> 00:05:47,375
１つのデータに対してだけ
勾配降下法を行いたい場合には

95
00:05:47,375 --> 00:05:49,280
このようにすることができます

96
00:05:49,280 --> 00:05:52,640
この式を使ってdz を計算して

97
00:05:52,640 --> 00:05:56,707
これらのdw1 dw2 db の式を使って

98
00:05:56,707 --> 00:06:01,170
この更新を行います

99
00:06:01,170 --> 00:06:04,538
w1はw1ひく

100
00:06:04,538 --> 00:06:06,575
学習率のαかけるdw1です

101
00:06:06,575 --> 00:06:09,245
w2 も同様に更新できます

102
00:06:09,245 --> 00:06:14,170
b:=b-αdbです

103
00:06:14,170 --> 00:06:18,860
これが１つのデータに対しての
勾配降下法を行う１つの段階です

104
00:06:18,860 --> 00:06:22,130
１つの教師データの時に

105
00:06:22,130 --> 00:06:27,200
どうやって微分を計算し
ロジステック回帰の勾配降下法を実装するかを見てきました

106
00:06:27,200 --> 00:06:28,987
ロジステック回帰のモデルを学習させるには

107
00:06:28,987 --> 00:06:34,700
１つの教師データではなく
m個の教師データを与えるでしょう

108
00:06:34,700 --> 00:06:36,120
見ての通り、ここある。

109
00:06:36,120 --> 00:06:39,350
１つの教師データだけからではなく

110
00:06:39,350 --> 00:06:40,760
教師データ全体から
学習させるように

111
00:06:40,760 --> 00:06:42,400
適用する方法を
見ていきましょう