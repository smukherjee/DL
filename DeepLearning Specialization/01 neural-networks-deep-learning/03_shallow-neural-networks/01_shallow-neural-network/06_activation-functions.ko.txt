여러분이 신경망을 부팅하는 경우에, 선택할 수 있는 것중 한가지는 숨겨진 레이어에서 어떤 activation 함수를 쓸지와 또한, 신경망의 결과값 유닛을 어떻게 할지에 대한 선택입니다. 현재까지는 시그모이드 activation 함수를 써왔는데요, 하지만 다른 선택이 더 잘 작동하는 경우가 간혹 있습니다. 그 옵션들을 한번 보겠습니다. 신경망 네트워크 전 방향전파 스텝 에서 여기 2가지 단계가 있는데요, 여기서는 시그모이드 함수를 씁니다. 이것은 activation 함수라고 하는데요 여기는 낯이 익은 시그모이드 함수인데요 a= 1 나누기 1 더하기 e의 -2승입니다. 더 일반적인 케이스 같은 경우엔, g의 z인 함수를 가질 수 있는데요, 여기 적을 텐데요, G는 비선형 함수로 시그모이드 함수가 아닐 수 있습니다. 예를 들어, 시그모이드 함수는 0 과 1사이 값을 갖는데요, 시그모이드 함수보다 거의 항상 잘 작동하는 activation 함수는 tanh h 함수입니다. 또는 쌍곡선의 탄젠트 함수인데요, 이것은 z이고, 이 건은 a, 그리고 이것은 a=tanh 의 z입니다. 그리고 이것은 -1과 1사이으 값을 갖습니다. tanh h의 공식은 e의 z 승 빼기 e의 -v승 나누기 그 둘의 합인데요, 수학적으로 이동한 시그모이드 함수 버전입니다. 이것과 같은 시그모이드 함수인데요. 이것이 이렇게 이동된 것입니다. 그러므로 이제는 0, 0을 지나도록 rescale되는 것인데요, 그러므로 이제 -1에서 1의 값을 갖게 됩니다. 그리고 이제 여러분이 숨겨진 유닛에 대해 g(z) 함수를 tan h (z)와 일치하게 해주면, 이것이 시그모이드 함수보다 거의 항상 더 잘 작동합니다. 그 이유는 숨겨진 레이어에서 나오는 activation의 평균값이, -1에서 1사이의 범위 경우, 0 이라는 평균값을 갖는데 더 가깝기 때문에, 러닝 알고리즘을 간혹 트레이닝 하는 경우, 데이터를 중앙화시켜 데이터가 0의 평균값을 갖도록 할 것입니다. 시그모이드 함수대한 tan h 함수를 이용해서 말이죠. 데이터를 중앙화시키는 효과가 있다고 보시면 됩니다. 그 평균값이 0.5와 같은 값이 아닌 0과 근접한 값이 될 수 있게 말이죠. 이렇게 진행하면, 다음 층에서의 러닝을 조금 더 수월하게 만들어줍니다. 이것과 관련된 내용은 두번째 코스에서 최적화 알고리즘을 이야기하면서 더 다루도록 하겠습니다. 한가지 기억하실 것은, 저는 이제 시그모이드 함수는 거의 쓰지 않습니다. tan h 함수가 거의 항상 더 우월합니다. 한가지 예외의 경우는 결과값 층의 경우입니다. 만약 Y가 0이거나 1인 경우, y hat 같은 경우, 출력하고 싶은 값인데요, 이 값이 0에서 1입니다. -1과 1 사이값 대신에 말이죠. 그렇기 때문에 한가지 예외 경우는, 시그모이드 함수를 이진분류법을 쓰는 경우에 사용할 것입니다. 이 경우, 시그모이드 함수는 결과값 층에 대해 쓸 것입니다. 그러므로 여기 g(z2)는 시그마의 z2입니다. 여기서 example을 볼수 있는 것은, tan h activation 함수가 hidden layer에 대해 있을 수 있고, 그리고 결과값 층에 대해서 시그모이드 함수가 있을 수 있습니다. 그렇기 때문에, 다른 층들에 대해서, activation 함수가 다를 수 있습니다. 그리고 가끔씩 다른 층에 대해서 activation 함수가 다르다는 점을 여기 대괄호 위첨자를 사용해 나타낼 수 있습니다. G의 대괄호 1이 G의 대괄호 2와 다르다는 것을 그리고, 대괄호 1 위첨자는 여기 층을 나타내며, 위첨자 대괄호 2는 결과값 층을 뜻합니다. 자 그러면 시그모이드 함수와 tan h 함수의 단점 중에 하나는 만약 z가 매우 크거나 매우 작은 값인 경우, derivative의 기울기나, 여기 이 함수의 기울기가 매우 작아집니다. 그렇기 때문에 z는 매우 크거나 매우 작습니다. 기울기는 그러면 거의 0에 가까워 질 수 있는데요, 이렇게 되면 기울기 강하가 늦춰질 수 있습니다. 그렇기 때문에 머신 러닝에서 인기있는 또 다른 방법은 rectified linear unit라고 하는 것인데요. ReLu 함수는 이렇게 생겼습니다. 그리고 그 공식은 a=max (0, 1) 입니다. derivative는 1인데요, Z가 양수이고, z 값이 음수인 경우엔, 기울기는 0이 됩니다. 이것을 기술적인 접근을 하는 경우, z가 0인 경우의 derivative는 잘 정의되어 있지 않습니다. 여러분이 컨퓨터에서 도입하는 경우, 자주 000000000 값을 갖는데요, 값이 매우 작기 때문에, 실제로는 걱정하지는 않을 것입니다. 만약 z의 값이 0인 경우에 derivative가 0 도는 1이라고 가정해도 되는데요, differentiate 할 수 없다고 가정하면 됩니다. activation 함수를 고르는데 있어서의 몇개의 경험에 의거한 규칙인데요, 만약 이진분류법을 이용해서 그 결과값이 0 과 1의 값을 갖을 때, 시그모이드 함수의 경우, 결과값에 대해서 자연스러운데요, ReLu 또는 rectified linear 유닛과 같은 경우, activation 함수의 기본값인 경우가 많은데요, 만약 여러분이 숨겨진 레이어로 무엇을 쓸지 모르시는 경우엔, 그냥 ReLu 함수를 쓸 것입니다. 요즘 대부분의 사람들이 사용하는 함수인데요, 물론 가끔씩 사람들이 tanh 함수를 사용하긴 합니다. ReLu의 단점중 하나는, derivative의 값이 0이라는 것입니다. z의 값이 마이너스 인 경우에 말이죠, 실제로는 괜찮습니다만, 또 다른 ReLu 함수가 있는데요 Leaky ReLu하고 하는
것이 있습니다. 공식은 다음 슬라이드에서 드리겠습니다. z가 마이너스인 경우에 derivative가 0 대신에, 이렇게 생긴 슬로프 값을 갖습니다. 이것은 아까 얘기했던 것처럼 Leaky ReLu라고 하는데요, 이 함수가 보통 ReLu activation 함수보다는 잘 작동하는데요, 실제로는 잘 사용되니는
않습니다. 둘다 사용해도 괜찮은데요, 만약 여러분이 줄중에 한개를 골라야 한다면 저 같은 경우엔 그냥 ReLu를 고르겠습니다. 그리고 두개 모두, ReLu와 Leaky ReLu의 장점은, 많은 z값에 대해서, activation 함수의 derivative가 즉, activation 함수의 기울기가 0가 매우 다릅니다. 실제로는 그러므로 ReLu activation 함수를 신경망에서 이용하는 것이 tan h 함수를 이용하는 것보다 훨씬 더 빠를 것입니다. 또는 시그모이드 activation 함수보다도 말이죠. 주된 이유는, 이렇게 함수의 기울기가 0으로 근접하는 효과가 덜하기 때문입니다. 이런 효과는 러닝의 속도를 느리게 하는데요, 저는 아는데요, 여기 z의 반 정도 되는 범위는 ReLu 의 기울기가 0인 것을 말이죠. 하지만 실제로는 숨겨진 유닛이 충분하게 Z가 0보다 더 큰 값을 갖는 경우인데요, 그렇기 때문에 대부분의 트레이닝 예시에 대해서
러닝의 속도가 계속 빠를 것입니다. 그러면 이제 activation 함수들의 장점과 단점에 대해서 복습해보겠습니다. 여기는 시그모이드 activation 함수인데요, 이것은 이진분류법에서 결과값 층 관련한 것 외에는 절대로 사용하지 말라고 말씀드리겠습니다. 거의 전혀 사용하지 마십시요. 제가 사용하지 않는 이유는, tan h 함수가 거의 항상 우월하기 때문입니다. 이것이 tan h 함수인데요, 그리고 기본값 함수, 사장 흔하게 사용되는 activation 함수는 ReLu 함수인데요, 여기 보이는 것인데요, 여러분이 어떤 것을 써야할지 잘 모르겠는 경우, 
이것을 쓰십시요, 그리고 또 Leaky ReLu를 언제든지 사용하세요, 여기가 0.01 z, z 가 될수도 있겠죠. 그러면 a 는 0.01 곱하기 z 와 z에서의 최대값입니다. 그런 값은 여기 함수에서 구부러지게 하는데요, 그러면 여러분은 여기 상수의 값이 왜 0.01인지 의문을 가질 수 있는데요, 이런 경우, 여러분은 또 하나의 러닝 알고리즘 파라미터를 만들 수도 있는데요, 어떤 사람들은 그렇게 하는 것이 더 잘 작동한다고도 
이야기 하는데요, 하지만 대부분의 사람들은 새로 파라미터를 만들지 않습니다. 여러분의 어플에서 만약 여러분이 직접 시도해보시겠다고 하면 얼마든지 그렇게 하셔도 좋습니다. 어떻게 작동하는 한번 확인해보려면 말이죠 좋은 결과를 주면, 계속 사용할 수도 있겠죠. 이런 내용들이 바라건대, 여러분이 신경망에서 사용할 수 있는 activation 함수의 초이스들에 대한 이해를 돕는데 일조했으면 좋겠습니다. 여러분이 딥러닝에서 볼 수 있는 테마 중 한가지는 바로 많은 선택권이 있다는 것인데요, 신경망을 어떻게 만드는지에 대해서 말이죠, 숨겨진 유닛의 개수를 어떻게 설정할지에서부터, 어떤 activation 함수를 기용할지와, 어떻게 초기화를 시킬지 등의 여러가지 결정을 필요로 합니다. 그리고 이런 선택권들이 있기 때문에 또 동시에 여러분의 문제에서 어떤 것들이 가장 잘 작동할지에 대한 좋은 가이드라인을 찾기가 자주 어렵습니다. 그러므로 해당 코스를 통해서 저는 여러분에게 제가 이 분야에서 보는 것들과 어떤 것이 더 수요가 있고, 또는 인기가 없는지 어플리케이션에 따라 확인하고, 사실 미리 어떤 것이 가장 잘 작동할지 여부를 아는 것은 굉장히 어려운 일이죠. 그러면 가장 흔하지만서도 좋은 방법은 이러한 activation 함수중에서 어떤 것이 가장 좋은지 확실하지 않으면, 
바로 다 시도해 보는 것입니다. 그러고나서 holdout validation set를 기준으로 평가하거나, devlopment set를 기준으로 평가합니다. 이 부분은 나중에 다루겠습니다. 어떤 것이 더 잘 작동하는지 알아내고, 직접 시도하는 것입니다. 이런 것들을 본인 어플에서 직접 시도해보면서 신경망의 구조를 증명하는 것이 미래에도 좋을 것입니다. 특이한 성향도 문제고, 알고리즘의 진화에 맞게 적용하는 것이 중요합니다 제가 만약에 여러분에게 ReLu activation 함수를 무조건 쓰고 다른 것들은 절대 사용하지 말라고 하면 여러분이 작업하고 있는 문제에 적용할 수도, 또는 전혀 적용이 안될 수 있습니다. 근접한 미래에서는 먼 미래에서든 말이죠, 그럼 이 내용이 activation 함수의 선택에 대한 내용이였는데요, 여러분은 가장 인기가 있는 activation 함수들을 봤는데요, 또 다른 질문을 할 수 있는 부분이 있는데요 왜 애초에 activation 함수를 사용해야 하는 것 일까요? 이런 내용에 대해서 다음 비디오에서 이야기 하겠습니다. 그럼 다음 비디오에서는 여러분이 왜 신경망에서는 어떤 유형의 비선형 activation 함수가 필요한지 보게될 것입니다.