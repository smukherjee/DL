前のビデオでは、最急降下法の数学的定義をしました。 予測値を計算するために
ベクトル化を使う方法を見てきました トレーニングデータ全体に対して
１度で小文字のa を計算する方法です このビデオでは
トレーニングデータ全体に対して 勾配計算の実行を
ベクトル化する方法を見ます 今回も１度に実行する方法です このビデオの終わりには これまでのことをまとめて
ロジステック回帰を 非常に効率的な実行ができるように
実装を行えるようになります 勾配の計算では
最初のデータに対して dz(1)を行うのを
覚えているでしょうか a(1)-y(1)です
次に dz(2)=a(2)-y(2)です m 個のトレーニングデータ全体に
続いていきます これの替わりに
新しい変数dZを定義します dZはdz(1)、dz(2)、dz(m)となります ここでもdzの変数は
水平に重ねられます ここでは1とmの行列になり
別の言い方ではm要素の列ベクトルです 前回のスライドから思い出してください 既に見たように大文字のA を計算するには
a(1)からa(m)とします 大文字のY は
y(1)からy(m)と定義します そして水平方向に重ねます これらの定義に基づいて 既に分かるかもしれませんが dz=A-Yですが
これはa(1)-y(1)を 最初の要素として a(2)-y(2)があり
続いていきます １つ目のa(1)-y(1)は
dz(1)の定義と同じで ２つ目のa(2)-y(2)は
dz(2)の定義と同じで以後続きます これを１行のコードで これらのすべてを
一度に計算できます 前回の実装では for loop を
１つ排除しましたが トレーニングデータに対して繰り返す
２つ目のfor loop がまだあります dwを０のベクトルで
初期化しますが トレーニングデータに対して繰り返して dw+=x(1)かけるdz(1)を 最初のトレーニングデータに実行し
dw+=x(2)かけるdz(2)と続けます これをm 回繰り返した後に
dm/=mを実行します dbに対しても同様で０で初期化し
db+=dz(1)をし db+=dz(2)をし
dz(m)まで続けます db/=m へと続きます
これが前回の実装方法です １つのfor loop は
すでに排除したので dwはベクトルになっていますが
元は個別にdw(1)、dw(2)のように 更新してました これはもう排除しましたが トレーニングセットのm 個のデータに対して
for loop が残っています ではこれらの命令を
ベクトル化しましょう こうやります dbのベクトル化の実装は
基本的にはdzを まず加算して
mで割ります dbは1/mです シグマでi=1からmまでで
dz(i)をし この列ベクトルの中のdzは
Python では こう実装できます 1/mかけるnp.sum(dz)で できます この変数を使って
この変数に対して np.sum関数を呼ぶだけで
dbが求められます dwはどうでしょうか 正しいやり方を確認できるように
正しい式を書き出します dwは1/mかける 行列Xかけるdzの
転置になります なぜこうなるでしょうか これは1/mと行列Xの x(1)からx(m)をこのように
列で重ねたようになり dzの転置はdz(1)からdz(m)になります この行列とベクトルを掛けると 1/mかける x(1)dz(1)から続けて
x(m)dz(m)を出したものです これはn×1のベクトルで
dwとなります ご存知のようにdwは x(i)dz(i)を合計したもので この行列ベクトルの積が行っているものです １行のコードで
dwが計算できます 微分の計算をベクトル化で実装すると
これだけです この式をdb で実装し この式でdw を実装します
トレーニングセットに対するfor loop が無いことに注目してください パラメーターを更新する値を
計算できます これまでの全体を纏めて
ロジステック回帰の実装方法を見ましょう これが元々で かなり非効率で
ベクトル化なしの実装です 前のビデオで最初に行ったのは
このfor loop を排除しました dw1、dw2と続けるループを やめて この部分をベクトル値のdwと置き換え
dw+=x(i)とし dz(i)と掛けました これから実施するのは
下のfor loop を排除するだけでなく このfor loop も排除します 次のようにやります 前のスライドにあるように 大文字のZが Z=w(転置)かけるX+bで
コードではnp.dot(w.T,X)+bと書きます Aは大文字Zのシグモイド関数です この２つの式のすべてのiに対して
計算ができました さらに前のスライドでは dz=A-Yを計算しました この行の全てのiに対する
計算を行います さらにdwは1/mかけるXかけるdzの転置です db=1/mとnp.sum(dz)と なります これで誤差順伝播法と誤差逆伝播法を
行ったことになります for loop 無しで
全てのトレーニングデータm に対して 予測と微分を
計算したことになります 勾配降下法の更新はこうなります w=w引く学習率かける
上で計算したdwになり bはb引く学習率かけるdbになります コロンを入れることで
代入であることを表します 表記方法について
一貫して記載していませんでしたが ここまでの作業によって ロジステック回帰での
１回分の勾配降下の更新を実装できました これまでの話では
できる限りのfor loop を 排除するように言ってきましたが 複数回分の勾配降下の 実行をするには
for loop で回数分を繰り返す必要があります 千回の勾配降下を
実行したい場合には 繰り返しの数に基づいて
for loop が必要になります ここのように一番外側の
for loop で このfor loop を排除する方法は
無いように思います ただfor loop を使うことなく 最低でも１回分の勾配降下法を
実行できるのは信じられないほど良いことだと思います さてこれで終わりです
これでロジステック回帰の かなりベクトル化されて
非常に効率化された実装ができました 次のビデオでは
あと１つだけ詳細を話します ここの記述で少しほのめかしましたが
ブロードキャストのことです ブロードキャストは
Python と numpy を使うと コードの一部を
はるかに効率的にすることができるテクニックです 次のビデオで
ブロードキャストの詳細を見ましょう