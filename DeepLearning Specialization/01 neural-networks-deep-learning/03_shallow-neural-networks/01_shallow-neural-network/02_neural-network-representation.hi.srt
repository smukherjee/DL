1
00:00:00,000 --> 00:00:03,116
आप देखें मुझे बनाते हुए कुछ चित्र न्यूरल नेटवर्क के.

2
00:00:03,116 --> 00:00:05,712
इस वीडियो में, हम बात करेंगे कि वास्तव में 
उन चित्रों का क्या मतलब है.

3
00:00:05,712 --> 00:00:06,728
दूसरे शब्दों में,

4
00:00:06,728 --> 00:00:11,235
वास्तव में क्या वे न्यूरल नेटवर्क्स 
जो हम बना रहे हैं रेप्रेज़ेंट करते हैं.

5
00:00:11,235 --> 00:00:15,014
और हम शुरू करेंगे फ़ोकस करते हुए 
न्यूरल नेटवर्क्स पर जिनमें है

6
00:00:15,014 --> 00:00:17,290
जिसे कहते हैं एक अकेली हिडन लेयर.

7
00:00:17,290 --> 00:00:19,667
यहाँ है एक पिक्चर एक न्यूरल नेटवर्क की.

8
00:00:19,667 --> 00:00:22,986
चलो देते हैं इस पिक्चर के 
विभिन्न हिस्सों को कुछ नाम.

9
00:00:22,986 --> 00:00:27,447
हमारे पास हैं इनपुट फ़ीचर्ज़ 
x1, x2, x3 रखे हुए लम्ब रूप से.

10
00:00:27,447 --> 00:00:30,694
और इसे कहते हैं इनपुट लेयर न्यूरल नेटवर्क की.

11
00:00:30,694 --> 00:00:35,764
तो शायद आश्चर्य नहीं है, 
इसमें शामिल हैं इन्पुट्स न्यूरल नेटवर्क की.

12
00:00:35,764 --> 00:00:37,990
फिर वहाँ वृत्तों की एक और लेयर है.

13
00:00:37,990 --> 00:00:41,663
और इसे कहते हैं एक हिडन लेयर न्यूरल नेटवर्क की.

14
00:00:41,663 --> 00:00:45,414
मैं वापिस आऊँगा कुछ देर में बताने के लिए कि 
हिडन शब्द का क्या मतलब है.

15
00:00:45,414 --> 00:00:49,509
लेकिन अंतिम लेयर यहाँ बनी है, 
इस केस में, केवल एक नोड से.

16
00:00:49,509 --> 00:00:53,894
और इस एक-नोड की लेयर को कहते हैं 
आउट्पुट लेयर, और यह उत्तरदायी है

17
00:00:53,894 --> 00:00:56,059
उत्पन्न करने के लिए प्रिडिक्ट की हुई वैल्यू y हैट.

18
00:00:56,059 --> 00:00:59,932
एक न्यूरल नेटवर्क में, जो आप ट्रेन करते हैं 
सूपर्वायज़्ड लर्निंग से,

19
00:00:59,932 --> 00:01:05,237
ट्रेनिंग सेट में होती हैं वैल्यूज़ इनपुट x की तथा 
टारगेट आउट्पुट y की.

20
00:01:05,237 --> 00:01:09,239
तो टर्म हिडन लेयर बताता है तथ्य कि ट्रेनिंग सेट में,

21
00:01:09,239 --> 00:01:12,702
सही वैल्यू इस नम्बर की मध्य लेयर में दिखती नहीं है.

22
00:01:12,702 --> 00:01:15,185
मतलब कि आप नहीं देखते हैं कि क्या है ट्रेनिंग सेट में.

23
00:01:15,185 --> 00:01:16,640
आप देखते हैं इन्पुट्स क्या हैं.

24
00:01:16,640 --> 00:01:18,094
आप देखते हैं क्या आउट्पुट होनी चाहिए.

25
00:01:18,094 --> 00:01:20,992
लेकिन हिडन लेयर में चीज़ें नहीं दिखती हैं 
ट्रेनिंग सेट में.

26
00:01:20,992 --> 00:01:25,542
तो वह एक प्रकार से समझाता है 
नाम हिडन लेयर वहाँ क्योंकि आप

27
00:01:25,542 --> 00:01:28,088
नहीं देखते हैं इसे ट्रेनिंग सेट में.

28
00:01:28,088 --> 00:01:30,262
चलो देखते हैं नोटेशन को.

29
00:01:30,262 --> 00:01:35,542
जहाँ पहले हम इस्तेमाल कर रहे थे वेक्टर X
 डिनोट करने के लिए इनपुट फ़ीचर्ज़ और

30
00:01:35,542 --> 00:01:37,226
वैकल्पिक नोटेशन

31
00:01:37,226 --> 00:01:41,987
इनपुट फ़ीचर्ज़ की वैल्यूज़ के लिए होगी 
A सूपरस्क्रिप्ट वर्ग कोष्ठक 0.

32
00:01:41,987 --> 00:01:44,934
और टर्म A का मतलब ऐक्टिवेशन भी है, और

33
00:01:44,934 --> 00:01:47,733
और यह संदर्भित करती है वैल्यूज़ को 
जो विभिन्न लेयर्स

34
00:01:47,733 --> 00:01:51,651
न्यूरल नेटवर्क की भेज रही हैं आगे की लेयर्स को.

35
00:01:51,651 --> 00:01:55,998
तो इनपुट लेयर भेजती है वैल्यू x हिडन लेयर को, तो

36
00:01:55,998 --> 00:02:01,110
हम कहेंगे उसे ऐक्टिवेशन 
इनपुट लेयर A सूपरस्क्रिप्ट 0 का.

37
00:02:01,110 --> 00:02:05,990
अगली लेयर, जो हिडन लेयर है वह आगे 
उत्पन्न करेगी कुछ सेट ऐक्टिवेशन्स के,

38
00:02:05,990 --> 00:02:09,601
मैं लिखूँगा A सुपरस्क्रिप्ट वर्ग कोष्ठक 1.

39
00:02:09,601 --> 00:02:13,306
तो विशेष रूप से, यह पहली यूनिट 
या यह पहला नोड,

40
00:02:13,306 --> 00:02:17,824
हम लेंगे एक वैल्यू A सुपरस्क्रिप्ट 
वर्ग कोष्ठक 1 सबस्क्रिप्ट 1.

41
00:02:17,824 --> 00:02:20,735
यह दूसरी हम लेंगे A वैल्यू.

42
00:02:20,735 --> 00:02:23,311
अब हम हैं सबस्क्रिप्ट 2 पर और इसी प्रकार आगे.

43
00:02:23,311 --> 00:02:26,488
और इसलिए, A सुपरस्क्रिप्ट वर्ग कोष्ठक 1,

44
00:02:26,488 --> 00:02:30,120
तो यह है एक चार डिमेन्शन्स का वेक्टर या 
आप चाहते हैं

45
00:02:30,120 --> 00:02:34,707
पाइथान में क्योंकि 4 .1 मेट्रिक्स 
आम वेक्टर की जो इस तरह दिखती है.

46
00:02:34,707 --> 00:02:39,205
और यह है चार डिमेन्शनल वेक्टर क्योंकि 
इस केस में हमारे पास हैं चार नोड्ज़, या

47
00:02:39,205 --> 00:02:42,684
चार यूनिट्स, या चार हिडन यूनिट्स 
इस हिडन लेयर में.

48
00:02:42,684 --> 00:02:46,302
और अंत में, खुली लेयर उत्पन्न करती है 
कुछ वैल्यू A2,

49
00:02:46,302 --> 00:02:47,948
है सिर्फ़ एक रियल नम्बर.

50
00:02:47,948 --> 00:02:51,658
तो y हैट लेगा वैल्यू A2 की.

51
00:02:51,658 --> 00:02:55,885
तो इस प्रकार यह जाता है रेग्रेशन में हमारे पास है 
y हैट बराबर A और

52
00:02:55,885 --> 00:03:00,349
लॉजिस्टिक रेग्रेशन जिसमें हमारे पास थी 
केवल वह एक आउट्पुट लेयर, तो

53
00:03:00,349 --> 00:03:03,583
तो हम उपयोग कर रहे थे 
सुपरस्क्रिप्ट वर्ग कोष्ठक का.

54
00:03:03,583 --> 00:03:07,916
लेकिन हमारे नए नेटवर्क में, 
हम इस्तेमाल करेंगे सूपरस्क्रिप्ट वर्ग

55
00:03:07,916 --> 00:03:11,653
कोष्ठक का स्पष्ट रूप से बताने के लिए कि 
कौन सी लेयर से यह आया.

56
00:03:11,653 --> 00:03:15,468
एक मज़ेदार बात नोटेशन कन्वेन्शन की 
न्यूरल नेटवर्क्स में

57
00:03:15,468 --> 00:03:20,194
है कि यह नेटवर्क जो आपने देखा है यहाँ 
कहलाता है एक दो लेयर न्यूरल नेटवर्क.

58
00:03:20,194 --> 00:03:23,541
और कारण है कि जब हम गिनते हैं 
लेयर्स न्यूरल नेटवर्क्स में,

59
00:03:23,541 --> 00:03:25,321
हम इनपुट लेयर नहीं गिनते.

60
00:03:25,321 --> 00:03:28,858
एक हिडन लेयर है लेयर एक, 
और एक आउट्पुट लेयर है लेयर दो.

61
00:03:28,858 --> 00:03:32,661
हमारे नोटेशन कन्वेन्शन में, हम कह रहे हैं 
इनपुट लेयर को लेयर ज़ीरो, तो

62
00:03:32,661 --> 00:03:35,887
तकनीकी तौर पर हो सकता है कि इस 
न्यूरल नेटवर्क में तीन लेयर्स हैं,

63
00:03:35,887 --> 00:03:39,649
क्योंकि वहां है इनपुट लेयर, हिडन लेयर 
और आउट्पुट लेयर.

64
00:03:39,649 --> 00:03:43,357
लेकिन कन्वेन्शनल इस्तेमाल में, 
यदि आप पढ़ते हैं रीसर्च पेपर और अन्य कहीं

65
00:03:43,357 --> 00:03:47,489
कोर्स में, आप देखेंगे लोग संदर्भित करते हैं इस 
विशेष न्यूरल नेटवर्क को एक दो लेयर का

66
00:03:47,489 --> 00:03:51,602
न्यूरल नेटवर्क, क्योंकि हम नहीं गिनते इनपुट लेयर 
एक औपचारिक लेयर.

67
00:03:51,602 --> 00:03:55,912
अंत में, कुछ जिस पर हम बाद में जाएँगे है 
कि हिडन लेयर और

68
00:03:55,912 --> 00:03:59,670
आउट्पुट लेयर से सम्बंधित होंगे उनके पेरामीटर्स.

69
00:03:59,670 --> 00:04:03,447
तो हिडन लेयर के होंगे उसके साथ 
सम्बंधित पेरामिटरज़ w और b.

70
00:04:03,447 --> 00:04:08,218
और मैं लिखूँगा A सुपरस्क्रिप्ट वर्ग कोष्ठक 1 
बताने के लिए कि ये

71
00:04:08,218 --> 00:04:12,395
हैं पेरामीटर्स सम्बंधित लेयर एक साथ, 
हिडन लेयर के साथ.

72
00:04:12,395 --> 00:04:15,416
हम देखेंगे बाद में कि w होगा एक 
4 बाई 3 मेट्रिक्स और

73
00:04:15,416 --> 00:04:18,016
b होगा एक 4 बाई 1 वेक्टर इस उदाहरण में.

74
00:04:18,016 --> 00:04:21,754
जहाँ पहला निर्देशांक चार आता है 
क्योंकि हमारे पास है

75
00:04:21,754 --> 00:04:24,503
चार नोड्ज़ हिडन लेयर में और

76
00:04:24,503 --> 00:04:28,120
तीन हैं क्योंकि हमारे पास हैं तीन इनपुट फ़ीचर्ज़.

77
00:04:28,120 --> 00:04:31,980
हम बाद में बात करेंगे डिमेन्शन्स के बारे में 
इन मेट्रिसीज़ की.

78
00:04:31,980 --> 00:04:33,844
और उस समय यह शायद ज़्यादा समझ आएगा.

79
00:04:33,844 --> 00:04:37,813
लेकिन कुछ आउट्पुट लेयर में जो 
सम्बंधित है इससे, पेरामिटर w

80
00:04:37,813 --> 00:04:41,663
सुपरस्क्रिप्ट वर्ग कोष्ठक 2 और 
b को भी सुपरस्क्रिप्ट वर्ग कोष्ठक 2.

81
00:04:41,663 --> 00:04:45,747
और ऐसा होता है डिमेन्शनस 
इनकी है 1 बाई 4 और 1 बाई 1.

82
00:04:45,747 --> 00:04:49,297
और ये 1 बाई 4 हैं क्योंकि हिडन लेयर में हैं 
चार हिडन यूनिट्स,

83
00:04:49,297 --> 00:04:51,177
आउट्पुट लेयर में केवल एक यूनिट.

84
00:04:51,177 --> 00:04:56,378
लेकिन हम बात करेंगे इसं मेट्रिसीज़ और वेक्टर्स के 
डिमेन्शन्स की एक बाद के वीडियों में.

85
00:04:56,378 --> 00:04:59,839
तो आपने अभी देखा कैसा एक दो-लेयर का
 न्यूरल नेटवर्क दिखता है.

86
00:04:59,839 --> 00:05:03,108
यह है एक न्यूरल नेटवर्क एक हिडन लेयर का.

87
00:05:03,108 --> 00:05:04,260
अगले वीडियो में,

88
00:05:04,260 --> 00:05:08,513
चलो देखेंगे गहराई से कि यह न्यूरल नेटवर्क 
क्या कम्प्यूट कर रहा है.

89
00:05:08,513 --> 00:05:11,223
इस प्रकार यह न्यूरल नेटवर्क, इनपुट करता है x 
और

90
00:05:11,223 --> 00:05:14,169
जाता है पूरा आगे कम्प्यूट करते हुए इसकी 
आउट्पुट y हैट.