lojistik regresyon modelini gördünüz. Tek eğitim örneğini ne kadar iyi yaptığını ölçen kayıp fonksiyonunu gördünüz. Ayrıca w ve b parametrelerinin eğitim seti üzerinde ne kadar doğru olduğunu ölçen maliyet fonksiyonunu gördünüz. Şimdi dereceli alçalmayı kullanarak nasıl eğitim setinde ki w ve b parametrelerini eğitileceğini ve öğretileceğini görelim. Tekrarlama amaçlı,lojistik regresyon algoritmasına benzeri bir algoritma. İkinci satırda w ve b parametresine bağlı maliyet fonksiyonumuz var. Ve ortalama olarak tanımlandı. Bu 1 den m e kadar kayıp fonksiyonun toplamı. Ve kayıp fonksiyonu senin algoritmanın çıktısı y şapka i nin her eğitim örnekleri üzerinde nasıl yığıldığını ölçer her eğitim örnekleri üzerindeki y(i) etiketi ile karşılaştırır. Ve tam formül sağa doğru genişletildi. Maliyet fonksiyonu w ve b parametrelerinin eğitim setindeki ne kadar iyi olduğunu ölçüyor. w ve b parametrelerini öğrenmek için doğal olarak maliyet fonksiyonunu J(w,b) mümkün olduğunca küçük yapan w ve b yi bulmak istiyoruz. Bu da dereceli alçalmanın bir çizimi. Bu grafikte yatay eksen uzaysal parametreleri w ve b yi temsil eder.. Pratikte,w daha büyük boyutlu olabilir,fakat çizimin amacı için w yü ve b yi gerçek sayı olarak tanımlayalım. Maliyet fonksiyonu J(w,b) w ve b yatay eksenlerinin üstündeki bazı yüzeylerdir. Yüzeyin yüksekliği J(w,b) nin kesin noktadaki değerini temsil eder. Ve bizim yapmak istediğimiz şey w ve b nin maliyet fonksiyonunu minimum yaptığı değerleri bulmak. Bu J maliyet fonksiyonunu dışbükey yapar. Bu sadece bir tek büyük kase, bu bir dışbükey ve Böyle bir dışbükey olmayan ve birçok farklı yerel optimumu olan bir fonksiyon düşünelim. Aslında bizim burada dışbükey olarak tanımlanan maliyet fonksiyonumuz J(w,b) neden özel maliyet fonksiyonu kullandığımızın büyük nedenlerinden biri lojistik regresyon için. parametreler için iyi bir değer bulmak için bizim yapacağımız şey w ve b bazı başlangıç sayılarla başlatmak, kırmızı nokta olarak gösterebiliriz. lojistik regresyon için, neredeyse bütün başlangıç metotları çalışır, genellikle başlangıç değerleri 0'dır. Rastgele başlangıçta çalışır,ama Genelde insanlar lojistik regresyon için kulanılmaz. Bu fonksiyon dışbükey olduğu için,nereden başladığın önemli değil ya aynı değeri almalısın ya da kabaca aynı noktaları. Ve dereceli alçalma başlangıç noktasından başlayarak dik yerden aşağıya doğru bir adım ilerler. Dereceli alçalmanın bir adım sonrasında, orada bitirebilirsiniz çünkü en dik inişi bir adımda yapmaya çalışıyor ya da aşağıya mümkün olduğunca hızlı inmeye çalışıyor. bu dereceli alçalmanın bir yinelemesi. Ve iki yineleme sonra belki orada olabilirsin üç yineleme ve böyle gider. Sanırım şimdi bu çizimin arkasında gizlenmiş olan şey sonunda ,umarım global optimuma yakınsayacaksınız(converge) ya da global optimuma yakın bir değer alacaksınız. Bu resim dereceli alçalmayı gösteriyor. Birazcık daha detaylı yazalım. resim için, minimize etmek istediğimiz böyle gözüken bir fonksiyon olsun J(w). Daha kolay çizmek için b yi şimdilik yok sayalım. Daha yüksek boyutlu çizim yerine bir boyutlu bir çizim yapalım. Dereceli alçaltma şunu yapar, Aynı güncelleştirmeleri tekrar tekrar uygulayacağız. w değerini alıp güncelleştireceğiz, iki nokta eşittir güncellenmiş w yu temsil eder. w yü w eksi alfa çarpı olarak belirleriz. J(w) nin w ya göre türevi. Bunu algoritma yakınsanana(converge) kadar tekrar edeceğiz. bazı formüldeki noktalar,buradaki alfa öğrenme katsayımız ve dereceli alçaltmada ne kadar büyük adım atılacağını kontrol eder. Öğrenme katsayısı seçme yollarından daha sonra bahsedeceğiz. 2. olarak,Bu bir türevdir. Bu basitçe güncellemedir ve w parametresini yapmak istediğimize çevirir. dereceli alçaltmanın uygulamasını yazmaya başladığımızda Değişken adlarında şöyle bir düzen kullanacağız dw türevi temsil edecek. kod yazdığınız şu tip şeyler yazacaksınız w iki nokta eşittir w eksi alfa çarpı dw. ve biz dw yi türev olarak kullanacağız. Şimdi biz dereceli alçalmanın doğru yapıldığından emin olalım. w burada olsun. maliyet fonksiyonu bu noktada olsun. türevin tanımı hatırlayalım fonksiyonun noktadaki eğimidir. Fonksiyonun eğimi gerçekte yükseklik bölü genişliktir buradaki üçgen J(w) nin bu noktadaki tanjantıdır. buradaki türev pozitifdir. w eksi öğrenme katsayısı çarpı türev olarak güncellenir. türev pozitif ve w da azalmayla sonuçlanır böylece sola bir adım atılır. Eğer w yu böyle büyük başlarsanız dereceli alçalma sizin algoritmanızın yavaş bir şekilde parametrenizi azaltır. Diğer bir örnek olarak,w burada olsaydı bu noktada ki türevin eğimi eksi olacak ve böylece dereceli alçaltma alfa çarpı negatif bir sayıdan çıkaracak w nün yavaşça artacak ve w büyüyerek başarılı bir uygulamayla sonuçlanacak. Umarım ki ,sizin sağda ya da solda başlamanıza bakmadan dereceli alçaltma sizi global minimuma doğru götürecek. Eğer siz türeve ya da yüksek matematiğe (calculus) dj(w)/dw sembolüne aşina değilseniz çok fazla bu konuda endişelenmeyin. Diğer videoda türev hakkında daha fazla şeylerden bahsedeceğiz. Eğer sizin derin yüksek matematik (calculus) bilginiz varsa yapay sinir ağlarının nasıl çalıştığı hakkında daha dip bir seziniz vardır. Fakat yüksek matematiğe (calculus) aşinalığınız yoksa da, ileride ki videolarda yeterli türev ve yüksek matematik (calculus) hakkında sezgiler alacaksınız yapay sinir ağlarını etkili bir şekilde kullanabileceksiniz. Fakat etraflı bir sezgi için şimdilik bu terim fonksiyonun eğimini temsil eder ve parametrenin geçerli ayarlarında fonksiyonun eğimini bilmek istiyoruz. böylece biz en dik iniş adımını atabiliriz,ve biz maliyet fonksiyonu j de aşağıya gidebilmek için hangi yöne adım atacağımızı biliyoruz. Dereceli alçaltmayı j(w) için yazdık. Lojistik regresyon da,maliyet fonksiyonu w ve b nin fonksiyonudur. Bu durumda,dereceli alçaltmanın iç döngüsünde, buradaki senin tekrarlamak zorunda olduğun şey şuna dönüşür. w yü w eksi öğrenme kat sayısı çarpı J(w,b) nin w ya göre türevine güncellersin. ve b yi b eksi öğrenme katsayısı çarpı maliyet fonksiyonun b ye göre türevine güncelleştirirsin. Aşağıdaki denklemler senin gerçekte uygulayacağın güncellemeler. Konu dışı olarak,Ben bir yüksek matematikteki (calculus) gelenekten bahsetmek istiyorum yüksek matematik (calculus) bazı insanlara kafa karıştırıcı geliyor. Senin yüksek matematiği (calculus) anlaman çokda önemli olduğunu düşünmüyorum Ben senın bunu çok fazla düşünmeni istemiyorum. yüksek matematikteki (calculus) bu terim aslında şöyle kısa ve eğri çizgi olarak yazabiliriz. bu sembol,aslında küçük d süslü ve stilize yazı tipinde bu ifadeyi gördüğünde anlamı j(w,b) fonksiyonunun gerçek eğimi değil w yönünde ne kadar eğimli olduğudur. bana göre tamamen mantıksal olmayan yüksek matematiğin(calculus) işaret sistemin bana göre daha fazla karmaşık yapan bu sistemde eğer J 2 veya daha fazla değişkene bağlıysa küçük d yerine bu sembolü kullanırız. buna kısmi türev denir. Fakat bunun hakkında endişelenmeyin, ve J sadece bir değişkenli fonksiyonsa, küçük d kullanabilirsiniz. kısmi türev sembolünün küçük d den tek farkı J iki ya da daha fazla değişkenin bir fonksiyonu mu yoksa değil mi. Bu durumda , bu kısmi türev sembolünü kullanın ya da eğer J bir değişkenin fonksiyonuysa küçük d kullanın. Bana göre olması gerekenden daha karmaşık olan yüksek matematikteki (calculus) garip işaret sistemlerinden biri. Kısmi türev sembolünü gördüğünüzde bunun anlamı bir değişene bağlı fonksiyonun eğimini ölçüyorsunuz. Benzer şekilde buraya yüksek matematikteki (calculus) matematiksel sembolü ekleyelim çünkü J 2 girdiye sahip bir tane değil. Aşağıdaki yere kısmi türev sembolü yazılmalı. Bunun anlamı neredeyse küçük d ile aynı. Sonuç olarak bunu kodda uyguladığınızda şöyle bir düzen kullanmalıyız kodunda dw senin w yu güncellediğin değeri temsil etmeli. ve bu nicelik. b yi güncellemek istediğin değer senin kodunda db olarak temsil edilecek. Bu senin nasıl dereceli alçaltmayı uygulaman gerektiğiydi. Eğer birkaç yıldır yüksek matematik (calculus) görmediysen, senin rahat hissedeceğinden daha fazla türev gibi gözüküyor biliyorum. Ama eğer böyle hissediyorsan endişelenme. Sonraki videoda, sana türev hakkında daha iyi sezi vereceğiz. derin bir şekilde yüksek matematik (calculus) matematiğini anlamadan, sezgisel olarak yüksek matematiği (calculus) anlayarak, etkili bir biçimde yapay sinir ağları yapabilirsin. Hadi biraz daha türev anlatacağımız sonraki videoya geçelim.