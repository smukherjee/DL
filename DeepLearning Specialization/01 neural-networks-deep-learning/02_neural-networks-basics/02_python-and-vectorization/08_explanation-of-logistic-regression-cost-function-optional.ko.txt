이전 비디오에서는 로지스틱 회귀분석에 대한 비용함수 공식을 적어드렸는데요 이번 선택적 비디오를 통해서는, 
왜 로지스틱 회귀분석에서 이러한 비용함수를 쓰는 것이 좋은지 입증하는 시간을
갖도록 하겠습니다. 빠르게 복습하자면, 로지스틱 회귀분석에서는 ŷ 이라는 예측을 w transpose x 더하기 b의 시그모이드라고 합니다. 시그모이드는 여기서 보이는 익숙함 함수이고요. 그리고 ŷ 를 p(y=1 | x)라고 해석한다고 했었죠. 궁극적으로 우리의 알고리즘은 ŷ 이 입력 값 x에 대해서 y가 1인 경우의 확률을 나타내주길 바랍니다. 다시 말해, y가 1인 경우, x가 ŷ 일 때의 y 확률이라고 할 수 있습니다. 반대로, 만약 y가 0이면, y가 0인 경우는 1-y겠죠. 맞죠? 그래서 만약 ŷ 이 y가 1일 확률이면 1-ŷ 은 y가 0일 확률이 됩니다. 자 그럼 마지막 2개의 공식을 가지고 와서
다음 슬라이드에 복사하겠습니다. 제가 무엇을 할 것이냐면
이 2가지의 공식을 갖고 각각 이 공식은 y=0 인 경우와 y=1인 경우에
p(y | x)를 정의해주는데요. 2개의 공식을 한가지 공식으로 요약해보겠습니다. 다시 말씀 드리자면, y는 반드시 0또는 1이여야 합니다.
2진법 분류에서는 2가지 가능한 케이스밖에 없기 때문이죠. 2개의 공식을 1개로 요약하면, 자, 먼저 일단 공식이 어떻게 생겼는지 적어보겠습니다. 왜 이 공식이 되는지 정확한 유는 나중에 설명 드리겠습니다. 그럼 (1-ŷ)(1-y) 이죠. 1가지의 선이 위의 2개의 공식을 요약해줍니다. 왜 그런지 말씀 드리죠. 첫 번째 경우, y=1이라고 해봅시다. 맞죠? 만약 y=1인 경우, 이 항은 ŷ 이 됩니다. 그 이유는 이 것이 ŷ 의 1승이기 때문이죠. 이 항은 1-ŷ 의 1승 빼기 1이 되는데요. 이것은 그럼 0승입니다. 하지만 어떤 한 값의 0승은 1이죠. 그럼 이건 사라집니다. 그러므로 이 공식은, y가 1인 경우 p(y | x)=ŷ 이죠. 저희가 원했던 대로 값이 나왔습니다. 2번째 경우는 어떨까요? y=0인 경우 말이죠. y=0이면, 위의 공식은 p(y | x) = ŷ 의 0승입니다. 어떠한 값의 0승은 1이기 때문에 이것은 1 곱하기 1-ŷ 의 1-y승입니다. 그래서 1-y는 1-0, 즉 1이 됩니다. 그럼 이 값은 1 곱하기 1-ŷ 그래서 1-ŷ 이 됩니다. 여기서는 y가 0인 경우, p (y | x) = 1- ŷ 이 됩니다. 정확히 저희가 원했던 값입니다. 여기서 보여드린 것은 이 공식이 p(y l x)의 올바른 정의라는 것입니다. 이 함수의 규칙은 단순 증가하는 함수이기 때문에 최대화를 통해… 비슷한 결과는 p(y | x) 최적화합니다. p(y | x)의 log를 계산하 보면 =log ŷ[y](1- ŷ)(1-y) 값인데요 그렇게 해서 y log ŷ 더하기 1 빼기 y 곱하기 로그 1 빼기 ŷ 이겠죠. 맞죠? 그래서 이것은 사실 loss함수의 마이너스 값인데요 이전에 찾아야 했던 값과 같습니다. 여기 마이너스 부호가 있는데요, 그 이유는 러닝 알고리즘을 트레이닝 시키는 경우 확률은 높이고 싶지만 로지스틱 회귀분석에서는 이것을 나타냅니다. loss 함수를 최소화시키고 싶은 것입니다. loss를 최소화시킨다는 의미는 확률의 log를 최대화시키는 것과 동일합니다. 단일의 예시에서의 loss 함수는 이렇게 생겼습니다. 이 비용함수는 어떨까요? m개의 예시를 갖춘 전체 트레이닝 세트에 대한 전체 비용함수 말이죠. 알아보도록 하겠습니다. 알아봅시다. 트레이닝 세트에 있는 모든 레이블에 대한 확률 대충 비형식적으로 쓰겠습니다. 따로 그린 트레이닝 예시들은 drawn IID로 그리는데요 identically independently distributed의 약자입니다. 예시의 확률은 확률들의 곱입니다. i값은 1에서 m까지 p(y(i) ) l x(i) 값입니다. 그러면 최대 확률 추정치를 구하는 경우 최대화 시켜야하는데요, 
먼저, observation과 트레이닝세트의 최대화 값 확률을 주는 매개변수를 찾아야 합니다. 이것을 최대화시키는 것은 로그를 최대화시키는 것과 동일합니다. 그러므로, 양쪽에 로그를 적용시킵니다. 그러면, 트레이닝 세트 레이블의 확률의 log값은 곱한 값의 로그는 로그의 합이죠. 그러면 i=1에서 m까지 log p(y(i)) l x(i) 공식에 적용시킵니다. 이전 슬라이드에서 찾았듯이 이 값은 –L (ŷ(i), y(i))입니다. 통계 원리가 있는데, 최대확률측정법 (principle of maximum likelihood estimation)
라고하는 원리가 있습니다. 단순히 이야기해서 이 값을 최대화 시킬 수 있는
매개변수를 고르라는 것입니다. 다르게 이야기하면, 
이 값을 최대화 시키는 것입니다. = - SUM i=1 m L(ŷ, <i>,y<i>) 합 공식에서 마이너스 부호를 바깥으로 빼면 됩니다. 이 값의 J(w, b)이기도 한 로지스틱 회귀분석을 기반으로 한 비용에 대해 합리화시켜줍니다. 이제는 비용을 최대화가 아닌 최소화하는 확률을 구해야 하기 때문에 이 마이너스 부호를 빼줍니다. 편의를 위해, 양에 대한 부분은 스케일 하여 1/m 에서 m은 추가 scaling factor입니다. 요약하자면, J(w, b)를 최소화하여 로지스틱 회귀분석모델을 바탕으로 최대확률측정법 을 진행합니다. 우리의 트레이닝 example이 IID 또는 identically independently distributed 됐다는 가정하에 말이죠. 이번 비디오가 선택적이었음에도 시청해주셔서 감사합니다. 로지스틱 회귀분석에서 왜 이런 비용함수를 쓰는지 조금 더 감이 오셨으면 좋겠는데요. 다음은, pro exercise와 이번주 퀴즈로 넘어가시길 바랍니다. 이어지는 퀴즈와 exercise에서 행운을 빕니다.