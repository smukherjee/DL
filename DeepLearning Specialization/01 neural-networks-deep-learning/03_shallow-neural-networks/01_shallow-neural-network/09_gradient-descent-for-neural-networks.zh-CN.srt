1
00:00:00,000 --> 00:00:03,840
我相信这会是一个令人激动的视频

2
00:00:01,800 --> 00:00:06,240
在本期视频中我们将学习

3
00:00:03,840 --> 00:00:08,730
如何在神经网络中

4
00:00:06,240 --> 00:00:10,530
使用一个隐含层实现梯度下降

5
00:00:08,730 --> 00:00:12,809
本视频中 我将写出所有方程

6
00:00:10,530 --> 00:00:14,639
你需要使用这些方程

7
00:00:12,809 --> 00:00:17,039
来实现梯度下降法中

8
00:00:14,639 --> 00:00:19,410
梯度的逆传播

9
00:00:17,039 --> 00:00:21,510
在之后的视频中

10
00:00:19,410 --> 00:00:24,150
我会解释为什么这些方程

11
00:00:21,510 --> 00:00:26,070
是精确的 换句话说 正确的

12
00:00:24,150 --> 00:00:27,630
它们可以用于计算

13
00:00:26,070 --> 00:00:28,289
神经网络中需要用到的梯度

14
00:00:27,630 --> 00:00:30,090
神经网络中需要用到的梯度

15
00:00:28,289 --> 00:00:32,520
现在 你的神经网络

16
00:00:30,090 --> 00:00:39,930
只包含一个隐含层

17
00:00:32,520 --> 00:00:44,760
且有参数 W1 b1 W2 b2

18
00:00:39,930 --> 00:00:50,399
回顾一下 它有nx(即n0)个

19
00:00:44,760 --> 00:00:56,640
输入特征

20
00:00:50,399 --> 00:00:59,149
n1个隐藏单元和n2个输出单元

21
00:00:56,640 --> 00:01:05,670
在我们的例子中 n2=1

22
00:00:59,149 --> 00:01:08,880
那么矩阵W1就是n1*n0的

23
00:01:05,670 --> 00:01:11,250
b1是一个n1维向量

24
00:01:08,880 --> 00:01:13,350
可以视作n1*1矩阵

25
00:01:11,250 --> 00:01:16,500
即一个列向量

26
00:01:13,350 --> 00:01:20,750
W2的维度是n2*n1

27
00:01:16,500 --> 00:01:26,759
b2的维度是n2*1

28
00:01:20,750 --> 00:01:28,590
我们目前只考虑

29
00:01:26,759 --> 00:01:30,930
n2=1的情况

30
00:01:28,590 --> 00:01:36,930
也就是说只有一个隐藏单元

31
00:01:30,930 --> 00:01:39,570
而且在这种神经网络中

32
00:01:36,930 --> 00:01:41,340
我们还有一个损失函数

33
00:01:39,570 --> 00:01:44,220
现在假设我们在做

34
00:01:41,340 --> 00:01:48,659
二分类问题

35
00:01:44,220 --> 00:01:51,740
那么参数整体的损失

36
00:01:48,659 --> 00:01:57,090
就是m分之一倍的

37
00:01:51,740 --> 00:01:59,969
平均损失函数

38
00:01:57,090 --> 00:02:03,420
这里的L表示

39
00:01:59,969 --> 00:02:06,240
当模型预测为y帽(即a2)

40
00:02:03,420 --> 00:02:07,649
但真实标签为y时的损失

41
00:02:06,240 --> 00:02:09,629
而且如果做二分类

42
00:02:07,649 --> 00:02:12,510
损失函数可以

43
00:02:09,629 --> 00:02:15,030
与逻辑回归完全一致

44
00:02:12,510 --> 00:02:18,420
所以为了训练算法的参数

45
00:02:15,030 --> 00:02:21,450
我们需要采用梯度下降法

46
00:02:18,420 --> 00:02:23,189
当我们训练神经网络时

47
00:02:21,450 --> 00:02:25,379
初始化参数

48
00:02:23,189 --> 00:02:26,129
随机在全0附近十分重要

49
00:02:25,379 --> 00:02:28,140
 

50
00:02:26,129 --> 00:02:30,030
我们之后再讨论其中的原因

51
00:02:28,140 --> 00:02:32,069
初始化完参数后

52
00:02:30,030 --> 00:02:34,140
梯度下降的每一次循环

53
00:02:32,069 --> 00:02:36,780
都要对样本做预测

54
00:02:34,140 --> 00:02:42,359
所以我们计算y帽i

55
00:02:36,780 --> 00:02:44,519
i取从1到m

56
00:02:42,359 --> 00:02:49,440
之后需要计算导数

57
00:02:44,519 --> 00:02:51,420
计算dW1 也就是

58
00:02:49,440 --> 00:02:54,359
损失函数

59
00:02:51,420 --> 00:02:56,489
对W1求导数

60
00:02:54,359 --> 00:02:59,220
我们还需计算另一个参数

61
00:02:56,489 --> 00:03:00,870
db1 也就是

62
00:02:59,220 --> 00:03:04,109
损失函数

63
00:03:00,870 --> 00:03:07,349
对b1求导 亦称为求斜率

64
00:03:04,109 --> 00:03:10,170
依此类推

65
00:03:07,349 --> 00:03:12,629
同样对W2和b2求导

66
00:03:10,170 --> 00:03:17,879
最后梯度下降的更新就是

67
00:03:12,629 --> 00:03:22,709
W1更新为W1减去𝛼

68
00:03:17,879 --> 00:03:26,129
即学习率 乘上dW1

69
00:03:22,709 --> 00:03:31,620
b1更新为b1减去学习率乘db1

70
00:03:26,129 --> 00:03:34,739
类似地还要更新W2和b2

71
00:03:31,620 --> 00:03:36,299
有时我写"冒号等号"

72
00:03:34,739 --> 00:03:38,489
有时我还写"等号"

73
00:03:36,299 --> 00:03:40,829
两种写法都可以

74
00:03:38,489 --> 00:03:42,510
这就是梯度下降的单次循环

75
00:03:40,829 --> 00:03:44,280
然后重复这个循环很多遍

76
00:03:42,510 --> 00:03:46,079
直到参数看似收敛了

77
00:03:44,280 --> 00:03:48,150
在之前的视频中

78
00:03:46,079 --> 00:03:50,099
我们讨论了如何计算预测值

79
00:03:48,150 --> 00:03:51,629
即如何计算输出

80
00:03:50,099 --> 00:03:54,060
我们还介绍了如何向量化

81
00:03:51,629 --> 00:03:56,269
所以关键只需了解

82
00:03:54,060 --> 00:04:00,180
如何计算这些

83
00:03:56,269 --> 00:04:04,079
偏导数dW1 db1

84
00:04:00,180 --> 00:04:06,780
以及偏导数dW2 db2

85
00:04:04,079 --> 00:04:09,419
我希望直接给你们

86
00:04:06,780 --> 00:04:12,150
计算这些导数的方程

87
00:04:09,419 --> 00:04:14,699
我将在下一个视频中

88
00:04:12,150 --> 00:04:17,430
那是个选修视频

89
00:04:14,699 --> 00:04:19,090
深入介绍我们如何推导

90
00:04:17,430 --> 00:04:21,400
这些公式

91
00:04:19,090 --> 00:04:26,169
那么我们总结一下

92
00:04:21,400 --> 00:04:33,250
用于传播的公式

93
00:04:26,169 --> 00:04:37,900
我们有Z1=W1X+b1

94
00:04:33,250 --> 00:04:41,680
A1等于那一层的激励函数

95
00:04:37,900 --> 00:04:49,690
作用于Z1的每一个元素

96
00:04:41,680 --> 00:04:53,530
然后Z2=W2A1+B2

97
00:04:49,690 --> 00:04:55,180
最后 这些都是

98
00:04:53,530 --> 00:05:01,210
对训练集做了向量化的

99
00:04:55,180 --> 00:05:02,740
A2=g2(Z2)

100
00:05:01,210 --> 00:05:04,870
我们假定在做二分类

101
00:05:02,740 --> 00:05:06,610
那么激励函数

102
00:05:04,870 --> 00:05:08,560
应当就是sigmoid函数

103
00:05:06,610 --> 00:05:11,080
所以我就直接写在这儿

104
00:05:08,560 --> 00:05:13,870
所以这些就是前向传播

105
00:05:11,080 --> 00:05:15,729
也就是神经网络

106
00:05:13,870 --> 00:05:18,430
从左向右进行前向计算

107
00:05:15,729 --> 00:05:24,750
我们来计算导数

108
00:05:18,430 --> 00:05:30,750
也就是逆向传播步骤

109
00:05:24,750 --> 00:05:33,610
dZ2等于A2减去真实标签Y

110
00:05:30,750 --> 00:05:36,580
我再提醒一下

111
00:05:33,610 --> 00:05:41,289
本例中 我们做了向量化

112
00:05:36,580 --> 00:05:45,280
所以矩阵Y是这个1*m矩阵

113
00:05:41,289 --> 00:05:51,370
其中水平拼接了所有的m个样例

114
00:05:45,280 --> 00:05:55,330
所以dW2就等于这个公式

115
00:05:51,370 --> 00:05:58,870
其实这里的前三个公式

116
00:05:55,330 --> 00:06:00,900
与逻辑回归的梯度下降十分类似

117
00:05:58,870 --> 00:06:00,900
与逻辑回归的梯度下降十分类似

118
00:06:00,960 --> 00:06:12,610
axis=1, keepdims=True

119
00:06:07,419 --> 00:06:15,580
这里我说一个小细节

120
00:06:12,610 --> 00:06:18,070
这个np.sum是一个Python numpy指令

121
00:06:15,580 --> 00:06:21,100
它作用于矩阵的某一维度

122
00:06:18,070 --> 00:06:24,810
在这里是水平方向求和

123
00:06:21,100 --> 00:06:27,600
而这里的keepdims是用于

124
00:06:24,810 --> 00:06:31,230
防止输出奇怪的

125
00:06:27,600 --> 00:06:34,650
一阶数组 也就是说

126
00:06:31,230 --> 00:06:37,010
数组的维度会是(n,)

127
00:06:34,650 --> 00:06:41,280
所以设定keepdims=True

128
00:06:37,010 --> 00:06:44,580
我们可以保证Python的db2输出为(n,1)维

129
00:06:41,280 --> 00:06:47,820
严格地说这里应写作

130
00:06:44,580 --> 00:06:50,130
n2*1 而这里则是一个

131
00:06:47,820 --> 00:06:53,520
1*1的数

132
00:06:50,130 --> 00:06:56,790
可能现在看来无所谓

133
00:06:53,520 --> 00:06:58,500
但我们之后就会解释这里很重要

134
00:06:56,790 --> 00:07:01,320
到此为止我们做的和逻辑回归很像

135
00:06:58,500 --> 00:07:03,919
但是当你继续计算

136
00:07:01,320 --> 00:07:14,370
梯度逆传播的时候

137
00:07:03,919 --> 00:07:19,380
你需计算这个 [读公式]

138
00:07:14,370 --> 00:07:20,880
所以这里的g1'就是

139
00:07:19,380 --> 00:07:22,919
你隐含层所使用的

140
00:07:20,880 --> 00:07:25,770
激励函数的导数

141
00:07:22,919 --> 00:07:27,030
对于输出层 我假定你在做

142
00:07:25,770 --> 00:07:29,400
二分类问题

143
00:07:27,030 --> 00:07:30,780
用的是sigmoid函数

144
00:07:29,400 --> 00:07:34,620
我已经把它代入了

145
00:07:30,780 --> 00:07:39,090
得到这里的dZ2公式

146
00:07:34,620 --> 00:07:43,050
而这个乘号是指每个元素对应相乘

147
00:07:39,090 --> 00:07:46,950
所以这里应当是n1*m的矩阵

148
00:07:43,050 --> 00:07:48,990
而这里是对每一个元素

149
00:07:46,950 --> 00:07:52,680
代入求导 所以同样也是

150
00:07:48,990 --> 00:07:54,720
n1*m的矩阵

151
00:07:52,680 --> 00:07:59,669
所以这里的乘号指矩阵对应元素相乘

152
00:07:54,720 --> 00:08:08,490
最后dW1就等于这个

153
00:07:59,669 --> 00:08:18,950
而db1就等于这个

154
00:08:08,490 --> 00:08:21,900
[读公式]

155
00:08:18,950 --> 00:08:23,430
所以尽管之前

156
00:08:21,900 --> 00:08:27,210
这里的keepdims可能不重要

157
00:08:23,430 --> 00:08:28,590
因为n2=1 这里只是个1*1矩阵

158
00:08:27,210 --> 00:08:35,729
是个实数

159
00:08:28,590 --> 00:08:38,370
这里db1是n1*1向量

160
00:08:35,729 --> 00:08:40,110
所以我们希望Python的

161
00:08:38,370 --> 00:08:43,110
np.sum输出这种维度

162
00:08:40,110 --> 00:08:46,529
而不是一个奇怪的

163
00:08:43,110 --> 00:08:48,360
如此处所示维度的矩阵

164
00:08:46,529 --> 00:08:50,580
那会导致之后的计算很混乱

165
00:08:48,360 --> 00:08:53,310
另一种方法就是

166
00:08:50,580 --> 00:08:56,910
你无需保留这些参数

167
00:08:53,310 --> 00:09:00,060
但需要显式地调用一个reshape函数

168
00:08:56,910 --> 00:09:04,400
来把np.sum的输出转变为

169
00:09:00,060 --> 00:09:08,310
你希望的db的维度

170
00:09:04,400 --> 00:09:11,339
所以这些就是前向传播的

171
00:09:08,310 --> 00:09:14,310
四个方程 以及逆向传播的

172
00:09:11,339 --> 00:09:16,680
六个方程 我直接写出了这些方程

173
00:09:14,310 --> 00:09:18,870
但在下一个选修视频中

174
00:09:16,680 --> 00:09:22,050
我们将回顾

175
00:09:18,870 --> 00:09:23,940
逆向传播算法的六个方程

176
00:09:22,050 --> 00:09:25,830
直觉上是如何推导的

177
00:09:23,940 --> 00:09:27,750
请自己决定是否观看

178
00:09:25,830 --> 00:09:30,000
但是无论如何

179
00:09:27,750 --> 00:09:32,730
只要你实现我写的算法

180
00:09:30,000 --> 00:09:34,650
你就能实现正确的前向和逆向传播

181
00:09:32,730 --> 00:09:36,750
而且你将能够计算

182
00:09:34,650 --> 00:09:39,029
梯度下降法所需要的导数

183
00:09:36,750 --> 00:09:41,520
从而学出你的神经网络参数

184
00:09:39,029 --> 00:09:43,680
你同样可以实现这个算法并使其生效

185
00:09:41,520 --> 00:09:45,209
你同样可以实现这个算法并使其生效

186
00:09:43,680 --> 00:09:47,130
即使你没有深入理解

187
00:09:45,209 --> 00:09:50,520
背后的代数计算

188
00:09:47,130 --> 00:09:52,320
许多成功的深度学习实践者就这么做的

189
00:09:50,520 --> 00:09:54,180
但如果你想理解 你可以看下一个视频

190
00:09:52,320 --> 00:09:56,580
从中学习这些方程导数

191
00:09:54,180 --> 00:09:58,820
的推导方法

192
00:09:56,580 --> 00:09:58,820
的推导方法