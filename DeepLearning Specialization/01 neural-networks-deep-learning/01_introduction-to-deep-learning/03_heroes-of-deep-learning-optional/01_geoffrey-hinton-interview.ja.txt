deeplearning.ai のコースの一部として 深層学習の技術的な考えを
教えるだけではなく 深層学習の英雄たちを
紹介したいと思っています このスペシャリゼーションや このコースで学ぶ考えの多くを
発明した人たちです これらのビデオでは
深層学習のリーダーに キャリアの助言をもらえるようにしたいです
深層学習に参入する方法や 深層学習の研究を行ったり
仕事が見つけられるようにする方法を聞きます このインタビューシリーズの
初回として ジェフリー・ヒントンさんをお迎えし
紹介するのを光栄に思います ようこそジェフ
deeplearning.ai のインタビューを受けてくれてありがとう お招きありがとうございます 現時点ではこの地球上で誰よりも
深層学習を支える多くのアイデアを 発明した人はいないように思います 多くの人が深層学習のゴッドファーザーと
呼んでると思っていました しかし数分前にお話して初めて
私が最初にそう呼んだと 分かりました
そう呼んでみてよかったです 多くの人が
あなたを伝説として見ています わたしが聞きたいのは
伝説の裏のあなた自身のお話です ずっと昔を振り返ってみて
どんな経緯でAIや機械学習やニューラルネットワークに 関わることになったのですか 高校生のときに
同級生がいて 何についても私より優れていて
優秀な数学者でした ある日彼が学校にきて
「脳がホログラムを使っているって知ってる」と聞いてきました たしか1966年頃だと思うのですが
私は「ホログラムって何？」という感じでした ホログラムがあると
脳を半分切り離しても全体像がまだ残ると 説明してくれました 脳にある記憶は
脳全体に分散されていると言いました 彼はラシュリーの実験を
読んだのだと思います 実験では
ラットの脳の一部を切り離し ある部分が特定の記憶を保管している場所を
特定するのが非常に困難と発見した実験です このことをきっかけに
脳がどう記録を保管するのかに興味を持ちました そして大学に進んだ時に 生理学と物理学を
学び始めました ケンブリッジ大学では この両方を勉強していたのは
学部では私だけでした その後これは諦めて 哲学を学びました
より洞察が得られると思ったためです ですが
実は 哲学では事実と違うことがあっても
分からないように思えました そして心理学に転向し 心理学では
非常に単純化された理論が多く 脳の動作を説明するには
絶望的に不適切と言ってもいいものでした その後少しのんびりして
大工になったあとに AIで試してみることにして
エジンバラでランガー・ヒギンスと AIを研究しました 彼はとても良いニューラルネットワークの
研究をしていましたが ちょうど諦めていたところでした
私はウィノグラードの論文に衝撃を受けていました 私が合流したときに
彼はわたしが古いことをしていると考えていましたが 私はシンボリックAIを
始めなければと考えました かなり戦いましたが
信じたものをただただ続けました その後どうなりました 最終的にAIで博士号をとり
イギリスでは仕事がありませんでしたが カリフォルニアのスローン・フェローシップの
素晴らしい広告を見て なんとかこれに
入ることができました カリフォルニアに行きましたが
何もかもが異なってました イギリスではニューラルネットワークは
馬鹿げたことだと見なされていましたが カリフォルニアでは
ドナルド・ノーマンや デビッド・ラメルハートは
ニューラルネットワークの考えにとてもオープンでした 脳がどう機能しているのか
これが心理学と関連がある可能性を考えることを 非常に有益なことと捉えられる所に来たのは 初めてでした とても楽しかったです デビッド・ラメルハートと協力するのは
特に素晴らしかったです なるほど
1982年にカリフォルニア大学サンディエゴ校に ラメルハートさんといて 誤差逆伝播法の学会論文を
書き上げたのですね 実はもっと複雑でした なにがありましたか 1982年初めのころ デビッド・ラメルハートと
ロナルド・ウィリアムスと私で 誤差逆伝播法のアルゴリズムを
開発しました 主としてデビッド・ラメルハートの考えでした 後になって
他にも多くの人が発明していたことを発見しました デビット・パーカーがそうです
行ったのは後でしたが私たちの発表よりも先でした ポール・ワーボスは
数年前にすでに発表していましたが 注目されませんでした 他にもよく似たアルゴリズムを
開発した人たちがいましたが 誤差逆伝播法が何を意味するのか
明らかではありませんでした 連鎖律を使って
微分を出すのは新しいアイデアではありませんでした なるほど
なぜあなたの論文だけが コミュニティが理解するのに
そんなに貢献したのでしょうか 相手がだれにしてもこのアルゴリズムが受け入れられたのは
まるで論文が伝染するかのよう です 1986年にネイチャーに
発表することができました 論文が受け入れられるように
政治的な仕事を随分としました 審査員の１人が有名な心理学者の
スチュアート・サザーランドだろうと 探りあてたので 話しに行き
長い時間を使って 彼に実際にどうなっているかを
説明しました 彼は非常に感銘を受けていました 誤差逆伝播法が
言葉の表現を学ぶことができる事実にです ベクトルであるこれらの表現を見ることができて 個々の特徴量の意味を理解できます 実際には家系図の中の３連の語を
学習させました 例えばマリーには
ビクトリアという母がいるなどです 最初の２語を与えて
最後の１語を予測するものでした 学習させると 個々の言葉を表現によって
特徴量を見ることができました 人の国籍があったり 何世代目か どの家系の子孫なのかが
特徴量で分かりました そのことによって
彼はこの事実に本当に感銘を受けました これが論文が
受け入れられた理由だと思います 非常に早期の分散表現ですね
既に学習アルゴリズムから 表われる学習した意味の特徴量を
見れたのですね 心理学者の視点からすると
知識がどのようなものかという 全く異なる意見の考えが
統合されたことを興味深いものでした つまり古い心理学者は
概念は大量の特徴量の塊という見解で この証拠は多くあります 当時のAIの考えでは
かなり構造的な考えで 概念というものは
お互いの概念がどう関連しているかというものでした 概念をとらえるには
意味ネットワークやグラフ構造のようにする必要が ありました 誤差逆伝播法の例が示したのは 今回は家系図だったグラフ構造に
入るような情報は この情報を特徴量に変換でき
この特徴量は 汎化のように一貫した情報を
取り出すのに使える形にできるということです 重要なことは
家系図のグラフ表現と家系図を木構造表現との間での行き来で 人々を表現したものが
巨大な特徴ベクトルによって 表わされるということでした グラフのような表現から
特徴ベクトルに 変換したければ、 できて
特徴ベクトルからはグラフのような表現が得られるという事実です それは1986年ですか 90年代の初期に
ベンジオが実データを 英語のテキストを使って
同じ手法を適用しました 英語のテキストから分散表現を作り
これは人々に大きな感銘を与えました GPUやスーパーコンピューターなどの高速計算機が
深層学習をどのように進展させているかが 近年よく語られていますが 1986年から90年代初期に既に
あなたとベンジオの間で この流れが始まっていたのは
知りませんでした 非常に大きな進展です 1986年には10分の１のメガフロップより遅いLispの機械を
使っていました 1993年の頃には
10メガフロップの機械がありました なるほど
100倍です これが使いやすくなってきた時で コンピューターが
速くなっていきました 過去数十年に渡って ニューラルネットワークと深層学習の
多くの手法を発明されていますが 発明されたものすべてに
私は興味を持っています 今でも一番興味があるのはどれですか 一番美しいのはテリー・セジュノスキーとの ボルツマンマシンの研究だと思います つまり本当にとても単純なアルゴリズムで 数個のノードしか見ることができないような
巨大な密結合ネットに 適用できる学習アルゴリズムを
発見しました これは隠れ表現を学びますし
非常に単純なアルゴリズムです 脳の中にも見つけられそうなものに
思えました 各シナプスが知る必要があるのは
これが繋がっている２つのニューロンの挙動を 知っておくだけでいいのです 伝播される情報は
同じものでした ２つの段階がありました
覚醒と睡眠と呼んでいましたが この異なる段階でも 同じ方法で情報を
伝播していました 誤差逆伝播法では
前方パスと後方パスがあります パス次第で
異なる動作をします 異なる信号を送ります これが最も美しいものだと思います 長年にわたって
これはただ好奇心としか言えないものでした 遅すぎるだろうからです その後 美しさを少し取り払って
諦めがついてきて いくらかは単純化したネットワークにし １回分の実行にしました 制限ボルツマン・マシンを生んで 実際に実践では
効果的に動作しました 例えばNetflix の競技では 制限ボルツマン・マシンは
勝利した組が組み入れていた方法の１つでした 2007年頃に始まった
ニューラルネットワークと深層学習が 最近 再び浮かび上がってきたのは
制限ボルツマン・マシンでした 制限なしボルツマン・マシンもそうでした
あなたとあなたのラボが作ったのでした そうです これもとても満足している研究の１つです １つだけ隠れ特徴の層がある制限付きボルツマン・マシンに
学習させることができ １つの特徴の層を
学習させることができる考えです この特徴をデータとして扱えて
同じことを再度できます 新しい特徴をまた新しい層で学習でき
もう１度同じことができます 何回でもできます とても良いもので
実際に使えるものでした UYテイ 変な種類のモデルでした こんなモデルです
最上部に制限ボルツマン・マシンがあり 下には何年も前に作られていたシグモイドのBelief ネットが あります これは直接モデルで これで制限付きボルツマン・マシンに
学習させることで シグモイドのBelief ネットで効率的に
予測を行う方法を作ることができました 当時は 密結合ネットを使おうと人たちが
ニューラルネットを使っている人はいましたが 確率的予測を行う
良い方法がありませんでした 私の子供たちと異なるように
グラフィカルモデルを行っていた人たちは 予測を正しく行えた人もいましたが
疎結合ネットでしかできませんでした 私たちはDeep Belief Net を学習させる方法
示すことができ 近似の形で行う予測でした
とても速いものでした １つの前方パスのみを行い
美しい結果を生みました また追加で特徴の層を学習することを
保証することができました 新しい層を学習するたびに
新しいBoundができ 新しいBoundは
つねに古いBoundよりも優れていました 層を足すことにできるVariational bound ですね ビデオを覚えています これが２つ目のとても嬉しかったことです ３つ目のものは
neilとの 変分法のものです
すでに統計学の学者が似た研究をしていたことが分かりましたが 私たちは知りませんでした 完全Eの計算を 行う必要がなく近似的なEを行うことで
ENを算出する随分と良い方法を 示すことができました ENは統計学では
巨大なアルゴリズムですが 私たちは大きな一般化を
示しました 特に1993年には
ヴァン・キャンプと 最初の変分ベイズ法の論文を
一緒に書きました ベイズの学習の方法の１つを行う方法を示し ガウスの真の事後を近似することで
より扱いやすいものでした これをニューラルネットで出来ました これにはとても嬉しかったです なるほど 私はそれらの論文を覚えています Neil,Hinton,yen/ian paperの論文を読むのに
長い時間を使いました こんにち使われて 毎日使うようなアルゴリズムには
ドロップアウトやReLUがありますが これもあなたのグループが
発表しているものですよね どちらとも言えます Rectified Linear Units は
他の人が考えたものです 私たちは制限付きボルツマン・マシンで
ReLU が ロジステックユニットでの層を重ねたもの全体と
ほぼ同じという研究をしました これはReLU が出回ったのに
貢献したものの１つです とても興味があります ReLU の論文は
多くの数学が含まれています この関数が非常に複雑な式によって
近似できることを示しています ご本人で数学的部分も行って
論文が学会で受け入れられるようにしたのですか max(0,x)の開発に影響を及ぼした
すべての数学の部分です この件は考えを開発するのに
数学が実は重要だった事例の １つでした つまり ロジステックユニットを知ってましたし
明らかにRectified Linear Units も 知っていました ボルツマン・マシンを
研究していたので 基本的な作りは
ロジステックユニットで行われていました そして命題は Rectified Linear Units で
学習アルゴリズムがきちんと動作するかでした Rectified Linear Units で表現することでも
ロジステックユニットで 行ったのとほとんど同じでした
同じになる方法を数学で表わしていました なるほど それによって多くの人がReLU を使うことを鼓舞されたのですね
ロジステックユニットを理解しなくても同じように動作しますし そうです 同じ目的で行うものを
必ずしも理解する必要がありません そうです
Google に行って１つ気付いたことがありました 2014年にGoogle でReLU についてと
単位行列で初期化することについて 話しました ReLU でいいことは
隠れ層をコピーし続けると 単位行列で初期化していくと 次の層にパターンを
コピーするだけになります そして300の隠れ層のネットワークを
学習させることができるのを示していました 単位行列で初期化すると
本当に効果的に学習させることができました これをそれ以上は追いませんでした
そうしなかったことを本当に後悔しています リーさんと論文を発表しました
再帰ネットワークを そのように初期化できることを
示しました さらにこれを追求すべきでした
後にResidual ネットワークができたからです そういうことでした 何年にも渡って
あなたが脳を語って 脳と逆誤差伝播法の関係を
語るのを聞いてきました この関係については
今はどんな考えがありますか 実はまさに今
論文を書いています 主な考えはこれです もし逆誤差伝播法が
学習に良いアルゴリズムと分かったならば もちろん進化についても つまり今は眼球や歯を
作れる細胞があります 細胞が変化できるなら
細胞に逆誤差伝播法を組み込むことができるはずです これに選択的な圧力がかかるはずです これが信じられるものではないという神経科学者の考えは
馬鹿げています 分かりくい実現方法が
あるのかもしれません 逆誤差伝播法と全く同じでないにしても
それにかなり近いものが 脳にはあると考えています 何年にもわたって
どうすればこれがあり得るのかという考えが浮かんでいます 1987年に
ジェームズ・マクレランドと 再循環アルゴリズムを作りました これの考えというのは
ループするものに情報を 送り 情報がこのループを回りながらも
構成は変わりません これを最も単純な形で表わすとしたら
入力ユニット２つと隠れユニット２つがあり 情報を入力１から隠れユニット１に送り
入力２に戻し 隠れ２に戻し入力に戻しというのを
繰り返します オートエンコーダーを学習させたいのですが 逆誤差伝播法なしで
学習させるというものです 学習させるのですが
活動での差異を取り除くものです シナプスの学習方法のルールは シナプス前入力への割合と
シナプス後入力の変更率の割合で 重みを変更します 再循環ではシナプス後入力を 古いものを良いものとして
新しいものを悪いものにします この方向で変更していきます このアルゴリズムを発明しました
スパイクタイミング依存可塑性を 神経科学者が発明する前にです スパイクタイミング依存可塑性は
同じアルゴリズムですが方向が逆です この学習ルールでは新しいものが良く
古いものは悪いとしています これでは重み変更は
シナプス前活動かける (新しいシナプス前活動から古い活動を引いたもの)を行います 後に2007年に分かったのですが 制限付きボルツマン・マシンを重ねて学習させて 学習済みになると
逆誤差伝播法を実装するのと 全く同じ状態になりました
再構築しようとするだけでできました 再構築誤差を観察すると
再構築誤差は 識別性能の違いを教えてくれます 2007年の深層学習ワークショップでは
これについて話したのですが ほぼ完璧なまでに
無視されました 後にヨシュア・ベンジオは
この考えを使い 私が私自身で研究しているよりも 実はより多くの研究を行いました このオートエンコーダーを積み重ねることで 逆方向に活動を送り誤差を出し
再構築誤差を突き止める考えは 本当に興味深い考えで
脳が行っている可能性は十分にあります 他の話題としては
あなたが色々と考え研究していると聞いていることで 深層学習で再帰結合型(multiple time scales)をどう扱うかです お考えを共有してもらえますか はい 実は大学院生の１年目のことでした 私がFast weight(速い重み)と呼ぶものを
使うことについて最初の講演をしました 重みが高速で適応するが
高速で忘れます このため 短期記憶を保持します 1973年に非常に簡単なシステムで示しました 重みを真に再帰処理できる仕組みでした 真の再帰処理というのは
特徴を表現するのに使われているニューロンが 再帰コアで特徴を表現するために
再利用されるという意味です 知識を表現するための重みは 再帰コアで再利用されます このことは次の疑問を導きます
再帰コアを引き抜いた時は 今まさに行っていたことを
どうやって覚えているのかという疑問です 記憶はどこか？ なぜなら再帰コアに
ニューロンを使っています 答えは この記憶を速い重みに入れれば 速い重みから
ニューロンの活動状態を復元できます つい最近では
ジミー・バーと一緒に研究し このように再帰で速い重みを使った方法について
NIPS に論文を発表しました なるほど これにはかなり長い時間差がありました 最初のモデルは1973年で
出版されませんでしたが ジミー・バーのモデルは
2015年か2016年と思います およそ40年後です もう１つの聞きたい考えは
随分前に そう ５年くらい前に 「カプセル」についてです
今はどんな状態でしょうか 分かりました これについては
昔の私と同じ状況に戻りました 本当に信じている考えがあるのですが
誰も信じていない状況です これの論文を提出しても
全て却下されましたが 本当にこの考えを信じていて
押し続けるつもりです これには要となる何点かの概念があります １つ目は多次元をどう表現するかで 少しベクトル操作をするだけで表現できます どれか１つが分かればいいのです この考えでは画像のそれぞれの領域で
何か１つの種類の特徴が あることを想定して 大量のニューロンを利用して その活動によって
特徴の異なる側面を表現させます その領域内で
x、y座標の値であったり 向きは？ 移動速度は？ 色は？ 明るさは？ などのものです 大量のニューロンを使って
同じものを様々な次元で表現することが できます ただし１つだけなのが条件です これは従来のニューラルネットを使った表現とは 非常に異なる表現方法です 通常はニューラルネットでは
非常に大きな層を使うだけで 全てのユニットは利用され
色々に動作します しかしユニットを
異なる座標を表現する小さなグループに 分けることは考えていません 追加される構造であるべきと考えています これと共にある他の概念は ということは
分散された表現が 別々のサブセットに 分割した表現が 表現するのですね それぞれのサブセットを「カプセル」と呼んでいます なるほど この概念「カプセル」によって
特徴のインスタンスを表現することができますが １つだけです これがその特徴の異なる属性を
表現します １つの属性のスカラーしか無ような 従来のニューロンとニューラルネットと違って
多くの属性を持つ特徴です なるほど これで出来るようになるのは
通常のニューラルネットでは非常に成績の悪いことでも このことを私が合意のルーチンと呼んでいる動作で
出来るようになります 例えばセグメンテーションを
行いたいとしましょう 口かもしれないものと
鼻かもしれないものがあるとしましょう これを組み立てて１つのものを
作るべきかどうか知りたいです この概念では 口のパラメーターを持つ口のカプセルと 鼻のパラメーターを持つ鼻のカプセルがあります 次に１つに組み立てるかどうかを決めるために カプセルのそれぞれが
顔にはどんなパラメーターとなるべきかに投票させます もし口と鼻が正しい空間的関係にあれば
合意するはずです このように １つのレベルで２つのカプセルが
同じパラメーターの組み合わせに合意するような時には 次のレベルに進みます
高次元空間で合意することは非常に起こりにくいので カプセルは十中八九正しいと
想定することができます これは通常ニューラルネットで行うフィルタリングとは 非常に異なる方法です この合意によるルーチンは
ニューラルネットが限定的なデータから 汎化するのに極めて重要になると考えています この方法は視点が変わることに対応したり
セグメンテーションを行うために 非常によいものだろうと考えています ニューラルネットで今行っているよりも
統計的に圧倒的に効率的にできることを 望んでいます 今だと視点の変更に対応するには 視点変更に合わせた変更を与えて
全てで学習させています なるほど
教師あり学習の誤差伝播法のみによって学習させるのではなく 異なる方法で学習させるのですね でも 依然 教師あり学習で
行おうかと考えていますが 前方パスの仕組みは
非常に異なります 純粋な前方パスではなく
少しだけ繰り返し処理を行います 口を見つけたと思う箇所と
鼻を見つけたと思う箇所について 一緒に組み合わせると顔になるかを
決定するために 繰り返し処理を少し行います 繰り返しでは
逆誤差伝播法を使うこともできるでしょう 識別力のあるものを
試して実行することもできるでしょう トロントの私のグループで
この研究を行っています ブレインチームの一部のGoogle のチームも
仲間にいます これが今楽しんでいることです なるほど  いいですね 論文が発表されるのを
楽しみにしています もし掲載されればね(笑) あなたは数十年に渡って
深層学習の研究をしています これらの間に AI の理解や考え方は どう変わって来たかに
非常に興味があります 私の知的な歴史の多くは
誤差逆伝播法やそれをどう使うかや その力を活かす方法に
関するもののように思えます 最初から話すと
まず80年代半ばに それを評価学習に使っていて
きちんと動作していました そして90年代の初めころまでに 人間の学習はそのほとんどが
教師なし学習だと確信しました そのことから教師なし学習に
より強く興味を持つようになりました これはウェグスティン(Wegstein)アルゴリズムに
取り組んでいたころです このころのあなたのコメントは
私の考え方にも大きく影響を与えてました あなたの影響で 私が率いていたGoogle ブレインでは 最初のプロジェクトは
教師なし学習でした そうですか
間違って導いたかもしれないですね なぜなら長い目で見れば 教師なし学習は
絶対に重要になると思いますが 現実を直視する必要も出てきます 過去10年くらいで上手くいったのは
教師あり学習で 評価学習でした
この場合にはラベルがあり 連続するもので次に何が来るのかを予測しようとするものでした
これもラベルでした これは非常にうまく動作しました 私は依然として 教師なし学習が
重要になると信じていますし もし適切に動作させることができれば
今よりも本当にかなり精度が上がるでしょうが まだです そうですね 私も含めて
深層学習で経験を積んできた人は 教師なし学習にかなり興奮しています どうやればいいのか
まだ誰もほどんど分からないだけです あなたは分かるかも
私は分かりません 変分オートエンコーダーは
reparameterization trick を使っていて とても良い考えに思えます 敵対的生成ネットワーク(GAN)も
とても良い考えに思えます 敵対的生成ネットワークは 深層学習の考え方では新しいものの中では
とても根本的に異なる考えの１つです 「カプセル」を同じように成功させられればと
考えていますが 今は敵対的生成ネットワークは
大きな飛躍的進歩だと思います スパース性と「slow features」については
どうでしょうか 教師なし学習のモデルを作る原則の２つです スパース性については
君ほど有名ではないですよ でも「slow features」は
「slow」を追うのは間違いだと 思います 基本的な考えは正しいですが
変化のない特徴を狙ってはいけません 予測できるように変化する特徴を
狙うべきです これはモデル化する場合の
基本的な原則のようなものですが この場合にはデータを使って
非線形の変換をこのデータに適用します これを状態ベクトルとなるまで適用します 状態ベクトルは実は線形なのです ただカルマン・フィルターを使いながらも
まるで線形かのように扱ってはいけません 観測データから変数への変形が見つかります 変数への行列積のような線形の演算が 役割を演じます 例えば視点を変更したいとします 他の視点からの画像を作成したいとすると ピクセルから座標へと行うべきです 座標表現にたどり着いたら これは「カプセル」でできればと
思っていることですが 行列積を行い
視点を変えます そしてピクセルへと割り戻します なるほど
だから これは本当に一般的な原則です だから顔の合成の研究を行っているのですね 顔を取り入れて
非常に低次元のベクトルに圧縮し ベクトルを入れて
顔を取得する方法ですね 研究している学生はいますが
私自身はそれほど研究してません この質問はよく受けるとは思いますが もし深層学習に参入したい人は
何をすべきですか どんな助言がありますか １対１の状況では
多くの助言をしているでしょうが 世界全体の視聴者に対して 参入しようとしている人には
どんな助言をしますか いいでしょう 私の助言は文献は読んでも
読みすぎないようにということです これは私の指導者からもらったもので
ほとんどの人は言いそうにないことです ほとんどの人は
何年かは文献を読んだ後で 自分の考えを
研究し始めるべきと言います 研究者にはこれが当てはまる人もいるでしょうが
創造的な研究者なら 少し文献を読んで 他の人たちは間違っていると
あなたが思うことを探します 人と正反対ということですが 見ていて
正しくないと感じることを探します そして正しい方法を探すのです 人がそれは良くないと言った時でも
ただ続けてください 続けるようにするための良い原則を
知っています それは直感が正しかろうが
正しくなかろうが 直感が正しければ
それに従うべきですし 最終的には
成功するでしょう 直感が間違っていたら
何をしても無駄でしょう なるほど(笑) 刺激的な助言です
した方がましということですね 直感を信じたほうがましです 信じない理由がありません なるほど 私は通常 論文を読むだけではなく
再現するように助言します これは自然と
読める量を制限しています というのも結果を再現するのは
とても時間のかかることだからです 正しいです
論文を再現しようとすると 動作させるための細かな技を
色々と発見できます 他の助言としては
プログラムは続けましょうということです 優秀でない学生に宿題をさせると 戻ってきて
動作しなかったと言うでしょう 動作しなかった原因は
重要なことだと気付かない細かな 決定かもしれません UY テイのような優秀な学生に与えれば 何を宿題に出しても
戻ってきて動作させれたと言うでしょう こうしたのを覚えています
私は「UYちょっとまって この間話した後に これこれの理由で
動作するはずがないと気づいた」と伝えると UYは「すぐに気づいたので
異なることを意図していたと想像しました」と言いました (笑)
それは面白いですね どうでしょう
他には AI や深層学習に
参入したい人に助言はありますか 基本的には十分な量を読んで
直感を発展させ始めて 直感を信じて
突き進んでください みんなが無意味だと言っても
気にしすぎないでください 無意味だと言ってる人たちが
正しいかどうかが 分かる方法はないですから
突き進めて見つけるしかないということですよね もうひとつあります
あなたが本当に良い考えだと思うことを 他のひとが完全に無意味だと言われる事柄なら 何か価値のあることなのだと
分かります それの１つの例はCamp(不明)と私が
変分法を思いついた時のことです 私の学生だったピーター・ブラウンに
これを説明するメールを送りました 彼はENをよく分かっていました 彼が研究を一緒にしている人たちに
見せたところ 相手はデラ・ピエトラ・ブラザーズで確か双子です ピーターは彼らが何と言ったか教えてくれて 彼らは「このひとは酔ってるか
ただ愚かだ」と言ったそうで 彼らは本当に本当に
無意味だと思ったのです 部分的には私の説明の仕方にも
よったのかもしれません というのも直感的な言葉で
説明していました 自分が良い考えだと思っていることを 他のひとが完全にゴミだと言った時は
本当に良い考えの兆しでもあります なるほど
研究の分野の話ですが 新卒の人たちは
何をするべきでしょうか 「カプセル」、教師なし学習
ほかには何があるでしょうか 新卒の方々への良い助言としては 自分と同じような信念を持っている助言者を
探してみてください というのは 助言者が思い入れのあることを
研究していれば 助言者は良い助言や時間を
多くくれるでしょう 助言者が興味のないことを研究していると 得られるのは助言はあるでしょうが
それほど役に立つものではないでしょう なるほど
助言について最後に１つ 学習者に対しては
博士課程に行く人に対して トップ企業やトップ研究機関に行く人もいます
この人たちについてはどう感じてますか これは複雑なことです
現在起こっていることは 大学で深層学習の教育が必要な人の全員を
教えるほどの数の人は 教育者側にはいないのです 今は学部が持つ教員の人員が
十分ではないのですが これは一時的なことだと思います これまで起こっていたのは
多くの学部では起こっている革命を 理解するのに
非常に時間がかかったのです 部分的には同意しますが
第２の産業革命とは言えないものですが それに近い規模のものではあります 根本的な大変化が起こっています われわれのコンピューターとの関係が
変わってしまったからです プログラムするのではなく
見せて 勝手に分かるようになりました これは全く異なるコンピューターの使い方です コンピューター科学の学部は
コンピューターをプログラムするという考えに基づいて作られています このコンピューターに見せることが コンピューターにプログラムすることと同じくらい重要になるとは
分かっていません コンピューターに見せることで作業をさせることを
学部の教員の半分は行っているべきだということが分かって いません 私のいる学部でも
見せることを出来る人を多く抱えるべきことを 認めたがりません これが出来る人は数人か もう少しいるかもしれませんが
多くはいません この状況では 大きな企業でも多くの訓練を行う必要があるのを
覚えておいてもらう必要があります Google はブレインのレジデンスの人たちを
訓練しています 大学は最終的には
追いつくのではと想定しています なるほど 実は
多くの学生がこれに気付いたかもしれません トップクラスの博士課程の多くが
半分以上の志願者は プログラムではなく
見せることをしたいと望んでいます そして称賛されるべき点を
称賛するために言いますが deeplearning.ai が
深層学習のスペシャリゼーションを作っている一方で 私の知る限り深層学習のMOOC は
実はあなたがCoursera で さかのぼること2012年に
行われました 奇妙なことに この年にあなたは
RMSProp のアルゴリズムを発表しました そうです 知ってるでしょうけど
MOOC に誘われたからで とても懐疑的でしたが
ずっと押し続けましたね 大変な作業が伴いましたが
行って非常によかったです そうですね 行ってくれてありがとうございます
作業がどれだけのものか 苦情を言われたのを覚えています あなたは夜も残ってましたが
多くの学習者が 最初のMOOC の恩恵を受けました
私はとても感謝しています ならよかったです AI のパラダイムシフトが起こる中で
何年にもわたってあなたがAIのパラダイムの議論に 巻き込まれるのを見てきました これに関するあなたの考えを
共有してもらえますか よろこんで
さかのぼって50年代の初期のころには フォン・ノイマンやチューリンなどの人々は
シンボリックAI を信じていませんでした 脳にはるかに強く
惹きつけられていました 残念ながら彼らは若すぎて亡くなり
意見は人に届きませんでした AI の初期には 知性に必要な表現は
なんらかの種類のシンボリックな表現が必要だと 人々は完全に納得していました 単調でない事柄を実行できるような場合には
整理されたロジックが必要だと考えられていました ロジックではないのですがロジックのようなもので
知性の本質は推論と考えられていました 今では起こっているのは
完全に異なる見解があるということです 考えというものは
神経活動の巨大なベクトルだということです 考えがシンボリック表現ということとは
対照的になるものです 考えがシンボリック表現と考えた人々は
大きな間違いをしてしまったと 思います 入力するのは文字列で
出力するのは文字列です このため 文字列は
明らかに事柄を表現する方法です その中にあるのは
文字列か文字列のような何かがあると 考えたのです その中にあるのは
文字列とは異なるものだと考えます 思考がなんらかの言語の中にあるという考えは
空間世界での配置の理解が ピクセルの中にあるという考えと同様に
馬鹿げています ピクセルが入ってくるのです 仮にドットマトリックスプリンターが
人間に接続できたら ピクセルが出力されてくるでしょうが
その中にあるのはピクセルではありません 私は考えというのは超巨大なベクトルだと
巨大なベクトルは因果を考える力があると 考えます 巨大なベクトルだからです 思考がシンボリック表現という標準的なAI への考え方
全くありえないものです なるほど AI はこの新しい見解に
確かに到達しつつありますと思えています 一部です AI 領域では多くの人が
思考はシンボリック表現の必要があると今でも考えていると思います このインタビューを受けていただいて
ありがとうございます 深層学習が何十年もかけて進化したことや
今でも未来に向けてあなたが進めていることを 聞くけたのは魅力的なことでした
ジェフさん ありがとうございました いえ この機会を与えてくれてありがと ありがとうございます。