1
00:00:00,550 --> 00:00:02,370
في الفيديوهات السابقة من هذا الأسبوع،

2
00:00:02,370 --> 00:00:05,890
وكذلك في الفيديوهات
من الأسابيع العديدة السابقة،

3
00:00:05,890 --> 00:00:09,500
لقد تعرّفتم على المكونات الأساسية
للانتشار الأمامي

4
00:00:09,500 --> 00:00:14,200
والانتشار الخلفي والمكونات الرئيسية التي
تحتاجونها لتطبيق شبكة عصبية عميقة.

5
00:00:14,200 --> 00:00:17,540
دعونا نرَ كيف يمكنكم تجميع
هذه المكونات معًا لبناء شبكة عميقة.

6
00:00:18,560 --> 00:00:20,440
إليكم شبكة تحتوي على طبقات قليلة.

7
00:00:20,440 --> 00:00:21,610
ودعونا نختر طبقة واحدة.

8
00:00:22,650 --> 00:00:27,330
وننظر إلى العمليات الحسابية مع
التركيز على هذه الطبقة فقط حتى الآن.

9
00:00:27,330 --> 00:00:32,130
بالنسبة للطبقة L،
لديكم بعض المتغيرات wl

10
00:00:33,250 --> 00:00:37,638
وbl وبالنسبة للانتشار الأمامي، يمكنكم إدخال

11
00:00:37,638 --> 00:00:44,762
معلمات التنشيط a l-1
من الطبقة السابقة

12
00:00:44,762 --> 00:00:48,930
وإخراج a l.

13
00:00:48,930 --> 00:00:54,618
حسنًا، طريقة تنفيذ هذا سابقًا
كانت من خلال حساب z l =

14
00:00:54,618 --> 00:00:59,136
w l ضرب al - 1 + b l.

15
00:00:59,136 --> 00:01:07,050
ثم al = g(z) l‎.

16
00:01:07,050 --> 00:01:08,180
حسنًا.

17
00:01:08,180 --> 00:01:13,100
إذًا، هكذا تنتقلون من الإدخال
al - 1 إلى الإخراج al.

18
00:01:13,100 --> 00:01:20,900
وقد تبين أنه للاستخدام اللاحق،
فسيكون من المفيد أيضًا أن يتم تخزين القيمة zl مؤقتًا.

19
00:01:20,900 --> 00:01:25,640
لذا، دعوني أدرج هذا التخزين المؤقت أيضًا
لأن تخزين قيمة zl

20
00:01:25,640 --> 00:01:30,930
سيكون مفيدًا للخلف
لخطوة الانتشار الخلفي لاحقًا.

21
00:01:30,930 --> 00:01:35,854
ثم للخطوة للخلف أو
لخطوة الانتشار الخلفي، مرة أخرى،

22
00:01:35,854 --> 00:01:37,760
من خلال التركيز على حساب هذه الطبقة l،

23
00:01:37,760 --> 00:01:43,530
فإنكم تنفّذون الدالة
التي تتضمن إدخال da(l)‎.

24
00:01:45,830 --> 00:01:53,750
وإخراج da(l-1)‎
وفقط للتوسع في التفاصيل،

25
00:01:53,750 --> 00:01:59,230
يكون الإدخال da(l)‎ بالفعل،
وكذلك التخزين المؤقت

26
00:01:59,230 --> 00:02:04,230
لذا فإنه تتوفر لكم قيمة zl
التي قمتم بحسابها

27
00:02:04,230 --> 00:02:10,150
وبالإضافة إلى ذلك، من خلال إخراج da(l)‎
ناقص 1، فإنكم تحضرون الإخراج

28
00:02:10,150 --> 00:02:13,100
أو التدرجات التي تريدونها
لتنفيذ انحدار التدرج

29
00:02:13,500 --> 00:02:14,330
للتعلم، حسنًا؟

30
00:02:14,330 --> 00:02:19,830
فهذه هي البنية الأساسية
لكيفية تنفيذ هذه الخطوة إلى الأمام،

31
00:02:19,830 --> 00:02:23,050
والتي نطلق عليها الدالة الأمامية
وكذلك هذه الخطوة للوراء،

32
00:02:23,050 --> 00:02:25,170
والتي سنطلق عليها الدالة الخلفية.

33
00:02:25,170 --> 00:02:28,150
إذًا لتلخيص ذلك فقط، في الطبقة l،

34
00:02:28,150 --> 00:02:32,250
ستكون لديكم الخطوة للأمام أو
الانتشار الأمامي للدالة الأمامية.

35
00:02:32,250 --> 00:02:39,360
والإدخال al- 1 والإخراج al،

36
00:02:39,360 --> 00:02:45,300
ولإجراء هذه العملية الحسابية
فإنكم تحتاجون إلى استخدام wl وbl.

37
00:02:45,300 --> 00:02:52,690
وكذلك إخراج تخزين مؤقت
يحتوي على zl، حسنًا؟

38
00:02:52,690 --> 00:02:56,300
ثم الدالة "خطوات للخلف"،
باستخدام خطوة الانتشار الخلفي،

39
00:02:56,300 --> 00:03:01,170
ستكون دالة أخرى تحتوي على

40
00:03:01,170 --> 00:03:08,460
المدخلات da(l)‎ والمخرجات da(l-1)‎.

41
00:03:08,460 --> 00:03:14,390
ويعني هذا، أنه في ضوء المشتقات
المتعلقة بحالات التنشيط هذه،

42
00:03:14,390 --> 00:03:17,400
da(l)‎ هذه، ما المشتقات؟

43
00:03:17,400 --> 00:03:18,540
ما المقدار الذي أريده؟

44
00:03:18,540 --> 00:03:23,170
تعلمون أن al- 1 يغيّر
المشتقات المحسوبة مع مراعاة معلمات إلغاء التنشيط

45
00:03:23,170 --> 00:03:24,580
من طبقة سابقة.

46
00:03:25,380 --> 00:03:26,800
داخل هذا المربع، حسنًا؟

47
00:03:26,800 --> 00:03:31,610
يجب عليكم استخدام wl وbl،
وقد تبين أنكم في نهاية المطاف

48
00:03:31,610 --> 00:03:36,910
تحسبون dzl، ثم هذا المربع،

49
00:03:36,910 --> 00:03:42,790
قد يكون إخراج دالة "خطوات للخلف"
هذه هو dwl وdbl،

50
00:03:42,790 --> 00:03:47,360
ولكنني أستخدم أحيانًا الأسهم الحمراء
للإشارة إلى التكرار الخلفي.

51
00:03:47,360 --> 00:03:50,520
لذا إذا كنتم تفضلون،
يمكننا رسم هذه الأسهم باللون الأحمر.

52
00:03:51,680 --> 00:03:55,450
إذا كان بإمكانكم تنفيذ
هاتين الدالتين

53
00:03:55,450 --> 00:03:59,660
فسيكون الحساب الأساسي
للشبكة العصبية كما يلي.

54
00:03:59,660 --> 00:04:05,120
ستأخذون الإدخال الذي يبرز a0،
وتعملون على تزويد ذلك،

55
00:04:05,120 --> 00:04:10,170
وسيؤدي ذلك إلى حساب حالات تنشيط
الطبقة الأولى، لنطلق عليها a1

56
00:04:10,170 --> 00:04:16,180
وللقيام بذلك، تحتاجون إلى w1
وb1 وبعد ذلك،

57
00:04:16,180 --> 00:04:19,760
ستقومون بتخزين z1 مؤقتًا، حسنًا؟

58
00:04:21,520 --> 00:04:26,340
الآن وبعد القيام بذلك، فإنكم تعملون على توفير ذلك
إلى الطبقة الثانية، ثم تستخدمون w2 وb2،

59
00:04:26,340 --> 00:04:34,330
فإنكم تحسبون حالات
إلغاء التنشيط في الطبقة التالية a2 وما إلى ذلك.

60
00:04:34,330 --> 00:04:38,110
حتى تُخرجوا في النهاية

61
00:04:38,110 --> 00:04:43,540
a l وهو ما يساوي y hat.

62
00:04:43,540 --> 00:04:50,160
وخلال سير العملية،
قمنا بتخزين جميع قيم z هذه مؤقتًا،

63
00:04:52,660 --> 00:04:55,030
فهذه هي خطوة الانتشار الأمامي.

64
00:04:55,370 --> 00:04:59,600
الآن، لخطوة الانتشار الخلفي
ما سنقوم به هو

65
00:04:59,600 --> 00:05:03,960
تسلسل للخلف لحالات التكرار

66
00:05:05,260 --> 00:05:09,960
التي تتجهون فيها للخلف
وحساب التدرجات مثل ذلك.

67
00:05:12,260 --> 00:05:17,560
لذا فإن ما ستقومون بتزويده هنا،
هو da(l)‎

68
00:05:17,560 --> 00:05:30,700
ثم سيعطينا هذا المربع da(l- 1)‎
وما إلى ذلك حتى نحصل على da(2) da(1)‎.

69
00:05:30,950 --> 00:05:36,940
ويمكنكم بالفعل الحصول على
نتيجة أخرى لحساب da(0)‎،

70
00:05:36,940 --> 00:05:38,650
ولكن هذه المشتقة بالنسبة إلى

71
00:05:38,650 --> 00:05:40,950
ميزات الإدخال، والتي
لا تعد مفيدة على الأقل

72
00:05:40,950 --> 00:05:46,700
لتدريب أوزان هذه
الشبكات العصبية الخاضعة للإشراف،

73
00:05:46,700 --> 00:05:49,180
وبالتالي يمكنكم التوقف هنا.
ولكن خلال سير العملية،

74
00:05:49,180 --> 00:05:54,680
فإن الانتشار الخلفي أيضًا
ينتهي به الحال بالناتج dwl، وdbl.

75
00:05:54,680 --> 00:05:59,170
فقط استخدمت المطالبة كـ wl وbl.

76
00:05:59,170 --> 00:06:06,750
وسيؤدي هذا إلى إخراج dw3 وdb3 وما إلى ذلك.

77
00:06:10,500 --> 00:06:13,510
حسنًا، لقد انتهيتم من حساب كل
المشتقات التي تحتاجون إليها.

78
00:06:16,560 --> 00:06:21,110
وفقط لتعبئة بنية
هذا بصورة أكثر قليلاً،

79
00:06:21,110 --> 00:06:24,380
ستستخدم هذه المربعات
هذه المتغيرات أيضًا.

80
00:06:26,180 --> 00:06:31,930
wl وbl وقد تبين أننا

81
00:06:31,930 --> 00:06:37,400
سنرى لاحقًا أننا داخل هذه المربعات
سينتهي بنا الحال ونحن نحسب dz أيضًا.

82
00:06:37,400 --> 00:06:42,250
لذا فإن تكرار واحد للتدريب من خلال
شبكة عصبية ينطوي على: البدء بـ

83
00:06:42,250 --> 00:06:46,930
a(0)‎ وهي x ومن خلال
المرور في الانتشار الأمامي كما يلي،

84
00:06:46,930 --> 00:06:50,840
وحساب ŷ ثم
استخدام هذا لحساب ذلك،

85
00:06:50,840 --> 00:06:56,480
ثم القيام بذلك في الانتشار الخلفي

86
00:06:56,480 --> 00:07:01,560
والآن، لديكم كل هذه المصطلحات المشتقة
وبالتالي فإنكم تعلمون،

87
00:07:01,560 --> 00:07:06,370
w سيتم تحديثها إلى w1 =
معدل التعلم مضروبًا في dw، حسنًا؟

88
00:07:06,370 --> 00:07:13,260
لكل الطبقات
وبالمثل لمعدل b.

89
00:07:13,260 --> 00:07:17,690
الآن يحتوي الانتشار الخلفي
المحسوب على كل هذه المشتقات.

90
00:07:17,690 --> 00:07:21,930
لذا، فإن هذا تكرار للانحدار
التدريجي للشبكة العصبية.

91
00:07:21,930 --> 00:07:25,390
الآن قبل المضي قدمًا، لدينا
واحدة من التفاصيل المعلوماتية الإضافية.

92
00:07:25,390 --> 00:07:30,110
نظريًا، سيكون من المفيد أن يتم
التفكير في التخزين المؤقت هنا كتخزين

93
00:07:30,110 --> 00:07:34,110
لقيمة z لدوال
"خطوات للخلف"،

94
00:07:34,110 --> 00:07:37,130
ولكن عند تنفيذ ذلك،
وترون ذلك في تمرين البرمجة،

95
00:07:37,130 --> 00:07:40,060
عندما تنفذون ذلك،
فإنكم تجدون أن التخزين المؤقت قد يمثل

96
00:07:40,060 --> 00:07:43,650
طريقة مريحة للحصول على هذه
القيمة الخاصة بمتغيرات w1 وb1،

97
00:07:43,650 --> 00:07:46,510
في دالة "خطوات للخلف" أيضًا.
لذا بالنسبة

98
00:07:46,510 --> 00:07:51,000
لهذا التمرين، فإنكم تخزنون بالفعل في ذاكرة
التخزين المؤقت لديكم لـ z كلٍ من w

99
00:07:51,000 --> 00:07:59,800
وb أيضًا. لذا فإن هذا يخزن z2 وw2 وb2.
ولكن من منطلق التنفيذ،

100
00:07:59,800 --> 00:08:03,790
وجدت هذه الطريقة مريحة
لنسخ المعلمات،

101
00:08:03,790 --> 00:08:08,630
إلى أينما تريدون استخدامها لاحقًا
عند حسابكم للانتشار الخلفي.

102
00:08:08,630 --> 00:08:12,030
وهذه مجرد تفاصيل تنفيذية
ترونها عند

103
00:08:12,030 --> 00:08:15,330
ممارسة تمرين البرمجة.

104
00:08:15,330 --> 00:08:18,000
إذًا، لقد رأيتم الآن
المكونات الأساسية

105
00:08:18,000 --> 00:08:19,910
لتنفيذ شبكة عصبية عميقة.

106
00:08:19,910 --> 00:08:22,040
وفي كل طبقة، هناك
خطوة انتشار أمامي

107
00:08:22,040 --> 00:08:24,270
وهناك خطوة
انتشار خلفي مقابلة.

108
00:08:24,270 --> 00:08:27,870
وتخزين مؤقت لإيداع المعلومات
من خطوة إلى الأخرى.

109
00:08:27,870 --> 00:08:28,810
ففي الفيديو التالي،

110
00:08:28,810 --> 00:08:32,190
سنتحدث حول كيف يمكنكم
تنفيذ هذه المكونات الأساسية.

111
00:08:32,190 --> 00:08:33,340
لننتقل الى الفيديو التالي.