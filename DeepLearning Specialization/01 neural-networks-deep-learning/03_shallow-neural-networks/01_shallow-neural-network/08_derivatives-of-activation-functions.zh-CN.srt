1
00:00:00,302 --> 00:00:04,340
当你在神经网络中用反向传播算法时，你需要

2
00:00:04,340 --> 00:00:07,813
计算激活函数的导数（斜率）。

3
00:00:07,813 --> 00:00:11,333
那就让我们一起看看有哪些可选择的激活函数，

4
00:00:11,333 --> 00:00:14,025
而又该怎样计算这些函数的斜率。

5
00:00:14,025 --> 00:00:17,239
这是大家都熟知的sigmoid激活函数。

6
00:00:17,239 --> 00:00:22,252
所以给定z一个任意值 比如这个值 这个函数将

7
00:00:22,252 --> 00:00:27,184
给出对应的某个斜率，或者某个导数，

8
00:00:27,184 --> 00:00:32,389
如果你在那里画一条短线，以短线为弦的小三角形的高与底边的比值就是斜率。

9
00:00:32,389 --> 00:00:39,823
所以如果g(z)是Sigmoid函数，那么函数的斜率就是d/dz g(z)。

10
00:00:39,823 --> 00:00:47,034
我们通过计算得到g(x)在z值上的斜率。

11
00:00:47,034 --> 00:00:52,513
如果你对微积分十分熟悉并且知道怎样计算导数，

12
00:00:52,513 --> 00:00:56,074
如果对sigmoid函数求导，

13
00:00:56,074 --> 00:01:00,300
可以导出和这里一样的公式。

14
00:01:00,300 --> 00:01:02,380
再一次声明，我不会在这里做演算步骤。

15
00:01:02,380 --> 00:01:04,839
但如果你熟悉微积分

16
00:01:04,839 --> 00:01:08,465
可以随意暂停视频，尝试自己证明求导过程。

17
00:01:08,465 --> 00:01:16,583
所以这里的导数刚好等于g(z)*(1-g(z))。

18
00:01:16,583 --> 00:01:20,923
让我们对这个表达式做个完整性检查看是否合理。

19
00:01:20,923 --> 00:01:26,343
首先，如果z非常大，假如z=10,

20
00:01:26,343 --> 00:01:30,315
那么g(z)将接近于1，并且

21
00:01:30,315 --> 00:01:38,758
左边的公式告诉我们d/dz g(z)的值接近于g(z)

22
00:01:38,758 --> 00:01:42,979
这里g(z)等于1,g(z)*(1-g(z))等于1乘以(1-1),

23
00:01:42,979 --> 00:01:46,851
因此非常接近于0。

24
00:01:46,851 --> 00:01:51,267
这确实是正确的，因为当z值非常大时,斜率接近于0。

25
00:01:51,267 --> 00:01:58,885
相反的，如果z=-10,远在那头，那么g(z)值接近于0。

26
00:01:58,885 --> 00:02:01,765
那左边的公式告诉我们，

27
00:02:01,765 --> 00:02:07,333
d/dzg(z)将接近于，这里g(z)等于0，g(z)*(1-g(z))等于0乘以(1-0)。

28
00:02:07,333 --> 00:02:10,005
所以这也非常接近于0，这也是正确的。

29
00:02:10,005 --> 00:02:16,889
最后如果z等于0，那么g(z)等于0.5。

30
00:02:16,889 --> 00:02:18,983
这就是sigmoid函数。

31
00:02:18,983 --> 00:02:24,040
因此导数等于0.5乘以(1-0.5),

32
00:02:24,040 --> 00:02:28,459
导数值等于1/4。

33
00:02:28,459 --> 00:02:33,135
这也是当z等于0时导数的正确取值

34
00:02:33,135 --> 00:02:35,831
或者说函数的斜率。

35
00:02:35,831 --> 00:02:38,839
最后，再介绍一点标记上的问题

36
00:02:38,839 --> 00:02:42,321
有时，代替d/dz g(z)这种写法

37
00:02:42,321 --> 00:02:46,087
导数的简写法是g'(z)。

38
00:02:46,087 --> 00:02:52,167
所以在微积分中，g'(z)这里的短撇也叫做prime(角分号)。

39
00:02:52,167 --> 00:02:55,283
因为g'(z)是微积分中的简写

40
00:02:55,283 --> 00:03:00,455
代表对函数g求对输入变量z的求导。

41
00:03:00,455 --> 00:03:07,513
在神经网络中，我们令a等于g(z),

42
00:03:07,513 --> 00:03:11,351
就像这样。

43
00:03:11,351 --> 00:03:17,145
那么这里也可以简写为a乘以(1-a)。

44
00:03:17,145 --> 00:03:20,109
所以书写上有时

45
00:03:20,109 --> 00:03:25,256
你会看到g'(z)等于a乘以(1-a)。

46
00:03:25,256 --> 00:03:29,040
那就说明，g'这个表达式

47
00:03:29,040 --> 00:03:33,428
即表示导数，就等于这边的式子。

48
00:03:33,428 --> 00:03:38,397
然后这个公式的优点在于
如果你已经计算出了a的值

49
00:03:38,397 --> 00:03:43,078
那么通过这个表达式，你可以很快算出

50
00:03:43,078 --> 00:03:45,038
g'的斜率。

51
00:03:45,038 --> 00:03:47,632
好了，所以这就是sigmoid激活函数的导数。

52
00:03:47,632 --> 00:03:51,172
现在我们来看tanh激活函数。

53
00:03:51,172 --> 00:03:56,311
和前面的讨论相似，d/dz g(z)的定义

54
00:03:56,311 --> 00:04:01,155
就是g(z)在特定z点上的斜率。

55
00:04:01,155 --> 00:04:07,747
观察一下双曲正切函数的例子，

56
00:04:07,747 --> 00:04:13,066
如果你微积分学得不错，你就可以求导

57
00:04:13,066 --> 00:04:17,359
并证明这个式子可以简化为这个式子。

58
00:04:20,813 --> 00:04:23,997
我们可以用之前的写法

59
00:04:23,997 --> 00:04:27,019
称这个为g'(z)。

60
00:04:27,019 --> 00:04:30,941
你可以去检查这个式子有没有错。

61
00:04:30,941 --> 00:04:37,005
例如，当z=10，tanh(z)就会非常接近1。

62
00:04:37,005 --> 00:04:41,309
这是从+1到-1的函数。

63
00:04:41,309 --> 00:04:45,183
那么根据这个公式，g'(z)

64
00:04:45,183 --> 00:04:48,148
大概就是1-1^2，即为0.

65
00:04:48,148 --> 00:04:53,930
所以如果z非常大，斜率就接近于0.

66
00:04:53,930 --> 00:04:58,760
相对地，如果z非常小，比如z=-10,

67
00:04:58,760 --> 00:05:02,440
tanh(z)就会接近-1

68
00:05:02,440 --> 00:05:07,871
那么g'(z)就约等于是1-(-1)^2，

69
00:05:07,871 --> 00:05:12,792
也就是约等于1-1，约等于0。

70
00:05:12,792 --> 00:05:18,421
最后，若z=0，那么tanh(z)=0.

71
00:05:18,421 --> 00:05:22,165
从而斜率就等于1，

72
00:05:22,165 --> 00:05:26,433
这就是z=0时的斜率。

73
00:05:26,433 --> 00:05:33,001
总而言之，如果a=g(z)，如果a等于这个tanh(z)

74
00:05:33,001 --> 00:05:38,300
那么导数g'(z)就等于1-a^2

75
00:05:38,300 --> 00:05:42,000
那么再次，如果你已经算出了a的值

76
00:05:42,000 --> 00:05:46,522
你可以用这个式子来很快算出导数。

77
00:05:46,522 --> 00:05:49,618
最后，我们来看看如何计算ReLU的导数

78
00:05:49,618 --> 00:05:51,338
以及带泄漏的ReLU激活函数的导数。

79
00:05:51,338 --> 00:05:57,866
对于ReLU, g(z)=Max(0,z)

80
00:05:57,866 --> 00:06:03,330
那么当z<0时，导数就是0

81
00:06:03,330 --> 00:06:09,066
z>0导数就是1

82
00:06:09,066 --> 00:06:15,498
然后在z精确等于0处，斜率是没有定义的

83
00:06:15,498 --> 00:06:18,339
但如果你在软件中实现这个算法 如果z刚好在0处

84
00:06:18,339 --> 00:06:21,398
可能数学上不是百分之百正确

85
00:06:21,398 --> 00:06:25,571
但实际上是行得通的

86
00:06:25,571 --> 00:06:30,501
你可以令导数为1，或者令导数为0

87
00:06:30,501 --> 00:06:31,781
这都是可以的。

88
00:06:31,781 --> 00:06:33,578
如果你对优化领域的术语很熟悉，

89
00:06:33,578 --> 00:06:37,109
g‘就变成所谓激活函数g(z)的次梯度

90
00:06:37,109 --> 00:06:41,360
这样梯度下降法仍然有效

91
00:06:41,360 --> 00:06:47,327
但你可以这样想，z精确为0的概率非常小

92
00:06:47,327 --> 00:06:52,805
所以你把z=0处的导数设为哪个值都无所谓

93
00:06:52,805 --> 00:06:54,303
<br />

94
00:06:54,303 --> 00:06:59,155
所以在实践中，人们一般这么定z的导数

95
00:06:59,155 --> 00:07:03,712
最后，如果你在训练自己的神经网络

96
00:07:03,712 --> 00:07:06,882
使用带泄漏的ReLU激活函数时

97
00:07:06,882 --> 00:07:12,244
g(z)就是max(0.01z,z)

98
00:07:12,244 --> 00:07:15,247
那么z<0时，g'(z)就等于0.01

99
00:07:15,247 --> 00:07:19,074
z>0时，g’(z)就等于1

100
00:07:19,074 --> 00:07:20,477
我们再来看看我们再来看看

101
00:07:20,477 --> 00:07:26,403
z精确为0时的导数也是没有定义的

102
00:07:26,403 --> 00:07:31,479
但你可以写一段代码去定义这个梯度

103
00:07:31,479 --> 00:07:38,353
令g'(z)等于0.01或1，都是可以的

104
00:07:38,353 --> 00:07:41,499
你的代码都可以运行

105
00:07:41,499 --> 00:07:45,594
掌握了这些式子，你应该可以计算

106
00:07:45,594 --> 00:07:48,400
你的激活函数的斜率或导数

107
00:07:48,400 --> 00:07:50,553
现在，你有了这个基础工具

108
00:07:50,553 --> 00:07:54,502
就已经准备好去学习如何在你的神经网络上实现梯度下降算法了。

109
00:07:54,502 --> 00:07:57,309
让我们来看下一个视频