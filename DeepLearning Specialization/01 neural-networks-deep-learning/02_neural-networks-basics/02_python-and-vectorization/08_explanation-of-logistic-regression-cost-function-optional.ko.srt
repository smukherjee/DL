1
00:00:00,390 --> 00:00:03,860
이전 비디오에서는 로지스틱 회귀분석에 대한

2
00:00:03,860 --> 00:00:05,230
비용함수 공식을 적어드렸는데요

3
00:00:05,230 --> 00:00:09,370
이번 선택적 비디오를 통해서는, 
왜 로지스틱 회귀분석에서 이러한 

4
00:00:09,370 --> 00:00:13,490
비용함수를 쓰는 것이 좋은지 입증하는 시간을
갖도록 하겠습니다.

5
00:00:13,490 --> 00:00:17,709
빠르게 복습하자면, 로지스틱 회귀분석에서는

6
00:00:17,709 --> 00:00:23,704
ŷ 이라는 예측을 w transpose x 더하기 b의 시그모이드라고 합니다.

7
00:00:23,704 --> 00:00:27,711
시그모이드는 여기서 보이는 익숙함 함수이고요. 

8
00:00:27,711 --> 00:00:34,353
그리고 ŷ 를 p(y=1 | x)라고 해석한다고 했었죠.

9
00:00:34,353 --> 00:00:39,776
궁극적으로 우리의 알고리즘은 ŷ 이 

10
00:00:39,776 --> 00:00:45,030
입력 값 x에 대해서 y가 1인 경우의 확률을 나타내주길 바랍니다.

11
00:00:45,030 --> 00:00:50,090
다시 말해, y가 1인 경우, 

12
00:00:50,090 --> 00:00:56,020
x가 ŷ 일 때의 y 확률이라고 할 수 있습니다.

13
00:00:56,020 --> 00:00:59,170
반대로, 만약 y가 0이면,

14
00:01:00,310 --> 00:01:05,840
y가 0인 경우는 1-y겠죠. 맞죠? 

15
00:01:05,840 --> 00:01:09,150
그래서 만약 ŷ 이 y가 1일 확률이면

16
00:01:09,150 --> 00:01:13,620
1-ŷ 은 y가 0일 확률이 됩니다.

17
00:01:13,620 --> 00:01:18,057
자 그럼 마지막 2개의 공식을 가지고 와서
다음 슬라이드에 복사하겠습니다.

18
00:01:18,057 --> 00:01:22,684
제가 무엇을 할 것이냐면
이 2가지의 공식을 갖고

19
00:01:22,684 --> 00:01:28,010
각각 이 공식은 y=0 인 경우와 y=1인 경우에
p(y | x)를 정의해주는데요.

20
00:01:28,010 --> 00:01:33,110
2개의 공식을 한가지 공식으로 요약해보겠습니다.

21
00:01:33,110 --> 00:01:37,543
다시 말씀 드리자면, y는 반드시 0또는 1이여야 합니다.
2진법 분류에서는 

22
00:01:37,543 --> 00:01:41,110
2가지 가능한 케이스밖에 없기 때문이죠. 

23
00:01:41,110 --> 00:01:44,653
2개의 공식을 1개로 요약하면, 

24
00:01:44,653 --> 00:01:48,774
자, 먼저 일단 공식이 어떻게 생겼는지 적어보겠습니다. 왜 이 공식이 되는지 정확한 유는 나중에 설명 드리겠습니다.

25
00:01:48,774 --> 00:01:54,040
그럼 (1-ŷ)(1-y) 이죠.

26
00:01:54,040 --> 00:01:58,920
1가지의 선이 위의 2개의 공식을 요약해줍니다. 

27
00:01:58,920 --> 00:02:00,500
왜 그런지 말씀 드리죠. 

28
00:02:00,500 --> 00:02:04,643
첫 번째 경우, y=1이라고 해봅시다. 맞죠?

29
00:02:04,643 --> 00:02:09,562
만약 y=1인 경우, 이 항은 ŷ 이 됩니다.

30
00:02:09,562 --> 00:02:13,970
그 이유는 이 것이 ŷ 의 1승이기 때문이죠. 

31
00:02:13,970 --> 00:02:21,120
이 항은 1-ŷ 의 1승 빼기 1이 되는데요. 이것은 그럼 0승입니다.

32
00:02:21,120 --> 00:02:26,320
하지만 어떤 한 값의 0승은 1이죠. 그럼 이건 사라집니다.

33
00:02:26,320 --> 00:02:33,030
그러므로 이 공식은, y가 1인 경우 p(y | x)=ŷ 이죠.

34
00:02:33,030 --> 00:02:37,480
저희가 원했던 대로 값이 나왔습니다.

35
00:02:37,480 --> 00:02:40,250
2번째 경우는 어떨까요? y=0인 경우 말이죠. 

36
00:02:40,250 --> 00:02:47,057
y=0이면, 위의 공식은 p(y | x) = ŷ 의 0승입니다.

37
00:02:47,057 --> 00:02:51,920
어떠한 값의 0승은 1이기 때문에

38
00:02:51,920 --> 00:02:58,267
이것은 1 곱하기 1-ŷ 의 1-y승입니다.

39
00:02:58,267 --> 00:03:02,770
그래서 1-y는 1-0, 즉 1이 됩니다.

40
00:03:02,770 --> 00:03:07,610
그럼 이 값은 1 곱하기 1-ŷ 그래서 1-ŷ 이 됩니다.

41
00:03:10,700 --> 00:03:17,230
여기서는 y가 0인 경우, p (y | x) = 1- ŷ 이 됩니다.

42
00:03:17,230 --> 00:03:21,570
정확히 저희가 원했던 값입니다.

43
00:03:21,570 --> 00:03:23,690
여기서 보여드린 것은 이 공식이 

44
00:03:25,330 --> 00:03:30,331
p(y l x)의 올바른 정의라는 것입니다.

45
00:03:30,331 --> 00:03:36,513
이 함수의 규칙은 단순 증가하는 함수이기 때문에

46
00:03:36,513 --> 00:03:43,223
최대화를 통해… 비슷한 결과는

47
00:03:43,223 --> 00:03:48,672
p(y | x) 최적화합니다. p(y | x)의 log를 계산하 보면

48
00:03:48,672 --> 00:03:54,330
=log ŷ[y](1- ŷ)(1-y) 값인데요

49
00:03:54,330 --> 00:03:59,818
그렇게 해서 y log ŷ 더하기 1

50
00:03:59,818 --> 00:04:05,881
빼기 y 곱하기 로그 1 빼기 ŷ 이겠죠. 맞죠?

51
00:04:05,881 --> 00:04:09,832
그래서 이것은 사실 loss함수의 마이너스 값인데요

52
00:04:09,832 --> 00:04:14,310
이전에 찾아야 했던 값과 같습니다.

53
00:04:14,310 --> 00:04:17,470
여기 마이너스 부호가 있는데요, 그 이유는

54
00:04:17,470 --> 00:04:20,460
러닝 알고리즘을 트레이닝 시키는 경우 확률은 높이고 싶지만

55
00:04:20,460 --> 00:04:23,980
로지스틱 회귀분석에서는 이것을 나타냅니다.

56
00:04:23,980 --> 00:04:25,820
loss 함수를 최소화시키고 싶은 것입니다.

57
00:04:25,820 --> 00:04:30,640
loss를 최소화시킨다는 의미는 확률의 log를 최대화시키는 것과 동일합니다.

58
00:04:30,640 --> 00:04:33,925
단일의 예시에서의 loss 함수는 이렇게 생겼습니다.

59
00:04:33,925 --> 00:04:35,435
이 비용함수는 어떨까요?

60
00:04:35,435 --> 00:04:40,435
m개의 예시를 갖춘 전체 트레이닝 세트에 대한 전체 비용함수 말이죠. 알아보도록 하겠습니다.

61
00:04:40,435 --> 00:04:41,385
알아봅시다.

62
00:04:41,385 --> 00:04:45,710
트레이닝 세트에 있는 모든 레이블에 대한 확률 

63
00:04:45,710 --> 00:04:47,750
대충 비형식적으로 쓰겠습니다.

64
00:04:47,750 --> 00:04:51,945
따로 그린 트레이닝 예시들은

65
00:04:51,945 --> 00:04:54,198
drawn IID로 그리는데요 identically independently distributed의 약자입니다.

66
00:04:54,198 --> 00:04:57,810
예시의 확률은 확률들의 곱입니다. 

67
00:04:57,810 --> 00:05:03,143
i값은 1에서 m까지 p(y(i) ) l x(i) 값입니다.

68
00:05:03,143 --> 00:05:07,970
그러면 최대 확률 추정치를 구하는 경우

69
00:05:07,970 --> 00:05:12,476
최대화 시켜야하는데요, 
먼저, observation과 트레이닝세트의 최대화 값 확률을 

70
00:05:12,476 --> 00:05:15,948
주는 매개변수를 찾아야 합니다.

71
00:05:15,948 --> 00:05:20,200
이것을 최대화시키는 것은 로그를 최대화시키는 것과 동일합니다.

72
00:05:20,200 --> 00:05:22,990
그러므로, 양쪽에 로그를 적용시킵니다.

73
00:05:22,990 --> 00:05:28,640
그러면, 트레이닝 세트 레이블의 확률의 log값은 

74
00:05:28,640 --> 00:05:30,990
곱한 값의 로그는 로그의 합이죠.

75
00:05:30,990 --> 00:05:39,000
그러면 i=1에서 m까지 log p(y(i)) l x(i) 공식에 적용시킵니다.

76
00:05:39,000 --> 00:05:43,582
이전 슬라이드에서 찾았듯이 

77
00:05:43,582 --> 00:05:47,630
이 값은 –L (ŷ(i), y(i))입니다. 

78
00:05:48,850 --> 00:05:55,220
통계 원리가 있는데, 최대확률측정법 (principle of maximum likelihood estimation)
라고하는 원리가 있습니다.

79
00:05:55,220 --> 00:06:00,720
단순히 이야기해서 이 값을 최대화 시킬 수 있는
매개변수를 고르라는 것입니다.

80
00:06:00,720 --> 00:06:04,220
다르게 이야기하면, 
이 값을 최대화 시키는 것입니다.

81
00:06:04,220 --> 00:06:09,510
= - SUM i=1 m L(ŷ, <i>,y<i>)

82
00:06:09,510 --> 00:06:11,840
합 공식에서 마이너스 부호를 바깥으로 빼면 됩니다.

83
00:06:11,840 --> 00:06:15,749
이 값의 J(w, b)이기도 한 로지스틱 회귀분석을 기반으로 한  

84
00:06:15,749 --> 00:06:21,235
비용에 대해 합리화시켜줍니다.

85
00:06:21,235 --> 00:06:27,349
이제는 비용을 최대화가 아닌 최소화하는 확률을 구해야 하기 때문에

86
00:06:27,349 --> 00:06:30,095
이 마이너스 부호를 빼줍니다.

87
00:06:30,095 --> 00:06:35,467
편의를 위해, 양에 대한 부분은 스케일 하여

88
00:06:35,467 --> 00:06:39,310
1/m 에서 m은 추가 scaling factor입니다.

89
00:06:39,310 --> 00:06:43,960
요약하자면, J(w, b)를 최소화하여 

90
00:06:43,960 --> 00:06:48,430
로지스틱 회귀분석모델을 바탕으로 최대확률측정법 을 진행합니다.

91
00:06:48,430 --> 00:06:53,120
우리의 트레이닝 example이 IID 또는 

92
00:06:53,120 --> 00:06:55,530
identically independently distributed 됐다는 가정하에 말이죠. 

93
00:06:55,530 --> 00:06:59,550
이번 비디오가 선택적이었음에도 시청해주셔서 감사합니다.

94
00:06:59,550 --> 00:07:03,845
로지스틱 회귀분석에서 왜 이런 비용함수를 쓰는지 

95
00:07:03,845 --> 00:07:05,200
조금 더 감이 오셨으면 좋겠는데요.

96
00:07:05,200 --> 00:07:09,287
다음은, pro exercise와 

97
00:07:09,287 --> 00:07:11,277
이번주 퀴즈로 넘어가시길 바랍니다.

98
00:07:11,277 --> 00:07:14,735
이어지는 퀴즈와 exercise에서 행운을 빕니다.