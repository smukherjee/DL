سمعنا جميعًا بأن
الشبكات العصبية العميقة تعمل بشكل جيد فيما يتعلق بكثير من المشاكل، والأمر لا يتعلق
بهذا فحسب فهي يجب أن تكون شبكات عصبية كبيرة على وجه التحديد، يجب أن تكون عميقة
أو أن تحتوي على الكثير من الطبقات المخفية. إذًا لماذا هذا؟ دعونا نستعرض بعض الأمثلة
ونحاول تخمين الأسباب التي تجعل الشبكات
العميقة قد تعمل جيدًا. أولاً، ما المقصود
بالحوسبة العميقة للشبكة؟ إذا كنتم تقومون بإنشاء نظام
للتعرف على الوجه أو اكتشاف الوجه، فإليكم
ما قد تفعله الشبكة العصبية العميقة. ربما تقومون بإدخال صورة لوجه، وبعد ذلك
يمكنكم اعتبار الطبقة الأولى من الشبكة العصبية بمثابة مكتشفة
للميزات أو كاشفة للحواف. في هذا المثال، سأرسم ما المقصود بالشبكة
العصبية من خلال ربما 20 وحدة مخفية، وهي ربما محاولة للحساب على هذه الصورة. وبالتالي تم تمثيل الـ 20 وحدة مخفية مرئيًا
من خلال هذه المربعات الصغيرة. فعلى سبيل المثال، يمثل هذا
الرسم المرئي الصغير وحدة مخفية تحاول معرفة أين توجد
حواف هذا التوجه في الصورة. وربما تحاول هذه
الوحدة المخفية معرفة أين توجد الحواف الأفقية
في هذه الصورة. وعندما نتحدث حول الشبكات الالتفافية
في دورة تدريبية لاحقة، فسيكون هذا التمثيل المرئي
منطقيًا أكثر. ولكن يمكنكم التفكير في الطبقة الأولى
للشبكة العصبية من خلال النظر إلى الصورة ومحاولة معرفة
أين توجد الحواف في هذه الصورة. والآن دعونا نفكر في مكان الحواف
في هذه الصورة من خلال تجميع وحدات البكسل معًا لتشكيل الحواف. ويمكن بعد ذلك الكشف عن الحواف
وتجميعها معًا لتشكيل أجزاء الوجه. فعلى سبيل المثال، قد تكون لديكم خلية عصبية
صغيرة تحاول معرفة ما إذا كان يمكنها اكتشاف عين، أو خلية عصبية مختلفة تحاول
اكتشاف هذا الجزء من الأنف. ومن خلال تجميع الكثير من الحواف معًا، يمكن البدء في اكتشاف
أجزاء مختلفة من الوجوه. وأخيرًا من خلال تجميع
أجزاء مختلفة من الوجوه، مثل عين أو أنف أو أذن أو ذقن،
يمكن بعدها أن تحاول معرفة أو اكتشاف أنواع مختلفة من الوجوه. لذا بشكل بديهي، يمكنكم التفكير في
الطبقات السابقة للشبكة العصبية باعتبارها تكتشف وظائف بسيطة، مثل الحواف. وبعد ذلك تعمل على تجميعها
في الطبقات اللاحقة للشبكة العصبية حتى يمكنها معرفة
المزيد من الوظائف المعقدة. ستبدو هذه المرئيات منطقية أكثر
عندما نتحدث حول الشبكات الالتفافية. ومن التفاصيل الفنية
لهذا التمثيل المرئي، فإن كاشفات الحواف تبحث في
مناطق صغيرة نسبيًا للصورة، والتي قد تكون مناطق صغيرة جدًا كهذه. وبعد ذلك قد تكون أجهزة كشف الوجه التي
يمكنكم النظر إليها تمثل مناطق أكبر من الصورة. ولكن الحدس الرئيسي الذي تنتزعه من هذا
هو اكتشاف الأشياء البسيطة مثل الحواف ثم تطويرها. وتجميعها معًا لاكتشاف أشياء
أكثر تعقيدًا مثل عين أو أنف وبعد ذلك تجميعها معًا
لاكتشاف المزيد من الأشياء المعقدة. وهذا النوع من التمثيل الهرمي
البسيط إلى المعقد، أو التمثيل التركيبي، يتم تطبيقه في أنواع بيانات أخرى
بخلاف الصور والتعرف على الوجه. على سبيل المثال، إذا كنتم تحاولون
إنشاء نظام تعرّف على الكلام، فمن الصعب إعادة تمثيل الكلام ولكن إذا قمتم بإدخال مقطع صوتي، فربما
يكون المستوى الأول من الشبكة العصبية قد تعلم اكتشاف ميزات الأشكال الموجية للصوت
منخفضة المستوى، مثل هل هذه النبرة ترتفع؟ هل هي تنخفض؟ هل هذا ضجيج أبيض أم
صوت شهيق مثل [صوت]. وما حدة الصوت هذه؟ وعندما يتعلق الأمر بهذا، يتم اكتشاف
ميزات الأشكال الموجية منخفضة المستوى مثل هذه. وبعد ذلك من خلال تجميع
الأشكال الموجية منخفضة المستوى، ربما تتعلمون اكتشاف
وحدات الصوت الأساسية. ويطلق على هذا في اللغويات الوحدات الصوتية. ولكن، على سبيل المثال، في الكلمة cat، الحرف
C هو وحدة صوتية والحرف A هو وحدة صوتية، والحرف T هو وحدة صوتية أخرى. ولكن تعلم اكتشاف
وحدات الصوت الأساسية، ثم تجميع ذلك معًا قد
يؤدي إلى معرفة الكلمات في الصوت. وبعد ذلك ربما يتم تجميعها معًا، للتعرف على العبارات
أو الجمل الكاملة. لذا فإن الشبكة العصبية العميقة
مع الطبقات المخفية المتعددة، ربما يدفع الطبقات السابقة لتعلم هذه
الميزات البسيطة منخفضة المستوى، مما يؤدي بعد ذلك إلى دفع الطبقات الأعمق
اللاحقة إلى تجميع الأشياء الأبسط التي اكتشفها معًا لاكتشاف الأشياء الأكثر تعقيدًا
مثل تمييز الكلمات المحددة أو حتى العبارات أو الجمل. النطق من أجل تنفيذ
التعرف على الكلام. وما نراه هو أنه بينما
تحسبون الطبقات الأخرى، ما يبدو مثل الوظائف البسيطة نسبيًا للإدخال
مثل أين توجد الحافة، عندما تتعمقون في الشبكة يمكنكم
القيام بأشياء معقدة على نحو مدهش. مثل اكتشاف الوجوه أو
اكتشاف الكلمات أو العبارات أو الجمل. يحب بعض الأشخاص المقارنة
بين الشبكات العصبية العميقة والعقل البشري، حيث نعتقد،
أو يعتقد علماء الأعصاب، أن العقل البشري يبدأ أيضًا في اكتشاف
الأشياء البسيطة مثل الحواف فيما تراه العيون، وبعد ذلك يعمل على تطويرها
لاكتشاف أشياء أكثر تعقيدًا مثل الوجوه التي ترونها. أعتقد أن المقارنات بين
التعلم العميق والعقل البشري أحيانًا
ما تكون خطيرة قليلاً. ولكن هناك الكثير من الحقيقة في هذا الأمر،
فيما يتعلق بكيفية تفكيرنا في عمل العقل البشري وأن العقل البشري ربما
يكتشف الأشياء البسيطة مثل الحواف أولاً وبعد ذلك يعمل على تجميعها معًا لتشكيل
أشياء أكثر تعقيدًا ويمكن اعتبار هذا بمثابة شكل غير محكم من
أشكال الإلهام لبعض أجزاء التعلم العميق أيضًا. سنعرف أكثر حول
العقل البشري أو حول العقل البيولوجي في
فيديو لاحق هذا الأسبوع. الجزء الآخر المتعلق بالحدس
حول الأسباب التي تجعل الشبكات العميقة تبدو عاملة بشكل جيد هو ما يلي. لذا فهذه النتيجة تأتي من نظرية
الدوائر والتي تتعلق بالتفكير في أنواع الوظائف التي يمكنكم
حسابها من خلال بوابات AND وبوابات OR
وبوابات NOT، بوابات منطقية أساسية. لذا بصورة غير رسمية، يتم حساب وظائفها
من خلال شبكة عصبية صغيرة نسبيًا ولكنها عميقة، وأعني بكلمة صغيرة أن عدد
الوحدات المخفية صغير نسبيًا. ولكن إذا حاولتم حساب نفس
الوظيفة باستخدام شبكة غير عميقة، لذلك إذا لم تكن هناك طبقات مخفية كافية، فحينها قد تتطلبون وحدات إضافية أكثر بصورة كبيرة للحساب. لذا، دعوني أعطِكم مثالاً واحدًا
وكذلك أوضح هذا بصورة غير رسمية. ولكن دعونا نفترض أنكم تحاولون
حساب OR الحصري أو تعادل جميع ميزات الإدخال. لذا فأنتم تحاولون حساب X1
وXOR وX2 وXOR وX3 وXOR حتى Xn
إذا كانت لديكم ميزات n أو n X. لذا إذا قمتم بدمج شجرة XOR كهذه،
لذا فبالنسبة لنا تحسب XOR الخاص بـ X1 وX2، وبعد ذلك تأخذ X3
وX4 وتحسب XOR الخاص بها. وبصورة تقنية، إذا كنتم تستخدمون
بوابة AND أو NOT، فقد تحتاجون إلى بعض الطبقات لحساب دالة
XOR بدلاً من طبقة واحدة، ولكن من خلال دائرة صغيرة نسبيًا،
يمكنكم حساب XOR وهكذا. وبعد ذلك يمكنكم بناء
شجرة XOR هكذا، حتى تكون لديكم في النهاية دائرة هنا
وتكون نتائجها، دعونا نطلق عليها Y. نتائج Y تساوي Y. OR الحصري
تعادل جميع أجزاء الإدخال هذه. لذلك لحساب XOR، فإن عمق
الشبكة سيكون في حدود log N. ستكون لدينا شجرة XOR فحسب. لذلك، فإن عدد العقد أو
عدد مكونات الدائرة أو عدد البوابات في هذه الشبكة
ليس كبيرًا للغاية. فلستم بحاجة إلى الكثير من البوابات
لحساب OR الحصري. ولكن الآن، إذا لم يكن مسموحًا لكم
باستخدام شبكة عصبية من خلال العديد من الطبقات المخفية، في هذه الحالة،
سجل الأوامر والطبقات المخفية، إذا فُرض عليكم حساب هذه الدالة
من خلال طبقة مخفية واحدة، فإن لديكم كل هذه الأشياء ذاهبة إلى
الوحدات المخفية. وتكون نتيجة هذه الأشياء Y. ولحساب دالة XOR هذه،
فإن هذه الطبقة المخفية ستحتاج إلى أن تكون كبيرة جدًا،
لأنه في الأساس، تحتاجون إلى تعداد تكوينات
2 إلى N الممكنة باستفاضة. لذا في ترتيب 2 إلى N،
فإن التكوينات الممكنة لأجزاء الإدخال التي تؤدي إلى
OR الحصري 1 أو 0. وبالتالي فإنكم تحتاجون في النهاية إلى طبقة مخفية
كبيرة للغاية في عدد الأجزاء. أعتقد من الناحية الفنية، يمكنكم تنفيذ ذلك
من خلال 2 إلى N ناقص 1 من الوحدات المخفية. ولكن هذه عمليات 2 إلى N القديمة،
والتي ستكون كبيرة جدًا في عدد الأجزاء. لذا آمل أن يعطي هذا صورة
عن وجود دوال رياضية، يمكن حسابها بصورة أسهل باستخدام الشبكات
العميقة عما هو الحال مع الشبكات غير العميقة. بالفعل لقد وجدت بصورة شخصية أن النتيجة
من نظرية الدوائر أقل فائدة للتمكّن من التخمين، ولكن
هذه واحدة من النتائج التي كثيرًا ما يستشهد بها الناس عند توضيح قيمة
امتلاك تمثيلات عميقة للغاية. الآن وبالإضافة إلى هذه الأسباب لتفضيل الشبكات العصبية العميقة،
لكي أكون أمينًا، أعتقد أن الأسباب الأخرى التي جعلت مصطلح
التعلم العميق بارزًا، هي العلامة التجارية فحسب. هذه الأشياء التي اعتدنا أن نطلق عليها الشبكات
العصبية والتي تحتوي على الكثير من الطبقات المخفية، ولكن العبارة "التعلم العميق"، كما تعلمون،
تعد علامة تجارية رائعة، فهي عميقة للغاية. لذا أعتقد أنه بمجرد اعتماد المصطلح
فإنه تمت إعادة تسمية الشبكات العصبية أو تمت إعادة الشبكات العصبية التي تحتوي
على العديد من الطبقات المخفية، على أمل جذب
خيال الناس أيضًا. ولكن بصرف النظر عن العلامة التجارية للعلاقات العامة
فإن الشبكات العميقة تعمل بشكل جيد. أحيانًا يذهب الناس بعيدًا
ويصرون على استخدام الكثير من الطبقات المخفية. ولكن عندما أبدأ مشكلة جديدة،
فإنني أبدأ كثيرًا بالانحدار اللوجستي
ثم أحاول بعد ذلك مع طبقة مخفية واحدة أو طبقتين مخفيتين
وأستخدم ذلك كمعلمة مفرطة. استخدام ذلك كمعلمة أو معلمة مفرطة
تعملون على مواءمتها، من أجل محاولة إيجاد العمق المناسب لشبكتكم العصبية. ولكن على مدار السنوات العديدة الأخيرة،
كان هناك ميل تجاه الأشخاص الذين يجدون أنه بالنسبة لبعض التطبيقات، فإن
الشبكات العصبية العميقة جدًا هنا التي قد تحتوي على الكثير من الطبقات،
قد تكون أحيانًا النموذج الأفضل لمشكلة. إذًا هذا كل شيء بالنسبة للأسباب
التي تجعل الفرضيات تبدو وأنها تعمل جيدًا للتعلم العميق. دعونا الآن نلقِ نظرة على آليات
كيفية تنفيذ ليس الانتشار الأمامي فحسب ولكن الانتشار الخلفي أيضًا.