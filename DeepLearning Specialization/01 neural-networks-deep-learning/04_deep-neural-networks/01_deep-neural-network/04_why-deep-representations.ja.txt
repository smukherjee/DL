深いニューラルネットワークが
多くの問題に対して本当に良い成績をあげていると みなさんも聞いていますよね
ただ大きいニューラルネットワークにする必要があるのではなく 特に隠れ層を多く持った深いネットワークに
する必要があると聞いたでしょう これはなぜでしょうか 何個か例を見て なぜ深層ネットワークが
成績が良いかの洞察を 得てみましょう まず 深層ネットワーク計算とは
なんでしょうか もし顔認識や顔位置認識のシステムを
作ろうとした場合には こんな感じのことを
深層ニューラルネットワークだと行っているでしょう 顔の写真を入力すると
ニューラルネットワークの最初の層は 特徴検出器や境界検出器と
考えてもらってもよいです この例では この画像に対して
20個相当の隠れ層ユニットがあるニューラルネットワークが 計算するだろうことを描画します 20個の隠れユニットは
この小さい四角で表現します 例えば この左上の表現する隠れ層ユニットが
行おうとしているのは 画像上のどこにこの向きの境界があるのかを
見つけようとしています この右下の隠れユニットは たぶん この水平の境界が
画像のどこにあるのかを探しています のちのコースで
畳み込みネットワークを話すときには この表現が
より理解できるでしょう 最初のニューラルネットワークの層が行うのは
このイメージの中を見て どこに境界があるのかを
検出しようとするものと考えてよいです ピクセルをグループ化することで
この画像の境界が 形成できたので 検出した境界を使ってグループ化して
顔の部品を形成できます 例えば 目を見つけようとする
神経ニューロンがあったり この鼻の一部を探そうとする別のニューロンがあるでしょう つまり 多くの境界を
組み合わせることで 様々な顔の部品を
検出し始めることができます 最後に 目や鼻や耳や顎などの
様々な顔の部品を組み合わせることで 様々な種類の顔を
認識したり検出することが できます 直感できるかと思いますが
始めのころのニューラルネットワークの層は 境界のような簡単な関数を
検出すると考えられます 次に 後々の層では
組み立てていると考えられ より複雑な関数を
学ぶようになれます 畳み込みネットワークに関して話すと
これらの表現がより理解できるでしょう この表現の技術的な点を話すと 境界検出器は
画像の比較的狭い範囲を見ていて 多分画像のこの部分くらいですが 顔の部分の検出器は
多分もっと広い領域を見ることができます この例から理解してほしい主な洞察は
境界のような簡単なものをまず見つけ それを積み上げていくということです それを組み合わせて
目や鼻などのより複雑なものを検出し 検出したものを組み合わせて
より複雑なものを組み立てます 単純なものから複雑なものへと行く
階層的表現や組成的表現などの この種類は 画像や顔認識以外にもまた
適用できます 例えば 音声認識システムを作ろうとする時にです 音声を表現するのは難しいですが 音のファイルを入力すると
最初のニューラルネットワークの層は 音量が上下するような音の波の形などの特徴を学習しようと するでしょう ホワイトノイズや
舌を滑らせる音だったり 周波数だったりするでしょう これらの細かいレベルの形の音の特徴になるでしょう 細かいレベルの音の形を
組み合わせることで 音の基本的ユニットを
検出することを学習します 言語学での
音素です 例えば 猫の単語cat では
cやaが音素です t も音素です 音の基本的なユニットを見つけたら 次にこれを組み合わせて
音の中の単語を学習します これをさらに組み合わせて 句全体や文全体を
認識します 複数の隠れ層を持つ
深層ニューラルネットワークであれば 始めのころの層は
細かいレベルの簡単な特徴を学び のちに 深めの層では
検出した簡単なものを組み立てて 特定の単語やさらに句や文や全体などの
より複雑なものを 検出して 音声認識を実施します ここに見られるのは
初期の層が計算しているのは 境界がどこにあるかなどの
入力の比較的簡単な関数です ネットワークの深いところに行くと
驚くほど複雑な関数となっています 顔を検出したり
単語や句や文を検出します 深層ニューラルネットワークと
人間の脳が類似していると 言いたい人もいます
私たちや神経科学者が信じているような 人の脳も目に入ってくる境界のように
比較的単純なものを最初に検出し その後で顔のようなより複雑なものを
検出するように積み上げていくという 考えです この深層学習と
人間の脳が 類似していると考えるのは
時に少し危険だと思っています もちろんどう人間の脳が動作するのかについて
事実な部分もあります 恐らくは 人間の脳は境界のように単純な形を検出し それを積み上げてより複雑な物を
構成するという事実です これが深層学習のゆるい形態での刺激として
役立ってきました 人間の脳や生物学的な脳のことは 今週の後から出てくるビデオで
もっと深く見ていきます もう１点の深層ネットワークが
良い成績を上げる理由の洞察は 次の通りです この結果は回路理論から来ています AND、OR、NOTゲートなどの様々なロジックゲートによって
どんな計算が可能かに関するものです 砕けた言い方をすれば
比較的狭いが深いニューラルネットワークで計算する関数があります 狭いで意味するのは
隠れユニットの数が比較的少ないということです この計算を浅いネットワークで
同じような関数を作る場合 十分な隠れ層がない場合 指数関数的なより多くの隠れユニットが
計算に必要ということです 砕けた方法で例示するために
１つ例をあげます 全ての入力特徴に関して
XORまたはパリティを 計算しようしています X1 XOR X2 XOR X3 に続き XOR Xn まで計算するとします
nx の特徴がある場合です XOR のツリーを作る場合
まずX1 とX2 のXORを計算し X3 とX4 のXOR を計算します 厳密にいえば AND ゲートと
NOT ゲートだけを使う場合には XOR を計算するには１層ではなく
複数の層が必要になりますが 比較的小さい回路で
XOR を計算できます 続けてブールのXOR のツリーを作って行き 最終的に出力する回路が出てくるまで続けます
出力はy と呼びましょう 出力yハット=yを出力します 全てのビットのXOR 
またはパリティの値です XOR を計算する深さは
log n の階数になります この形のXOR ツリーです このネットワークではノードの数
または回路の部品の数 もしくはゲートの数は
それほど多くはありません XOR を計算するには
ゲートは多く必要ありません もし複数の隠れ層を持ったニューラルネットワークが使えない場合には この場合のlog n の階数の層でしたが
１つの隠れ層で この関数を計算するのを
強いられたらどうでしょうか この場合 全ての入力が
隠れ層に行った後で これがYを出力します そしてXOR 関数を計算するには この隠れ層が
指数関数的に大きい必要があります というのは網羅的に
２のN乗の可能な組み合わせを列挙する必要があるからです つまり入力ビットの可能な組み合わせで XOR で１か０のどちらかの結果ができます このため隠れ層には
ビット数が指数関数的に大きいものが 必要になります 厳密には ２のn-1乗の
隠れユニットでできますが ２のｎ階数の指数関数的に
大きい数のビット数になります 深いネットワークで計算する方が
浅いネットワークで計算するよりも 簡単に計算できる数学的関数があることの
意味が分かるようになったでしょうか 回路理論の結果は
洞察を得るにはそれほど役に立たないと 気付いていましたが
深層表現の価値を説明するときに よく使われる結果の１つです これらの理由に加え 深層ニューラルネットワークを
好む理由の１つは私たちにもあります 深層学習という用語が流行した他の理由は
ただブランディングされたからでもあります これらは昔は多層の隠れ層のニューラルネットワークと
呼ばれていましたが 深層学習という語句はただただ凄いブランドです
もの凄く深いという響きです この語句が流行って
多層の隠れ層のニューラルネットワークが 新しくブランド化されたのと同時に よく使われる想像を
掴むのに一役かったと思います PR のブランディングと関係なく
深層ネットワークは実に成績が良いです あまりに多くの隠れ層を使うことを
主張するほどにやり過ぎてしまう人も時々いますが わたしは新しい問題に取り組むときには
まずはロジステック回帰から 始めることも多いです
そして１つか２つの隠れ層を試し これをハイパーパラメーターに使います これを調整を行うパラメーターか
ハイパーパラメーターに使って ニューラルネットワークの
適切な深さを探すのに使います 過去数年にわたって
特定の利用方法については 非常に深いニューラルネットワークが
時に膨大な数の層にも上りますが これが問題に対する最良のモデルとなることを
発見する流れがあります 今回は深層学習が良い成績を
上げているように思える理由の洞察でした 次は 順誤差伝播法だけでなく逆誤差伝播法を
実装する方法の仕組みを 見ていきましょう