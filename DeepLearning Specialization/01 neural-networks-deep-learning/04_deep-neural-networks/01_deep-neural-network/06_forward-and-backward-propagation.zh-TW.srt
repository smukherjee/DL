1
00:00:00,000 --> 00:00:01,350
在先前的影片中,

2
00:00:01,350 --> 00:00:05,065
您看到了基本的礎石
在建置一個深度神經網路

3
00:00:05,065 --> 00:00:07,696
對於每一層正向傳播步驟

4
00:00:07,696 --> 00:00:09,975
跟相對的反向傳播步驟

5
00:00:09,975 --> 00:00:12,370
讓我們來看看您如何實際去建置這些步驟

6
00:00:12,370 --> 00:00:14,230
我們從正向傳播開始

7
00:00:14,230 --> 00:00:20,045
記得這個做的是輸入 a[l-1], 輸出 a[l]

8
00:00:20,045 --> 00:00:21,795
然後快取 z[l]

9
00:00:21,795 --> 00:00:24,615
我們之前也說過用建置觀點來看

10
00:00:24,615 --> 00:00:28,145
也許也存 w[l] 跟 b[l] 在快取上

11
00:00:28,145 --> 00:00:31,585
讓函數取值容易一些在程式練習裡

12
00:00:31,585 --> 00:00:35,438
所以, 這個方程式應該看起來很眼熟

13
00:00:35,438 --> 00:00:46,813
建置正向函數的方式就是
這個等於 w[l] * a[l-1] + b[l]

14
00:00:46,813 --> 00:00:53,620
然後 a[l] 等於啟動函數應用到 z 

15
00:00:53,620 --> 00:00:57,250
如果您想做向量化建置

16
00:00:57,250 --> 00:01:06,218
就會是那個乘上 a[l-1] + b

17
00:01:06,218 --> 00:01:09,930
加上 b 使用 Python 傳播

18
00:01:09,930 --> 00:01:15,296
然後 a[l] = g 逐元素應用到 z[l]

19
00:01:15,296 --> 00:01:20,128
您記得, 在正向步驟的圖形上

20
00:01:20,128 --> 00:01:22,565
記得我們用一些箱子向前連接起來

21
00:01:22,565 --> 00:01:26,775
您初始時餵入 a[0]

22
00:01:26,775 --> 00:01:29,305
也就是等於 X

23
00:01:29,305 --> 00:01:31,440
所以您初始化這個

24
00:01:31,440 --> 00:01:33,380
真的, 什麼是第一個輸入值?

25
00:01:33,380 --> 00:01:40,207
其實是 a[0] 也就是輸入特徵對於一個訓練例子

26
00:01:40,207 --> 00:01:42,255
如果您一次做一個例子

27
00:01:42,255 --> 00:01:45,800
或者 A[0], 整個訓練集

28
00:01:45,800 --> 00:01:48,215
如果您同時處理整個訓練集

29
00:01:48,215 --> 00:01:51,990
這個是第一個連鎖正向函數的輸入

30
00:01:51,990 --> 00:01:53,970
然後只要重複這個會讓您

31
00:01:53,970 --> 00:01:57,085
計算正向傳播從左到右

32
00:01:57,085 --> 00:02:00,110
下一步, 我們來談反向傳播步驟

33
00:02:00,110 --> 00:02:03,585
這裡您的目標是輸入 da[l]

34
00:02:03,585 --> 00:02:08,040
然後輸出 da[l-1] 跟 dw[l], db[l]

35
00:02:08,040 --> 00:02:16,240
讓我寫下這些您需要做的步驟
去計算這些東西: dz[l] = da[l]

36
00:02:16,240 --> 00:02:23,866
逐元素乘積 g[l] 一撇 z[l]

37
00:02:23,866 --> 00:02:27,405
然後計算導數

38
00:02:27,405 --> 00:02:34,420
dw[l] = dz[l] * a[l-1]

39
00:02:34,420 --> 00:02:37,445
我沒有明顯將這個放在快取， 但實際上

40
00:02:37,445 --> 00:02:39,114
您需要這樣做

41
00:02:39,114 --> 00:02:47,560
然後 db[i] = dz[l], 最後

42
00:02:47,560 --> 00:02:59,089
da[l-1] = w[l]轉置 乘 dz[l], 對吧?

43
00:02:59,089 --> 00:03:02,310
我不去詳細談怎樣做這些導數

44
00:03:02,310 --> 00:03:06,605
但實際上, 如果您拿這個
 da 的定義代入這個公式

45
00:03:06,605 --> 00:03:10,260
您會得到跟我們前一個禮拜
用的相同的公式

46
00:03:10,260 --> 00:03:16,617
對於您如何計算 dz[l] 是個函數
對於前一個 dz[l], 實際上

47
00:03:16,617 --> 00:03:18,135
如果我將其代入這裡

48
00:03:18,135 --> 00:03:33,817
您最後得到 dz[l] = w[l+1]轉置 dz[l+1] 乘 g[l] 一撇 z[l]

49
00:03:33,817 --> 00:03:36,165
我知道這有很多的代數

50
00:03:36,165 --> 00:03:38,360
實際上您可以自己檢查

51
00:03:38,360 --> 00:03:40,820
這就是我們之前寫下的公式

52
00:03:40,820 --> 00:03:43,055
在上個禮拜反向傳播， 當我們

53
00:03:43,055 --> 00:03:45,930
作用一個神經網路在單一隱藏層時

54
00:03:45,930 --> 00:03:48,602
提醒一下, 這個乘
是逐元素乘積

55
00:03:48,602 --> 00:03:54,950
您所需的是這四個方程式
來建置您的反向函數

56
00:03:54,950 --> 00:03:58,735
最後, 我會寫直接寫下向量化版本

57
00:03:58,735 --> 00:04:04,135
第一行變成 dz[l] = dA[l]

58
00:04:04,135 --> 00:04:11,045
逐元素乘積 g[l] 一撇 Z[l]

59
00:04:11,045 --> 00:04:13,060
也許不意外

60
00:04:13,060 --> 00:04:23,715
dw[l] 變成 1/m dz[l] 乘 A[l-1]轉置

61
00:04:23,715 --> 00:04:30,844
db[l] 變成 1/m np.sum dZ[l]

62
00:04:30,844 --> 00:04:37,970
然後 axis=1, keepdims = True

63
00:04:37,970 --> 00:04:44,095
我們在上個禮拜談過用 np.sum 來計算 db

64
00:04:44,095 --> 00:04:56,155
最後, dA[l-1] 是 w[l]轉置 乘 dz[l]

65
00:04:56,155 --> 00:05:02,435
這會讓您輸入這個量 dA, 這裡

66
00:05:02,435 --> 00:05:07,950
輸出 dw[l], db[l]

67
00:05:07,950 --> 00:05:10,100
您需要的導數

68
00:05:10,100 --> 00:05:16,022
跟 dA[l-1] 像這樣

69
00:05:16,022 --> 00:05:18,905
這是您如何來建置反向函數

70
00:05:18,905 --> 00:05:20,560
總結一下

71
00:05:20,560 --> 00:05:23,585
拿輸入 X

72
00:05:23,585 --> 00:05:25,060
您也許用第一層

73
00:05:25,060 --> 00:05:28,445
也許是 ReLU 啟動函數

74
00:05:28,445 --> 00:05:30,570
然後到第二層

75
00:05:30,570 --> 00:05:33,340
也許另一個 ReLU 啟動函數

76
00:05:33,340 --> 00:05:35,175
到第三層

77
00:05:35,175 --> 00:05:39,595
也許是S形函數如果您做二元分類的話

78
00:05:39,595 --> 00:05:41,875
而這會輸出 y-hat

79
00:05:41,875 --> 00:05:43,890
然後, 用 y-hat

80
00:05:43,890 --> 00:05:46,265
您可以計算損失

81
00:05:46,265 --> 00:05:49,685
而這可以讓您開始反向的遞迭

82
00:05:49,685 --> 00:05:51,775
我先來畫這些箭頭

83
00:05:51,775 --> 00:05:54,320
這樣我不用常常換畫筆

84
00:05:54,320 --> 00:06:03,430
您會用反向傳播來計算導數, 計算

85
00:06:03,430 --> 00:06:16,063
dw[3], db[3], dw[2], db[2], dw[1], db[1]

86
00:06:16,063 --> 00:06:18,865
一路下來，您也計算了

87
00:06:18,865 --> 00:06:24,820
快取會傳遞 z[1], z[2], z[3]

88
00:06:24,820 --> 00:06:32,250
這裡反向的傳送 da[2], da[1]

89
00:06:32,250 --> 00:06:34,730
這也可以計算 da[0]

90
00:06:34,730 --> 00:06:35,880
但我們用不到

91
00:06:35,880 --> 00:06:37,935
所以您可以忽略它，是吧?

92
00:06:37,935 --> 00:06:40,785
這是您如何去建置正向傳播跟反向傳播

93
00:06:40,785 --> 00:06:44,040
對於一個三層的神經網路

94
00:06:44,040 --> 00:06:46,140
有最後一個細節我還沒談到

95
00:06:46,140 --> 00:06:48,735
也就是對於正向遞迴

96
00:06:48,735 --> 00:06:52,420
我們用輸入資料 X 來初始化

97
00:06:52,420 --> 00:06:54,090
反向遞迴呢?

98
00:06:54,090 --> 00:06:59,235
實際上 da[l]

99
00:06:59,235 --> 00:07:01,065
當您做羅吉斯迴歸分析時

100
00:07:01,065 --> 00:07:02,977
當您做二元分類時

101
00:07:02,977 --> 00:07:09,685
是等於 y/a + 1-y/1-a

102
00:07:09,685 --> 00:07:12,575
事實上損失函數的導數

103
00:07:12,575 --> 00:07:14,180
相對於輸出

104
00:07:14,180 --> 00:07:17,475
相對於 y-hat, 可以證明是等於這樣的

105
00:07:17,475 --> 00:07:19,105
如果您熟悉微積分

106
00:07:19,105 --> 00:07:21,328
如果您查損失函數 l

107
00:07:21,328 --> 00:07:24,150
求導數相對於 y-hat, 或相對於 a

108
00:07:24,150 --> 00:07:26,505
您可以證明您得到這個公式

109
00:07:26,505 --> 00:07:31,350
所以您應該用這個公式在最後一層, 大寫 L 層的 da

110
00:07:31,350 --> 00:07:35,715
當然，如果您用向量化建置

111
00:07:35,715 --> 00:07:38,351
您初始這個反向遞迴

112
00:07:38,351 --> 00:07:43,706
不是這個, 而是 dA 大寫A, 對於 L 層

113
00:07:43,706 --> 00:07:48,465
會是對每個不同的例子做同樣的事

114
00:07:48,465 --> 00:07:54,011
第一個訓練例子 除以 a + 1-y

115
00:07:54,011 --> 00:07:55,285
對於第一個例子

116
00:07:55,285 --> 00:07:58,153
除以 1-a  對於第一個訓練例子

117
00:07:58,153 --> 00:08:05,185
... 直到 m 訓練例子,  1-a(m)

118
00:08:05,185 --> 00:08:09,423
這是您用來建置向量化的版本

119
00:08:09,423 --> 00:08:13,055
這是您用來初始
向量化版本的反向傳播

120
00:08:13,055 --> 00:08:16,100
現在您看到基本的建構礎石對於

121
00:08:16,100 --> 00:08:20,030
正向傳播跟反向傳播

122
00:08:20,030 --> 00:08:22,340
如果您建置這些方程式

123
00:08:22,340 --> 00:08:24,530
您會得到正確的

124
00:08:24,530 --> 00:08:27,640
正向傳播，反向傳播建置
來獲得您需要的導數

125
00:08:27,640 --> 00:08:29,660
您也許會想這麼多的方程式

126
00:08:29,660 --> 00:08:32,080
我略感困惑， 我不是很清楚這些如何作用

127
00:08:32,080 --> 00:08:34,645
如果您覺得如此, 我的建議是

128
00:08:34,645 --> 00:08:37,205
當您做這個星期的程式作業時

129
00:08:37,205 --> 00:08:40,175
您會自己建置這些

130
00:08:40,175 --> 00:08:42,020
它們會變得更具體

131
00:08:42,020 --> 00:08:43,805
我知道有很多的方程式

132
00:08:43,805 --> 00:08:46,265
也許一些方程式看起來並不合理

133
00:08:46,265 --> 00:08:49,055
但您可以通過微積分

134
00:08:49,055 --> 00:08:50,905
跟線性代數來驗證, 雖然真的不容易

135
00:08:50,905 --> 00:08:52,430
不妨試試

136
00:08:52,430 --> 00:08:56,390
但這真的是在機器學習裡
最難推導的部分之一

137
00:08:56,390 --> 00:08:57,950
實際上推導這些方程式

138
00:08:57,950 --> 00:09:02,685
就是在反向傳播時用微積分方程式
來計算這些導數

139
00:09:02,685 --> 00:09:04,750
但再一次說明, 
如果覺得這有一點抽象

140
00:09:04,750 --> 00:09:06,400
您覺得有一點點神秘

141
00:09:06,400 --> 00:09:09,108
我的建議是, 當您做完程式練習後

142
00:09:09,108 --> 00:09:11,465
您會覺得它具體了一些

143
00:09:11,465 --> 00:09:14,120
雖然我必須說, 即使在今天

144
00:09:14,120 --> 00:09:16,805
當我建置學習算法, 有時候

145
00:09:16,805 --> 00:09:18,000
我也會驚訝於

146
00:09:18,000 --> 00:09:21,170
我的學習算法建置可行， 因為

147
00:09:21,170 --> 00:09:25,670
機器學習的複雜度來自於資料
而不是幾行程式

148
00:09:25,670 --> 00:09:27,095
所以有時候, 您覺得

149
00:09:27,095 --> 00:09:28,685
您建置了幾行程式

150
00:09:28,685 --> 00:09:30,110
不確定它們做了什麼

151
00:09:30,110 --> 00:09:31,625
但幾乎神奇的運作了

152
00:09:31,625 --> 00:09:35,296
因為許多的神奇實際上
並非來自於您寫的幾行程式

153
00:09:35,296 --> 00:09:37,090
通常不會太長的程式

154
00:09:37,090 --> 00:09:38,705
它並不是真的簡單

155
00:09:38,705 --> 00:09:40,730
但也不是那種上萬

156
00:09:40,730 --> 00:09:42,115
或幾十萬行的程式

157
00:09:42,115 --> 00:09:44,750
但您餵了這麼多資料, 有時候

158
00:09:44,750 --> 00:09:46,850
即使我做了這麼久時間
的機器學習

159
00:09:46,850 --> 00:09:49,400
有時候，它還是讓我驚艷當

160
00:09:49,400 --> 00:09:53,274
我的學習演算法可行因為
您的學習演算法很多複雜度

161
00:09:53,274 --> 00:09:55,970
來自於資料而

162
00:09:55,970 --> 00:10:01,020
不一定來自於成千上萬行的程式

163
00:10:01,020 --> 00:10:05,935
好的, 這是您如何建置一個深度神經網路

164
00:10:05,935 --> 00:10:10,325
當您完成這個禮拜的程式作業
這些會變得更具體

165
00:10:10,325 --> 00:10:14,220
更進一步之前，在下一段影片

166
00:10:14,220 --> 00:10:17,480
我想討論超參數及參數

167
00:10:17,480 --> 00:10:19,681
實際上當您訓練深度網路時

168
00:10:19,681 --> 00:10:22,225
能夠組織好您的超參數

169
00:10:22,225 --> 00:10:25,400
也會幫助您建置您的網路更有效率

170
00:10:25,400 --> 00:10:29,000
下一段影片，我們來談談這是什麼意思