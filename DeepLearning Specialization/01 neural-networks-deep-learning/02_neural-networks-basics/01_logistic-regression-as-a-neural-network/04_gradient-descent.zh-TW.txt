您已經看過羅吉斯迴歸分析模型 您看過用損失函數來
衡量您做得如何 在單一訓練例子上 您也看過成本函數來
衡量您的參數 w 跟  b 作用到整個訓練集的表現 現在讓我們談談您如何使用
梯度下降演算法來訓練 或者說學習, 參數 w跟 
b 在您的訓練集上 回顧一下，這是熟悉的
羅吉斯迴歸分析演算法 我們在第二
行有成本函數，J， 就是一個函數對於
您的參數 w 跟 b 而這定義為平均數 所以是 1 除以 m 乘上和的
這個損失函數 這損失函數
衡量了您的演算法 輸出 y-hat(i) 在每一個訓練例子作用得如何, 疊起來或者 跟標準答案標籤 y(i) 比較
在每一個訓練例子 完整的公式
展開在右邊 成本函數衡量
您的參數 w 跟 b 作用在訓練集的表現如何 為了要學習參數
 w 跟 b 似乎自然的方式是我們想要去 找 w 跟 b 使得成本
函數 j(w,b) 越小越好 這裡是
梯度下降法的圖示 在這個圖形中
水平軸代表了 您的參數 w 跟 b 實際上 w 可能是更高
維度, 但為了 畫圖方便, 讓我們畫成單一
實數, b 也是單一實數 成本函數 J(w,b) 是 一個曲面在
水平軸 w 跟 b 上方 這個曲面的高代表
 j(w,b) 的值在一定點 我們要做的是實際上
去找到 w 跟 b 的值相對於這個
成本函數的最小值 實際上這個成本
函數是凸形函數 也就是一個碗狀
這是一個凸形函數 這跟這種函數
像這樣相反 這種是非凸函數
有許多的不同的局部最佳化 事實是我們成本
函數這樣定義 是凸形函數也是重要的原因
這是為什麼我們用這個特殊的函數 j 用在羅吉斯迴歸分析上 為了找到好的值來當
參數 我們要做的是初始化 w 跟
 b 一些初始值 也許記為一個小紅點 而對於羅吉斯迴歸分析幾乎
任何初始化都可行 通常您初始值為 0 隨機初始化也行, 但 人們在羅吉斯迴歸分析不這樣做 因為這個函數是凸形函數
不論您初始哪裡 您應該會到達同一點
或者大約同一點 而梯度下降做法是
它開始於起始點 然後往最陡的下坡方向邁出一步 所以在梯度下降的第一步後
您可能會在這裡, 因為 它試圖在最速下降
方向邁出一步往下坡或 盡可能的儘快下山 這是一個梯度下降的一次迴圈 經過兩次迴圈梯度
下降, 您也許走到這裡 三次迴圈等等 我猜現在是隱藏在
圖的背後直到最終 收斂到這個全域最佳化或者
接近於全域最佳化 這個圖形說明了
梯度下降演算法 讓我們寫詳細一點 為了進行說明, 假設
有一些函數 J(w) 您想要最小值,而
或許函數像這樣 為了便於繪製
我先忽略 b 只畫一維
圖形, 而非多維圖形 梯度下降法做這件事 我們要反覆進行下列更新 我們取 w 的值然後
更新它 用 冒號等號 
來代表更新 w 設 w 為 w 減 alpha 乘上 這個導數 dj(w)/dw 我會重複做這樣
直到演算法收斂 一些關於符號的註解
alpha 是學習率 控制了步伐大小
在每個梯度下降的迴圈 我們將來會談論一些方式
來選擇學習率 alpha 第二, 這一個量
這是導數 這基本上是您想做的更新或改變
到參數 w 當我們開始寫程式去
建置梯度下降 我們的約定會用
這個變數名稱在我們的程式 dw 會用來代表
這個導數項目 當我們寫程式時
您寫成 w 冒號等號 w 減 alpha 乘 dw 所以我們用 dw 這個變數
名稱來代表這個導數項目 現在讓我們來確定這個
梯度下降更新是合理的 假設 w 在這裡 您在
成本函數 J(w) 這一點 記得導數的定義 是函數在這一點的斜率 函數的斜率實際上
是高除以寬, 是吧 底下三角形這裡
是j(w)在這裡的切線 這裡的導數是正的 w 更新為 w 減 學習
率 乘上這個導數 導數是正的, 所以
您從 w 減去, 所以 您最終會往左邊走 所以梯度下降會
慢慢的 減低這個參數如果您
從這個大的 w 值開始 另一個例子, 如果 w 在這裡 那這一點的斜率
dj/dw 是負的 梯度下降更新會
減去 alpha 乘上一個負數 所以 w 會慢慢的增加
您最終會讓 w 越來 越大在幾次
梯度下降的迴圈之後 所以不管您的
初始值在左邊 或是在右邊梯度下降會
將您移動到全域最小值這裡 如果您不熟悉導數或者
微積分 這個項目 dj(w)/dw 的意義
不用擔心 我們在下一段影片會談更多 如果您已經有深厚的微積分知識 您或許能有更深的
直觀關於神經網路如何作用 但即使如果您不
熟悉微積分 在下幾個影片中我們會給您
足夠的直觀有關於導數跟 有關的微積分您能夠用來
有效的使用神經網路 但整體的直觀在 現在是這個項目代表
函數的斜率而 我們想要知道函數的斜率
在目前的參數設定 來讓我們可以做最陡下坡的下一步
也就是讓我們知道 哪個方向為了往
下坡在成本函數 J 上 我們寫我們的梯度下降對於
 J(w) w 是您的參數 在羅吉斯迴歸分析, 您的成本
函數是一個 w 跟 b 的函數 在這個情況下, 這個梯度下降的內迴圈
就是這個東西 這個東西您必須
重複變成以下 您要更新 w 成為 w
減去 學習率乘上 這個 J(w, b) 的導數 相對於 w 您更新 b 為 b 減去
學習率乘上 這個成本函數對於 b 的導數 這下面兩個公式
實際上更新您的建置 順便說一句，我只想講微積分學
中的一個符號公約 對於一些人會有點混亂 我想這不是超級重要
對於您理解微積分 假設您看怪怪的我想確定
您不用想太多 在微積分裡,這個項目 實際上寫成
一個滑稽的波形曲線符號 這個符號
實際上只是小寫的 d 用花俏的字體, 形式化的字體
當您看到這個運算式 這代表的意思是 J(w,b) 偏導數
實際上就是函數 J(w,b) 的斜率, 函數是多少的
斜率在 w 方向 而這個在微積分的符號規則
我覺得一點都不合邏輯 這個在微積分的符號規則
我想會讓事情 更加不必要的複雜
當如果函數是兩個或 更多的變數時, 與其用
小寫的 d 您用這個有趣的符號 這是稱為偏
導數符號 但不用擔心 如果 J 是只有一個變數的函數時
您用小寫的 d 唯一的區別在於
不管您用這個有趣 偏導數符號或
小寫 d 像我們上面寫的 是在於函數是否是兩個
或更多的變數 這種情況下, 您用這個符號
偏導數符號 如果 J 是一個變數的函數
您用小寫的 d 這是一個滑稽的規則
在微積分的符號上 我想這只會讓事情
變得不必要的複雜 但如果您看到這個偏導數
符號, 它的意思是衡量 函數的斜率
相對於一個變數 同樣的執著於
正確的數學 符號在微積分, 因為這個
J 有兩個輸入不只一個 在底下這個應該寫成
用這個偏導數符號 但其實意思是同樣的
幾乎跟小寫的 d 一樣 最後, 當您建置這些程式 我們約定將
這個項目, 實際是您 更新 w 的量記為
變數 dw 在程式中 而這個項目 您想更新 b 的量 記為變數
 db 在程式中 好的, 這是您如何
建置梯度下降 如果您幾年沒用過微積分
我知道這似乎 很多的微積分的導數
您可能不適應 如果您這麼覺得
不用擔心 在下一段影片, 我將給您
好一點的直觀有關於導數 甚至不需要深的微積分數學
的理解 就只是一些
理解微積分的直觀 您會能夠讓神經
網路有效的工作 讓我們近到下一段影片
我將談談更多的 微分