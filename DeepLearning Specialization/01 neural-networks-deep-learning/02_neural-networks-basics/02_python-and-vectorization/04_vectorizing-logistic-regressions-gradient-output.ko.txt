이전 강의에서 여러분에게 이번 비디오에서는, vectorization을 이용하여, 예측수치를 산출하는 방법을 배웠는데요, 전체 트레이닝 세트 O에 대해서 소문자 a들을 상대로 말이죠. 이번 비디오에서는 vectorization을 이용해서 어떻게 M 트레이닝 샘플들 데 대해 기울기를 산출할 수 있는지 보겠습니다. 다시 한번, 모두 한꺼번에 말이죠. 그리고 비디오 마지막 부분에서는, 모두 취합하여, 매우 효율적인 로지스틱 회귀분석의 도입을 가능케 할지 한번 보여드리겠습니다. 기울기 강하 산출에는 기억하시겠지만, 첫 번째 예시에 대해서 dz1을 계산하고, 이 값은 a1 빼기 y1 그리고 dz2는 a2 빼기 y2 등등 말이죠. 모들 m 트레이닝 예시에 대해서 말이죠. 이제 저희는 새로운 변수를 정의할 것입니다. dZ는 dz1, dz2, dz2이 될 텐데요, 다시 한번, d 소문자 z의 변수는 가로로 쌓입니다. 그러면 이것은 1xm 매트릭스 또는 m차원의 벡터입니다. 이전 슬라이드에서 아시겠지만, 대문자 A를 구하는 방법은 알아냈는데요, a1에서 am까지 말이죠. 그리고 여기 대문자 Y는 y1에서 ym까지의 값입니다. 이것 또한, 가로로 쌓인 것이죠. 이러한 정의를 바탕으로, 직접 보실 수도 있겠지만, dz은 A 빼기 Y로 계산되는데요, a1 - y1이기 때문에 이것은 first element가 되고, a2 - y2는 second element이렇게 이어집니다. 그렇기 때문에 first element인 a1-y1은 dz1의 정의와 똑같습니다. second element 는 dz2와 동일하구요, 그렇게 해서 한 줄의 코드로, 모든 값을 한번에 산출해낼 수 있습니다. 이전의 도입에서 보면, 1개의 for loop를 이미 없앴는데요, 여기 두 번째 for loop은 트레이닝 예시에 대해 아직 있었습니다. 그래서 dw를 0으로 초기화하고, 0의 벡터로 말이죠. 하지만 아직도 트레이닝 예시 에 대해 loop over해야 합니다. dw 플러스는 x1 곱하기 dz1 인데요, 첫 번째 트레이닝 예시에 대해 말이죠, dw 플러스는 x2 dz2 등등으로 이어집니다. 이것을 M번 반복하여 dw 나누기=M과 같이 됩니다. B도 비슷하게 말이죠. 맞죠? db는 0으로 포기화됐고, db 플러스는 dz1, db 플러스는 dz2 이렇게 말이죠, dz(m)까지 이어지는 데요, db /=M이 됩니다. 이것이 이전 도입에 있었던 내용인데요, 1개의 for loop은 이미 없앤 상태입니다. 이제 적어도 dw은 벡터이고 개별적으로 dw1, dw2, 등등을 업데이트 해줬습니다. 이제 이미 없앴긴 했지만, 아직도 트레이닝세트에서 m 개의 예시에 대해서는 for loop이 있었습니다. 그러면 여기 절차를 이용해서 vectorize 시켜 보겠습니다. 저희가 할 수 있는 것은 이렇습니다. db의 vectorize 도입을 위해서는, 저희는 기본적으로 이것을 더하는 것입니다. 여기 모든 dz들을 더한 후에, m으로 나누어 주는 것이죠, db는 1 나누기 그리고 합의 공식은 i=1 에서 m까지 적용되는데요, 그 공식은 dzi 입니다. 여기서 모든 dz는 row vector입니다. 그러므로 파이썬에서는, 여러분이 할 것은, 1/m np. dz의 합을 도입시킵니다. 그러면 여기 변수를 가지고 np.sum 함수라고 부르십시오. 그러면 db를 줄 것입니다. dw는 어떨까요? 올바른 공식을 적을 테니 나중에 확인하셔도 됩니다. 1 나누기 m, 곱하기 X매트릭스 곱하기 dz transpose입니다. 왜 이런 건지 조금 보자면, 이건은 1 나누기 m, 그 다음에 매트릭스 x들인데요, x1에서 xm까지 세로로 쌓이고, dz transpose는 이렇게 dz1에서 dzm까지 이어질 것입니다. 그러면 여기 이 매트릭스와 여기 매트릭스를 곱하면 되는 값을 보자면, 1 나누기 M 곱하기 x1dz1 더하기... 더하기 xm dzm이 됩니다. 그러면 이것은 n x 1벡터인데요, 이것이 남게 됩니다. dw와 같이 말이죠, 그 이유는 dw는 xi dzi와 같은 값을 더해서 생기는 것인데요, 여기 이 매트릭스를 곱하면서 그런 작용이 일어납니다. 그러면 가시, 한 줄의 코드를 이용해서, dw를 계산할 수 있습니다. 그래서 derivative 계산에 대한 vectorized 도입은 이렇습니다, 여기 이 라인을 이용해서 db를 도입하고, 여기 이 라인을 이용해서 dw를 도입합니다. 그리고 아시겠지만, 트레이닝 세트에
거쳐서 for loop를 이용해 이제는 파라미터에서 원하는 업데이트를 계산할 수 있습니다. 이제, 모든 것을 취합해서 로지스틱 회귀분석을 어떻게 도입할지 보겠습니다. 이것이 원본인데요, 매우 비효율적인 non-vectorized 도입 방식입니다. 이전 비디오에서 처음으로 한 것은 여기 이 볼륨을 없앴습니다. 맞죠? 그러면 dw1, dw2 등에 looping over하는 대신에, 이 값을 벡터 값인 dw, 그 값이 dw+=xi인 값으로 대체했습니다. 이제는 이 값이 벡터 곱하기 dz(i)죠. 하지만 이제, 어떤 줄에서의 for loop를 없애는 것뿐만 아니라 여기 전체 for loop를 없앨 수 있다는 것을 알죠. 이렇게 하는 것입니다. 이전 슬라이드에 있는 내용을 사용하면, 대문자 Z가, w transpose X 더하기 B이고, 사용하는 코드는 np w transpose X 더하기 B입니다. 그리고 a는 대문자 Z의 시그마입니다. 그럼 여러분은 이제 여기 모든 값을 구했고, 모든 i값에 대해 여기 있는 값들을 구했습니다. 다음으로, 이전 슬라이드에서는, dz는 A -Y로 계산한다고 했죠. 그리고 여기 있는 모든 것을 i의 모든 값에 대해 구했습니다. 마지막으로, dw=1 나누기 m 곱하기 dz transpose t 그리고 db는 1 나누기 m 그리고 sum (dz) 입니다. 그러면 여러분은 전 방향전파 과 후 방향전파를 했고, 예측 값과 derivative를 계산함으로써, 모든 M 트레이닝 샘플에 대해서 말이죠, for loop사용 없이 말입니다. 그러면, 기울기 강하 업데이트는 , w 는 w 빼기 러닝속도 곱하기 dw dw는 위에서 계산했고요, b는 b 빼기 러닝속도 곱하기 db로 업데이트 됩니다. 특정 방법 이 있는 걸 볼 수 있습니다. 저도 생각해보면 이 표기법을 그리 일관되게 쓰진 않았네요. 이렇게 하여, 로지스틱 회귀분석에서 single iteration의 기울기 강하를 방금 도입했습니다. 제가 일정한 for loop을 없앨 수 있을 때 무조건 없애야 한다고 했는데요, 만약 여러분이 복수의 iteration을 통해 도입하고 싶은 경우 몇몇의 iteration에서 for loop이 필요할 수 있습니다. 여러분이 만약 기울기 강하의 1000개의 iteration을 갖고 싶으면, 이 숫자까지 for loop이 필요할 것입니다. 이렇게 가장 바깥 쪽의 for loop이 있는 경우엔, 여러분이 아마 for loop를 없앨 수 있는 방법이 없을 것입니다. 하지만 제 생각에는 적어도 한번의 기울기 강하의 iteration을 for loop없이 도입할 수 있다는 것은 꽤 멋진 것 같습니다. 자 이게 본 강의 내용의 전부인데요, 
이제 여러분은 vectorize되고, 매우 효율적인 로지스틱 회귀분석 기울기 강하 도입을 압니다. 한가지 더 내용이 있는데요, 이것은 다음 비디오에서 이야기하겠습니다. 여기에서 앞서 언급한 broadcasting에 대한 내용인데요, 파이썬 넘파이가 특정 부분의 코드에서 더 효율적으로 될 수 있도록 가능케 하는 기술이 바로 broadcasting 입니다. broadcasting에 대한 더 자세한 내용은 다음 비디오를 통해 보겠습니다.