1
00:00:00,390 --> 00:00:03,860
Раніше я записав формулу функції Витрат для

2
00:00:03,860 --> 00:00:05,230
логістичної регресії (ЛР).

3
00:00:05,230 --> 00:00:09,370
В цьому необов'язковому відео я хочу дати коротке обґрунтування

4
00:00:09,370 --> 00:00:13,490
чому ми використовуємо функцію Витрат для ЛР.

5
00:00:13,490 --> 00:00:17,709
Згадаємо, що в ЛР ми маємо передбачення ŷ

6
00:00:17,709 --> 00:00:23,704
= сигмоїді від w транспоноване x плюс b,

7
00:00:23,704 --> 00:00:27,711
де сигмоїда - ось ця знайома функція.

8
00:00:27,711 --> 00:00:34,353
І ми казали, що будемо розцінювати ŷ як ймовірність того,

9
00:00:34,353 --> 00:00:39,776
що y=1 при заданому x. Тож ми хочемо, щоб наш алгоритм вивів y як

10
00:00:39,776 --> 00:00:45,030
імовірність, що y=1 для заданих вхідних ознак x.

11
00:00:45,030 --> 00:00:50,090
Іншими словами, якщо y=1,

12
00:00:50,090 --> 00:00:56,020
тоді ймовірність від y при заданому x =ŷ.

13
00:00:56,020 --> 00:00:59,170
І навпаки, якщо y=0,

14
00:01:00,310 --> 00:01:05,840
тоді ймовірність (того, що y=0) =1-ŷ.

15
00:01:05,840 --> 00:01:09,150
Тож якщо ŷ є ймовірністю, що y=1,

16
00:01:09,150 --> 00:01:13,620
тоді 1-ŷ є ймовірністю, що y=0.

17
00:01:13,620 --> 00:01:18,057
Тож давай візьмемо 2 останніх рівняння і 
скопіюємо на наступний слайд.

18
00:01:18,057 --> 00:01:22,684
Тож я візьму ці 2 рівняння, які,

19
00:01:22,684 --> 00:01:28,010
по суті, визначають p(y|x) для двох випадків: y=0 та y=1.

20
00:01:28,010 --> 00:01:33,110
Тож я візьму ці 2 рівняння і об'єднаю в одне рівняння.

21
00:01:33,110 --> 00:01:37,543
Зазначимо, що y має бути або 0 або1, 
тому що в нас двійкова класифікація.

22
00:01:37,543 --> 00:01:41,110
Тож y= або 0 або 1 - лише 2 можливих випадки.

23
00:01:41,110 --> 00:01:44,653
Якщо взяти ці 2 рівняння і об'єднати їх наступним чином...

24
00:01:44,653 --> 00:01:48,774
Давай я спочатку запишу це, а потім поясню чому це так.

25
00:01:48,774 --> 00:01:54,040
Тож 1-ŷ в степені 1-y.

26
00:01:54,040 --> 00:01:58,920
Виявляється цей один рядок відображає два верхніх.

27
00:01:58,920 --> 00:02:00,500
Давай поясню чому.

28
00:02:00,500 --> 00:02:04,643
Тож, перший випадок: коли y=1. Тож, якщо

29
00:02:04,643 --> 00:02:09,562
y=1, тоді цей вираз буде =

30
00:02:09,562 --> 00:02:13,970
ŷ, тому що це буде ŷ в степені 1.

31
00:02:13,970 --> 00:02:21,120
Оцей вираз буде: 1-ŷ в степені 1-1, тобто в степені 0.

32
00:02:21,120 --> 00:02:26,320
Так як будь-яке число в степені 0 буде 1, то цей вираз відпадає.

33
00:02:26,320 --> 00:02:33,030
І, отже, це рівняння стане p(y|x)=ŷ при y=1.

34
00:02:33,030 --> 00:02:37,480
А це саме те, що нам треба.

35
00:02:37,480 --> 00:02:40,250
Як щодо другого випадку? Що буде якщо y=0?

36
00:02:40,250 --> 00:02:47,057
Якщо y=0, тоді це рівняння вгорі буде p(y|x)=...

37
00:02:47,057 --> 00:02:51,920
ŷ в степені 0. Але будь-яке число в степені 0 =1. Тому

38
00:02:51,920 --> 00:02:58,267
це просто = 1 помножене на 1-ŷ в степені 1-y. А

39
00:02:58,267 --> 00:03:02,770
1-y буде 1-0, тобто просто =1.

40
00:03:02,770 --> 00:03:07,610
І, отже, це = 1 помножити на 1-ŷ, а це =1-ŷ.

41
00:03:10,700 --> 00:03:17,230
І, отже, ми маємо, що при y=0, p(y|x)=1-ŷ,

42
00:03:17,230 --> 00:03:21,570
А це саме те, що ми записали вгорі.

43
00:03:21,570 --> 00:03:23,690
Тож ми щойно показали, що ось це рівняння

44
00:03:25,330 --> 00:03:30,331
є правильним визначенням для p(y|x).

45
00:03:30,331 --> 00:03:36,513
Тепер, врешті, через те, що логарифмічна функція є 
строго монотонно зростаючою функцією,

46
00:03:36,513 --> 00:03:43,223
яку ми намагаємось максимізувати, то логарифм від y 
при заданому x повинен дати результат схожий

47
00:03:43,223 --> 00:03:48,672
на p(y|x) при оптимізації. 
І ми можемо обчислити log(p(y|x)), який =

48
00:03:48,672 --> 00:03:54,330
log(ŷʸ(1-ŷ)⁽¹⁻ʸ⁾).

49
00:03:54,330 --> 00:03:59,818
А це спрощується до y*log(ŷ)+(1-y)*log(1-ŷ),

50
00:03:59,818 --> 00:04:05,881
правильно?

51
00:04:05,881 --> 00:04:09,832
А це, насправді, протилежне

52
00:04:09,832 --> 00:04:14,310
до функції Втрати, яку ми раніше визначили.

53
00:04:14,310 --> 00:04:17,470
Знак мінус тут тому, що зазвичай при тренуванні навчального

54
00:04:17,470 --> 00:04:20,460
алгоритму, ми хочемо щоб імовірності були якомога більшими,

55
00:04:20,460 --> 00:04:23,980
а для ЛР це значить, що

56
00:04:23,980 --> 00:04:25,820
ми намагаємось мінімізувати функцію Втрати.

57
00:04:25,820 --> 00:04:30,640
Тож мінімізація Втрати 
означає максимізацію логарифму від ймовірності.

58
00:04:30,640 --> 00:04:33,925
Тож ось що являє собою функція Втрати для одного зразка.

59
00:04:33,925 --> 00:04:35,435
А що ж з функцією Витрат,

60
00:04:35,435 --> 00:04:40,435
загальною функцією Витрат для всього тренувального набору 
з m зразків?

61
00:04:40,435 --> 00:04:41,385
Давай обчислимо.

62
00:04:41,385 --> 00:04:45,710
Тож імовірності всіх позначок тренувального зразка

63
00:04:45,710 --> 00:04:47,750
(запишемо це дещо неформально),

64
00:04:47,750 --> 00:04:51,945
якщо припустити, що тренувальні зразки я намалював незалежно 
або що вони - НРВ/IID

65
00:04:51,945 --> 00:04:54,198
(нормально розподілені величини/identically independently distributed),

66
00:04:54,198 --> 00:04:57,810
то імовірність всіх зразків буде добутком їх імовірностей.

67
00:04:57,810 --> 00:05:03,143
Отже, добуток по i від 1 до m усіх p(y⁽ⁱ⁾|x⁽ⁱ⁾).

68
00:05:03,143 --> 00:05:07,970
Якщо ми хочемо застосувати метод максимальної правдоподібності,

69
00:05:07,970 --> 00:05:12,476
то нам потрібно максимізувати або знайти параметри,

70
00:05:12,476 --> 00:05:15,948
що максимізують ймовірність спостережень і тренувального набору.

71
00:05:15,948 --> 00:05:20,200
Проте, максимізація цього - це те саме, що максимізація логарифму.

72
00:05:20,200 --> 00:05:22,990
Тож поставмо логарифм з обох боків.

73
00:05:22,990 --> 00:05:28,640
Логарифм ймовірності позначок тренувального набору =

74
00:05:28,640 --> 00:05:30,990
логарифму добутку, тобто сумі логарифмів, -

75
00:05:30,990 --> 00:05:39,000
сумі по i від 1 до m усіх log(p(y⁽ⁱ⁾|x⁽ⁱ⁾)).

76
00:05:39,000 --> 00:05:43,582
Нещодавно ми побачили (на попередньому слайді),

77
00:05:43,582 --> 00:05:47,630
що це =-L(ŷ⁽ⁱ⁾, y⁽ⁱ⁾).

78
00:05:48,850 --> 00:05:55,220
В статистиці, цей метод називається 
методом максимальної правдоподібності,

79
00:05:55,220 --> 00:06:00,720
що означає, що щоб вибрати параметри, які максимізують ось це,

80
00:06:00,720 --> 00:06:04,220
або іншими словами, максимізують ось це:

81
00:06:04,220 --> 00:06:09,510
від'ємна сума по i від 1 до m усіх L(ŷ⁽ⁱ⁾, y⁽ⁱ⁾).<i><i>

82
00:06:09,510 --> 00:06:11,840
Я просто виніс мінус перед суму.

83
00:06:11,840 --> 00:06:15,749
Тож це обґрунтування наших Витрат для

84
00:06:15,749 --> 00:06:21,235
ЛР, тобто J(w, b)= оцьому.

85
00:06:21,235 --> 00:06:27,349
А через те, що ми хочемо мінімізувати Витрати, 
а не максимізувати правдоподібність,

86
00:06:27,349 --> 00:06:30,095
ми позбудемося знаку мінус.

87
00:06:30,095 --> 00:06:35,467
І, нарешті, згідно конвенції, 
щоб звести наш результат до відповіднішого діапазону,

88
00:06:35,467 --> 00:06:39,310
додамо тут масштабуючий фактор 1/m.

89
00:06:39,310 --> 00:06:43,960
Тож підсумуємо. Мінімізуючи функцію Витрат J(w, b), ми, насправді,

90
00:06:43,960 --> 00:06:48,430
обчислюємо максимальну правдоподібність для моделі ЛР.

91
00:06:48,430 --> 00:06:53,120
При цьому ми припускаємо, що тренувальні зразки - НРВ/IID або

92
00:06:53,120 --> 00:06:55,530
нормально розподілені величини/identically independently distributed.

93
00:06:55,530 --> 00:06:59,550
Тож дякую за перегляд цього відео, хоча це було й необов'язково.

94
00:06:59,550 --> 00:07:03,845
Надіюсь, 
ти отримав/ла розуміння чому ми використовуємо функцію Витрат

95
00:07:03,845 --> 00:07:05,200
для ЛР.

96
00:07:05,200 --> 00:07:09,287
І, надіюсь, ти перейдеш до вправ (програмувальної вправи) і

97
00:07:09,287 --> 00:07:11,277
цьоготижневого опитування.

98
00:07:11,277 --> 00:07:14,735
Успіхів тобі на опитуванні і вправах!