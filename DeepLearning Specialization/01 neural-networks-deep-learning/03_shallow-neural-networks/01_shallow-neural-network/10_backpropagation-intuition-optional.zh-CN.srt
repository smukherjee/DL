1
00:00:00,000 --> 00:00:01,230
上个视频中

2
00:00:01,230 --> 00:00:03,720
你已经看过了反向传播的等式

3
00:00:03,720 --> 00:00:06,900
在这个视频中 我们会用计算图

4
00:00:06,900 --> 00:00:10,515
来更直观地学习这些式子的推导过程

5
00:00:10,515 --> 00:00:12,385
这个视频不是必须要看的

6
00:00:12,385 --> 00:00:14,106
所以 看或者不看这个视频都可以

7
00:00:14,106 --> 00:00:16,360
不看这个视频你也应该可以完成所有任务

8
00:00:16,360 --> 00:00:19,410
那么 回忆一下我们讨论的逻辑回归

9
00:00:19,410 --> 00:00:23,685
我们首先从这个向前的步骤算出z

10
00:00:23,685 --> 00:00:26,145
然后算出a 还有损失函数

11
00:00:26,145 --> 00:00:27,445
之后我们要计算导数

12
00:00:27,445 --> 00:00:32,520
从这个反向的步骤中我们可以先算出da

13
00:00:32,520 --> 00:00:35,400
然后去计算dz

14
00:00:35,400 --> 00:00:40,720
然后去计算dw和db

15
00:00:40,720 --> 00:00:46,970
这个损失函数L(a,y)

16
00:00:46,970 --> 00:00:52,655
等于-yloga-(1-y)log(1-a)

17
00:00:52,655 --> 00:00:57,440
-yloga-(1-y)log(1-a)

18
00:00:57,440 --> 00:00:59,750
如果你熟悉

19
00:00:59,750 --> 00:01:03,600
微积分并对a进行求导的话

20
00:01:03,600 --> 00:01:06,156
你就会得到da的表达式

21
00:01:06,156 --> 00:01:09,060
所以da就等于这个

22
00:01:09,060 --> 00:01:12,750
如果你算出这个微积分的话就会知道这个是

23
00:01:12,750 --> 00:01:18,808
-y/a+(1-y)/(1-a)

24
00:01:18,808 --> 00:01:23,040
这就是对这个进行求导的结果

25
00:01:23,040 --> 00:01:26,680
当你再反向推导一步去计算dz时

26
00:01:26,680 --> 00:01:32,430
我们已经弄清楚了dz等于a-y
我之前解释过了为什么

27
00:01:32,430 --> 00:01:37,920
但是事实证明从微积分的链式求导法则来说dz等于

28
00:01:37,920 --> 00:01:45,425
da乘上g(z)的导数

29
00:01:45,425 --> 00:01:50,535
这里的g(z)等于sigmoid(z)

30
00:01:50,535 --> 00:01:56,245
也就是对于这个输出单元的逻辑回归的激活函数 对吧？

31
00:01:56,245 --> 00:02:00,570
所以记住这仍然是那个输入x1 x2 x3

32
00:02:00,570 --> 00:02:05,757
然后通过一个sigmoid单元输出a

33
00:02:05,757 --> 00:02:07,400
也就是y-hat的逻辑回归

34
00:02:07,400 --> 00:02:11,400
这里激活函数是个sigmoid函数

35
00:02:11,400 --> 00:02:12,960
顺便说一下

36
00:02:12,960 --> 00:02:17,205
对于那些熟悉微积分的链式求导法则的同学

37
00:02:17,205 --> 00:02:22,520
这个的原因是a等于sigmoid(z)

38
00:02:22,520 --> 00:02:29,310
因此 L关于z的偏导数等于

39
00:02:29,310 --> 00:02:36,800
L关于a的偏导数乘da/dz

40
00:02:36,800 --> 00:02:39,611
这里的a等于sigmoid(z)

41
00:02:39,611 --> 00:02:42,970
这个等于d(g(z))/dz

42
00:02:42,970 --> 00:02:49,080
即g'(z)

43
00:02:49,080 --> 00:02:54,060
这就是为什么这个表达式 也就是我们代码中的dz

44
00:02:54,060 --> 00:02:59,484
等于这个表达式 也就是我们代码中的da乘g'(z)

45
00:02:59,484 --> 00:03:05,860
于是这个就是这个

46
00:03:05,860 --> 00:03:09,172
所以只有你熟悉微积分中的链式求导法则

47
00:03:09,172 --> 00:03:13,510
才能理解那个最后的推导过程

48
00:03:13,510 --> 00:03:15,325
但如果你不熟悉的话别担心

49
00:03:15,325 --> 00:03:18,853
在需要的地方我会具体解释

50
00:03:18,853 --> 00:03:22,315
最后在计算完了这个dz之后

51
00:03:22,315 --> 00:03:26,335
我们会计算dw 也就是

52
00:03:26,335 --> 00:03:31,470
dz乘上x 还有db
当只有一个训练样本时它就是dz

53
00:03:31,470 --> 00:03:33,822
那么以上就是逻辑回归了

54
00:03:33,822 --> 00:03:36,700
现在当我们要计算神经网络的反向传播时

55
00:03:36,700 --> 00:03:40,090
我们要做的其实很像这个

56
00:03:40,090 --> 00:03:46,995
但是我们要做两次因为现在我们的x
不单单是一个输出单元

57
00:03:46,995 --> 00:03:50,930
它会先成为一个隐藏层之后再变成一个输出单元

58
00:03:50,930 --> 00:03:58,405
所以我们会在这个有两层的神经网络里做两步

59
00:03:58,405 --> 00:04:04,483
而不是像在这里一样只计算一步

60
00:04:04,483 --> 00:04:08,586
那么在这个有一个输入层 一个隐藏层

61
00:04:08,586 --> 00:04:10,138
和一个输出层的两层神经网络里

62
00:04:10,138 --> 00:04:12,070
回想一下计算步骤

63
00:04:12,070 --> 00:04:17,210
首先用这个等式算出z1

64
00:04:17,210 --> 00:04:22,177
然后算出a1 再算出z2

65
00:04:22,177 --> 00:04:25,505
注意这里的z2也取决于参数W2和b2

66
00:04:25,505 --> 00:04:27,530
然后根据z2

67
00:04:27,530 --> 00:04:32,815
计算出a2然后最终得到损失函数

68
00:04:32,815 --> 00:04:41,560
而反向传播做的事是它会从反方向计算da2和dz2

69
00:04:41,560 --> 00:04:48,805
然后向后去计算dW2和db2

70
00:04:48,805 --> 00:04:53,232
再向后计算da1

71
00:04:53,232 --> 00:04:57,278
dz1 等等

72
00:04:57,278 --> 00:05:00,290
我们不需要关于输入x进行求导

73
00:05:00,290 --> 00:05:03,745
因为对于监督学习来说x是不变的

74
00:05:03,745 --> 00:05:07,845
所以我们不会去优化x
至少对于监督学习来说

75
00:05:07,845 --> 00:05:09,655
我们不会关于x进行求导

76
00:05:09,655 --> 00:05:15,605
我将会跳过具体计算da2的步骤

77
00:05:15,605 --> 00:05:18,110
如果你想的话 你其实可以算出来

78
00:05:18,110 --> 00:05:20,750
da2然后用它去计算dz2 但是在实际使用中

79
00:05:20,750 --> 00:05:25,760
你可以把这两步合并成一步然后就会得到

80
00:05:25,760 --> 00:05:31,715
dz2等于a2-y 就像以前一样

81
00:05:31,715 --> 00:05:33,620
而且

82
00:05:33,620 --> 00:05:38,615
我先把dw2和db2写在这下面

83
00:05:38,615 --> 00:05:46,700
你会得到dw2等于dz2乘以

84
00:05:46,700 --> 00:05:52,040
a1的转置 然后db2等于dz2

85
00:05:52,040 --> 00:05:55,990
这步和我们在逻辑回归中计算dw=dz*x

86
00:05:55,990 --> 00:06:03,550
的步骤挺像的 只是现在

87
00:06:03,550 --> 00:06:08,770
a1在这里替代了x而且这里有个多出来的转置步骤

88
00:06:08,770 --> 00:06:14,125
因为我们的这个大写的W矩阵和单个的w参数

89
00:06:14,125 --> 00:06:16,660
之间需要进行转置 对吧？

90
00:06:16,660 --> 00:06:24,370
因为在只有一个输出的逻辑回归的情况下w是一个行向量

91
00:06:24,370 --> 00:06:26,980
dw2是这样的 然而

92
00:06:26,980 --> 00:06:32,440
这里的w是一个列向量
所以这就是为什么a1有个转置

93
00:06:32,440 --> 00:06:36,980
而在逻辑回归的x这里没有

94
00:06:36,980 --> 00:06:40,335
这样我们就完成了一半的反向传播

95
00:06:40,335 --> 00:06:44,045
然后 如果你想的话你可以去计算da1

96
00:06:44,045 --> 00:06:49,440
虽然在实际操作中da1和dz1的计算

97
00:06:49,440 --> 00:06:52,330
通常被合并到一步里

98
00:06:52,330 --> 00:06:57,130
你实际上要实现的是 dz1=W2

99
00:06:57,130 --> 00:07:03,480
的转置乘上dz2 然后再用它的每个元素与

100
00:07:03,480 --> 00:07:10,383
g1'(z1)的每个元素相乘

101
00:07:10,383 --> 00:07:13,960
然后检查一下维度就行了 对吧？

102
00:07:13,960 --> 00:07:19,510
如果你有一个像这样的神经网络

103
00:07:19,510 --> 00:07:23,000
输出y

104
00:07:23,000 --> 00:07:28,265
如果你有n0个输入的特征 这里nx=n0

105
00:07:28,265 --> 00:07:30,230
n1个隐藏单元

106
00:07:30,230 --> 00:07:34,275
和n2

107
00:07:34,275 --> 00:07:36,740
这种情况下的n2

108
00:07:36,740 --> 00:07:38,565
只是一个输出单元

109
00:07:38,565 --> 00:07:48,795
然后W2矩阵的大小就是n2*n1

110
00:07:48,795 --> 00:07:57,490
z2和dz2的大小就是n2*1

111
00:07:57,490 --> 00:07:59,850
在我们实现二元分类的时候这个的大小其实是1*1

112
00:07:59,850 --> 00:08:04,750
z1还有

113
00:08:04,750 --> 00:08:10,045
dz1就是n1*1维的 对吧？

114
00:08:10,045 --> 00:08:16,115
注意任意的变量foo和dfoo都会有相同的维度

115
00:08:16,115 --> 00:08:20,850
这就是W和dW的维度永远相同的原因 而相似地

116
00:08:20,850 --> 00:08:23,680
对于b和db与z和dz都是一样的

117
00:08:23,680 --> 00:08:26,895
为了确认它们的维度相同

118
00:08:26,895 --> 00:08:35,430
我们有dz1等于W2的转置乘以dz2

119
00:08:35,430 --> 00:08:44,490
这里是与g1'(z1)进行一个单独元素间的乘法

120
00:08:44,490 --> 00:08:47,040
现在与上面的维度匹配一下的话

121
00:08:47,040 --> 00:08:52,575
这里就应该是n1*1 等于W2的转置

122
00:08:52,575 --> 00:08:57,945
我们对这个矩阵进行一下转置 它就会变成n1*n2维的

123
00:08:57,945 --> 00:09:05,790
dz2就是n2*1维的 然后这个

124
00:09:05,790 --> 00:09:07,230
这个东西的维度应该与z1相同

125
00:09:07,230 --> 00:09:11,820
这个也是n1*1维的 这里有个元素间的乘法

126
00:09:11,820 --> 00:09:14,350
现在维数就对了吧？

127
00:09:14,350 --> 00:09:18,330
n1*1维的向量可以通过

128
00:09:18,330 --> 00:09:23,520
n1*n2维的矩阵乘以n2*n1因为它们两个

129
00:09:23,520 --> 00:09:28,890
的乘积是一个n1*1维的矩阵 之后这就变成了

130
00:09:28,890 --> 00:09:34,618
两个n1*1的向量的单独元素间的乘法

131
00:09:34,618 --> 00:09:36,060
这样维度就匹配了

132
00:09:36,060 --> 00:09:40,620
在写反向传播时有一个注意事项

133
00:09:40,620 --> 00:09:44,790
就是如果你确认你的矩阵维度匹配了的话

134
00:09:44,790 --> 00:09:47,190
如果你从头到尾想一想你的矩阵

135
00:09:47,190 --> 00:09:50,430
比如W1 W2 z1

136
00:09:50,430 --> 00:09:54,180
z2 a1 a2等等的维度 然后确认一下

137
00:09:54,180 --> 00:09:58,642
这些矩阵运算结果的维度都可以相互匹配

138
00:09:58,642 --> 00:10:03,390
有时就已经可以消除许多反向传播过程中的bug了

139
00:10:03,390 --> 00:10:06,960
那么 我们现在得到了dz1 最后

140
00:10:06,960 --> 00:10:12,160
总结一下 dW1和db1

141
00:10:12,160 --> 00:10:13,965
也许我们应该把它们写在这

142
00:10:13,965 --> 00:10:17,200
但现在地方不够 我还是把它们写在幻灯片的右边吧

143
00:10:17,200 --> 00:10:21,965
dW1和db1可以用以下公式表示

144
00:10:21,965 --> 00:10:25,950
这个就会等于dz1乘以x的转置

145
00:10:25,950 --> 00:10:28,905
这个等于dz

146
00:10:28,905 --> 00:10:34,045
你也许注意到了这些等式之间的相似之处

147
00:10:34,045 --> 00:10:37,095
其实这是必然的因为x

148
00:10:37,095 --> 00:10:41,660
在这里是a0的角色
所以x的转置就是a0的转置

149
00:10:41,660 --> 00:10:45,484
所以这些方程实际上非常相似

150
00:10:45,484 --> 00:10:50,260
这些给了我们一些关于推导反向传播的理解

151
00:10:50,260 --> 00:10:54,530
我们一共有6个等式 分别是dz2

152
00:10:54,530 --> 00:11:00,190
dW2 db2 dz1 dW1和db1

153
00:11:00,190 --> 00:11:05,767
我现在要把这六个等式给放到下一页 好了

154
00:11:05,767 --> 00:11:08,950
目前为止 我们已经推导出了对于

155
00:11:08,950 --> 00:11:13,959
只有一个训练样本的反向传播算法

156
00:11:13,959 --> 00:11:21,530
但是不足为奇的是
比起一次只能训练一个样本

157
00:11:21,530 --> 00:11:27,810
我们更想要可以向量化多个不同样本的训练模型

158
00:11:27,810 --> 00:11:30,971
回想一下在前向传播中

159
00:11:30,971 --> 00:11:33,545
我们一次处理一个训练样本时

160
00:11:33,545 --> 00:11:41,665
我们有这样的等式 a1=g1(z1)

161
00:11:41,665 --> 00:11:43,655
为了进行向量化

162
00:11:43,655 --> 00:11:51,260
我们取出这些z 然后把它们以列的方式

163
00:11:51,260 --> 00:12:00,775
堆在一起 然后把它命名为大写的Z

164
00:12:00,775 --> 00:12:04,960
然后我们发现通过把这些以列堆叠

165
00:12:04,960 --> 00:12:10,240
之后在用一个大写字母来定义它们

166
00:12:10,240 --> 00:12:17,093
我们就可以直接得到Z1=W1X+b

167
00:12:17,093 --> 00:12:24,700
A1=g1(Z1) 对吧？

168
00:12:24,700 --> 00:12:28,645
我们在这节课中非常仔细地定义符号来确保

169
00:12:28,645 --> 00:12:35,550
把样本以列堆叠成一个矩阵可以把所有这些解决

170
00:12:35,550 --> 00:12:40,105
事实表明如果你仔细研究一下其中的数学

171
00:12:40,105 --> 00:12:46,645
相同的技巧对于反向传播同样适用
因此向量化的等式如下

172
00:12:46,645 --> 00:12:52,250
首先 如果你把这些不同训练样本的dz

173
00:12:52,250 --> 00:12:58,339
以列堆叠成一个矩阵
而且对这个和这个也进行相同操作的话

174
00:12:58,339 --> 00:13:03,070
那么这就是它向量化的实现
然后 这里是

175
00:13:03,070 --> 00:13:05,569
计算dW2的方式

176
00:13:05,569 --> 00:13:11,130
这里有一个1/m因为代价函数J

177
00:13:11,130 --> 00:13:18,410
等于1/m乘以损失函数从i=1到n的和

178
00:13:18,410 --> 00:13:20,615
在计算导数时

179
00:13:20,615 --> 00:13:23,885
我们会加上1/m 就像我们在

180
00:13:23,885 --> 00:13:27,982
计算逻辑回归的权重时那样

181
00:13:27,982 --> 00:13:31,790
这是db2

182
00:13:31,790 --> 00:13:40,640
就是把这里dz的值都加起来乘以1/m
然后dZ1是这么算的

183
00:13:40,640 --> 00:13:49,109
这里与以前一样是一个单独元素间的乘法

184
00:13:49,109 --> 00:13:56,595
只是之前的幻灯片上它是n1*1维的向量

185
00:13:56,595 --> 00:14:03,185
现在它是一个n1*m维的矩阵

186
00:14:03,185 --> 00:14:09,045
这两项都是n1*m维的

187
00:14:09,045 --> 00:14:19,310
这就是这个星号代表着单独元素间的乘法的原因

188
00:14:19,310 --> 00:14:21,454
最后 这是剩下的两个

189
00:14:21,454 --> 00:14:25,836
它们看起来不太让人吃惊

190
00:14:25,836 --> 00:14:29,510
我希望以上这些给你了一些关于反向传播推导过程的理解

191
00:14:29,510 --> 00:14:32,205
在所有机器学习算法中

192
00:14:32,205 --> 00:14:34,820
我认为反向传播算法的推导

193
00:14:34,820 --> 00:14:38,465
实际上是我见过用到的数学最复杂的推导之一

194
00:14:38,465 --> 00:14:42,470
你需要熟悉线性代数以及

195
00:14:42,470 --> 00:14:46,830
对于矩阵的求导来从最基本的原理开始推导

196
00:14:46,830 --> 00:14:50,165
如果你十分熟悉矩阵微积分的话

197
00:14:50,165 --> 00:14:54,255
根据这个过程你也许可以自己推出这个算法

198
00:14:54,255 --> 00:14:57,500
但是我认为实际上许多看过像视频里这个级别的

199
00:14:57,500 --> 00:15:01,060
推导过程的深度学习的从业人员

200
00:15:01,060 --> 00:15:04,100
已经完全有能力

201
00:15:04,100 --> 00:15:08,580
对这个算法有一个直观的理解并且有效地实现它了

202
00:15:08,580 --> 00:15:10,070
所以如果你特别熟悉微积分的话

203
00:15:10,070 --> 00:15:13,395
看看你是否可以从头开始推导这个算法

204
00:15:13,395 --> 00:15:15,665
它运用了极难的数学

205
00:15:15,665 --> 00:15:20,010
是我见过在所有机器学习算法中很难推导的之一

206
00:15:20,010 --> 00:15:22,861
但是无论如何 如果你要实现这个

207
00:15:22,861 --> 00:15:27,260
你已经有足够的经验去调试并且让它正常工作

208
00:15:27,260 --> 00:15:30,830
最后 我还想在你自己写神经网络之前

209
00:15:30,830 --> 00:15:34,190
与你分享最后一个细节

210
00:15:34,190 --> 00:15:37,720
就是怎样去初始化你的神经网络的权重

211
00:15:37,720 --> 00:15:40,600
事实表明随机初始化你的参数

212
00:15:40,600 --> 00:15:42,560
而不是直接初始为0

213
00:15:42,560 --> 00:15:45,515
对于训练你的神经网络十分重要

214
00:15:45,515 --> 00:15:48,000
在下一个视频中 你将看到原因