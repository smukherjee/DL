Bir sinir ağı neden lineer olmayan bir etkinleştirme fonksiyonuna ihtiyaç duyar? Sinir ağınızın ilginç fonksiyonları hesaplayabilmesi için lineer olmayan bir etkinleştirme fonksiyonuna ihtiyaç duyduğu ortaya çıkar. Peki neden? Burada sinir ağının dayanak denklemlerini görelim. Bunlardan neden kurtulmuyoruz? g fonksiyonundan kurtulalım ve a1 eşittir z1 diyelim. Ya da alternatif olarak g(z) eşittir z diyelim, doğru mu? Bazen buna lineer etkinleştirme fonksiyonu denir. Belki bunun için daha iyi bir isim olarak kimlik etkinleştirme fonksiyonunu kullanabiliriz çünkü girdi neyse çıktı da odur. Bu doğrultuda, ya a2 z2'ye eşit olsaydı? Görünüyor ki, bunu yaparsanız, bu model x verildiğinde lineer bir fonksiyon olarak sadece y veya y-şapkayı hesaplar. İlk iki fonksiyonu ele aldığımızda a1 eşittir z1 o da eşittir w1 ise, x artı b ve buradan a2 eşittir z2 o da eşittir w2, a1 artı b, sonrasında a1'in tanımını alıp buraya yerleştirirseniz a2'nin w2 çarpı w1'e eşit olduğunu bulursunuz. x artı b1, doğru mu? Öyleyse bu a1 artı b2' dir ve w2'ye sadeleştirilir. w1, x artı w2, b1 artı b2. O zaman, bu sadece, hadi buna w', b' diyelim. O zaman bu w'x artı b' olur. Eğer lineer etkinleştirme fonksiyonlarını kullanacak olsaydınız, kimlik etkinleştirme fonksiyonu da dediğimiz, o zaman sinir ağı sadece girdinin lineer fonksiyonunun çıktısını üretirdi. Derin ağlar hakkında daha sonra konuşacağız. Birçok katmanın, birçok gizli katmanın sinir ağları. Öyle görünüyor ki eğer lineer etkinleştirme fonksiyonunu kullanırsanız ya da alternatif olarak etkinleştirme fonksiyonunuz yoksa sinir ağınızın kaç katmanının bulunduğunun bir önemi yoktur, sinir ağınızın bütün yaptığı sadece lineer etkinleştirme fonksiyonunu hesaplamaktır. Hiç gizli katmanınız da olmayabilir. Bazı durumlarda kısaca bahsedildiği gibi, Görünen o ki, eğer burada bir lineer etkinleştirme fonksiyonu ve şurada bir sigmoid (s-biçimli) fonksiyonu varsa, o zaman bu model gizli katmanı olmayan standart lojistik regresyondan daha anlamlı değildir. Bu yüzden bunu kanıtlamak için uğraşmayacağım ama istersen bunu yapmaya çalışabilirsiniz. Alınması gereken ders, lineer bir gizli katman üç aşağı beş yukarı işe yaramazdır. çünkü iki lineer fonksiyonun birleşimi kendisine yani bir lineer fonksiyona eşittir. Yani buraya bir lineersizlik atmazsanız ağda daha derine gitmenize rağmen daha ilginç fonksiyonları hesaplamıyor olursunuz. Lineer etkinleştirme fonksiyonunu kullanabileceğiniz sadece bir yer olabilir, g(z) eşittir z ve bu sadece regresyon probleminde makine öğrenmesi uygulamak. yani eğer y bir reel sayı ise, Örneğin, konut fiyatlarını tahmin ediyorsanız, y sıfır değildir, ama bir reel sayıdır, sıfır TL' den bir evin fiyatı nereye kadar çıkıyorsa kadardır. Tahminimce evler milyonlarca TL olabilir. Bununla birlikte, veri kümenizde ne kadar ev fiyatı olursa, eğer y bu reel değerleri alırsa o zaman burada lineer etkinleştirme fonksiyonunu kullanmak normal olabilir, böylece çıktınız y' da eksi sonsuzla artı sonsuz arasında bir reel sayı olur. Ama sonra gizli birimler lineer etkinleştirme fonksiyonunu kullanmamalıdır. Gizli birimler Relu, Tanh veya Lihi değeri ya da başka bir şey kullanabilir. Yani lineer etkinleştirme fonksiyonunu kullanabileceğiniz yer genellikle çıktı katmanıdır. Bunun yanı sıra lineer etkinleştirme fonksiyonunu, bazı çok özel durumlar sıkıştırmayla ilgili daha önce konuştuğumuz kullanım şekli haricinde, gizli bir katmanda kullanımı çok nadirdir. Tabi, eğer gerçekten ev fiyatlarını birinci hafta videolarında gördüğünüz gibi tahmin ediyorsanız, çünkü ev fiyatları negatif değildir, belki o zaman onların değer etkinleştirme fonksiyonlarını kullanabilirsiniz, böylece çıktılarınız y' sıfıra eşit veya sıfırdan büyük olur. Umarım bu size lineer olmayan bir etkinleştirme fonksiyonuna sahip olmanın sinir ağlarının önemli bir parçası olduğuna dair bir anlayış kazandırmıştır. Devamında, azalan eğim hakkında konuşmaya başlayacağız ve bunu yapmak için, azalan eğim hakkında bir tartışma başlatmak için, sıradaki videoda, eğimi veya etkinleştirme fonksiyonlarını nasıl tahmin edeceğinizi, hesaplayacağınızı göstermek istiyorum. Hadi sıradaki videoya devam edelim.