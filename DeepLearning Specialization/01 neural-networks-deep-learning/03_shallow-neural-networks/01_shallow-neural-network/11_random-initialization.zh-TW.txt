當您訓練您的神經網路 怎樣隨機初始
這些權重是很重要的 對於羅吉斯迴歸分析, 
初始這些權重為0 是可以的 但對於神經網路初始
這些參數權重都為0 然後應用到梯度下降
這樣是不可行的 讓我們看看為什麼 您有兩個輸入特徵 n0 = 2, 兩個隱藏單元, n1 = 2 相關於隱藏層的矩陣 w1 將會是 2 乘 2 假設您初始化它們
都為0, 0 0 0 0, 2 乘 2 矩陣 假設 b1 也是等於 0 實際上初始這個偏差值
 b 設為 0 是可以的 但初始 w 全為  0 是個問題 這種樣子的初始化的問題是對於 任何例子您給予
您會有 a1, 1, 跟 a1, 2 會是相等的, 對吧? 這個啟動跟
這個啟動會一樣 因為這兩個隱藏單元
用同樣的函數來計算 然後
當您計算反向傳播時 會變成 dz1,1跟 dz1, 2 會是ㄧ樣
像是對稱般 兩個隱藏單元
用一樣的方式初始 技術上來說 我假設輸出權重也是相同 也就是 w2 等於 0 0 但如果您用這樣初始
神經網路 這個隱藏單元跟
這個隱藏單元是完全相等 有時候您說它們
完全對稱 也就是說它們
完全相同的函數 然後用數學歸納法證明 實際上經過單一
遞迴的訓練您的兩個隱藏 單元還是計算
完全一樣的函數 因為這是可以證明 dw 
會是一個矩陣像這樣 每一個行的值都相同 我們在執行權重更新時 我們在執行權重更新時
w1 更新為 w1 - alpha 乘 dw 您發現到 w1 經過每次遞迴 都會是第一行
等於第二行 所以這是可以用
數學歸納法證明如果您 初始所有 w 的值為 0 因為兩個隱藏單元開始於
一樣的函數計算 兩個隱藏單元有
同樣的影響到輸出單元 經過一次的遞迴
同樣的情況 兩個隱藏單元還是對稱 所以用數學歸納法, 經過兩次
遞迴, 三次遞迴等等 不管您
訓練您個神經網路多久 兩個隱藏單元還是
用同樣的函數計算 這種情況下, 真的沒有
意義使用一個以上的隱藏單元 因為它們都
計算同樣的東西 當然對於大型的神經網路
假設說有三個特徵跟 也許很多個隱藏單元 同樣的議論顯示
像這樣的神經網路 [聽不清]畫所有的線
如果您初始權重為 0 那所有您的隱藏
單元是對稱的 不管您跑多久的
梯度下降 還是持續的
計算同樣的函數 這並沒有幫助
因為您要不同的 隱藏單元來計算
不同函數 解決方式是
隨機初始您的參數 您可以這樣做 您可以設 w1 = np.random.randn 這會產生一個高斯
隨機變數 (2,2) 然後通常您會乘上一個
很小的數字, 比如說 0.01 所以您初始它們
為很小的隨機值 然後對於 b, 實際上 b
並沒有對稱問題 稱為對稱
斷裂問題 初始
b 都為 0 是沒有關係的 因為
只要 w 隨機初始 您開始用不同的隱藏
單元計算不同的事 您不再有這種
對稱斷裂問題 同樣地, 對於 w2
您將它們隨機初始 b2 您可以初始為 0 您也許會疑惑, 這個
常數來自何方, 為什麼是 0.01？ 為什麼不是 100 或是 1000？ 實際上我們通常
喜歡初始 這些權重為很小的隨機值 因為如果您使用 tanh 或
S型啟動函數 或者其他S型函數
即使只在輸出層 如果權重太大 當您計算
啟動值 記得 z[1] = w1 x + b 然後 a1 是用啟動
函數應用到 z1 如果 w 值很大
z 也會很大, 或者說至少 z 的值不是很大
就是很小 在這種情況下, 您大概會
在這個平坦的部分在 tanh 函數或者S型函數, 而
那裡的斜率或者說梯度會很小 意思是梯度
下降會很慢 所以學習會很慢 總結一下, 如果 w 很大
您很可能 在開始學習時
有很大的 z 值 會使得您的 tanh 或者 sigmoid 
啟動函數卡住 因此放慢學習 如果您不用S型函數或 tanh 啟動函數在您的
神經網路這比較不會是問題 但如果您處理二元分類問題
您的輸出單元是 sigmoid 函數, 那您不希望
這些初始參數的值太大 這是為什麼乘上 0.01會是
合理的值試試看 或者其他小的數字 w2 也是一樣, 對吧? 這個可以是 random.randn 我猜這會是 1 乘 2
在這個例子， 乘上 0.01 這裡少了個 s 最後, 實際上有時候
有可能比 0.01 有更好的常數 當您訓練神經
網路只有一個隱藏層時 這真的是淺層的神經網路
並沒有很多的隱藏層 設這個為 0.01 也許是可行的 但當您訓練一個
很深的神經網路時 您也許需要選
不同的常數而不是 0.01 在下一個禮拜的課程
我們將會提到一點點如何跟 何時您需要選擇
不同於 0.01 的常數 但不管如何, 通常
這都會是一個很小的數字 所以這是這一週的影片 您現在知道了如何去設定一個神經
網路有一層的隱藏層 初始化參數
使用正向傳播做預估 跟計算導數及
建置梯度下降 使用反向傳播 所以
您應該可以做測驗 跟這個禮拜的
程式練習 祝您幸運 我希望您覺得
程式練習很好玩 期望在第四週的課程再見到您