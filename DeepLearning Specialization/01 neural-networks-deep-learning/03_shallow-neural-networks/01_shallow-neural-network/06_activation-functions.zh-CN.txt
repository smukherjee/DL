当你建立一个神经网络的时候 你需要做的众多选择之一 是对隐藏层使用什么样的激活函数 以及对输出层的神经元使用什么样的激活函数 到现在为止我们都（对输出层） 使用了Sigmiod函数 但是有时候使用别的激活函数会更好 我们一起来看看 其中的一些选择 在神经网络正向传递步骤中 我们在这三个步骤中使用了Sigmiod函数 因此Sigmiod函数在这里也被称为激活函数 这是一个大家熟悉的Sigmiod函数曲线 a等于1除于1加欧拉常数e的负z次方 在一般情况下 我们可以使用其他的关于z的函数g 我们把它写在这里 g为一个非线性函数 但是不一定是Sigmiod函数 比如说，Sigmoid函数值的范围在0和1之间 一个通常情况下效果比Sigmoid函数 效果更好的激活函数 是tanh函数 也叫做双曲正切函数 这是z，a和关于z的tanh函数 函数值的范围在1和-1之间 tanh函数的公式为 e的z次方和e的-z次方之差 除以他们的和 它其实在数学上看是一个 移位后的Sigmoid函数 也就是一个普通的Sigmoid函数 但是通过移位后 经过原点并且重新调整比例 使函数输出范围在-1和1之间 如果在隐藏单元中
令关于z的函数g 等于tanh(z) 那么最终效果基本都会好过 使用Sigmoid函数的结果
这是因为此时函数的值 介于1和-1之间 因此隐藏层激活函数输出的平均值 会更加逼近于0 有时候当你 训练一个 学习算法的时候 你可能会中心化你的数据 并且使用tanh替代Sigmiod函数 以达到数据中心化的效果 数据中心化使数据的平均值 更加逼近零，而不是比如说0.5 这个会使得下一层的学习 变的更简单一点 我们将在第二个课程中具体讨论 彼时我们会在讨论优化算法时
进行进一步的研究 这里面一个需要知道的是 我几乎再也不使用Sigmoid函数 作为激活函数 再也不使用 因为tanh函数在大多数情况下几乎 都相当严格地优于Sigmoid
一个例外是 在输出层上
因为y的值是0或者1 因此可以理解 你需要输出的y帽（拟合值） 的输出在0和1之间 而不是在-1和1之间 在一个特例下我会使用Sigmoid 作为激活函数，就是当你使用 二元分类的时候 在这种情况下你可以使用Sigmoid激活函数 用于输出层 关于z2的函数g等于Sigmoid(z2) 你看到的例子这里 你可能会在隐藏层 用tanh作为激活函数 将Sigmoid用作输出层的激活函数 因此不同的层会使用不同的激活函数 有时候需要标明 每个层使用的激活函数 都可能不同 我们可以使用方括号的上标 来表明G[1] 和G[2]可能是不尽相同的函数 上标标明在哪个层 上标表示为层 带有方括号的上标2表示输出层 输出层 Sigmoid和tanh函数的 缺点之一是 如果Z的值非常大或者非常小 那么关于这个函数导数的梯度或者 斜率会变的很小 当Z很大或者很小的时候 函数的斜率值 会接近零 这会使得梯度下降变的缓慢 一个在机器学习界很流行的 小工具叫做线性整流函数 线性整流函数大概是这么样子的 ReLU的公式为 a等于0和z中较大一个值 当z为正时导数为1 反之当z为负的时候 斜率，或说导数就为0 当技术上去实现的时候 虽然z正好为0时候的导数并不存在 但是当你在计算机上实现的时候 你通常会得到 z正好为000...的非常小的数值 所以你不需要去担心
（遇到z恰好为0的情况） 在实际应用当中 你可以认为当z为0时候的 导数为1或者0 这样做并不会有什么大的问题 即使这个函数是不可微的 这里有一些大致规则 来选取激活函数 当你处理的问题是二分类问题
此时的输出为0和1 那么Sigmoid激活函数在输出层 的使用是顺理成章的选择 而在其他神经元上使用ReLU，或者 也叫做线性整流函数 会是一个用来做激活函数的更好选择 如果你不确定用什么样的激活函数 用在你的隐藏层上 我推荐使用线性整流函数函数 ReLU是目前广泛被人们使用的一个方法 虽然有时候 人们也会使用双曲函数 作为激活函数 ReLU的缺点之一是 当z为负数的时候 其导数为0,但在实际应用中并不是问题 不过还有另外一个版本的ReLU 叫做Leaky Relu 我们将在下一张幻灯片上给出公式 当z为负值时，函数值不是为0 而是包含像这样的一个很小斜率 这个叫做Leaky ReLU Leaky ReLU效果通常要好于 ReLU激活函数 虽然在实践中使用的相对少 这是因为无论使用哪个，问题都是不大的 如果你必须从2个中选取一个 我通常会选用ReLU ReLU和Leaky ReLU的共有的优势是 在z的数值空间里面 激活函数的导数或者说 激活函数的斜率 离0比较远 因此在实践当中使用普通的 ReLU激活函数的话 那么神经网络的学习速度通常会比使用 双曲函数tanh或者Sigmoid函数来的更快 主要原因是使学习变慢的斜率趋向0的现象 变少了 激活函数的导数趋向于0会降低学习的速度 我们知道，一半z的数值范围 ReLU的斜率为0 但是在实际使用中 大多数的隐藏单元的z值 将会大于0，因此学习仍然可以很快 让我们快速的回顾一下 不同激活函数的优缺点 这是一个Sigmoid激活函数 我会建议 不要使用这样的函数，除了输出层上并且 你要解决的是二分类问题 或者干脆完全不使用这个函数 我几乎不使用这个函数的原因是 tanh函数相比要好很多 tanh激活函数 大概是这个样子的 而默认的最经常使用的激活函数 则是ReLU函数，它是这个样子的 当你没有特别好的选择的时候
可以使用ReLU 你也可以尝试使用 Leaky ReLU函数 其a是0.01z，z值中的 最大的0.01z和z中最大的那个 0.01z和z使得函数略微弯曲 你可能会说 为什么要使用0.01这个常数 你完全可以将其看作 算法的另外一个参数 有人说那样做效果更好 但是我很少看到有人真正去做这件事 如果你想在你的应用里面尝试一下 大胆的去尝试吧！ 你能发现其是否有效 如果效果好的话继续使用它 我希望本课程给你一些 可选的激活函数 你可以用于你自己的神经网络 事实上在深度学习中 你通常会有很多选择来
建立你的深度神经网络 你通常会有很多选择来
建立你的深度神经网络 从隐藏单元的数量 到激活函数的选择 到我们将讨论的如何初始化权重 有很多的选择 还有很多相类似的选择 多数时候很难得到现成的实践指导 来精确的适配你的问题 并最好的解决它 通过这个系列的课程 我会让你了解我在业界看到的方法 哪些更流行，哪些少流行一些 但是对于你的应用 以及你应用的特征 其实是很难事先去断定 什么方法是最好的， 通常的建议通常是 如果你不确定哪个激活函数效果最好 你可以逐个尝试使用他们 并用交叉验证集或说开发数据集 去验证他们的效果 我们将在后面讲到 看哪个方法的效果最好并选择它 我认为通过测试 这些不同的选择 你能够更好的 设计面向未来的神经网络架构 来对应你问题的特征和算法的演进 而不是我来告诉你 例如使用ReLU激活函数 而不要使用别的方法 这样的建议不一定对于 你想要在不久或者长远的将来 解决的问题 这个就是关于选择 激活函数的内容 你看到了最为流行 的几种激活函数 还有一个时常被提起的问题 为什么你需要一个 激活函数 为什么不能干脆不用它 我们会在下一个视频中讨论 （音乐） 你会理解 为什么神经网络需要某种 非线性的激活函数
GTC字幕组翻译