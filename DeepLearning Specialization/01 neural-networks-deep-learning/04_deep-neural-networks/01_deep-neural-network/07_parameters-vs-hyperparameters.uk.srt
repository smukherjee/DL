1
00:00:00,060 --> 00:00:04,380
Щоб ефективно розробляти глибокі

2
00:00:02,669 --> 00:00:06,870
нейронні мережі (НМ), необхідно

3
00:00:04,380 --> 00:00:09,269
добре налаштовувати не лише параметри, але й

4
00:00:06,870 --> 00:00:11,759
гіперпараметри. Тож що таке

5
00:00:09,269 --> 00:00:15,170
гіперпараметри? Давай подивимось. Тож

6
00:00:11,759 --> 00:00:17,820
параметрами нашої моделі є W і b. Але

7
00:00:15,170 --> 00:00:21,720
є й інші речі, які потрібно повідомляти

8
00:00:17,820 --> 00:00:26,220
навчальному алгоритму, такі як

9
00:00:21,720 --> 00:00:28,920
навчальний темп α - тому що нам потрібно

10
00:00:26,220 --> 00:00:32,329
вказати α і це буде

11
00:00:28,920 --> 00:00:34,890
визначати як розвиваються параметри;

12
00:00:32,329 --> 00:00:38,190
а також кількість ітерацій

13
00:00:34,890 --> 00:00:40,170
Градієнтного спуску, який ми втілюємо.

14
00:00:38,190 --> 00:00:42,629
Наш навчальний алгоритм має й інші

15
00:00:40,170 --> 00:00:47,340
величини, які потрібно встановити. Такі як

16
00:00:42,629 --> 00:00:50,629
кількість прихованих шарів, яку ми позначаємо

17
00:00:47,340 --> 00:00:56,039
великою L, кількість прихованих вузлів -

18
00:00:50,629 --> 00:00:59,670
n[1], n[2] і т.д.,

19
00:00:56,039 --> 00:01:03,329
також потрібно вибрати

20
00:00:59,670 --> 00:01:05,610
функцію активації - чи то

21
00:01:03,329 --> 00:01:06,869
ВЛВ [ReLU], чи гіперболічний тангенс, чи сигмоїду,

22
00:01:05,610 --> 00:01:11,760
чи щось інше для використання в

23
00:01:06,869 --> 00:01:13,590
прихованих шарах. І, отже, всі ці величини

24
00:01:11,760 --> 00:01:15,990
є величинами, які треба повідомити нашому

25
00:01:13,590 --> 00:01:19,640
навчальному алгоритму. Вони є

26
00:01:15,990 --> 00:01:22,200
параметрами, які контролюють остаточні

27
00:01:19,640 --> 00:01:25,640
параметри W і b, тож ми називаємо всі

28
00:01:22,200 --> 00:01:29,340
ці величини внизу гіперпараметрами.

29
00:01:25,640 --> 00:01:30,750
Через те, що ці величини -

30
00:01:29,340 --> 00:01:32,369
навчальний темп α, кількість ітерацій,

31
00:01:30,750 --> 00:01:36,000
кількість прихованих шарів та ін. -

32
00:01:32,369 --> 00:01:39,290
є параметрами, що контролюють W і b,

33
00:01:36,000 --> 00:01:41,970
ми називаємо їх гіперпараметрами.

34
00:01:39,290 --> 00:01:44,250
Тому, що ці гіперпараметри

35
00:01:41,970 --> 00:01:46,950
певним чином визначають остаточні

36
00:01:44,250 --> 00:01:50,100
значення параметрів W і b.

37
00:01:46,950 --> 00:01:53,520
Фактично, глибоке навчання (ГН) має

38
00:01:50,100 --> 00:01:55,470
багато різноманітних гіперпараметрів. Пізніше,

39
00:01:53,520 --> 00:01:57,899
в подальших курсах ми побачимо інші

40
00:01:55,470 --> 00:02:02,990
гіперпараметри. Такі як

41
00:01:57,899 --> 00:02:02,990
імпульсний коефіцієнт, розмір міні-серії,

42
00:02:05,150 --> 00:02:13,020
різноманітні форми параметрів

43
00:02:07,220 --> 00:02:14,700
регуляризації і т.д. Якщо ці

44
00:02:13,020 --> 00:02:16,020
поняття внизу поки що незрозумілі,

45
00:02:14,700 --> 00:02:18,810
не переймайся, ми про них поговоримо

46
00:02:16,020 --> 00:02:21,870
в 2-ому курсі. Через те, що ГН

47
00:02:18,810 --> 00:02:24,120
має так багато гіперпараметрів,

48
00:02:21,870 --> 00:02:26,370
порівняно з ранніми етапами машинного

49
00:02:24,120 --> 00:02:28,890
навчання (МН), я буду старатись

50
00:02:26,370 --> 00:02:31,050
завжди називати навчальний темп α

51
00:02:28,890 --> 00:02:33,480
гіперпараметром, а не просто

52
00:02:31,050 --> 00:02:35,250
параметром. Я думаю, що на ранніх

53
00:02:33,480 --> 00:02:37,920
етапах МН, коли не було

54
00:02:35,250 --> 00:02:39,600
так багато гіперпараметрів, більшість з нас

55
00:02:37,920 --> 00:02:42,120
зазвичай

56
00:02:39,600 --> 00:02:44,610
називали α параметром. Технічно,

57
00:02:42,120 --> 00:02:47,580
α і є параметром, але параметром,

58
00:02:44,610 --> 00:02:50,280
який визначає справжні параметри. Тож

59
00:02:47,580 --> 00:02:51,570
тепер ми намагаємось називати ці

60
00:02:50,280 --> 00:02:54,180
величини - α, кількість

61
00:02:51,570 --> 00:02:55,769
ітерацій та ін. - гіперпараметрами.

62
00:02:54,180 --> 00:02:57,810
Отже, коли ми тренуємо глибоку НМ для свого

63
00:02:55,769 --> 00:02:59,940
застосунку, то виявляємо, що є

64
00:02:57,810 --> 00:03:01,560
багато можливих значень

65
00:02:59,940 --> 00:03:04,440
гіперпараметрів, які просто потрібно

66
00:03:01,560 --> 00:03:07,230
перевірити на практиці. Тож застосування ГН сьогодні -

67
00:03:04,440 --> 00:03:09,840
дуже емпіричний процес. Часто ми просто

68
00:03:07,230 --> 00:03:12,150
маємо ідею, наприклад, вона може

69
00:03:09,840 --> 00:03:13,549
полягати в тому, щоб знайти найкраще значення для

70
00:03:12,150 --> 00:03:16,739
навчального темпу. Наприклад, ми припускаємо, що

71
00:03:13,549 --> 00:03:20,670
α має бути 0,01. І таким його і встановлюємо.

72
00:03:16,739 --> 00:03:22,530
Потім перевіряємо,

73
00:03:20,670 --> 00:03:23,910
дивимось як воно працює. А потім, отримавши

74
00:03:22,530 --> 00:03:25,890
результат, ми можемо

75
00:03:23,910 --> 00:03:28,620
змінити наше припущення і збільшити

76
00:03:25,890 --> 00:03:30,930
навчальний темп до 0,05. Тож, якщо

77
00:03:28,620 --> 00:03:32,970
ми не впевнені яке найкраще значення

78
00:03:30,930 --> 00:03:35,010
для навчального темпу використати, то можна

79
00:03:32,970 --> 00:03:37,680
спробувати одне значення навчального темпу α

80
00:03:35,010 --> 00:03:39,690
і побачити, що функція Витрат J прямує вниз

81
00:03:37,680 --> 00:03:41,820
ось так, потім можна спробувати більше

82
00:03:39,690 --> 00:03:43,650
значення для навчального темпу α і

83
00:03:41,820 --> 00:03:45,060
побачити, що функція Витрат йде вгору або

84
00:03:43,650 --> 00:03:47,250
розбігається, а потім можна спробувати ще якесь

85
00:03:45,060 --> 00:03:49,709
значення і побачити, що вона різко спускається вниз,

86
00:03:47,250 --> 00:03:51,780
але збігається до більшого значення. Можна

87
00:03:49,709 --> 00:03:53,670
спробувати ще якесь значення і побачити,

88
00:03:51,780 --> 00:03:55,530
що функція Витрат J робить отак. Потім,

89
00:03:53,670 --> 00:03:57,840
після кількох спроб, можна

90
00:03:55,530 --> 00:04:00,870
буде зробити висновок, що, наприклад, оце значення α

91
00:03:57,840 --> 00:04:02,790
дає нам досить швидке навчання

92
00:04:00,870 --> 00:04:04,290
і дозволяє функції Витрат J збігатись

93
00:04:02,790 --> 00:04:06,720
до меншого значення J, тож його ми й

94
00:04:04,290 --> 00:04:08,040
будемо використовувати. На попередньому

95
00:04:06,720 --> 00:04:10,170
слайді ми бачили багато

96
00:04:08,040 --> 00:04:11,489
різноманітних гіперпараметрів. Виявляється,

97
00:04:10,170 --> 00:04:13,830
що коли ми починаємо вирішувати робити новий

98
00:04:11,489 --> 00:04:15,450
застосунок, то буває дуже

99
00:04:13,830 --> 00:04:17,940
складно точно знати наперед

100
00:04:15,450 --> 00:04:20,580
які найкращі значення мають мати гіперпараметри.

101
00:04:17,940 --> 00:04:22,169
Тож часто ми просто

102
00:04:20,580 --> 00:04:24,570
перевіряємо багато різних

103
00:04:22,169 --> 00:04:26,970
значень і весь час проходимо оце коло.

104
00:04:24,570 --> 00:04:28,440
Ми пробуємо певні значення. Можливо, 5 прихованих

105
00:04:26,970 --> 00:04:31,140
шарів з певною кількістю прихованих

106
00:04:28,440 --> 00:04:34,140
вузлів, реалізуємо їх і дивимось чи вони працюють.

107
00:04:31,140 --> 00:04:36,180
І знову по колу. Тож назва цього слайду -

108
00:04:34,140 --> 00:04:38,340
"Прикладне ГН - дуже

109
00:04:36,180 --> 00:04:40,740
емпіричний процес". Емпіричний процес,

110
00:04:38,340 --> 00:04:42,419
можливо, - вигадливий спосіб сказати, що

111
00:04:40,740 --> 00:04:45,330
потрібно просто перевіряти багато значень і дивитись

112
00:04:42,419 --> 00:04:47,190
які спрацюють. Також я помітив, що

113
00:04:45,330 --> 00:04:48,810
ГН зараз застосовується до

114
00:04:47,190 --> 00:04:51,990
величезної кількості задач: від комп'ютерного

115
00:04:48,810 --> 00:04:53,789
бачення до розпізнавання мовлення, до обробки

116
00:04:51,990 --> 00:04:55,500
людської мови, до багатьох

117
00:04:53,789 --> 00:04:59,250
застосунків для структурованих даних, таких як

118
00:04:55,500 --> 00:05:02,430
онлайн реклами або інтернет пошуку,

119
00:04:59,250 --> 00:05:05,640
або рекомендації продуктів і т.д.

120
00:05:02,430 --> 00:05:08,190
Я помітив, що, по-перше,

121
00:05:05,640 --> 00:05:10,080
дослідники з будь-якої з цих дисциплін

122
00:05:08,190 --> 00:05:12,060
часом переходять до іншої

123
00:05:10,080 --> 00:05:14,400
і інколи їх розуміння гіперпараметрів

124
00:05:12,060 --> 00:05:16,590
може бути застосовним там, а інколи - ні.

125
00:05:14,400 --> 00:05:17,849
Тож я часто раджу людям,

126
00:05:16,590 --> 00:05:20,970
особливо коли вони починають розв'язувати нову

127
00:05:17,849 --> 00:05:23,550
задачу, пробувати набір

128
00:05:20,970 --> 00:05:25,500
значень і дивитись як це працює. І в наступному

129
00:05:23,550 --> 00:05:27,930
курсі ми побачимо систематизований спосіб,

130
00:05:25,500 --> 00:05:30,780
кілька систематичний способів підбору

131
00:05:27,930 --> 00:05:32,070
набору величин. А, по-друге,

132
00:05:30,780 --> 00:05:33,570
навіть якщо ти працюєш над одним

133
00:05:32,070 --> 00:05:35,220
застосунком довгий час,

134
00:05:33,570 --> 00:05:37,979
скажімо, ти працюєш над онлайн

135
00:05:35,220 --> 00:05:39,930
рекламою, то, досягши певного прогресу в

136
00:05:37,979 --> 00:05:41,580
задачі, дуже ймовірно, що найкращі

137
00:05:39,930 --> 00:05:43,830
величини для навчального темпу α, кількості

138
00:05:41,580 --> 00:05:46,440
прихованих вузлів та ін., можуть змінитись. Тож,

139
00:05:43,830 --> 00:05:49,229
навіть якщо ти налаштував систему з найкращими

140
00:05:46,440 --> 00:05:51,750
значеннями гіперпараметрів сьогодні,

141
00:05:49,229 --> 00:05:53,430
то, можливо, через рік ти відмітиш, що

142
00:05:51,750 --> 00:05:55,650
найкращі значення змінились. Можливо

143
00:05:53,430 --> 00:05:57,840
це станеться через обчислювальну інфраструктуру -

144
00:05:55,650 --> 00:05:59,789
зміниться кількість бітів ЦП [CPU] або тип ГП [GPU],

145
00:05:57,840 --> 00:06:01,560
на якому проводиться навчання, чи ще щось.

146
00:05:59,789 --> 00:06:03,659
Через це одним з практичних правил є

147
00:06:01,560 --> 00:06:05,070
час від часу (можливо раз в кілька

148
00:06:03,659 --> 00:06:06,659
місяців, якщо робота над задачею триватиме

149
00:06:05,070 --> 00:06:09,030
досить довго, скажімо, багато

150
00:06:06,659 --> 00:06:10,800
років) просто підставляй кілька величин в

151
00:06:09,030 --> 00:06:12,570
значення гіперпараметрів і перевіряй чи

152
00:06:10,800 --> 00:06:15,150
нема кращих значень для гіперпараметрів.

153
00:06:12,570 --> 00:06:17,280
Роблячи так, ти поступово

154
00:06:15,150 --> 00:06:18,779
здобудеш розуміння того, які гіперпараметри

155
00:06:17,280 --> 00:06:19,870
працюють для твого типу

156
00:06:18,779 --> 00:06:21,820
задач.

157
00:06:19,870 --> 00:06:24,010
Я знаю, що здається ніби

158
00:06:21,820 --> 00:06:25,510
це не надто приємна частина ГН,

159
00:06:24,010 --> 00:06:27,940
бо потрібно перепробувати так багато значень

160
00:06:25,510 --> 00:06:30,160
для цих гіперпараметрів, але, можливо,

161
00:06:27,940 --> 00:06:32,200
це - одна з областей, в якій

162
00:06:30,160 --> 00:06:33,850
дослідження ГН все ще просувається вперед і, можливо,

163
00:06:32,200 --> 00:06:36,190
через деякий час ми зможемо давати кращі

164
00:06:33,850 --> 00:06:38,350
поради для підбору найкращих гіперпараметрів.

165
00:06:36,190 --> 00:06:41,260
Проте, також може бути, що

166
00:06:38,350 --> 00:06:43,630
через те, що ЦП, ГП, НМ

167
00:06:41,260 --> 00:06:45,910
і набори даних постійно змінюються,

168
00:06:43,630 --> 00:06:47,680
інструкції не будуть

169
00:06:45,910 --> 00:06:49,360
структуруватись ще певний час і нам необхідно

170
00:06:47,680 --> 00:06:50,860
продовжувати пробувати різні значення і

171
00:06:49,360 --> 00:06:52,479
оцінювати їх набором

172
00:06:50,860 --> 00:06:54,100
перехресної перевірки або чимсь іншим і

173
00:06:52,479 --> 00:06:56,350
знаходити працюючі значення, які підходять під конкретну задачу.

174
00:06:54,100 --> 00:06:58,870
Отже, це був короткий огляд

175
00:06:56,350 --> 00:07:01,030
гіперпараметрів. В 2-ому курсі

176
00:06:58,870 --> 00:07:03,280
ми дамо також кілька підказок як

177
00:07:01,030 --> 00:07:06,040
систематично досліджувати набір

178
00:07:03,280 --> 00:07:07,570
гіперпараметрів. Тож на даний момент ти вже

179
00:07:06,040 --> 00:07:09,430
маєш практично всі потрібні інструменти,

180
00:07:07,570 --> 00:07:11,470
щоб виконати програмувальну вправу. Перед тим як

181
00:07:09,430 --> 00:07:14,050
ти приступиш я поділюсь ще одним

182
00:07:11,470 --> 00:07:16,150
набором ідей, які часто обдумую,

183
00:07:14,050 --> 00:07:18,660
а саме, наскільки ГН подібне до

184
00:07:16,150 --> 00:07:18,660
людського мозку.