歡迎來到這個課程的第四週 您已經看過正向傳播跟
反向傳播在 使用一個隱藏層的神經網路
跟羅吉斯迴歸分析 您學過向量化跟 隨機初始權值是很重要的 如果您做了前幾個星期個
作業, 您已經建置且 自己看過這些觀念是可行的 所以, 現在 您已經看了大部分個觀念
您需要建置一個深度學習網路 我們這一週要做的是, 把
這些觀念放在一起 您可以建置您自己的深度學習網路 因為這個禮拜的程式
作業比較長 也比較多工作
我將會讓這個禮拜的影片 短一點您可以比較快看完
這些影片然後 有更多的時間花在
最後的重大的問題練習, 我希望 會讓您建置一個讓您驕傲的
深度學習網路 什麼是深度學習網路 您看過這個
羅吉斯迴歸分析的圖形 您也看過神經網路
用一個單一隱藏層 這個例子是神經
網路用兩個隱藏層 五個隱藏層的神經網路 我們說羅吉斯迴歸分析
是很淺層的模型 而這個模型是
更深層的模型 淺跟深的差別
在於幾層 神經網路
用一個隱藏層 這會是兩層的神經網路 記得當我們數神經
網路幾層時，我們不算輸入層 我們只算隱藏層
跟輸出層 所以這會是兩層神經
網路還是很淺 但不像羅吉斯迴歸分析那樣淺 技術上來講羅吉斯迴歸分析
是一層的神經網路 過去這些年來在 AI 在機器學習社群
了解到這項功用 亦即在很深的神經網路可以學的
比較淺的通常沒辦法做到 雖然對於一件問題
很難事前去預估 幾層的網路是您需要的 以下這樣做是合理, 先試
羅吉斯迴歸分析, 試一個 兩個隱藏層, 然後將
多少層當作是超 參數您可以試
不同的數字, 然後 用交叉驗證資料來評估
或者用您的開發集 我們以後會談到 現在讓我們看一下我們
用來描述深度神經網路的記號 這是一個 一, 二,  三
四層的神經網路 有三層隱藏層
而這些隱藏層的單元數目是 我猜 5, 5 ,3
然後一個輸出單元 我們要用的記號是 用大寫 L 來記為
網路的層數 在這個例子, L = 4 
也就是幾層的數字 我們將用 n 上標
[l] 來記為節點的數目 或者說單元的數目
在小寫 l 層上 如果我們指標這個
輸入層是 "0" 層 這是 層1, 層2, 
這是層3, 這是層4 然後我們用這個， 舉個例子
n[1], 會是這個 第一個, 會是等於5
因為有五個隱藏單元 對於這一個我們說 n[2] 單元數目在
第二隱藏層 也是等於 5, n[3] = 3 n[4] = n[L] 
 輸出單元的數目是 1 因為您的大寫 L 等於 4 我們同樣也對於 輸入層設為 n[0] = nx = 3 這是我們用來描述
節點的數目在我們不同 層上 對於每一層 L, 我們將使用 a[l] 記為 l 層的啟動值 我們稍後會看到
正向傳播時 您計算 a[l] 是
啟動值 g(z[l]) 也許啟動函數
也標上 l 層 然後我們用 w[l] 來表示
權重對於 計算 z[l] 的值在 l 層 同樣的  b[l] 是用來計算 z[l] 最後, 總結一下這些記號
輸入特徵稱為 x 但 x 也是
第0層的啟動值, 所以 a[0] = x 而最後一層的啟動值
a[L] = y-hat 所以a[L] 等於預估值
預估 y-hat 在這個神經網路 您現在知道
深度神經網路的樣子 跟這些記號來描述
跟計算深度網路 我知道我引進了很多記號
在這段影片中， 但如果您忘記了 一些符號的意義， 我們也放在
課程網站裡, 記號表或 記號導引, 您可以查表
來看這些不同符號的意義 下一個， 我要描述正向
傳播在這樣網路 的樣子 讓我們進入下一段影片