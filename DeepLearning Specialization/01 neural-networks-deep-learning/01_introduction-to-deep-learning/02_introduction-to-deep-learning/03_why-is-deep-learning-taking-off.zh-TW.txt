如果在深度學習背後的基本技術思維, 在新興網絡背後的基本技術思維已經存在好幾十年 為什麼它們只剛開始起飛 在這段影片中, 讓我們看一些 深度學習發展背後的主要驅動力，因為我認為 這能幫助你找到最好的機會 在你所在的組織來運用這些驅動力 在過去幾年 很多人問過我 "Andrew, 為什麼深度學習突然作用得這麼好" 當我被問到這個問題時，我通常會畫這張圖給他們 我們畫的這張圖，在橫軸 我們畫出一件任務的資料量 在縱軸 我們畫出我們學習表現的演算 像是我們垃圾資訊分類的準確度，或是我們 廣告點選預測，或是 我們神經系統對於找出其他車子與我們正在開的車子的相對位置的準確度 結果發現，如果你畫的是傳統的學習表現演算，像是 支援向量機器，或是 羅吉斯迴歸分析當成是一個資料量的函數 你可能會得出像是這樣的一條曲線 一開始當你增加數據，學習表現會增加，但是過一陣子 學習表現會到達高原期 這應該是一條水平線，我沒畫好 這就像是他們不知道怎麼處理大量的數據 我們的社群在 過去20年間所遇到的問題是 我們從擁有相對少量的數據 到擁有非常大量的數據 而這是歸功於社會的數位化 因為很多人類活動都發生在數位領域 我們花很多的時間在我們的電腦, 在網站, 在行動裝置, 在電子裝置上創建資料 多虧有了很多便宜的相機 與我們的手機，加速度計 以及所有在網路裡的感測器 我們一直在蒐集更多更多的資料 所以，在過去的20年間，因為有很多應用程式 我們收集了非常多的數據，數量超過 傳統的學習演算法能夠有效的利用 有了神經網絡，如果你訓練 一個小的神經網絡，他的表現可能會是這樣 如果你訓練一個大一點的神經網絡 我們稱之為中型神經網絡 表現通常會比較好，如果你訓練一個非常大的神經網絡 他的表現就會不斷地變好 所以，有幾個觀察發現，其中一個是 如果你想要達到非常高的表現 你需要兩件事 首先, 你往往需要能夠訓練 一個夠大的神經網絡，以至於能夠 有效利用大量的數據，第二 你必須要在x軸的外邊 你需要很多數據 所以，我們說推動深度學習進步的是規模。當我說規模， 我一是指是神經網路的大小， 意思是神經網路有很多的隱藏單元，很多參數， 很多的連結,二是資料的規模 事實上，現在獲得神經網路更好性能最可靠的方法之一 是訓練更大的神經網路，或是往裡面丟更多數據 而這只可行到一定程度。因為 最終你會用盡所有數據或神經網路會 大到需要花太長的時間來訓練，但是 僅僅提高規模已經幫我們在深度學習的世界裡達成了許多。 為了要讓這個圖增加一點技術上的精確度 讓我再加一些東西 我在Ｘ軸上寫下數據量 技術上來說，這是有標籤的數據量 所謂「標籤」過的資料，我指的是訓練資料有輸入 x， 以及其相對應的標籤輸出 y 。 接下來引入往後課程中會使用的符號： 我們會用小寫的 m 描述訓練資料集的大小。 也就是說，這裡我們的橫軸「訓練資料量」就是 m。 一些其他有關這個圖的細節 在這個區域是小的訓練集 這相對的演算法排名實際上不是那麼好定義 所以, 如果你沒有很多的訓練資料, 它往往是 依賴您的技術來手工打造特徵決定表現 所以, 很有可能, 如果有人利用'支援向量機'(SVM:Support Vector Machine) 訓練會 更有促動力去用手工設計特徵，相比於使用更大的神經網路訓練, 有可能在小訓練集小的時候 支援向量機(SVM)可以做得更好。 所以在這個圖形的靠左邊區域 演算法的效果排序 並不是那麼確定，更多取決於你在 選取特徵的技巧、或是其他細節上。 只是在大量資料的區域 很大的訓練集 也就是圖上右邊，m 很大的區域 我們一般會看到大型神經網路會有比較好的表現。 因此，如果有朋友問你「為甚麼神經網路正在興起」 我會鼓勵您畫這個圖讓他們了解 我會說在早期 現代深度學習的興起 是因為資料規模跟計算規模 反映在我們訓練大型神經網路的能力上 無論是在 cpu 還是在 gpu 上, 使我們取得了很大的進步。 但是逐漸地，尤其是最近幾年 我們也看到很多演算法上的巨大革新 所以, 我也不想低估這一點。 有趣的是，許多演算法的革新 一直在嘗試讓神經網路運行得更快。 舉一個具體的例子 一個重大的突破在神經網路是從 從 S型函數(Sigmoid), 看起來是這樣的, 換到 一個線性整流(ReLU)數學函數, 我們在先前的影片中談到 , 並進行了簡要介紹, 如下所示。 如果您不知道這些我將談到的細節, 不用擔心 但實際上使用S型函數(Sigmoid function)的問題之一在 機器學習是有些區域像這裡這個函數的斜率 其中梯度接近零, 所以學習變得非常緩慢 因為當你執行梯度下降,而梯度是零, 參數會改變得很慢學習也會很慢 而通過改變神經網路的啟動函數, 使用此函數稱為 "值函數"或 "線性整流單位", ReLU, 對於輸入的所有正值, 漸變梯度等於 1, 對吧。 因此, 梯度逐漸縮小到零的可能性要低得多。 這裡的坡度, 這條線的斜率是零 在左邊, 但事實證明, 只是通過換 S型函數(Sigmoid)到ReLU函數(ReLU) 會使梯度下降演算法工作得快得多. 所以, 這是一個例子, 也許一個相對簡單的演算法創新, 但 最終的影響, 這種演算法的創新是它真對計算非常有幫助。 所以, 實際上已經有相當多像這樣的例子, 在那裡, 我們改變演算法, 因為它允許我們的程式運行的 更快, 這讓我們訓練更大的神經網路或推論,在 合理的時間內, 即使我們有一個大的網路或大量的資料。 另一個理由是快速的計算對於 常反覆訓練的神經網路過程是很重要的 通常您有一個想法在 神經網路架構而您建立您的想法在程式裡邊 實行您的想法, 然後讓您進行實驗 它告訴你你的神經網路設計有多好, 然後通過看它, 你回去更改 你的神經網路細節, 然後你繞著這個循環步驟一遍又一遍。 當你的新網路需要很長時間來訓練時, 它是需要很長的時間去經過這個循環週期, 有 在你建立有效的神經網路時有一個很大的效率差異, , 每當你 有一個想法, 並嘗試實驗它, 在10分鐘內看看它是否有效 或者最多用一天; 相對於如你用一個月的時間訓練神經網路。 好吧, 有時它確實發生的, 因為當你得到 結果在10分鐘或是一天內, 您可以試很多想法而更能夠 發現非常適合您應用的一個神經網路 所以快速的計算能力確實已經幫助 加快了你可以得到實驗結果回來的速度, 而這非常幫助於神經網路的從業者, 以及 在深度學習領域工作的研究人員,循環得 更快, 改進想法更快。 所以, 這一切是一個巨大的利多對於 整個深度學習研究社區, 我認為這是令人難以置信的, 發明新的演算法, 並在這方面取得不斷的進展。 這一些力量增強了深度學習的興起, 但好消息是 這些力量仍然在有力地工作, 使深度學習更好。用資料作例子, 社會仍在創作越來越多的數位資料。或在 計算能力的例子, 像GPU等特殊硬體的興起和更快的網路, 很多種硬體 我實際上很有信心, 我們的能力來建立非常大的神經網路, 從一個純粹的 計算的角度來看, 將不斷變得更好。在演算法方面, 整個深度學習研究社區 在演算法方面的創新仍然是驚人的。 所以, 正因為如此, 我認為我們可以樂觀。 我當然樂觀地認為, 深度學習將保持 在未來的許多年裡變得越來越好。 所以, 有了這個, 讓我們繼續到這一節的最後一段影片 在這裡我們將更多地談談你從這門課程中學到的東西。