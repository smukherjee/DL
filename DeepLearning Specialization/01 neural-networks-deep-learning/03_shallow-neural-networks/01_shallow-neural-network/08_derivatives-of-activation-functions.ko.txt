여러분이 신경망을 위해서 후 
방향전파을 도입하는 경우 여러분은 정말로 activation 함수의 
derivative나 기울기를 구해야 하는데요, 이제 activation 함수의 선택권을 한번 보고 이러한 함수의 기울기를 
어떻게 구하는지 살펴보겠습니다. 여러분이 익숙한 시그모이드 함수인데요 어떠한 z의 값에 대해, 
여기 z값은 어떤 기울기를 갖거나 여기 조금한 선을 그리면, 
여기에 해당하는 derivative 값을 가질 수 있습니다. 이 경우, 높이와 너비를 알 수 있는데요 
여기 삼각형에서 말이죠. 그러면, g(z)가 시그모이드 함수인 
경우 기울기는 ddz g(z) 입니다. 저희는 미적분을 통해 이 값이 g(x) 함수에서 여러분이 미적분에 익숙하고 
derivative를 어떻게 구하는지 아시면, 여기 시그모이드 함수에서 derivative를 구하면, 여기 이 공삭과 동일하다는 것을 알 수 있습니다. 다시 한번 여기서는 미적분 
단계를 진행하지 않을 것입니다. 하지만 여러분이 미적분에 익숙하시면 언제든지 잠시 비디오를 멈추시고 직접 
한번 계산해서 증명해보세요. 그럼 여기는 g(z) 곱하기 1 빼기 g(z)인데요, 이 표현이 말이 되는지 체크를 한번 해보겠습니다. 처음으로, 만약 z가 매우 큰 값이면, 이 경우, z가 10이라고 해보겠습니다. 그러면 g(z)는 1과 가까울텐데요, 
그러면 왼쪽에 있는 공식은 저희에게 알려주는데요, ddz g(z) 는 1 곱하기 1빼기 1과 
가깝다는 것을 알려줄 것입니다. 이것은 결국 0과 근접한 값인데요, 이것은 틀리지 않습니다. 왜냐면 z가 매우 
큰값인 경우, 기울기가 0이기 때문입니다. 반대로 z가 -10인 경우, 여기까지 가있다고 
하는 것입니다. 그러면 g(z)는 0과 가깝습니다. 그러면 왼쪽에서는 dzz g(z) 가 g(z)와 가까울 텐데요, g(z)는 0 곱하기 (1-0) 
인데요 이 값은 역시 0과 근접합니다. 동일하게 맞죠. 그리고 마지막으로 z가 0인 경우, 
g(z)는 2분의 1 인데요 여기 시그모이드 함수인데요. 그러면 derivative는 1 나누기 2 곱하기 1 빼기 1/2 이고 이 값은 4분의 1입니다. 이 값은 이 함수의 derivative 또는 기울기가 맞습니다. z가 0일때 말이죠. 마지막으로 한가지 더 표기 관련하여 소개하자면, 가끔은 여기 이것을 적는 것 대신해서 짧게 derivative를 쓰는 방법은 g 프라임 z 입니다. 그러면 g'(z)는 미적분에서 여기 대쉬를 
위에 붙히면 이것을 프라임이라고 하며, g 프라임 z 는 미적분에서 g 함수의 derivative를 뜻합니다. 
입력값 z 변수에 대해서 말이죠. 그리고 신경망에서는 a 가 g(z)와 일치하고 이 값이 또한 곱하기 (1-a)를 심플하게 정리됩니다. 가끔씩 도입하는 과정에서 이렇게 g'(z)=a 곱하기 1 빼기 a 를 보실텐데요, 이것은 g 프라임이 observation인 경우를 뜻하는데요 이 것은 derivative를 뜻하고. 그 값은 이것입니다. 이 공식의 장점은 만약 a를 이미 구했을 때, 여기 이 표현을 쓰면서, 굉장히 빨리 
이것의 기울기도 구할 수 있는 것입니다. g 프라임 값을 말이죠. 여기까지 내용이 시그모이드 
activation 함수의 내용이였는데요. 이제 tan h activation 함수를 보겠습니다. 이전에 봤던 것과 비슷하게, ddz g(z)의 정의는 g(z)에서 그 값이 z일때의 기울기를 쯧합니다. 그 값이 z일때의 기울기를 쯧합니다. 
그리고 미적분학을 알면, 이것의 derivative값을 구하여 공식을 간단하세 표현하면, 
이렇게 간단하게 표현되는데요, 그리고 이전에 배운대로 이것을 g 프라임 z로 부르겠습니다. 원하시면 이 공식이 말이 
되는지 체크하셔도 좋습니다. 예를 들어 z 가 10인 경우 tan h (z)는 1과 가까울 것입니다. 이것은 1 에서 -1로 가는데요, 그러면 g'(z)는 이 공식에 의하면, 1-1의 제곱이므로 그 값이 0과 가깝게 됩니다. 
이것은 z가 큰 경우입니다. 1-1의 제곱이므로 그 값이 0과 가깝게 됩니다. 
이것은 z가 큰 경우입니다. 반대로 만약 z의 값이 매우 작다면 z가 -10이라면 tan h(z) 는 -1 과 가까울 것입니다. 그러므로 g'(z) 는 1 빼기 -1 의 제곱입니다. 그러므로 1 - 1 과 가까운 것인데요 
그러면 마찬가지로 그값은 0이 됩니다. 마지막으로 z가 0인 경우인데요, 
그러면 tan h(z)=0 인데요, 이 경우에는 기울기가 1입니다. 이 부분은 z의 값이 0일때의 기울기 점을 나타냅니다. 요약하자면, 만약 a 가 g(z)라면, 
만약 a=tan h (z), 그러면 derivative g 프라임 z 는 1- 제곱인데요 만약 a의 값을 이미 계산한 경우에는 이 공식을 사용해서 재빨리 
derivative의 값을 구할 수 있습니다. 마지막으로 여기서 ReLu 함수와 Leaky ReLu 함수의 derivative를 
찾는 방법을 말씀 드리겠습니다. g(z)=max (0, z)의 값에 대해서 이것의 기울기는 만약 z가 0보다 작으면 그 값은 0입니다. 그리고 만약 z의 
값이 0보다 큰 경우, 기울기 값이 1이 됩니다. 그리고 undefined인데요 엄밀히 이야기하면 
정의 할 수 없는 undefined 입니다. z가 정확히 0일 경우에 말이죠. 이것을 소프트웨어에 도입시키는 경우, 
수학적으로 100퍼센트 올바르지 않을 수 있습니다. 하지만 z의 값이 0인 부분에 대해서 기울기 값을 정확히 1로 
지정하거나 또는 0으로 지정해도 됩니다. 대해서 기울기 값을 정확히 1로 
지정하거나 또는 0으로 지정해도 됩니다. 최적화 전문가라면 엄밀히 이야기하면 g 프라임은 그러면 
g(z) 함수에 대해 sub gradient 함수 가 됩니다. 그렇기 때문에 기울기 강하가 계속 작동할 것입니다. 이것을 생각하는 방법은, 
z가 정확히 0이 되는 확률이 너무나도 작은 확률이기 때문에 별로 상관이 없다고 
생각하면 됩니다. z가 0일때 어떤 derivative값을 설정하는지 별로 상관 없다는 관점인 것입니다. 실제로는, 사람들이 z의 derivative에 
대해서 이렇게 도입합니다. 마지막으로 여러분이 직접 
Leaky ReLu activation 함수에서 트레이닝 하는 경우에, 마지막으로 여러분이 직접 
Leaky ReLu activation 함수에서 트레이닝 하는 경우에, g(z)는 max (0.01z, z)가 될텐데요, 그러므로 g 프라임 z는 만약 z값이 0보다 작을때 0.01일 텐데요, 0.01일 텐데요, 0보다 클 경우, 0보다 클 경우, 1일 것입니다. 다시 한번, 기울기는 z의 값이 
0일 때는 정의되지 않습니다. 그러므로 코드를 도입해서 기울기의 값 또는 g'(z)의 값이 z의 값이 0인 경우의 기울기로 반영하면 코드가 잘 작동할 것입니다. 이제 이런 공식으로 무장되었으니, 여러분은 activation 함수의 기울기를 구하실 수 있을 것입니다. 이제 기본 블럭이 되는 내용을 다루었으니 여러분은 기울기 강하를 신경망에서 
도입하는 방법을 보실 준비가 되셨을 것입니다. 다음 비디오를 통해서 보도록 하겠습니다.