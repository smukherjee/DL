1
00:00:00,590 --> 00:00:03,210
您已經看過羅吉斯迴歸分析模型

2
00:00:03,210 --> 00:00:06,560
您看過用損失函數來
衡量您做得如何

3
00:00:06,560 --> 00:00:08,780
在單一訓練例子上

4
00:00:08,780 --> 00:00:13,530
您也看過成本函數來
衡量您的參數 w 跟 

5
00:00:13,530 --> 00:00:16,590
b 作用到整個訓練集的表現

6
00:00:16,590 --> 00:00:21,600
現在讓我們談談您如何使用
梯度下降演算法來訓練

7
00:00:21,600 --> 00:00:25,730
或者說學習, 參數 w跟 
b 在您的訓練集上

8
00:00:25,730 --> 00:00:30,030
回顧一下，這是熟悉的
羅吉斯迴歸分析演算法

9
00:00:31,130 --> 00:00:34,700
我們在第二
行有成本函數，J，

10
00:00:34,700 --> 00:00:37,879
就是一個函數對於
您的參數 w 跟 b

11
00:00:37,879 --> 00:00:39,960
而這定義為平均數

12
00:00:39,960 --> 00:00:44,140
所以是 1 除以 m 乘上和的
這個損失函數

13
00:00:44,140 --> 00:00:48,470
這損失函數
衡量了您的演算法

14
00:00:48,470 --> 00:00:53,170
輸出 y-hat(i) 在每一個訓練例子作用得如何, 疊起來或者

15
00:00:53,170 --> 00:00:58,000
跟標準答案標籤 y(i) 比較
在每一個訓練例子

16
00:00:58,000 --> 00:01:00,886
完整的公式
展開在右邊

17
00:01:00,886 --> 00:01:04,130
成本函數衡量
您的參數 w 跟

18
00:01:04,130 --> 00:01:06,760
b 作用在訓練集的表現如何

19
00:01:06,760 --> 00:01:11,510
為了要學習參數
 w 跟 b 似乎自然的方式是我們想要去

20
00:01:11,510 --> 00:01:17,930
找 w 跟 b 使得成本
函數 j(w,b) 越小越好

21
00:01:17,930 --> 00:01:21,320
這裡是
梯度下降法的圖示

22
00:01:21,320 --> 00:01:25,320
在這個圖形中
水平軸代表了

23
00:01:25,320 --> 00:01:28,510
您的參數 w 跟 b

24
00:01:28,510 --> 00:01:32,350
實際上 w 可能是更高
維度, 但為了

25
00:01:32,350 --> 00:01:38,190
畫圖方便, 讓我們畫成單一
實數, b 也是單一實數

26
00:01:38,190 --> 00:01:40,770
成本函數 J(w,b) 是

27
00:01:40,770 --> 00:01:45,130
一個曲面在
水平軸 w 跟 b 上方

28
00:01:45,130 --> 00:01:50,720
這個曲面的高代表
 j(w,b) 的值在一定點

29
00:01:50,720 --> 00:01:55,070
我們要做的是實際上
去找到 w 跟

30
00:01:55,070 --> 00:01:59,730
b 的值相對於這個
成本函數的最小值

31
00:02:00,830 --> 00:02:06,050
實際上這個成本
函數是凸形函數

32
00:02:06,050 --> 00:02:10,327
也就是一個碗狀
這是一個凸形函數

33
00:02:10,327 --> 00:02:13,717
這跟這種函數
像這樣相反

34
00:02:13,717 --> 00:02:18,120
這種是非凸函數
有許多的不同的局部最佳化

35
00:02:18,120 --> 00:02:22,240
事實是我們成本
函數這樣定義

36
00:02:22,240 --> 00:02:27,020
是凸形函數也是重要的原因
這是為什麼我們用這個特殊的函數

37
00:02:27,020 --> 00:02:29,610
j 用在羅吉斯迴歸分析上

38
00:02:29,610 --> 00:02:33,810
為了找到好的值來當
參數

39
00:02:33,810 --> 00:02:39,160
我們要做的是初始化 w 跟
 b 一些初始值

40
00:02:39,160 --> 00:02:43,360
也許記為一個小紅點

41
00:02:43,360 --> 00:02:47,562
而對於羅吉斯迴歸分析幾乎
任何初始化都可行

42
00:02:47,562 --> 00:02:50,690
通常您初始值為 0

43
00:02:50,690 --> 00:02:52,910
隨機初始化也行, 但

44
00:02:52,910 --> 00:02:55,630
人們在羅吉斯迴歸分析不這樣做

45
00:02:55,630 --> 00:02:59,310
因為這個函數是凸形函數
不論您初始哪裡

46
00:02:59,310 --> 00:03:02,180
您應該會到達同一點
或者大約同一點

47
00:03:02,180 --> 00:03:06,450
而梯度下降做法是
它開始於起始點

48
00:03:06,450 --> 00:03:10,310
然後往最陡的下坡方向邁出一步

49
00:03:10,310 --> 00:03:15,290
所以在梯度下降的第一步後
您可能會在這裡, 因為

50
00:03:15,290 --> 00:03:19,320
它試圖在最速下降
方向邁出一步往下坡或

51
00:03:19,320 --> 00:03:21,250
盡可能的儘快下山

52
00:03:21,250 --> 00:03:23,600
這是一個梯度下降的一次迴圈

53
00:03:23,600 --> 00:03:27,084
經過兩次迴圈梯度
下降, 您也許走到這裡

54
00:03:27,084 --> 00:03:28,830
三次迴圈等等

55
00:03:28,830 --> 00:03:32,640
我猜現在是隱藏在
圖的背後直到最終

56
00:03:32,640 --> 00:03:38,880
收斂到這個全域最佳化或者
接近於全域最佳化

57
00:03:38,880 --> 00:03:42,300
這個圖形說明了
梯度下降演算法

58
00:03:42,300 --> 00:03:44,310
讓我們寫詳細一點

59
00:03:44,310 --> 00:03:47,750
為了進行說明, 假設
有一些函數 J(w)

60
00:03:47,750 --> 00:03:51,700
您想要最小值,而
或許函數像這樣

61
00:03:51,700 --> 00:03:54,650
為了便於繪製
我先忽略 b

62
00:03:54,650 --> 00:03:59,210
只畫一維
圖形, 而非多維圖形

63
00:03:59,210 --> 00:04:01,240
梯度下降法做這件事

64
00:04:01,240 --> 00:04:06,740
我們要反覆進行下列更新

65
00:04:06,740 --> 00:04:09,467
我們取 w 的值然後
更新它

66
00:04:09,467 --> 00:04:12,508
用 冒號等號 
來代表更新 w

67
00:04:12,508 --> 00:04:17,426
設 w 為 w 減 alpha 乘上

68
00:04:17,426 --> 00:04:22,200
這個導數 dj(w)/dw

69
00:04:22,200 --> 00:04:26,230
我會重複做這樣
直到演算法收斂

70
00:04:26,230 --> 00:04:30,666
一些關於符號的註解
alpha 是學習率

71
00:04:30,666 --> 00:04:36,820
控制了步伐大小
在每個梯度下降的迴圈

72
00:04:36,820 --> 00:04:41,200
我們將來會談論一些方式
來選擇學習率 alpha

73
00:04:41,200 --> 00:04:44,490
第二, 這一個量
這是導數

74
00:04:44,490 --> 00:04:48,010
這基本上是您想做的更新或改變
到參數 w

75
00:04:48,010 --> 00:04:52,700
當我們開始寫程式去
建置梯度下降

76
00:04:52,700 --> 00:04:57,380
我們的約定會用
這個變數名稱在我們的程式

77
00:04:58,620 --> 00:05:02,300
dw 會用來代表
這個導數項目

78
00:05:02,300 --> 00:05:06,551
當我們寫程式時
您寫成

79
00:05:06,551 --> 00:05:10,046
w 冒號等號 w 減 alpha 乘 dw

80
00:05:10,046 --> 00:05:14,750
所以我們用 dw 這個變數
名稱來代表這個導數項目

81
00:05:14,750 --> 00:05:19,330
現在讓我們來確定這個
梯度下降更新是合理的

82
00:05:19,330 --> 00:05:21,880
假設 w 在這裡

83
00:05:21,880 --> 00:05:26,060
您在
成本函數 J(w) 這一點

84
00:05:26,060 --> 00:05:29,270
記得導數的定義

85
00:05:29,270 --> 00:05:31,420
是函數在這一點的斜率

86
00:05:31,420 --> 00:05:36,190
函數的斜率實際上
是高除以寬, 是吧

87
00:05:36,190 --> 00:05:40,290
底下三角形這裡
是j(w)在這裡的切線

88
00:05:40,290 --> 00:05:43,900
這裡的導數是正的

89
00:05:43,900 --> 00:05:48,830
w 更新為 w 減 學習
率 乘上這個導數

90
00:05:48,830 --> 00:05:53,310
導數是正的, 所以
您從 w 減去, 所以

91
00:05:53,310 --> 00:05:55,260
您最終會往左邊走

92
00:05:55,260 --> 00:05:59,380
所以梯度下降會
慢慢的

93
00:05:59,380 --> 00:06:04,450
減低這個參數如果您
從這個大的 w 值開始

94
00:06:04,450 --> 00:06:08,545
另一個例子, 如果 w 在這裡

95
00:06:08,545 --> 00:06:15,050
那這一點的斜率
dj/dw 是負的

96
00:06:15,050 --> 00:06:22,771
梯度下降更新會
減去 alpha 乘上一個負數

97
00:06:22,771 --> 00:06:27,122
所以 w 會慢慢的增加
您最終會讓 w 越來

98
00:06:27,122 --> 00:06:31,530
越大在幾次
梯度下降的迴圈之後

99
00:06:31,530 --> 00:06:34,387
所以不管您的
初始值在左邊

100
00:06:34,387 --> 00:06:39,000
或是在右邊梯度下降會
將您移動到全域最小值這裡

101
00:06:39,000 --> 00:06:43,100
如果您不熟悉導數或者
微積分

102
00:06:43,100 --> 00:06:49,710
這個項目 dj(w)/dw 的意義
不用擔心

103
00:06:49,710 --> 00:06:53,770
我們在下一段影片會談更多

104
00:06:53,770 --> 00:06:56,761
如果您已經有深厚的微積分知識

105
00:06:56,761 --> 00:07:02,321
您或許能有更深的
直觀關於神經網路如何作用

106
00:07:02,321 --> 00:07:05,471
但即使如果您不
熟悉微積分

107
00:07:05,471 --> 00:07:10,091
在下幾個影片中我們會給您
足夠的直觀有關於導數跟

108
00:07:10,091 --> 00:07:14,980
有關的微積分您能夠用來
有效的使用神經網路

109
00:07:14,980 --> 00:07:16,410
但整體的直觀在

110
00:07:16,410 --> 00:07:21,520
現在是這個項目代表
函數的斜率而

111
00:07:21,520 --> 00:07:26,760
我們想要知道函數的斜率
在目前的參數設定

112
00:07:26,760 --> 00:07:31,140
來讓我們可以做最陡下坡的下一步
也就是讓我們知道

113
00:07:31,140 --> 00:07:35,450
哪個方向為了往
下坡在成本函數 J 上

114
00:07:36,660 --> 00:07:42,520
我們寫我們的梯度下降對於
 J(w) w 是您的參數

115
00:07:42,520 --> 00:07:47,150
在羅吉斯迴歸分析, 您的成本
函數是一個 w 跟 b 的函數

116
00:07:47,150 --> 00:07:50,894
在這個情況下, 這個梯度下降的內迴圈
就是這個東西

117
00:07:50,894 --> 00:07:53,302
這個東西您必須
重複變成以下

118
00:07:53,302 --> 00:07:57,970
您要更新 w 成為 w
減去 學習率乘上

119
00:07:57,970 --> 00:08:02,030
這個 J(w, b) 的導數 相對於 w

120
00:08:02,030 --> 00:08:07,460
您更新 b 為 b 減去
學習率乘上

121
00:08:07,460 --> 00:08:12,270
這個成本函數對於 b 的導數

122
00:08:12,270 --> 00:08:17,300
這下面兩個公式
實際上更新您的建置

123
00:08:17,300 --> 00:08:22,320
順便說一句，我只想講微積分學
中的一個符號公約

124
00:08:22,320 --> 00:08:24,560
對於一些人會有點混亂

125
00:08:24,560 --> 00:08:28,387
我想這不是超級重要
對於您理解微積分

126
00:08:28,387 --> 00:08:32,411
假設您看怪怪的我想確定
您不用想太多

127
00:08:32,411 --> 00:08:35,519
在微積分裡,這個項目

128
00:08:35,519 --> 00:08:40,730
實際上寫成
一個滑稽的波形曲線符號

129
00:08:40,730 --> 00:08:46,160
這個符號
實際上只是小寫的 d

130
00:08:46,160 --> 00:08:51,070
用花俏的字體, 形式化的字體
當您看到這個運算式

131
00:08:51,070 --> 00:08:56,145
這代表的意思是 J(w,b) 偏導數
實際上就是函數

132
00:08:56,145 --> 00:09:01,580
J(w,b) 的斜率, 函數是多少的
斜率在 w 方向

133
00:09:01,580 --> 00:09:06,640
而這個在微積分的符號規則
我覺得一點都不合邏輯

134
00:09:06,640 --> 00:09:11,780
這個在微積分的符號規則
我想會讓事情

135
00:09:11,780 --> 00:09:16,940
更加不必要的複雜
當如果函數是兩個或

136
00:09:16,940 --> 00:09:21,550
更多的變數時, 與其用
小寫的 d 您用這個有趣的符號

137
00:09:21,550 --> 00:09:24,380
這是稱為偏
導數符號

138
00:09:24,380 --> 00:09:26,120
但不用擔心

139
00:09:26,120 --> 00:09:31,090
如果 J 是只有一個變數的函數時
您用小寫的 d

140
00:09:31,090 --> 00:09:33,960
唯一的區別在於
不管您用這個有趣

141
00:09:33,960 --> 00:09:38,040
偏導數符號或
小寫 d 像我們上面寫的

142
00:09:38,040 --> 00:09:41,570
是在於函數是否是兩個
或更多的變數

143
00:09:41,570 --> 00:09:45,900
這種情況下, 您用這個符號
偏導數符號

144
00:09:45,900 --> 00:09:51,480
如果 J 是一個變數的函數
您用小寫的 d

145
00:09:51,480 --> 00:09:55,410
這是一個滑稽的規則
在微積分的符號上

146
00:09:55,410 --> 00:09:58,540
我想這只會讓事情
變得不必要的複雜

147
00:09:58,540 --> 00:10:03,300
但如果您看到這個偏導數
符號, 它的意思是衡量

148
00:10:03,300 --> 00:10:07,290
函數的斜率
相對於一個變數

149
00:10:07,290 --> 00:10:12,530
同樣的執著於
正確的數學

150
00:10:12,530 --> 00:10:18,070
符號在微積分, 因為這個
J 有兩個輸入不只一個

151
00:10:18,070 --> 00:10:22,540
在底下這個應該寫成
用這個偏導數符號

152
00:10:22,540 --> 00:10:28,290
但其實意思是同樣的
幾乎跟小寫的 d 一樣

153
00:10:28,290 --> 00:10:31,360
最後, 當您建置這些程式

154
00:10:31,360 --> 00:10:36,220
我們約定將
這個項目, 實際是您

155
00:10:36,220 --> 00:10:41,980
更新 w 的量記為
變數 dw 在程式中

156
00:10:41,980 --> 00:10:44,220
而這個項目

157
00:10:44,220 --> 00:10:47,230
您想更新 b 的量

158
00:10:47,230 --> 00:10:50,740
記為變數
 db 在程式中

159
00:10:50,740 --> 00:10:55,580
好的, 這是您如何
建置梯度下降

160
00:10:55,580 --> 00:10:59,830
如果您幾年沒用過微積分
我知道這似乎

161
00:10:59,830 --> 00:11:03,770
很多的微積分的導數
您可能不適應

162
00:11:03,770 --> 00:11:06,330
如果您這麼覺得
不用擔心

163
00:11:06,330 --> 00:11:10,150
在下一段影片, 我將給您
好一點的直觀有關於導數

164
00:11:10,150 --> 00:11:13,560
甚至不需要深的微積分數學
的理解

165
00:11:13,560 --> 00:11:16,310
就只是一些
理解微積分的直觀

166
00:11:16,310 --> 00:11:19,130
您會能夠讓神經
網路有效的工作

167
00:11:19,130 --> 00:11:22,743
讓我們近到下一段影片
我將談談更多的

168
00:11:22,743 --> 00:11:23,470
微分