이번 비디오에서는, 로지스틱 회귀분석법을 보았는데요, 회귀분석 모델에서 W와 B 파라미터를 트레이닝 시키기 위해서는 먼저 비용함수를 정의해야 합니다. 로지스틱 회귀 분석에 사용할 수 있는 비용 함수에 대해 살펴보도록 하죠. 복습하자면, 이전 슬라이드에서 찾았던 내용인데요. 결과값 ŷ은 w의 시그모이드 transpose x 더하기 b 이고, Z의 시그모이드는 이렇게 정의됩니다. 모델에 대한 파라미터를 학습하기 위해 m개의 트레이닝 example로 이루어진 트레이닝 세트가 주어지고, 트레이닝세트에서 매개 변수 W와 B를 찾는 것이 자연스러운 것 같습니다. 여러분의 트레이닝세트에 대한 예상치는 ŷ (i) 이라고 표기하는데요 트레이닝세트에서 나온 ground truth label y_i값과 비슷할 것입니다. 위에 있는 공식에대해 조금더 상세히 알려드리자면, 앞서 ŷ은 위처럼 정의된다고 했었죠, x 트레이닝샘플에 대해서 말이죠. 그리고 각각의 크레이닝 샘플에 대해서요. 밑에 이 아래첨자에 괄호를 사용해서 인덱싱하고 example들을 차별화하시켰습니다. 트레이닝 샘플 (i)의 예상치는 ŷ(i)이며, 기 값은 시그모이드 함수를 W transpose X값에 적용하여, (i) 트레이닝 example 입력값 더하기 V를 통해 Z(i)도 정의할 수 있습니다. Z(i)는 W transpose x (i) 플러스 b입니다. 이번 코스를 통해 우린 규칙화된 표기법을 쓸 것인데요, 위 첨자의 괄호 i는 데이터를 뜻합니다. X 또는 Y 또는 Z 또는 i번째 트레이닝 example과 연관된 i번째 example인거죠. 이것이 위 첨자 i가 의미하는 것입니다. 자 그럼 이제 알고리즘이 얼마나 잘 작동하고 있는지를 알아내기 위해 사용할 수 있는 loss 함수 또는 오류함수를 보도록 하겠습니다. 한가지 방법은 loss를 알고리즘의 결과값이 ŷ이고 true label인 Y가 제곱 오류 또는 0.5제곱 오류가 되게하는 것입니다. 이렇게 할 수 있는데요, 로지스틱 회귀분석법에서는 사람들이 이렇게 잘 하지 않습니다. 파라미터를 배우면서 알게되면, 최적한 문제에서 나중에 배우겠지만 비볼록하게 됩니다. 결과적으로 복수 국부 최적의 optimization 문제를 갖게 됩니다. 그렇게해서 gradient descent가 전역 최적값을 못 찾을 수 있습니다. 방금 이야기한 것들이 잘 이해가 안 가셨다고 하면 너무 걱정하지 마십시요. 추후 강의에서 더 자세히 다루도록 하겠습니다. 지금 이해햐셔야할 직관적인 부분은 L이라는 loss 함수는 true label y를 갖는 경우, 얼마나 정확히 ŷ 결과값을 산출하는지 정의할 때 사용합니다. 제곱 오류 값이 오히려 더 합리적인 방법이라고 생각할 수 있지만 gradient descent가 잘 안나온다는 단점이 있습니다. 그러므로 로지스틱 회귀분석법에서는, 여러가지 제곱 오류와 비슷한 역할을하는 loss함수를 정의해서 볼록한 최적화 문제를 주도록 할 것입니다. 그렇게되면 나중에 비디오 강의를 통해 보시겠지만, 최적화를 하기가 훨씬 더 쉬워집니다. 로지스틱 회귀분석법에서 사용하는 것은 여기 위에 있는 loss함수입니다. 여기서는 마이너스 y 로그 ŷ 더하기 1, 이 선은 y log 이 선은 ŷ 입니다. 직관적으로 이 loss 함수가 왜 말이 되는지 설명해드리겠습니다. 아셔야 할 것은, 제곱오류를 사용하는 경우, 이 값이 최대한 작아야 좋습니다. regression loss함수를 이용하면, 이 것 또한 작은 값일 수록 좋죠. 이 것이 왜 말이 되는지, 2가지의 케이스를 통해서 보도록 하겠습니다. 첫번째 경우에는 Y가 1, 그리소 loss 함수 ŷ 컴마 y는 이 마이너스 부호를 쓸 수 있도록 해주죠. 그러므로 마이너스 로그 ŷ 만약 y가 1인 경우 말이죠. 그 이유는 y가 1이면, 두번째 항인 1-Y 는 0이 됩니다. 그럼 y가 1이면, 마이너스 로그 ŷ은 큰 값이되게 하는게 좋습니다. 그 뜻은 즉 log ŷ을 큰 값으로 만드는게 좋은것 인데요 최대한 크게 만드는 것이 좋기 때문에 ŷ이 큰 값을 갖게 하는 것이 좋겠죠. 그렇지만 ŷ은 아시다시피, 시그모이드 함수이기 때문에 1보다 큰 값을 가질 수 없습니다. 즉, y가 1인 경우 ŷ 값이 최대한 큰 값을 갖도록 하는 것이 좋다는 것을 알게 됩니다. 그렇지만 이 값은 절대로 1보다 큰 값이 될 수 없기 때문에, ŷ을 1과 최대한 가깝게 
하는 것이 목표라고 해석할 수도 있습니다. 2번째 경우는 y가 0인 경우인데요, 만약 y가 0인 경우, loss 함수의 첫번째 항이 0이 됩니다. y 가 0, 그러면 2번째 항이 loss함수를 정의하게 되죠. 이렇게 되면 loss는 마이너스 로그 1 마이너스 ŷ이 됩니다. 여러분의 러닝 단계에서 loss 함수를 작게 만들려고 한다면, 로그 1 마이너스 ŷ이 최대한 값을 갖길 원한다는 뜻입니다. 그리고 여기가 마이너스 부호이기 때문에 이 작은 이유 하나로 해당 loss 함수가 ŷ의 값을 최대한 작은 값으로 하려고 하는 것을 알 수 있습니다. 또, ŷ이 0에서 1사이 값을 가져야 하기 때문에 만약 y가 0이라면 loss 함수는 파라미터가 일을해서 ŷ이 최대한 0에 가까운 값이되도록 할 것입니다. Rafidah's effect가 작용하는 여러가지 함수가 있는데요, 만약 y가 1인 경우, ŷ을 최대한 크게하고, Y가 0인 경우 ŷ을 작게하는 것입니다. 여기 초록색으로 된 부분을 아주 편안하게 비용함수에 대해 정의했는데요, 이 부분은 선택적인 비디오 강의 시청을 통해 조금 더 공식적인 배경을 설명하겠습니다. 왜 로지스틱 회귀분석법에서 이런 형식의 loss 함수를 
사용하는지 말이죠. 마지막으로, loss 함수는 single training example을 바탕으로 정의되었었는데요, single training example에서 얼마나 잘 작동하는지 여부를 측정합니다. 비용함수를 정의해볼텐데요, 이것은 전체적인 트레이닝세트에서 얼마나 잘 작동하는지 여부를 측정해 줄 것입니다. 비용함수 J는 W 파라미터에 적용될텐데요, B 파라미터는 각각의 트레이닝 샘플에 
적용된 m개의 loss 함수의 합과 함께 평균치가 될 것입니다. 그리고 여기 ŷ은 당연히, 로지스텍 회귀분석법 알고리즘에 의거한 결과값의 예상치입니다, W와 B 파라미터세트를 사용해서 나온 값 말이죠. 부가 설명을 드리자면, 이 것은 1 나누기 m이 비용함수가 1에서 m까지의 합인 경우입니다. 그러므로 이값은 y (i) Log ŷ (i) 더하기 1 선을 적용하면 y (i) log one 선을 적용하여 ŷ (i)가 됩니다.. 여기에 대괄호를 기입할 수도 있겠죠. 여기 마이너스 부호는 밖에 위치시킵니다. 여기서는 이런 표현를 쓸텐데요. loss 함수가 single training example에 적용되었다고 말이죠. 비용함수는 파라미터의 비용을 나타낸 것이구요, 그러므로 로지스틱 회귀분석법 모델을 트레이닝 시키는데 있어, W와 B라는 파라미터를 찾고 밑에 있는 전체적인 기계의 비용 J를 줄일 것입니다. 이제까지 로지스틱 회귀분석법 알고리즘의 세팅, 트레이닝 example의 비용함수, 그리고 알고리즘의 전체 비용함수의 파라미터 알고보니 로지스틱 회귀분석법은 아주 아주 작은 신경망 네트워크로 볼 수 있겠습니다. 다음 비디오에서는 이 내용에 대해 다룰텐데요 신경망의 직관적인 부분은 미리 생각해 놓으실 수 있습니다. 자 그럼 다름 비디오로 넘어가서 어떻게 로지스틱 회귀분석법을 아주 작은 신경망으로 볼 수 있는지 알아보겠습니다.