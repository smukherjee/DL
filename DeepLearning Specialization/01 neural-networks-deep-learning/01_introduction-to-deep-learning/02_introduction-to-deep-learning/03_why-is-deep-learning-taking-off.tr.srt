1
00:00:00,320 --> 00:00:04,140
deep learning ve neural networks arkasındaki

2
00:00:04,140 --> 00:00:07,110
temel fikirler onlarca yıldır mevcut ise

3
00:00:07,110 --> 00:00:08,970
neden şuanda popülerliğini kazanıyor

4
00:00:08,970 --> 00:00:11,310
bu videoda, deep learning'deki yükselen popülerliğini nedenlerini

5
00:00:11,310 --> 00:00:14,690
ve arkasındaki sebepleri inceleyeceğiz Çünkü, inanıyorum ki bu size

6
00:00:14,690 --> 00:00:17,540
organizasyonunuzun içerisinde hangi fırsatları

7
00:00:17,540 --> 00:00:20,560
daha iyi kullanabileceğiniz konusunda yardımcı olacaktır.

8
00:00:20,560 --> 00:00:21,690
Son birkaç yıldır

9
00:00:21,690 --> 00:00:23,095
bir çok kişi bana

10
00:00:23,095 --> 00:00:25,880
"Andrew, neden derin öğrenmenin çok daha iyi sonuç veriyor" şeklinde sorular soruyor

11
00:00:25,880 --> 00:00:30,460
ve bana bu soru sorulduğunda genellikle bu resmi onlar için çizerim.

12
00:00:30,460 --> 00:00:33,830
Diyelim ki yatay eksende bir görev için elimizde mevcut olan

13
00:00:33,830 --> 00:00:37,675
veri miktarını belirttiğimiz bir şekil çizelim.

14
00:00:37,675 --> 00:00:40,040
ve diyelim ki dikey eksende

15
00:00:40,040 --> 00:00:43,610
öğrenme algoritmamızın performansını çizelim

16
00:00:43,610 --> 00:00:47,660
örneğin spam sınıflandırıcısı veya

17
00:00:47,660 --> 00:00:50,870
reklam tıklamamızın ya da sürücüsüz araba sistemimizin

18
00:00:50,870 --> 00:00:55,980
diğer arabaların konumlarının belirlenmesindeki keskinlik oranı gibi.

19
00:00:55,980 --> 00:01:00,680
Görünen o ki eğer geleneksel öğrenme algoritmasının performansını veri miktarına göre çizecek olursak

20
00:01:00,680 --> 00:01:02,090
örneğin destek yöney makinası(support vector machine)

21
00:01:02,090 --> 00:01:06,340
geleneksel bir öğrenme algoritmasının performansını çizerseniz,

22
00:01:06,340 --> 00:01:09,185
buna benzer bir eğriye elde edebiliriz.

23
00:01:09,185 --> 00:01:14,220
Bu eğride veri ekledikçe performans artış gösterir fakat belli bir süre sonunda

24
00:01:14,220 --> 00:01:17,565
performans plato şekline dönüşür yani düz şekilde ilerler.

25
00:01:17,565 --> 00:01:20,100
Bunun düz bir çizgi olması gerekiyordu, çok iyi çizemedim.

26
00:01:20,100 --> 00:01:26,760
Burada algoritma sanki çok fazla veriyi nasıl kullanacağını bilmiyormuş gibi davranış sergilemektedir.

27
00:01:26,760 --> 00:01:29,270
Toplumumuzda son 20 yılda olan şey ise,

28
00:01:29,270 --> 00:01:32,360
birçok problem için eskiden az miktarda

29
00:01:32,360 --> 00:01:35,900
veriye sahipken şu anda çok daha fazla

30
00:01:35,900 --> 00:01:39,985
miktarda veriye sahibiz ve bunun en temel

31
00:01:39,985 --> 00:01:43,430
sebebi ise toplumumuzdaki dijitalleşmeden kaynaklı

32
00:01:43,430 --> 00:01:48,020
artık insan aktivitelerinin birçoğunun verisi dijital ortamda mevcut

33
00:01:48,020 --> 00:01:50,810
çünkü artık bilgisayarda, internet sitelerinde,

34
00:01:50,810 --> 00:01:56,475
mobil uygulamalarda, çok daha fazla vakit geçiriyoruz

35
00:01:56,475 --> 00:01:59,720
ve bu dijital cihazlar fazla miktarda veri oluşturuyorlar ayrıca

36
00:01:59,720 --> 00:02:01,970
artık telefonlarımıza dahil olan ucuz kameralara sahip olmamız

37
00:02:01,970 --> 00:02:05,645
ve nesnelerin interneti çağında(internet of things)

38
00:02:05,645 --> 00:02:09,350
telefonlarımızda ivmeölçer gibi diğer birçok sensöre sahip olmamız da bir diğer sebep.

39
00:02:09,350 --> 00:02:12,590
Dolayısıyla, son 20 yılda birçok uygulamada 

40
00:02:12,590 --> 00:02:15,935
geleneksel öğrenme algoritmalarının avantaj 

41
00:02:15,935 --> 00:02:20,015
sağlayabileceğinden daha fazla veriyi biriktirmiş bulunuyoruz.

42
00:02:20,015 --> 00:02:23,465
Sinir ağlarında ise eğer küçük bir sinir ağı eğitirseniz,

43
00:02:23,465 --> 00:02:28,865
bu durumda performans şuna benzeyecektir

44
00:02:28,865 --> 00:02:31,765
ve bir miktar daha büyük bir sinir ağı eğitirseniz,

45
00:02:31,765 --> 00:02:34,840
diyelim ki orta büyüklükte bir sinir ağı olsun,

46
00:02:34,840 --> 00:02:40,275
performans bir miktar daha iyi olur ve çok büyük bir sinir ağı eğitirseniz

47
00:02:40,275 --> 00:02:43,750
bu durumda yine performans daha iyi olmaya devam edecektir.

48
00:02:43,750 --> 00:02:45,965
Birkaç gözlemde bulunalım; birinci olarak,

49
00:02:45,965 --> 00:02:49,150
eğer yüksek seviye bir performansı hedefliyorsanız

50
00:02:49,150 --> 00:02:50,720
bu durumda iki şeye sahip olmalısınız,

51
00:02:50,720 --> 00:02:53,660
birincisi büyük miktardaki veriden

52
00:02:53,660 --> 00:02:56,540
yararlanabilmek için büyük bir

53
00:02:56,540 --> 00:02:59,630
sinir ağını eğitebilmelisiniz

54
00:02:59,630 --> 00:03:01,760
ikincisi ise fazla miktarda veriye

55
00:03:01,760 --> 00:03:03,850
sahip olmalısınız ve

56
00:03:03,850 --> 00:03:09,590
genellikle bu durumu 'ölçek' derin öğrenme ilerleyişinin temel sebebidir olarak açıklıyoruz ve ölçekten kastetiğimiz şey

57
00:03:09,590 --> 00:03:12,380
hem sinir ağının büyüklüğü

58
00:03:12,380 --> 00:03:16,090
yani çok fazla gizli üniteye sahip,çok fazla bağlantı ve çok fazla parametreye sahip

59
00:03:16,090 --> 00:03:21,055
bir sinir ağı ve aynı zamanda verinin ölçeği.

60
00:03:21,055 --> 00:03:25,190
Günümüzde bir sinir ağından daha iyi performans almanın yöntemi

61
00:03:25,190 --> 00:03:29,690
ya daha büyük bir sinir ağı eğitmek
 ya da daha fazla veriyle beslemekten geçiyor 

62
00:03:29,690 --> 00:03:32,210
ve bunlar sadece belli bir noktaya kadar çalışır çünkü eninde sonunda

63
00:03:32,210 --> 00:03:35,510
elinizdeki veri miktarı yeterli gelmemeye
başlar ya da elinizdeki sinir ağı

64
00:03:35,510 --> 00:03:38,120
çok büyük olduğundan eğitmesi çok uzun sürer

65
00:03:38,120 --> 00:03:43,325
fakat sadece 'ölçeği' geliştirmek bize derin öğrenme dünyasında çok fazla şey kazandırmıştır

66
00:03:43,325 --> 00:03:46,860
Bu şemayı teknik olarak daha doğru bir hale getirmek için

67
00:03:46,860 --> 00:03:48,815
birkaç şey eklememe izin verin.

68
00:03:48,815 --> 00:03:51,465
X eksenine veri miktarı yazdım,

69
00:03:51,465 --> 00:03:56,240
teknik olarak buna etiketli veri miktarı diyebiliriz

70
00:03:56,240 --> 00:04:02,795
burada etiketli veriden kastım input x ve etiket y ikilisidir.

71
00:04:02,795 --> 00:04:07,500
Bu kursun ilerleyen kısımlarında da kullanılmak üzere bir notasyon paylaşmam gerekirse,

72
00:04:07,500 --> 00:04:12,460
eğitim setimizin boyutunu ya da eğitim örneklerinin sayısını belirtmek için

73
00:04:12,460 --> 00:04:17,390
küçük m harfini kullanacağız.Yani yatay ekseni burada m ile belirtebiliriz.

74
00:04:17,390 --> 00:04:20,300
Bu şekil hakkında birkaç detay daha vermek gerekirse,

75
00:04:20,300 --> 00:04:24,080
burada az sayıda eğitim örneğinin olduğu durumlarda,

76
00:04:24,080 --> 00:04:28,970
algoritmaların sıralanışı aslına bakılırsa çok belli değildir.

77
00:04:28,970 --> 00:04:32,690
Eğer çok fazla eğitim veriniz yoksa

78
00:04:32,690 --> 00:04:37,120
elle mühendislikle oluşturulmuş özellikler performansın belirleyici faktörü olacaktır

79
00:04:37,120 --> 00:04:41,810
dolayısıyla örnek verirsek az sayıda eğitim örneğinde

80
00:04:41,810 --> 00:04:46,640
elle mühendislik yaparak SVM algoritması ağiten biri büyük derin öğrenme ağı

81
00:04:46,640 --> 00:04:49,260
eğiten birinden az verinin olduğu durumlarda

82
00:04:49,260 --> 00:04:50,815
daha iyi performansa ulaşabilir.

83
00:04:50,815 --> 00:04:54,140
Dolayısıyla bunun gibi az sayıda örneğe sahip olduğumuz durumlarda

84
00:04:54,140 --> 00:04:56,990
algoritmaların performanslarının karşılaştırılması

85
00:04:56,990 --> 00:05:00,800
çok iyi tanımlanmamıştır ve performans elle mühendislikle bulunmuş özelliklerden

86
00:05:00,800 --> 00:05:03,380
ve algoritmanızın diğer küçük ayrıntılarından 

87
00:05:03,380 --> 00:05:07,700
çok daha fazla etkilenecektir. Sadece çok fazla veriye sahip olduğumuzda

88
00:05:07,700 --> 00:05:09,080
daha kararlı olarak

89
00:05:09,080 --> 00:05:12,320
büyük sinir ağlarının daha iyi performans göstereceğini

90
00:05:12,320 --> 00:05:16,960
ve listede üst sıralara yerleşeceğini söyleyebiliriz.

91
00:05:16,960 --> 00:05:21,315
Eğer arkadaşlarınızdan biri gelip size sinir ağlarının neden popülerleştiğini sorarsa,

92
00:05:21,315 --> 00:05:24,820
size onlar için bu çizimi çizmenizi tavsiye ediyorum.

93
00:05:24,820 --> 00:05:27,695
Modern derin öğrenmenin yükselişinin

94
00:05:27,695 --> 00:05:29,450
ilk zamanlarında veri miktarındaki

95
00:05:29,450 --> 00:05:33,395
artış ve işlem kapasındaki artış 

96
00:05:33,395 --> 00:05:36,425
yani Cpu ya da Gpu üzerinde büyük sinir ağları

97
00:05:36,425 --> 00:05:38,420
eğitebiliyor olmamız

98
00:05:38,420 --> 00:05:41,555
bize epey ilerleme kaydettirdi

99
00:05:41,555 --> 00:05:44,320
fakat özellikle son birkaç yıldır

100
00:05:44,320 --> 00:05:47,980
algoritmalarda büyük innovasyonlara şahit olduk

101
00:05:47,980 --> 00:05:50,830
bunun önemini de azımsamak istemem.

102
00:05:50,830 --> 00:05:54,995
İlginç olarak algoritmalardaki innovasyonların birçoğu

103
00:05:54,995 --> 00:06:00,285
sinir ağının daha hızlı çalışması için yapılıyor.

104
00:06:00,285 --> 00:06:02,940
Somut bir örnek vermek gerekirse,

105
00:06:02,940 --> 00:06:05,210
sinir ağlarındaki en büyük devrimlerden

106
00:06:05,210 --> 00:06:08,125
bir tanesi burada gördüğünüz

107
00:06:08,125 --> 00:06:09,840
sigmoid fonksiyonundan

108
00:06:09,840 --> 00:06:13,535
önceki videoda kısaca bahsemiş olduğumuz

109
00:06:13,535 --> 00:06:17,855
şuraya gözüken relu fonksiyonuna geçmekti.

110
00:06:17,855 --> 00:06:22,010
Söyleyeceklerimin detaylarını anlamıyorsanız bunun hakkında endişelenmeyin.

111
00:06:22,010 --> 00:06:25,820
Makine öğreniminde sigmoid fonksiyonunu kullanmanın

112
00:06:25,820 --> 00:06:29,980
problemlerinden bir tanesi fonksiyonun eğimininin neredeyse sıfır olması

113
00:06:29,980 --> 00:06:31,905
dolayısıyla buradaki bölgelerde

114
00:06:31,905 --> 00:06:34,685
öğrenme çok yavaş bir hale gelir

115
00:06:34,685 --> 00:06:37,945
çünkü gradyan inişini burada uyguladığınız zaman eğim sıfır olduğundan

116
00:06:37,945 --> 00:06:41,820
parametreler çok yavaş değişir yani öğrenme çok yavaşlar

117
00:06:41,820 --> 00:06:47,090
bunun yanısıra Rectified Linear Unit (RELU)(doğrultulmuş lineer ünite) adı verilen

118
00:06:47,090 --> 00:06:52,835
bu fonksiyonu kullandığımızda ise gradyan inişi

119
00:06:52,835 --> 00:06:58,350
pozitif her girdi için 1 e eşitlenmiş olur

120
00:06:58,350 --> 00:07:03,130
dolayısıyla gradyan çok daha az bir ihtimalle yavaşça sıfıra iner.

121
00:07:03,130 --> 00:07:05,720
Bu bölgedeki eğim, doğrunun eğimi sıfırdır

122
00:07:05,720 --> 00:07:08,960
fakat görünen o ki sadece sigmoidten relu ye geçmemiz

123
00:07:08,960 --> 00:07:12,320
gradyan inişi algoritmasının

124
00:07:12,320 --> 00:07:16,380
çok daha hızlı ve verimli çalışmasına vesile olur.

125
00:07:16,380 --> 00:07:21,350
Bu örnek göreceli olarak basit bir algoritmasal innovasyondu

126
00:07:21,350 --> 00:07:26,755
fakat sonuç olarak bu innovasyonun amacı işlem zamanını azaltmaktı

127
00:07:26,755 --> 00:07:29,765
buna benzer şekilde algoritmaları daha hızlı çalışması

128
00:07:29,765 --> 00:07:33,230
için değiştirdiğimiz bir sürü örnek mevcut

129
00:07:33,230 --> 00:07:37,400
çünkü algoritmaların daha hızlı çalışması daha büyük sinir ağları eğitebilmemizi

130
00:07:37,400 --> 00:07:42,065
ya da büyük sinir ağlarını kabul edilebilir zaman içerinde çalıştırmamızı sağladı.

131
00:07:42,065 --> 00:07:47,060
Hızlı işlem yapabilmenin önemli olmasının bir diğer sebebine gelelim.

132
00:07:47,060 --> 00:07:52,525
görünen o ki Sinir ağlarını eğitmek tekrarlayıcı bir işlemdir.

133
00:07:52,525 --> 00:07:54,050
Genellikle sinir ağı mimarisi için

134
00:07:54,050 --> 00:07:58,285
bir fikriniz olur ve bu fikri uygularsınız ve kodlarsınız

135
00:07:58,285 --> 00:08:01,999
bu size sizir ağınızın nasıl sonuç verdiğini 

136
00:08:01,999 --> 00:08:05,880
anlamanızı sağlayan bir deneyi çalıştırma imkanı verir ve buna bakarak

137
00:08:05,880 --> 00:08:08,240
sinir ağınızın detaylarında değişikliğe gidersiniz

138
00:08:08,240 --> 00:08:13,190
ve daha sonra bu döngüde tekrar tekrar devam edersiniz.

139
00:08:13,190 --> 00:08:17,089
Eğer sinir ağınız eğitim için çok fazla zaman alıyorsa,

140
00:08:17,089 --> 00:08:21,140
bu döngü etrafında hareket etmek çok fazla zaman alır

141
00:08:21,140 --> 00:08:27,215
ve bir fikriniz olduğunda bunu uygulayıp çalışıp çalışmadığına

142
00:08:27,215 --> 00:08:32,455
örneğin 10 dk da bakabilmekle 1 günde bakabilmek

143
00:08:32,455 --> 00:08:38,570
veya belki de 1 ayda bakabilmek(bazen 1 aya kadar sürebilir) arasında

144
00:08:38,570 --> 00:08:41,480
üretkenliğiniz açısından ciddi fark vardır.

145
00:08:41,480 --> 00:08:44,630
Kısa zamanda sonuç alarak

146
00:08:44,630 --> 00:08:47,750
çok daha fazla farklı fikir deneyebilir ve uygulamanız için

147
00:08:47,750 --> 00:08:52,005
iyi çalışan bir sistem geliştirme ihtimaliniz artacaktır.

148
00:08:52,005 --> 00:08:57,320
Dolayısıyla hızlı işlem yapabilme yeteneği

149
00:08:57,320 --> 00:09:02,495
deneysel sonuçları alabilme hızınızı artırır ve bu

150
00:09:02,495 --> 00:09:06,950
hem sinir ağı uygulayamacılarına hem de

151
00:09:06,950 --> 00:09:10,550
derin öğrenme üzerine çalışan araştırmacılara

152
00:09:10,550 --> 00:09:14,530
çok daha hızlı ve tekrarlayıcı çalışabilmek adına yardım etmektedir.

153
00:09:14,530 --> 00:09:17,570
Tüm bunlar tüm derin öğrenme topluluğuna 

154
00:09:17,570 --> 00:09:22,390
yeni algoritmalar inşa etme ve durma olmaksızın

155
00:09:22,390 --> 00:09:27,470
ilerleme kaydetme konusunda büyük faydalar sağlamaktadır.

156
00:09:27,470 --> 00:09:33,200
Bunlar derin öğrenmenin yükselişinde etkili olan bazı güçlerdir.

157
00:09:33,200 --> 00:09:38,825
İyi haber ise bu güçlerin derin öğrenmenin gelişmesinde hala epey etkili olduklarıdır.

158
00:09:38,825 --> 00:09:42,890
Veriyi düşünün, toplum gün geçtikçe daha fazla

159
00:09:42,890 --> 00:09:47,630
dijital veri üretmektedir ya da işlem kapasitesini düşünün

160
00:09:47,630 --> 00:09:49,595
Gpu'lar ve hızlı ağlar gibi birçok özelleştirilmiş 

161
00:09:49,595 --> 00:09:54,260
donanımla beraber eminim ki büyük sinir ağları

162
00:09:54,260 --> 00:09:59,300
eğitme kapasitemiz daha da gelişecektir.

163
00:09:59,300 --> 00:10:02,150
Ve algoritmaları ele alın çok sayıda araştırma topluluğu

164
00:10:02,150 --> 00:10:06,170
yeni algoritmalar keşfetme konusunda çok iyi işler çıkarmakta.

165
00:10:06,170 --> 00:10:07,550
Bence tüm bunlardan dolayı

166
00:10:07,550 --> 00:10:09,530
gelecekte uzun bir süre boyunca

167
00:10:09,530 --> 00:10:11,720
derin öğrenmenin daha iyiye gideceği

168
00:10:11,720 --> 00:10:14,570
konusunda iyimser olabiliriz.

169
00:10:14,570 --> 00:10:17,855
Bunlarla beraber, hadi bir sonraki videoda bu kursta

170
00:10:17,855 --> 00:10:21,200
neler öğreneceğiniz hakkında konuşalım.