1
00:00:00,000 --> 00:00:02,389
4주차 강의에 오신 것을 환영합니다.

2
00:00:02,389 --> 00:00:06,354
지금까지 여러분은 신경망과 관련된
4개의 promulgation과 back promulgation을 보았고,

3
00:00:06,354 --> 00:00:10,807
한개의 숨겨진 유닛으로 이루어진 신경망, 
또한 로즈스틱 회귀분석에 대한 내용, 

4
00:00:10,807 --> 00:00:13,064
벡터화와 관련된 내용, 

5
00:00:13,064 --> 00:00:15,936
그리고 언제 무작위로 초기화를 하는 것이 좋은지도 
배웠습니다.

6
00:00:15,936 --> 00:00:19,417
지난 2주간의 숙제를 모두 마치셨다면, 

7
00:00:19,417 --> 00:00:21,378
이러한 아이디어를 실제로 적용하고
도입해보셨을 텐데요.

8
00:00:21,378 --> 00:00:21,977
지금쯤이면, 

9
00:00:21,977 --> 00:00:26,718
심층신경망을 도입하기 위해 필요한 아이디어를 살펴보았는데요, 

10
00:00:26,718 --> 00:00:30,453
이번주에는 이러한 아이디어를 바탕으로

11
00:00:30,453 --> 00:00:33,669
본인이 직접 심층신경망을 도입할 수 있도록 그 내용을 
취합해보도록 하겠습니다.

12
00:00:33,669 --> 00:00:36,406
이번주 연습학습이 조금 더 길고, 

13
00:00:36,406 --> 00:00:39,457
더 많은 작업을 요구하기 때문에, 
비디오 강의를 조금 짧게 진행하겠습니다.

14
00:00:39,457 --> 00:00:43,784
비디오를 조금 더 빨리 마치고 나서

15
00:00:43,784 --> 00:00:48,107
끝나고 나오는 연습 학습을 진행하는데 조금 더 시간을 가질 수 있도록
하겠습니다.

16
00:00:48,107 --> 00:00:52,537
여러분이 이 학습을 통해 신경망에 대해 자부심을 갖고 
그 내용을 깊게 생각을 하실 수 있으면 좋겠습니다.

17
00:00:52,537 --> 00:00:55,576
자 그러면 심층신경망은 무엇일까요?

18
00:00:55,576 --> 00:00:59,225
이 로지스틱 회귀분석에 관한 사진을 보셨을텐데요. 

19
00:00:59,225 --> 00:01:03,439
또한, 하나의 숨겨진 층이 있는 신경망도 이과 같이 보셨을 것입니다.

20
00:01:03,439 --> 00:01:07,925
여기서 보이는 그림은 2가지의 숨겨진 층으로 이루어진 신경망입니다.

21
00:01:07,925 --> 00:01:10,661
이것은 5개의 숨겨진 층으로 이루어진 그림이구요. 

22
00:01:10,661 --> 00:01:15,744
우리는 로지스틱 회귀분석 모델을 굉장히 "shallow"한 모델이라고 표현합니다.

23
00:01:15,744 --> 00:01:19,947
반면에 이 모델은 "deeper model"이라고 표현합니다.

24
00:01:19,947 --> 00:01:23,585
shallow와 depth의 차이는 결국 그 정도의 차이입니다.

25
00:01:23,585 --> 00:01:26,952
이것은 싱글 숨겨진 레이어 네트워크는, 

26
00:01:26,952 --> 00:01:30,052
2 개의 층으로 이루어진 신경망입니다.

27
00:01:30,052 --> 00:01:34,880
기억하실 것은, 신경망의 층을 세는 경우, 
우리는 입력값의 층을 세지 않습니다.

28
00:01:34,880 --> 00:01:38,076
그저 결과값 그대로의 층의 수를 셉니다.

29
00:01:38,076 --> 00:01:42,849
그러면 이것은 2개의 신경망 층으로 
조금 shallow 하지만

30
00:01:42,849 --> 00:01:45,961
로지스틱 회귀분석 모델보다는 shallow하지 않습니다.

31
00:01:45,961 --> 00:01:50,086
엄밀히 이야기하면 로지스틱 회귀분석 모델은
1개의 층으로 이루어진 신경망입니다.

32
00:01:50,086 --> 00:01:53,536
하지만 지난 몇 년간 머신러닝 커뮤니티의

33
00:01:53,536 --> 00:01:58,561
AI는 shallow한 모델과는 달리 

34
00:01:58,561 --> 00:02:03,590
심층신경망이 배울 수 있는 함수가 
많다는 것을 깨달았습니다.

35
00:02:03,590 --> 00:02:08,119
어떠한 특정 문제에 대해서 미리 신경망의 

36
00:02:08,119 --> 00:02:10,163
적합한 깊이를 예측하기는 어렵겠지만

37
00:02:10,163 --> 00:02:14,305
로지스틱 회귀분석 모델을 먼저 시도해보고

38
00:02:14,305 --> 00:02:19,200
1개의 숨겨진 층, 2개의 층 순으로 시도해보고
층의 개수를 또 다른 매개변수로 

39
00:02:19,200 --> 00:02:22,739
지정하여 다른 값을 시도해보는 방법이 있습니다.

40
00:02:22,739 --> 00:02:27,515
이렇게해서 hold-out cross validation data에 이르러 모두 시도해보고, 
아니면 development set에 시도해보는 것도 방법입니다.

41
00:02:27,515 --> 00:02:29,447
나중에 더욱 자세히 보도록 하겠습니다. 

42
00:02:29,447 --> 00:02:33,998
이제 심층신경망을 나타내는 표기법에대해 알아보겠습니다.

43
00:02:33,998 --> 00:02:39,147
여기는 하나, 둘, 셋, 네개의 층으로 이루어진 신경망인데요.

44
00:02:40,974 --> 00:02:45,729
3개의 숨겨진 층으로 이루어져 있습니다.

45
00:02:45,729 --> 00:02:50,842
이 층에서 숨겨진 유닛은 5, 5, 3
그리고 1 유닛입니다.

46
00:02:50,842 --> 00:02:52,731
여기서 쓸 표기법은, 

47
00:02:52,731 --> 00:02:56,591
대문자 L을 쓸것이구요, 이 것은 네트워크에서의
층 개수를 나타낼 것입니다.

48
00:02:56,591 --> 00:03:03,881
이 경우 L=4입니다. 이것은 
층의 개수이죠. 

49
00:03:03,881 --> 00:03:11,880
그리고 N의 위첨자 [l]을 사용해서 노드의 개수를 나타낼 것입니다.

50
00:03:11,880 --> 00:03:17,101
또는 소문자 l로 나타낸 유닛의 개수입니다.

51
00:03:17,101 --> 00:03:22,501
이것을 인덱스화하면, 
입력값을 "0" 층으로하고 

52
00:03:22,501 --> 00:03:28,950
이것을 1층, 이것은 2층, 이것은 3층, 이것은 4층

53
00:03:28,950 --> 00:03:33,822
그렇게되면 이것처럼 
n[1] 이 있겠죠.

54
00:03:33,822 --> 00:03:39,529
첫번째 층에서 나온 것이구요, 
이 값은 5입니다. 그 이유는 5개의 숨겨진 유닛이 있기 때문입니다.

55
00:03:39,529 --> 00:03:43,623
이것은 n[2]인데요

56
00:03:43,623 --> 00:03:48,810
2번째 숨겨진 숨겨진 층에서의 유닛 개수인데요

57
00:03:48,810 --> 00:03:53,315
이 값은 마찬가지로 5입니다. 
n[3]는 3입니다.

58
00:03:53,315 --> 00:03:59,459
n[4]는 n[L]입니다.
upper unit의 개수는 1입니다.

59
00:03:59,459 --> 00:04:04,101
대문자 L은 4와 같기 때문입니다.

60
00:04:04,101 --> 00:04:08,878
그리고 여기에서는 입력 층으로

61
00:04:08,878 --> 00:04:13,003
n[0]은 nx이고 이 값은 3이 됩니다.

62
00:04:13,003 --> 00:04:17,879
이렇게해서 다른 층마다 있는 노드의 개수를

63
00:04:17,879 --> 00:04:18,463
표기합니다.

64
00:04:18,463 --> 00:04:23,913
각각의 L층에 대해서, a[l]을 이용해서

65
00:04:23,913 --> 00:04:30,196
l 층에 대한 activation을 표기할 것입니다.

66
00:04:30,196 --> 00:04:34,669
나중에 보겠지만, 방향전파를 위해 

67
00:04:34,669 --> 00:04:40,791
a[l]을 activation g(z[l]) 을 계산하게 될 것입니다.

68
00:04:40,791 --> 00:04:46,440
아마도 activation은 l층에 대해서도 이렇게 인덱싱 될 것입니다.

69
00:04:46,440 --> 00:04:51,736
그리고 W[l]를 이용해서 

70
00:04:51,736 --> 00:04:55,973
l 층에서의 z[l] 값을 계산하는 비중을 표기하도록 하겠습니다. 

71
00:04:55,973 --> 00:05:00,714
유사하게 b[l]은 z[l]을 계사한는데 쓰입니다.

72
00:05:00,714 --> 00:05:07,114
표기에 대한 내용을 마무리하자면, 
입력 특성 값이 x라고 불립니다.

73
00:05:07,114 --> 00:05:12,215
x는 또한 0층의 activation인데요, 즉 a[0]은 x입니다.

74
00:05:12,215 --> 00:05:17,133
그리고 마지막 층의 activation은 
a[L]=ŷ입니다.

75
00:05:17,133 --> 00:05:25,275
결과적으로 a[L]은 신경망의 예상 결과값 ŷ이 되는거죠. 

76
00:05:25,275 --> 00:05:28,200
이제 여러분은 심층신경망이 어떻게 생겼는지 
보셨는데요, 

77
00:05:28,200 --> 00:05:32,427
신경망에서 계산할 때 쓰는 각종 표기법 또한 익히셨습니다. 

78
00:05:32,427 --> 00:05:36,457
이 비디오에서는 굉장히 많은 표기 방법에 대해 
다뤘는데요, 

79
00:05:36,457 --> 00:05:40,916
만약 공부하시면서 잘 기억이 나지 않는 심볼이 있는 경우, 
코스 홈페이지에 표기시트 또는 노테이션 가이드를 

80
00:05:40,916 --> 00:05:45,089
업로드 해놓았으니 확인하실 수 있습니다.

81
00:05:45,089 --> 00:05:48,958
다름으로는 이런 네트워크에서 전 방향전파가 

82
00:05:48,958 --> 00:05:49,620
어떤지 보겠습니다.

83
00:05:49,620 --> 00:05:51,019
다음 비디오로 넘어가겠습니다.