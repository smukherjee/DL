深層ニューラルネットワークの実装で
コードが正しいかを確認するために 私がよく使うデバッグツールの一つは
紙を持ってきて 自分が扱っている行列の次元を
確認していくという方法です これによってあなたもより簡単に
深層ニューラルネットワークを 実装できるようになってほしいので
その方法を説明します 大文字のLは入力層を含めないので
５になりますね ５つの層があります
４つの隠れ層と１つの出力層です なので 順伝播を実装すると 一つ目のステップは
z[1]=W[1]x+b[1]です バイアス項のbを無視して
パラメータWに着目しましょう この一つ目の隠れ層には
三つの隠れユニットがあります これが０層目
そして１、２、３、４、５層目です 前回の動画での表記法を用いると １層目の隠れユニットの数であるn[1]は
３となります n[2]は５で n[3]は４、n[4]は２、
n[5]は１です ここまでは出力ユニットが一つだけの
ニューラルネットワークでしたが 後のコースでは出力層が複数
あるものについてもお話しします 最後に入力層は n[0]=nx=2です ではz、W、xの次元を考えましょう zは一つ目の隠れ層への
活性化のベクトルなので zは3×1になります つまり３次元のベクトルです なのでn[1]×1次元のベクトルを書きます n[1]×1次元の行列ですね
この場合3×1です それでは入力の特徴量xはどうでしょう
二つの特徴量があります そのためこの例ではxは2×1ですが
一般化するとn[0]×1になります するとW[1]は
n[0]×1を掛けたときに n[1]×1のベクトルになる行列
ということになりますね ３次元のベクトルが 何かに２次元のベクトルをかけたものと
等しくなります 行列の掛け算の法則を考えると これは3×2の行列となります 3×2の行列に
2×1の行列またはベクトルをかけると 3×1のベクトルが得られるからです 一般化すると これは
n[1]×n[0]次元の行列です ここでわかったことは W[1]はn[1]×n[0]でなければいけない
ということです より一般的に言うと
W[l]はn[l]×n[l-1]次元になります 例えば この場合W[2]は 5×3次元となります というのは
n[2]×n[1]次元ということです なぜならz[2]は W[2]×a[1]から計算されるからです ここでもバイアス項は無視しましょう これは3×1となり これは5×1となるので これは5×3になるはずだからです 同様にW[3]も 次の層の次元数 コンマ
前の層の次元数になるので これは4×5ですね W[4]は2×4 そしてW[5]は1×2となります
いいですか？ つまり確認すべきことは l層目についての行列を
実装するとき その行列がn[l]×n[l-1]となっているか
ということです ではベクトルbの次元数についても
考えましょう これは3×1のベクトルなので
答えに3×1のベクトルを得るには 別の3×1のベクトルを
足さなければなりません この例では これは5×1なので これも5×1のベクトルになります すると四角で囲った二つの合計が ちゃんと5×1となります より一般的なルールとしては
左の例では b[1]はn[1]×1 つまり3×1ですね 二つ目の例ではn[2]×1です 一般化すると b[l]はn[l]×1次元になる
ということです これら二つの式が
行列Wとベクトルbの次元を 確認する助けになればと思います もちろん逆伝播の実装でも dWはWと同じ次元になるはずです dWはWと同じ次元数で dbはbと同じ次元数です 他に次元数を確認すべきものは
ここではあまり話しませんでしたが z、x、そしてa[l]です z[l]はg[l]がa[l]に
要素ごとに適用されたものなので こういう種類のネットワークでは
zとaは同じ次元数を持つはずです では一回に複数のサンプルについて
計算する ベクトル化された実装では
どうでしょう ベクトル化された実装でも もちろんW、b、dW、dbの
次元数は同じです しかしz、a、そしてxの次元は 少し変わります 前回は z[1]=W[1]x+b[1]でした そしてこれはn[1]×1 これはn[1]×n[0] xはn[0]×1で
bはn[1]×1でした そしてベクトル化された 実装では Z[1]=W[1]X+b[1]という式になります Z[1]はそれぞれのサンプルについての
z[1]を取ることで得られます つまりz[1](1)、z[1](2)から z[1](m)までをこのように並べると
Z[1]になるのです するとZ[1]はn[1]×1ではなく n[1]×mになります
ここでmは教師セットのサイズです W[1]の次元数はそのままなので
n[1]×n[0]です Xはn[0]×1ではなくなって 全ての教師サンプルが
水平に並べられたものなので 今はn[0]×m次元になっています n[1]×n[0]の行列に
n[0]×mの行列をかけると 期待通りn[1]×mの行列が
得られることがわかりますね 最後になりますが
b[1]はn[1]×1のままですが これをbに足すと
Pythonのブロードキャスティングによって n[1]×mに複製され
要素ごとに足されることになります 前のスライドではW、b、dW、dbの
次元数について話しました ここでお話ししているのは
z[l]とa[l]は n[l]×1なのに対して 大文字のZ[l]とA[l]は
n[l]×mだということです 特殊なのはlが０のときです そのときはA[0]は 教師サンプルの特徴量Xに等しく 予想できる通りn[0]×m次元になります もちろん逆伝播で実装するときにも 後で学びますが
dZやdAを計算します これらは当然 ZとAと同じ次元数となります ここまで見てきた例によって 扱う行列の次元数を明確に
できたならうれしいです 深層ニューラルネットワークを
実装するときに 全ての行列が一貫していることを
確認するのは 助けになるはずです きっと あり得るバグの原因を
潰してくれます いろいろな行列の次元数を求める
練習問題が 役に立っていたら幸いです 深層ニューラルネットワークの実装で これらの行列やベクトルの次元数を
きちんと整えていたら いくつかのバグの原因を消すのに
役立つでしょう 実際私のコードを正しくするのを
助けてくれています ここまではニューラルネットワークで
順伝播を行う 方法を見てきました しかしなぜ深層ニューラルネットワークは
こんなに効果的で 浅いものよりも効果があるのでしょう 次の動画で数分間
そのことについて話しましょう