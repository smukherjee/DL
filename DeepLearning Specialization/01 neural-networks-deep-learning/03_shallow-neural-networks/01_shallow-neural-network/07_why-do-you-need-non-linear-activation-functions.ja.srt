1
00:00:00,000 --> 00:00:04,360
なぜニューラルネットワークに
非線形活性化関数が必要なのでしょうか

2
00:00:04,360 --> 00:00:07,425
ニューラルネットワークに興味深い関数を
計算させようとすると

3
00:00:07,425 --> 00:00:10,335
非線形の活性化関数が必要だと分かります

4
00:00:10,335 --> 00:00:15,830
これはニューラルネットワークの
フォーワードプロパゲーションの式です

5
00:00:15,830 --> 00:00:17,770
この式を削除してみましょう

6
00:00:17,770 --> 00:00:22,315
関数gを削除して
a[1]=z[1]とします

7
00:00:22,315 --> 00:00:27,690
またはg(z)=zとします

8
00:00:27,690 --> 00:00:31,813
これは時として線形活性化関数と呼ばれます

9
00:00:31,813 --> 00:00:33,205
より良い名前としては

10
00:00:33,205 --> 00:00:37,800
恒等活性化関数があります
入力をそのまま出力しているからです

11
00:00:37,800 --> 00:00:39,600
この目的のためには

12
00:00:39,600 --> 00:00:43,310
a[2]=z[2]
だとしたらどうでしょうか

13
00:00:43,310 --> 00:00:45,183
この場合には

14
00:00:45,183 --> 00:00:53,620
モデルはy hat を
入力特徴量x の線形関数として計算しているだけです

15
00:00:53,620 --> 00:00:55,940
この最初の２つの式を考えます

16
00:00:55,940 --> 00:01:04,547
a[1]=z[1]=w[1]x+b
だとして

17
00:01:04,547 --> 00:01:15,965
a[2]=z[2]=w[2]a[1]+b[2]だとします

18
00:01:15,965 --> 00:01:25,370
そしてa[1]の定義を
下の式に代入すると

19
00:01:25,370 --> 00:01:32,585
a[2]=w[2](w[1]x+b[1])

20
00:01:32,585 --> 00:01:35,695
になりますよね

21
00:01:35,695 --> 00:01:40,985
この部分はa[1]です

22
00:01:40,985 --> 00:01:47,460
そして+b[2]です
単純化すると

23
00:01:47,460 --> 00:01:53,120
(w[2]w[1])x + (w[2]+b[1]+b[2])

24
00:01:53,120 --> 00:01:57,966
(w[2]w[1])x + (w[2]+b[1]+b[2])
となります

25
00:01:57,966 --> 00:02:01,930
結果

26
00:02:01,930 --> 00:02:06,726
この部分をwプライム、bプライムと呼びましょう

27
00:02:06,726 --> 00:02:10,935
結果wプライムxとbプライムになります

28
00:02:10,935 --> 00:02:13,720
ここで万が一線形活性化関数を使う場合や

29
00:02:13,720 --> 00:02:17,095
恒等活性化関数と呼ぶものを使う場合には

30
00:02:17,095 --> 00:02:23,335
ニューラルネットワークは単に
入力を線形関数で出力するだけです

31
00:02:23,335 --> 00:02:26,260
ニューラルネットワークに数多くの隠れ層のある

32
00:02:26,260 --> 00:02:27,460
深いネットワークについては

33
00:02:27,460 --> 00:02:29,167
後に話しますが

34
00:02:29,167 --> 00:02:34,460
もし線形活性化関数を使うか

35
00:02:34,460 --> 00:02:36,760
活性化関数を使わない場合には

36
00:02:36,760 --> 00:02:39,250
ニューラルネットワークに
どれだけ多くの層があっても

37
00:02:39,250 --> 00:02:42,970
線形活性化関数の計算を
しているだけになります

38
00:02:42,970 --> 00:02:45,905
隠れ層が全く無いものに
しても構いません

39
00:02:45,905 --> 00:02:49,335
次のような場合もあります

40
00:02:49,335 --> 00:02:50,880
線形活性化関数を

41
00:02:50,880 --> 00:02:55,170
ここに使い
シグモイド関数をここに使うと

42
00:02:55,170 --> 00:02:58,275
このモデルは標準的な

43
00:02:58,275 --> 00:03:02,505
隠れ層なしのロジステック回帰と
同じものになります

44
00:03:02,505 --> 00:03:05,910
わざわざ証明しませんが
試してみても構いません

45
00:03:05,910 --> 00:03:07,465
ここでは

46
00:03:07,465 --> 00:03:11,265
隠れ層に線形を使うのは
意味がありません

47
00:03:11,265 --> 00:03:17,130
２つの線形関数を合わせても
それ自体が線形関数になるからです

48
00:03:17,130 --> 00:03:19,950
非線形のものを入れない限り

49
00:03:19,950 --> 00:03:21,235
特になにかを

50
00:03:21,235 --> 00:03:25,350
関数が計算するわけではありません
ネットワークを深くした場合でもです

51
00:03:25,350 --> 00:03:29,820
線形活性化関数を
使うかもしれない箇所が１つあります

52
00:03:29,820 --> 00:03:36,810
g(z)=z これは回帰問題に
機械学習を使う時です

53
00:03:36,810 --> 00:03:39,420
y が実数の時です

54
00:03:39,420 --> 00:03:42,675
例えば住宅価格を予測する時です

55
00:03:42,675 --> 00:03:46,935
y は０、１ではなく
実数です

56
00:03:46,935 --> 00:03:54,660
住宅価格は０ドルから
可能なだけ高くなります

57
00:03:54,660 --> 00:03:58,640
数百万ドルの場合もありえますよね

58
00:03:58,640 --> 00:04:04,580
でもデータセット内で
どれだけ高くても

59
00:04:04,580 --> 00:04:09,705
y は実数を取ります

60
00:04:09,705 --> 00:04:14,700
この場合には線形活性化関数を
使っても良いかもしれません

61
00:04:14,700 --> 00:04:17,805
y hat も

62
00:04:17,805 --> 00:04:24,215
実数でマイナス無限大、プラス無限大に
なりえます

63
00:04:24,215 --> 00:04:28,700
しかし隠れ層のユニットは
活性化関数を使ってはいけません

64
00:04:28,700 --> 00:04:34,380
ReLU、Tanh、Leaky ReLU や
他のものを使います

65
00:04:34,380 --> 00:04:39,995
線形活性化関数を使う可能性のある個所は
通常は出力層です

66
00:04:39,995 --> 00:04:41,595
それ以外では

67
00:04:41,595 --> 00:04:44,730
線形活性化関数を

68
00:04:44,730 --> 00:04:50,135
隠れ層で使うのは
非常に特別な場合のみです

69
00:04:50,135 --> 00:04:52,320
圧縮に関してはありますが

70
00:04:52,320 --> 00:04:56,250
線形活性化関数を使うのは
非常に稀なことです

71
00:04:56,250 --> 00:04:59,130
１週目のビデオで見た
住宅価格の予測では

72
00:04:59,130 --> 00:05:03,795
住宅価格はマイナス値になりませんが

73
00:05:03,795 --> 00:05:07,060
この場合でもReLU 活性化関数を使って

74
00:05:07,060 --> 00:05:11,580
y hat が
常に０以上になるようにします

75
00:05:11,580 --> 00:05:13,980
これで非線形活性化関数が

76
00:05:13,980 --> 00:05:19,290
ニューラルネットワークに必須なことが
分かってもらえたかと思います

77
00:05:19,290 --> 00:05:23,945
次からは勾配降下法について
扱い始めます

78
00:05:23,945 --> 00:05:27,440
次のビデオで勾配降下法を話すために

79
00:05:27,440 --> 00:05:29,230
個々の活性化関数の

80
00:05:29,230 --> 00:05:34,105
傾きや微分の推定方法を
示したいと思います

81
00:05:34,105 --> 00:05:35,600
次のビデオに行きましょう