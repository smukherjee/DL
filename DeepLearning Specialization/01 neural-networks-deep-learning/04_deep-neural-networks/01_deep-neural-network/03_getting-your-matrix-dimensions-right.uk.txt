При реалізації глибинних нейронних мереж (НМ)
я частенько беру в руки інструмент для перевірки правильності свого коду - аркуш паперу - і записую розміри тих матриць, з якими працюю. Давай покажу, як я це роблю, бо сподіваюсь полегшити реалізацію і твоєї глибокої НМ. Тож L=5. Правильно? 
Це легко порахувати. Вхідний шар не враховуємо. Тож ми маємо тут 5 шарів: 4 прихованих і 1 вихідний. Отже, якщо ми реалізуємо пряме поширення, то першим кроком буде z[1]=w[1]x+b[1]. Давай поки проігноруємо параметр зсуву 
і сфокусуємось на параметрі w. Тож, перший прихований шар має три приховані вузли. 
Рахуємо спочатку - це нульовий шар, це перший, другий, третій, четвертий і п'ятий шари. Використовуючи позначення з попереднього відео, 
матимемо, що n[1], що є кількістю прихованих вузлів в 1-ому шарі, =3. Дня наступного ми матимемо n[2]=5, n[3]=4, n[4]=2 і, нарешті, n[5]=1. Досі ми бачили НМ лише з одним вихідним вузлом, але пізніше, в наступних лекціях, ми говоритимемо про НМ 
з багатьма вихідними вузлами. Нарешті, для вхідного шару матимемо n[0]=n[x]=2. Тепер давай подумаємо про розміри z, w та x. z - це вектор активаторів для 1-ого прихованого шару, тож матиме розмір (3, 1). Буде 3-елементним вектором. Я запишу це як вектор розміру (n[1], 1), як матриця розміру (n[1], 1). В цьому випадку - (3, 1). А як щодо вхідних ознак x? Ми маємо 2 вхідні ознаки. Тож x в цьому випадку - (2, 1), а в загальному - (n[0], 1). Тож нам необхідно, щоб матриця w[1] була такою, щоб при множенні на вектор (n[0], 1), ми отримали вектор (n[1], 1). 
Правильно? Тож ми маємо щось на зразок 3-елементного вектору помноженого на 2-елементний вектор. І, згідно правил множення матриць, в резултаті повинна бути матриця (3, 2). Правильно? Бо матриця (3, 2) помножена на матрицю (2, 1) (або на вектор (2, 1)), дає вектор (3, 1). В загальному випадку це буде матриця розмірності (n[1], n[0]). Тож ми вияснили, що розміри w[1] мають бути (n[1], n[0]). В більш загальному випадку розміри w[l] мають бути (n[l], n[l-1]). Тож, наприклад, розміри w[2] повинні бути (5, 3). Або ж це буде (n[2], n[1]). Тепер обчислимо z[2], що дорівнює w[2]a[1] І, знову ж, давай поки проігноруємо зсув. Тож це буде (3, 1). А оце потрібно щоб було (5, 1). Тож оце буде (5, 3). І, аналогічно, w[3] буде (розмір наступного шару, розмір попереднього шару), тобто буде (4, 5). w[4] буде (2, 4), а w[5] буде (1, 2). Добре? Тож загальна формула для перевірки розмірів матриці l-ого шару: розмір матриці має бути (n[l], n[l-1]). Тепер давай подумаємо про розмір вектора b. Оце буде розміру (3, 1). Тож ми маємо додати вектор такого ж розміру - (3, 1), щоб і на виході отримати вектор (3, 1). А в цьому випадку, нам треба додавати до оцього. 
Значить це буде (5, 1). Тож це має бути ще один вектор (5, 1), щоб після додавання цих обведених доданків ми отримали оцей вектор (5, 1). Тож загальне правило (на прикладі зліва): b[1] матиме розмір (n[1], 1). В другому випадку це буде (n[2], 1). І, отже, загальне правило: b[l] має мати розмір (n[l], 1). Я сповнений надії, що ці 2 рівняння допоможуть впевнитись, 
що розміри твоїх матриць w і векторів b мають правильні розміри. І, звичайно, при реалізації зворотного поширення, 
розмір dw[l] повинно мати розмір w[l]. Повторю, dw[l] має мати такий же розмір як w[l], а db[l] має мати розмір такий же як b[l]. Тепер. Наступний набір величин буде для перевірки розмірів z, x і a[l], про яке ми мало сьогодні говорили. Але, через те, що z[l]=g[l](a[l]) застосовується поелементно, то z і a повинні мати 
однакові розміри в таких типах НМ. Тепер давай подивимось що буде 
при векторизованій реалізації (для багатьох зразків одночасно). Навіть для векторизованої реалізації, звичайно, розміри w, b, dw і db залишаються незмінними. Але розміри z, a і x дещо зміняться при векторизованій реалізації. Тож на попередньому слайді ми мали z[1]=w[1]x+b[1], де оце було розміру (n[1], 1), це було (n[1], n[0]), x було (n[0], 1), а b було (n[1], 1). Тепер у векторизованій реалізації матимемо Z[1]=W[1]X+b[1], де тепер Z[1] утворюється із z[1] для окремих зразків - z[1]⁽¹⁾, z[1]⁽²⁾ і до z[1]⁽ᵐ⁾ складених ось так. І це дає нам Z[1]. Тож розмір Z[1] замість (n[1], 1) стає (n[1], m), де m - розмір тренувального набору. Розмір W[1] залишається тим же - (n[1], n[0]). А X замість (n[0], 1) тепер міститиме всі тренувальні зразки складені по горизонталі, тож буде (n[0], m). Тож зверни увагу, коли ми беремо матрицю (n[1], n[0]) і множимо на матрицю (n[0], m), то отримаємо матрицю (n[1], m), як і очікується. І остання ремарка. b[1] все ще (n[1], 1). Але коли ми візьмемо оце і додамо b, з допомогою посіву Python, це посіється до матриці (n[1], m) і поелементно додасться. Тож на попередньому слайді ми говорили про розмірність w, b, dw і db. Тепер можемо бачити, що якщо z[l] і a[l] мають розмір (n[l], 1), то зараз ми маємо, що Z[l] і A[l] будуть (n[l], m). Окремий випадок - коли L=0. В цьому випадку A[0] (наш тренувальний набір, вхідні ознаки X) буде (n[0], m), як і очікується. І, звичайно, коли ми це реалізуємо в зворотному поширенні і отримаємо dZ і dA, то побачимо, що вони мають ті ж розміри, що й Z і A. Отже, надіюсь, що ця маленька вправа допомогла зрозуміти де брати розміри тих багатьох матриць, з якими ти працюватимеш. Коли ти реалізуєш зворотне поширення для глибокої НМ, перевіряй код і переконуйся, 
що матриці мають відповідні розміри. Це, зазвичай, допомагає запобігати появі певних можливих помилок. Я радий, якщо вправа з визначення розмірів цих різноманітних матриць була корисною. Якщо ти реалізуватимеш глибоку НМ і чітко дотримуватимешся розмірів матриць і векторів, з якими працюватимеш, то, надіюсь, це допоможе уникнути певних можливих помилок. Безумовно, це допомагає і мені писати коректний код. Ти вже знайомий/а з деякими механізмами прямого поширення НМ. Але чому ж глибокі НМ такі ефективні? Чому вони працюють краще ніж "мілкі"? Давай витратимо кілька хвилин і 
в наступному відео про це поговоримо.