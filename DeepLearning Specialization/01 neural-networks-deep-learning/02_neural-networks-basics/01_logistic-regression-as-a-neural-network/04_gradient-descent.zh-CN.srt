1
00:00:00,590 --> 00:00:03,210
我们已经讲了逻辑回归模型。

2
00:00:03,210 --> 00:00:06,560
我们已经讲了如何通过损失函数来界定你的模型

3
00:00:06,560 --> 00:00:08,780
对单一样本的训练效果。

4
00:00:08,780 --> 00:00:13,530
我们还讲了代价函数，代价函数

5
00:00:13,530 --> 00:00:16,590
可以用来衡量参数w与b在你设计的整个模型中的作用效果

6
00:00:16,590 --> 00:00:21,600
现在，我们继续来看看如何使用梯度下降模型去训练，

7
00:00:21,600 --> 00:00:25,730
或者去学习，来调整你的训练集中的参数w和b。

8
00:00:25,730 --> 00:00:30,030
总的来说，这里有一个我们已经熟悉的逻辑回归算法。

9
00:00:31,130 --> 00:00:34,700
在第二行我们看到了代价函数 J

10
00:00:34,700 --> 00:00:37,879
代价函数J有参数w和b，

11
00:00:37,879 --> 00:00:39,960
并且定义为平均值。

12
00:00:39,960 --> 00:00:44,140
计算从1到m的损失函数之和

13
00:00:44,140 --> 00:00:48,470
损失函数可以衡量你的算法的效果

14
00:00:48,470 --> 00:00:53,170
对每一个训练样例都输出y^(i)

15
00:00:53,170 --> 00:00:58,000
对每一个训练样例上的真实结果y(i)进行比较

16
00:00:58,000 --> 00:01:00,886
完整的公式在等号右边展开

17
00:01:00,886 --> 00:01:04,130
所以代价函数可以衡量你的参数w和

18
00:01:04,130 --> 00:01:06,760
b在训练集上的效果

19
00:01:06,760 --> 00:01:11,510
要使得参数w和b的设置变得合理

20
00:01:11,510 --> 00:01:17,930
自然地想到要去找到使得代价函数 J(w, b)
尽可能小所对应的w和b

21
00:01:17,930 --> 00:01:21,320
接下来给出梯度下降法(gradient descent)的说明

22
00:01:21,320 --> 00:01:25,320
在这个图中横轴表示

23
00:01:25,320 --> 00:01:28,510
你的空间参数w和b

24
00:01:28,510 --> 00:01:32,350
在实践中w可以是更高的维度，但是为了更好地绘图

25
00:01:32,350 --> 00:01:38,190
我们定义w和b都是单一实数

26
00:01:38,190 --> 00:01:40,770
代价函数J(w,b)是在

27
00:01:40,770 --> 00:01:45,130
水平轴w和b上的曲面

28
00:01:45,130 --> 00:01:50,720
因此曲面的高度就是 J(w,b)在某一点的值

29
00:01:50,720 --> 00:01:55,070
我们所想要做的就是找到这样的w和b

30
00:01:55,070 --> 00:01:59,730
使得对应的代价函数J值是最小值

31
00:02:00,830 --> 00:02:06,050
我们可以看到代价函数J是一个凸函数(convex function)

32
00:02:06,050 --> 00:02:10,327
像这样的一个大碗，因此这是一个凸函数

33
00:02:10,327 --> 00:02:13,717
并且这与看起来像这样的函数相反

34
00:02:13,717 --> 00:02:18,120
它是非凸的，并且有很多不同的局部最优

35
00:02:18,120 --> 00:02:22,240
因此我们的成本函数J(w,b)，之所以定义为凸函数

36
00:02:22,240 --> 00:02:27,020
一个重要原因是我们使用对于逻辑回归

37
00:02:27,020 --> 00:02:29,610
这个特殊代价函数J造成的

38
00:02:29,610 --> 00:02:33,810
为了去找到优的参数值

39
00:02:33,810 --> 00:02:39,160
我们将会用一些初始值来初始化w和b

40
00:02:39,160 --> 00:02:43,360
可能是用那个小红点表示的

41
00:02:43,360 --> 00:02:47,562
对于逻辑回归几乎所有的初始化方法都有效

42
00:02:47,562 --> 00:02:50,690
通常用0来进行初始化

43
00:02:50,690 --> 00:02:52,910
随机初始化也有效

44
00:02:52,910 --> 00:02:55,630
但是对于逻辑回归我们通常不这么做

45
00:02:55,630 --> 00:02:59,310
但是因为函数是凸函数，无论在哪里初始化

46
00:02:59,310 --> 00:03:02,180
你应该达到同一点或大致相同的点

47
00:03:02,180 --> 00:03:06,450
梯度下降法以初始点开始

48
00:03:06,450 --> 00:03:10,310
然后朝最陡的下坡方向走一步

49
00:03:10,310 --> 00:03:15,290
因此在梯度下降法一步后，你或许会停在那里

50
00:03:15,290 --> 00:03:19,320
因为它正试图沿着最陡下降的方向走下坡路

51
00:03:19,320 --> 00:03:21,250
或者尽可能快地下坡

52
00:03:21,250 --> 00:03:23,600
这是梯度下降的一次迭代

53
00:03:23,600 --> 00:03:27,084
两次迭代或许会到达那里

54
00:03:27,084 --> 00:03:28,830
三次或更多

55
00:03:28,830 --> 00:03:32,640
我猜想这是隐藏在曲线后面

56
00:03:32,640 --> 00:03:38,880
希望你收敛到这个全局最优值或接近全局最优值

57
00:03:38,880 --> 00:03:42,300
所以这张图片说明了梯度下降模型

58
00:03:42,300 --> 00:03:44,310
让我们多说一些细节

59
00:03:44,310 --> 00:03:47,750
为了更好地说明，让我们来看一些函数

60
00:03:47,750 --> 00:03:51,700
你想要找到J(w)的最小值，可能函数会看起来像这样

61
00:03:51,700 --> 00:03:54,650
为了画起来容易些，我现在忽略b

62
00:03:54,650 --> 00:03:59,210
仅仅是用一维曲线代替多维曲线

63
00:03:59,210 --> 00:04:01,240
梯度下降是这样做的

64
00:04:01,240 --> 00:04:06,740
我们将重复执行以下更新的操作

65
00:04:06,740 --> 00:04:09,467
我们更新w的值

66
00:04:09,467 --> 00:04:12,508
使用“:=”表示w进行迭代

67
00:04:12,508 --> 00:04:17,426
设置w为w-α dJ(w)/dw(公式如图)

68
00:04:17,426 --> 00:04:22,200
（dJ(w)/dw表示函数J(w)对w求导）

69
00:04:22,200 --> 00:04:26,230
在算法收敛之前我会重复这样做。

70
00:04:26,230 --> 00:04:30,666
公式中有两点是我要提一下的，首先在这里的α表示学习率(learning rate)

71
00:04:30,666 --> 00:04:36,820
学习率可以控制我们在每一次迭代或者梯度下降法中步长大小

72
00:04:36,820 --> 00:04:41,200
我们之后讨论如何选择学习率α

73
00:04:41,200 --> 00:04:44,490
其次在这里的这个数是导数

74
00:04:44,490 --> 00:04:48,010
这就是对参数w的基本更新或者改变

75
00:04:48,010 --> 00:04:52,700
当我们开始编写代码来实现梯度下降

76
00:04:52,700 --> 00:04:57,380
我们将使用代码中变量名的约定

77
00:04:58,620 --> 00:05:02,300
dw表示导数

78
00:05:02,300 --> 00:05:06,551
因此你会像这样编写代码

79
00:05:06,551 --> 00:05:10,046
w:=w-α*dw(公式如图)

80
00:05:10,046 --> 00:05:14,750
我们用dw作为导数的变量名

81
00:05:14,750 --> 00:05:19,330
现在我们确保梯度下降法更新是有意义的

82
00:05:19,330 --> 00:05:21,880
w在这

83
00:05:21,880 --> 00:05:26,060
对应的代价函数J(w)在曲线上的这一点

84
00:05:26,060 --> 00:05:29,270
记住导数的定义

85
00:05:29,270 --> 00:05:31,420
是函数在这个点上的斜率

86
00:05:31,420 --> 00:05:36,190
而函数的斜率是高除宽

87
00:05:36,190 --> 00:05:40,290
在这个点相切于 J(w)的一个小三角形

88
00:05:40,290 --> 00:05:43,900
在这里导数是正的

89
00:05:43,900 --> 00:05:48,830
w通过w自身减去学习率乘导数来更新

90
00:05:48,830 --> 00:05:53,310
导数是正的所以你每一次从w中减去这个乘积

91
00:05:53,310 --> 00:05:55,260
接着每一次都向左边走一步

92
00:05:55,260 --> 00:05:59,380
如果在一开始你参数w的值就非常的大的话

93
00:05:59,380 --> 00:06:04,450
像这样梯度下降法会使你的算法渐渐地减小这个参数w。

94
00:06:04,450 --> 00:06:08,545
另一个例子，如果w的位置是在这里

95
00:06:08,545 --> 00:06:15,050
这个点处的斜率 dJ/dw将会是负的

96
00:06:15,050 --> 00:06:22,771
并且梯度下降法在更新参数时
w将会减去α乘上一个负数

97
00:06:22,771 --> 00:06:27,122
并慢慢地使得参数w增加

98
00:06:27,122 --> 00:06:31,530
所以这样的迭代和梯度下降法会使得参数w逐步变大

99
00:06:31,530 --> 00:06:34,387
无论你初始化的位置是在左边还是右边

100
00:06:34,387 --> 00:06:39,000
梯度下降法会朝着全局最小值方向移动

101
00:06:39,000 --> 00:06:43,100
如果你不熟悉导数或者微积分

102
00:06:43,100 --> 00:06:49,710
你也不熟悉dJ(w)/dw的含义，别急

103
00:06:49,710 --> 00:06:53,770
在下一节课我们会讨论更多关于导数的知识

104
00:06:53,770 --> 00:06:56,761
如果你深入了解过微积分

105
00:06:56,761 --> 00:07:02,321
你应该可以对神经网络如何工作有更深刻更直观的认识

106
00:07:02,321 --> 00:07:05,471
但是即使你并不熟悉微积分

107
00:07:05,471 --> 00:07:10,091
通过下面的几节课，我们也会对导数

108
00:07:10,091 --> 00:07:14,980
和微积分有足够直接的认识
使你能够有效的使用神经网络

109
00:07:14,980 --> 00:07:16,410
但是现在所有的直观认识

110
00:07:16,410 --> 00:07:21,520
便是这个术语，表示的是函数的斜率

111
00:07:21,520 --> 00:07:26,760
并且我们希望知道
在当前的参数条件下，斜率是怎样的

112
00:07:26,760 --> 00:07:31,140
所以我们可以采用下降速度最快的步长

113
00:07:31,140 --> 00:07:35,450
我们也可以知道下一步更新的方向
让代价函数J逐渐减小

114
00:07:36,660 --> 00:07:42,520
因此目前对于(w)J的梯度下降法
我们写出来的参数中假设了只有w

115
00:07:42,520 --> 00:07:47,150
在逻辑回归中你的代价函数是一个含有w和b的函数

116
00:07:47,150 --> 00:07:50,894
在这种情况下，梯度下降的内部循环
就是这里的

117
00:07:50,894 --> 00:07:53,302
这个公式，你需要不断重复迭代

118
00:07:53,302 --> 00:07:57,970
我们可以把w的迭代公式写成这样

119
00:07:57,970 --> 00:08:02,030
w:=w-α*dJ(w,b)/dw

120
00:08:02,030 --> 00:08:07,460
以及把b的迭代公式写成这样

121
00:08:07,460 --> 00:08:12,270
b:=b-α*dJ(w,b)/db

122
00:08:12,270 --> 00:08:17,300
这两个等式是你实际迭代更新参数时进行的操作

123
00:08:17,300 --> 00:08:22,320
另外我想提到的是，在微积分的符号约定中

124
00:08:22,320 --> 00:08:24,560
某些符号可能会让人疑惑。

125
00:08:24,560 --> 00:08:28,387
我不认为目前理解微积分（符号约定）是非常重要的

126
00:08:28,387 --> 00:08:32,411
如果你看到这些，希望你不要想太多

127
00:08:32,411 --> 00:08:35,519
在微积分中在这的术语

128
00:08:35,519 --> 00:08:40,730
作为这一有趣的花体标志我们实际上这么写<br />（如图所写的是偏微分符号,读作round）

129
00:08:40,730 --> 00:08:46,160
所以这个符号这实际上只是一个小写d

130
00:08:46,160 --> 00:08:51,070
当你看到这个表达式的时候
用一个花哨的、样式化的字体

131
00:08:51,070 --> 00:08:56,145
所有的含义都是J(w,b)的导数或者函数J(w,b)的斜率

132
00:08:56,145 --> 00:09:01,580
也即是函数在w这一点的斜率

133
00:09:01,580 --> 00:09:06,640
在微积分中这个符号的规则
我认为并不是完全符合逻辑的

134
00:09:06,640 --> 00:09:11,780
同时对于微积分中这个符号的规则
我认为会让理解变得

135
00:09:11,780 --> 00:09:16,940
更为困难。也就是说 当函数J有两个以上的变量
（使用偏导数符号）

136
00:09:16,940 --> 00:09:21,550
我们不使用小写字母d
而使用更加花哨的符号

137
00:09:21,550 --> 00:09:24,380
这个就称作偏导数符号

138
00:09:24,380 --> 00:09:26,120
但是别担心

139
00:09:26,120 --> 00:09:31,090
如果J只有一个变量，就使用小写字母d

140
00:09:31,090 --> 00:09:33,960
唯一的区别就是你是用偏导数符号

141
00:09:33,960 --> 00:09:38,040
还是小写字母d

142
00:09:38,040 --> 00:09:41,570
取决于你的函数J是否含有两个以上的变量

143
00:09:41,570 --> 00:09:45,900
变量超过两个，就使用偏导数符号

144
00:09:45,900 --> 00:09:51,480
如果你的函数只有一个变量你就使用小写字母d

145
00:09:51,480 --> 00:09:55,410
这是在微积分上一个有趣的符号规则

146
00:09:55,410 --> 00:09:58,540
我认为它使学习和理解变得更加复杂了

147
00:09:58,540 --> 00:10:03,300
但是如果你看到了偏导数符号
其含义就是计算函数

148
00:10:03,300 --> 00:10:07,290
关于其中一个变量在对应点处的斜率

149
00:10:07,290 --> 00:10:12,530
类似在这里，这里的微积分符号正式地写的话

150
00:10:12,530 --> 00:10:18,070
应该改用另一个数学符号
因为在这里J有两个输入参数而不是一个

151
00:10:18,070 --> 00:10:22,540
屏幕底部的这个东西应该用这个偏导数来写

152
00:10:22,540 --> 00:10:28,290
但是这其实表达了和小写字母d同样的含义

153
00:10:28,290 --> 00:10:31,360
最后当你编写代码想要实现

154
00:10:31,360 --> 00:10:36,220
我们屏幕写出的这个公式时
通常在迭代更新w的值的时候

155
00:10:36,220 --> 00:10:41,980
我们会用dw这个变量来代替这个式子

156
00:10:41,980 --> 00:10:44,220
同样的情况，还有下面这个公式

157
00:10:44,220 --> 00:10:47,230
当你想去更新b的数值

158
00:10:47,230 --> 00:10:50,740
我们将会用db这个变量来代表下面这个公式

159
00:10:50,740 --> 00:10:55,580
所以，这就是梯度下降法的实现方法

160
00:10:55,580 --> 00:10:59,830
如果现在你已经有很多年没接触微积分了

161
00:10:59,830 --> 00:11:03,770
我明白你看到这个视频中一黑板的导数公式会有些焦虑

162
00:11:03,770 --> 00:11:06,330
如果你有这种感觉，不要担心

163
00:11:06,330 --> 00:11:10,150
我们会在下一个教程中，给你更多关于导数的解释

164
00:11:10,150 --> 00:11:13,560
甚至不用通过深度数学知识来理解微积分

165
00:11:13,560 --> 00:11:16,310
而是通过直观图示来理解微积分

166
00:11:16,310 --> 00:11:19,130
这样学习完后，你将会更容易理解什么是神经网络

167
00:11:19,130 --> 00:11:22,743
让我们进入下一个视频，我们会讨论导数的知识
GTC字幕组翻译

168
00:11:22,743 --> 00:11:23,470
恩