1
00:00:00,000 --> 00:00:01,230
В останньому відео

2
00:00:01,230 --> 00:00:03,720
ти бачив/ла рівняння зворотного поширення.

3
00:00:03,720 --> 00:00:06,900
В цьому відео давай спробуємо зрозуміти з допомогою

4
00:00:06,900 --> 00:00:10,515
Діаграми обчислень як ці рівняння були виведені.

5
00:00:10,515 --> 00:00:12,385
Це відео абсолютно необов'язкове,

6
00:00:12,385 --> 00:00:14,106
тож дивитимешся ти його чи ні,

7
00:00:14,106 --> 00:00:16,360
ти, все одно, зможеш виконати домашнє завдання.

8
00:00:16,360 --> 00:00:19,410
Тож згадаймо, що коли ми говорили про логістичну регресію (ЛР),

9
00:00:19,410 --> 00:00:23,685
ми повинні були здійснити пряме поширення, де обчислювали z,

10
00:00:23,685 --> 00:00:26,145
потім a, а потім - Втрату.

11
00:00:26,145 --> 00:00:27,445
А потім, щоб взяти похідні,

12
00:00:27,445 --> 00:00:32,520
ми повинні були здійснити зворотне поширення, 
де спочатку могли обчислити da,

13
00:00:32,520 --> 00:00:35,400
а потім - dz,

14
00:00:35,400 --> 00:00:40,720
а потім - dw і db.

15
00:00:40,720 --> 00:00:46,970
Тож визначенням Втрати було

16
00:00:46,970 --> 00:00:52,655
L(a, y)=-y*log(a)-

17
00:00:52,655 --> 00:00:57,440
(1-y)*log(1-a).

18
00:00:57,440 --> 00:00:59,750
Тож, якщо ти знайомий/а з

19
00:00:59,750 --> 00:01:03,600
диференціальним численням (диф.численням), 
то можеш взяти похідні по a,

20
00:01:03,600 --> 00:01:06,156
що дасть тобі формулу da.

21
00:01:06,156 --> 00:01:09,060
Тож da= оцьому.

22
00:01:09,060 --> 00:01:12,750
І, якщо ми це розв'яжемо згідно диф.числення, 
то зможемо показати, що це

23
00:01:12,750 --> 00:01:18,808
=-y/a+(1-y)(1-a).

24
00:01:18,808 --> 00:01:23,040
Це виводиться диф.численням, якщо просто взяти похідні.

25
00:01:23,040 --> 00:01:26,680
Виявляється, якщо пройти ще назад і обчислити dz,

26
00:01:26,680 --> 00:01:32,430
то ми отримаємо, що dz=a-y. Я не пояснював цього раніше,

27
00:01:32,430 --> 00:01:37,920
проте, виявляється, що, згідно ланцюгового правила диф.числення,

28
00:01:37,920 --> 00:01:45,425
dz=da*g'(z),

29
00:01:45,425 --> 00:01:50,535
де g(z)=σ(z) і є

30
00:01:50,535 --> 00:01:56,245
нашою функцією активації вихідного вузла ЛР. Правильно?

31
00:01:56,245 --> 00:02:00,570
Тож просто запам'ятай, при ЛР ми маємо x₁, x₂, x₃

32
00:02:00,570 --> 00:02:05,757
і далі лише один сигмоїдальний вузол, який дає нам a,

33
00:02:05,757 --> 00:02:07,400
що дає нам ŷ.

34
00:02:07,400 --> 00:02:11,400
Тож тут функцією активації була сигмоїда.

35
00:02:11,400 --> 00:02:12,960
Трохи не по темі,

36
00:02:12,960 --> 00:02:17,205
тільки для тих, хто знайомий з ланцюговим правилом диф.числення:

37
00:02:17,205 --> 00:02:22,520
підставою для цього є те, що a=σ(z),

38
00:02:22,520 --> 00:02:29,310
тож частинна похідна L по z (∂L/∂z) =

39
00:02:29,310 --> 00:02:36,800
частинній похідній L по a (∂L/∂a) помножене на da/dz.

40
00:02:36,800 --> 00:02:39,611
Це - a, що =σ(z),

41
00:02:39,611 --> 00:02:42,970
тобто це =

42
00:02:42,970 --> 00:02:49,080
d/dz*g(z), що =g'(z).

43
00:02:49,080 --> 00:02:54,060
Ось чому оцей вираз, що в нашому коді буде dz, =

44
00:02:54,060 --> 00:02:59,484
оцьому виразу, що в нашому коді буде da, помноженому на g'(z).

45
00:02:59,484 --> 00:03:05,860
Тож оце - це оце.

46
00:03:05,860 --> 00:03:09,172
Останнє виведення матиме сенс, якщо

47
00:03:09,172 --> 00:03:13,510
ти знайомий/а з диф.численням, 
а, точніше, - з ланцюговим правилом.

48
00:03:13,510 --> 00:03:15,325
Але якщо ти незнайомий/а - не хвилюйся,

49
00:03:15,325 --> 00:03:18,853
я намагаюсь трохи пояснювати де це потрібно.

50
00:03:18,853 --> 00:03:22,315
І, нарешті, обчисливши dz для цієї регресії,

51
00:03:22,315 --> 00:03:26,335
ми обчислюватимемо dw, яке, виявляється, було

52
00:03:26,335 --> 00:03:31,470
dw*x, і db, яке просто =dz, 
якщо ми маємо лише один тренувальний зразок.

53
00:03:31,470 --> 00:03:33,822
Тож це була ЛР.

54
00:03:33,822 --> 00:03:36,700
Обчислюючи зворотне обчислення ми будемо

55
00:03:36,700 --> 00:03:40,090
проводити дуже схожі обчислення,

56
00:03:40,090 --> 00:03:46,995
але робити це будемо двічі, 
бо тепер x буде входити не в вихідний вузол,

57
00:03:46,995 --> 00:03:50,930
а у прихований шар, а вже потім в вихідний вузол.

58
00:03:50,930 --> 00:03:58,405
Тож замість цих обчислень, складених в 1 крок отут,

59
00:03:58,405 --> 00:04:04,483
ми матимемо вже 2 кроки. Це вже буде НМ з двома шарами.

60
00:04:04,483 --> 00:04:08,586
І в цій двошаровій НМ ми маємо вхідний шар,

61
00:04:08,586 --> 00:04:10,138
прихований шар і вихідний шар.

62
00:04:10,138 --> 00:04:12,070
Запам'ятай кроки обчислень.

63
00:04:12,070 --> 00:04:17,210
Спочатку ми обчислюємо z[1] з допомогою оцих рівнянь,

64
00:04:17,210 --> 00:04:22,177
потім обчислюємо a[1], потім - z[2]

65
00:04:22,177 --> 00:04:25,505
(пам'ятай, що z[2] теж залежить від параметрів w[2] і b[2]),

66
00:04:25,505 --> 00:04:27,530
а потім, базуючись на z[2],

67
00:04:27,530 --> 00:04:32,815
обчислюємо a[2] і вкінці отримуємо Втрату.

68
00:04:32,815 --> 00:04:41,560
А зворотне поширення піде назад (справа-наліво), 
щоб обчислити da[2], потім dz[2],

69
00:04:41,560 --> 00:04:48,805
потім ще назад, щоб обчислити dw[2] і db[2]

70
00:04:48,805 --> 00:04:53,232
і ще назад, щоб обчислити da[1],

71
00:04:53,232 --> 00:04:57,278
dz[1] і т.д.

72
00:04:57,278 --> 00:05:00,290
Немає потреби брати похідні по

73
00:05:00,290 --> 00:05:03,745
вхідних x, бо вхідні x для контрольованого навчання є сталими.

74
00:05:03,745 --> 00:05:07,845
Тож ми не намагаємось оптимізувати x 
і не будемо надокучати похідними,

75
00:05:07,845 --> 00:05:09,655
принаймі, для контрольованого навчання, по x.

76
00:05:09,655 --> 00:05:15,605
Я пропущу явне обчислення da[2].

77
00:05:15,605 --> 00:05:18,110
Якщо хочеш, можеш обчислити da[2],

78
00:05:18,110 --> 00:05:20,750
а потім використати його, щоб обчислити dz[2], але на практиці

79
00:05:20,750 --> 00:05:25,760
ці кроки можна об'єднати в один і отримати

80
00:05:25,760 --> 00:05:31,715
dz[2]=a[2]-y (як і раніше).

81
00:05:31,715 --> 00:05:33,620
Ми також

82
00:05:33,620 --> 00:05:38,615
(запишу dw[2] і db[2] внизу)

83
00:05:38,615 --> 00:05:46,700
маємо dw[2]=dz[2]*a[1]ᵀ,

84
00:05:46,700 --> 00:05:52,040
а db[2]=dz[2].

85
00:05:52,040 --> 00:05:55,990
Тож цей крок дуже схожий на той, що ми мали в ЛР.

86
00:05:55,990 --> 00:06:03,550
Ми мали dw=dz*x. А зараз

87
00:06:03,550 --> 00:06:08,770
маємо a[1] замість x і додатково маємо транспонування, бо

88
00:06:08,770 --> 00:06:14,125
зв'язок між матрицею велике W і окремими параметрами w

89
00:06:14,125 --> 00:06:16,660
тут є транспонованим. Правильно?

90
00:06:16,660 --> 00:06:24,370
Тому що w[2] є вектором-рядком 
(у випадку ЛР з одним вихідним вузлом),

91
00:06:24,370 --> 00:06:26,980
в той час як тут

92
00:06:26,980 --> 00:06:32,440
w було вектором-стовпцем. 
Ось чому ми додатково транспонуємо a[1],

93
00:06:32,440 --> 00:06:36,980
хоча не транспонували x в ЛР.

94
00:06:36,980 --> 00:06:40,335
Тож це - половина зворотного поширення.

95
00:06:40,335 --> 00:06:44,045
Знову ж, ти можеш обчислити da[1], якщо хочеш.

96
00:06:44,045 --> 00:06:49,440
Хоча на практиці обчислення da[1] і dz[1],

97
00:06:49,440 --> 00:06:52,330
як правило, об'єднують в один крок і

98
00:06:52,330 --> 00:06:57,130
реалізувати доводиться лише dz[1]=

99
00:06:57,130 --> 00:07:03,480
w[2]ᵀ помножене на dz[2] і потім поелементно помножити

100
00:07:03,480 --> 00:07:10,383
на g'(z[1]).

101
00:07:10,383 --> 00:07:13,960
Давай зробимо перевірку розмірів.

102
00:07:13,960 --> 00:07:19,510
Якщо ми маємо ось таку НМ,

103
00:07:19,510 --> 00:07:23,000
що видає ŷ.

104
00:07:23,000 --> 00:07:28,265
Якщо ми маємо nₓ=n[0] вхідних ознак,

105
00:07:28,265 --> 00:07:30,230
n[1] прихованих вузлів,

106
00:07:30,230 --> 00:07:34,275
і n[2], що

107
00:07:34,275 --> 00:07:36,740
в нашому випадку

108
00:07:36,740 --> 00:07:38,565
має лише 1 вихідний вузол,

109
00:07:38,565 --> 00:07:48,795
то матриця w[2] матиме розмір n[2]*n[1],

110
00:07:48,795 --> 00:07:57,490
z[2] і, отже, dz[2] матимуть розмір n[2]*1

111
00:07:57,490 --> 00:07:59,850
(насправді розмір буде 1*1

112
00:07:59,850 --> 00:08:04,750
для двійкової класифікації), і z[2] і, отже,

113
00:08:04,750 --> 00:08:10,045
dz[1] матимуть розмір n[1]*1.

114
00:08:10,045 --> 00:08:16,115
Варто запам'ятати, що змінні foo і dfoo 
(матриця величини і матриця похідних цієї величини) 
завжди будуть однакового розміру.

115
00:08:16,115 --> 00:08:20,850
Тож тому w і dw завжди матимуть однакові розміри і, аналогічно,

116
00:08:20,850 --> 00:08:23,680
b та db, z та dz і т.п.

117
00:08:23,680 --> 00:08:26,895
Щоб пересвідчитись, що всі розміри співпадають,

118
00:08:26,895 --> 00:08:35,430
ми маємо dz[1]=w[2]ᵀ*dz[2]

119
00:08:35,430 --> 00:08:44,490
і далі поелементне множення на g[1]'(z[1]).

120
00:08:44,490 --> 00:08:47,040
Тож, підставивши розміри згори,

121
00:08:47,040 --> 00:08:52,575
матимемо n[1]*1= w[2]ᵀ.

122
00:08:52,575 --> 00:08:57,945
Після транспонування матимемо розмір n[1]*n[2].

123
00:08:57,945 --> 00:09:05,790
dz[2] матиме розмір n[2]*1. А ось це

124
00:09:05,790 --> 00:09:07,230
матиме той же розмір, що й z[1].

125
00:09:07,230 --> 00:09:11,820
Тож це також має розмір n[1]*1

126
00:09:11,820 --> 00:09:14,350
(тут поелементне множення). 
Тож розміри мають значення, правильно?

127
00:09:14,350 --> 00:09:18,330
Вектор n[1]*1 може бути отриманий

128
00:09:18,330 --> 00:09:23,520
множенням матриці розміру n[1]*n[2] на n[2]*n[1], 
тому що їх добуток

129
00:09:23,520 --> 00:09:28,890
дасть матрицю розміру n[1]*1. Це зробить можливим

130
00:09:28,890 --> 00:09:34,618
поелементний добуток двох векторів розміру n[1]*1.

131
00:09:34,618 --> 00:09:36,060
Значить розміри співпадають.

132
00:09:36,060 --> 00:09:40,620
Дам одну пораду для реалізації зворотного поширення.

133
00:09:40,620 --> 00:09:44,790
Якщо ми хочемо пересвідчитись, що розміри співпадають,

134
00:09:44,790 --> 00:09:47,190
то треба подивитись які розміри

135
00:09:47,190 --> 00:09:50,430
всіх матриць: w[1], w[2], z[1],

136
00:09:50,430 --> 00:09:54,180
z[2], a[1], a[2] і т.д. і пересвідчитись,

137
00:09:54,180 --> 00:09:58,642
що розміри матриць підходять для математичних операцій.

138
00:09:58,642 --> 00:10:03,390
Інколи це дозволяє позбутись багатьох помилок 
при реалізації зворотного поширення.

139
00:10:03,390 --> 00:10:06,960
Добре. Тож це дає нам dz[1] і, нарешті

140
00:10:06,960 --> 00:10:12,160
(вже допишемо), dw[1] і db[1].

141
00:10:12,160 --> 00:10:13,965
Думаю, їх краще записати ось тут.

142
00:10:13,965 --> 00:10:17,200
Але так як я вже не маю місця, то запишу їх справа.

143
00:10:17,200 --> 00:10:21,965
dw[1] і db[1] отримуються наступними формулами:

144
00:10:21,965 --> 00:10:25,950
оце буде =dz[1]xᵀ,

145
00:10:25,950 --> 00:10:28,905
а це - =dz.

146
00:10:28,905 --> 00:10:34,045
Можна помітити подібність між цими і оцими рівняннями.

147
00:10:34,045 --> 00:10:37,095
Що не є випадковістю, бо x

148
00:10:37,095 --> 00:10:41,660
грає роль a[0]. Тож xᵀ - це a[0]ᵀ.

149
00:10:41,660 --> 00:10:45,484
Тож ці рівняння дійсно дуже схожі.

150
00:10:45,484 --> 00:10:50,260
Тож ми отримали певне розуміння виведення зворотного поширення.

151
00:10:50,260 --> 00:10:54,530
Тепер ми маємо 6 головних рівнянь для dz[2], dw[2],

152
00:10:54,530 --> 00:11:00,190
db[2], dz[1], dw[1] і db[1].

153
00:11:00,190 --> 00:11:05,767
Давай скопіюємо ці 6 рівнянь на наступний слайд. Ось вони.

154
00:11:05,767 --> 00:11:08,950
До цього моменту ми виводили зворотне поширення

155
00:11:08,950 --> 00:11:13,959
для тренування 1-ого тренувального зразка.

156
00:11:13,959 --> 00:11:21,530
Тож не буде дивним, що замість роботи над одним тренувальним зразком

157
00:11:21,530 --> 00:11:27,810
нам захочеться векторизувати це для всіх тренувальних зразків.

158
00:11:27,810 --> 00:11:30,971
Як пам'ятаємо, для прямого поширення,

159
00:11:30,971 --> 00:11:33,545
коли ми працювали над 1-им зразком за ітерацію,

160
00:11:33,545 --> 00:11:41,665
у нас були наступні рівняння: z[1]=w[1]x+b[1], a[1]=g[1](z[1]).

161
00:11:41,665 --> 00:11:43,655
Щоб векторизувати, ми

162
00:11:43,655 --> 00:11:51,260
брали z і складали їх

163
00:11:51,260 --> 00:12:00,775
ось так в стовпці і називали це великою Z.

164
00:12:00,775 --> 00:12:04,960
І подібним складанням в стовпці

165
00:12:04,960 --> 00:12:10,240
визначали також решту великобуквенних величин.

166
00:12:10,240 --> 00:12:17,093
Тож отримували Z[1]=W[1]X+b[1]

167
00:12:17,093 --> 00:12:24,700
і A[1]=g[1](Z[1]). Правильно?

168
00:12:24,700 --> 00:12:28,645
Ми використовуємо позначення дуже уважно 
в цьому курсі, щоб пересвідчитись,

169
00:12:28,645 --> 00:12:35,550
що складення зразків в стовпці матриці правильно працюватиме.

170
00:12:35,550 --> 00:12:40,105
Виявляється, якщо уважно виконувати математичні обчислення,

171
00:12:40,105 --> 00:12:46,645
то подібні дії підходять і для зворотного поширення. 
То векторизовані рівняння виглядають наступним чином.

172
00:12:46,645 --> 00:12:52,250
По-перше, якщо взяти dz різних тренувальних зразків і скласти

173
00:12:52,250 --> 00:12:58,339
їх в стовпці матриці (і так само для A і для Y),

174
00:12:58,339 --> 00:13:03,070
то отримаємо векторизовану реалізацію. А ось визначення

175
00:13:03,070 --> 00:13:05,569
або формула обчислення dW[2].

176
00:13:05,569 --> 00:13:11,130
Тут також буде 1/m, тому що функція Витрат J є

177
00:13:11,130 --> 00:13:18,410
1/m від суми від 1 до m Втрат.

178
00:13:18,410 --> 00:13:20,615
І при обчисленні похідних

179
00:13:20,615 --> 00:13:23,885
маємо вираз 1/m, як було і при

180
00:13:23,885 --> 00:13:27,982
обчисленні коригування w для ЛР.

181
00:13:27,982 --> 00:13:31,790
А ось коригування db[2].

182
00:13:31,790 --> 00:13:40,640
Тут теж сума dZ від 1 до m. 
І потім dZ[1] обчислюється отаким чином.

183
00:13:40,640 --> 00:13:49,109
Знову ж, це - поелементний добуток (на відміну від випадку,

184
00:13:49,109 --> 00:13:56,595
що ми бачили на попередньому слайді, 
де це було вектором розміру n[1]*1).

185
00:13:56,595 --> 00:14:03,185
І тепер це - матриця розміру n[1]*m.

186
00:14:03,185 --> 00:14:09,045
І ці обидві матриці теж мають розмір n[1]*m.

187
00:14:09,045 --> 00:14:19,310
Ось чому тут зірочка. Це поелементне множення. 
І, нарешті,

188
00:14:19,310 --> 00:14:21,454
останні 2 коригування.

189
00:14:21,454 --> 00:14:25,836
Сподіваюсь - не несподівані.

190
00:14:25,836 --> 00:14:29,510
Надіюсь, ти отримав/ла певне розуміння 
як виводиться алгоритм зворотного поширення.

191
00:14:29,510 --> 00:14:32,205
Всюди в машинному навчанні (МН),

192
00:14:32,205 --> 00:14:34,820
на мою думку, виведення алгоритму зворотного поширення

193
00:14:34,820 --> 00:14:38,465
є, насправді, однією з найскладніших математичних частин 
з тих, що я бачив.

194
00:14:38,465 --> 00:14:42,470
Воно вимагає знання і лінійної алгебри, і

195
00:14:42,470 --> 00:14:46,830
матричних похідних, які треба виводити з нуля, 
керуючись базовими принципами.

196
00:14:46,830 --> 00:14:50,165
Якщо ти експерт матричного диф.числення,

197
00:14:50,165 --> 00:14:54,255
то використовуючи базові принципи, ти, 
можливо виведеш алгоритм самостійно,

198
00:14:54,255 --> 00:14:57,500
але, на мою думку, насправді, 
більшість практиків глибокого навчання,

199
00:14:57,500 --> 00:15:01,060
що бачили виведення на тому ж рівні, що й ти

200
00:15:01,060 --> 00:15:04,100
в цьому відео, вже отримали

201
00:15:04,100 --> 00:15:08,580
достатнє розуміння і можуть реалізувати цей алгоритм 
дуже ефективно.

202
00:15:08,580 --> 00:15:10,070
Тож, якщо ти експерт з диф.числення,

203
00:15:10,070 --> 00:15:13,395
то спробуй вивести весь алгоритм з нуля.

204
00:15:13,395 --> 00:15:15,665
Це одна з найважчих його математичних частин.

205
00:15:15,665 --> 00:15:20,010
Одне з найважчих виведень, які я зустрічав в МН.

206
00:15:20,010 --> 00:15:22,861
Проте, незважаючи на те, чи вдасться тобі це вивести,

207
00:15:22,861 --> 00:15:27,260
алгоритм працюватиме і я маю надію, 
що ти маєш достатнє розуміння, щоб змусити його працювати.

208
00:15:27,260 --> 00:15:30,830
Отже, залишається ще одна деталь, якою я хочу

209
00:15:30,830 --> 00:15:34,190
з тобою поділитись перед тим, як ти почнеш реалізувати свою НМ,

210
00:15:34,190 --> 00:15:37,720
і це - як ініціалізувати коефіцієнти НМ.

211
00:15:37,720 --> 00:15:40,600
Виявляється, що ініціалізація параметрів

212
00:15:40,600 --> 00:15:42,560
не в 0, а у випадкове значення,

213
00:15:42,560 --> 00:15:45,515
буде грати значну роль в тренуванні твоєї НМ.

214
00:15:45,515 --> 00:15:48,000
В наступному відео ти побачиш чому.