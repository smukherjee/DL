Коли ми будуємо нейронну мережу (НМ), то неодмінно мусимо вибрати функції активації і для прихованих шарів, і для вихідних вузлів НМ. Досі ми використовували лише сигмоїду як функцію активації. Проте, інколи інші функції можуть працювати набагато краще. Давай розглянемо деякі опції. В кроці прямого поширення НМ ми маємо оці 3 кроки, в яких використовуємо сигмоїду. Ця сигмоїда називається функцією активації. Ось вже знайома сигмоїдальна функція. Вона =1/(1+e⁻ᶻ). Тож в більш загальному випадку ми можемо мати іншу функцію g(z), яку я запишу отут, де g може бути нелінійною функцією і не сигмоїдою. Приклад. Сигмоїда приймає значення від 0 до 1. А, в більшості випадків, кращою функцією активації ніж сигмоїда є функція th - функція гіперболічного тангенсу. Тобто, оце - z, оце - a, оце - a=th(z) з областю значень від -1 до +1. Формула функції th =(eᶻ-e⁻ᶻ)/(eᶻ+e⁻ᶻ). І, насправді, математично, є зсунутою версією сигмоїди. Тобто дуже подібна на сигмоїду, але дещо зміщена (і тепер перетинає точку (0, 0)) і розтягнута 
(тобто має область значень між -1 і +1) Виявляється, що для прихованих вузлів, 
якщо дозволити функції g(z) дорівнювати th(z), то вона майже завжди буде працювати краще ніж сигмоїда. Тому що при області значень між -1 і +1 
середнє значення активації, що виводиться з прихованих вузлів, буде близьким до 0. І, отже, інколи, при тренуванні навчального алгоритму, ми можемо відцентрувати дані відносно 0-ого середнього з допомогою th, а не сигмоїди. Тобто вона може типу центрувати дані, тож середнє значення даних буде ближче до 0, замість 0,5. А це робить навчання наступного шару дещо легшим. Ми більше поговоримо про це в 2-ому курсі, 
коли говоритимемо про оптимізацію алгоритмів. Але зауважу, що я майже вже не використовую сигмоїду як функцію активації. Функція th майже завжди є функцією вибору. Одне з виключень - вихідний шар. 
Бо якщо y може бути лише 0 або 1, то має сенс, щоб ŷ приймала значення (тобто ми так хочемо) між 0 і 1, замість між -1 і 1. Тож винятком, в якому я використав би сигмоїду як функцію активації - це двійкова класифікація. В цьому випадку можна використовувати сигмоїду 
як функцію активації для вихідного шару. Тож g(z[2]) тут =σ(z[2]). І, отже, на цьому прикладі видно, що th може бути функцією активації для прихованого шару, 
а сигмоїда - для вихідного шару. Тож функції активації можуть бути різними в різних шарах. Тож занотуй, що іноді функції активації різні в різних шарах. Ми будемо використовувати 
верхні індекси в квадратних дужках, щоб позначати, що g[1] може відрізнятись від g[2]. І знову ж, верхній індекс 1 в квадратних дужках 
відноситься до оцього шару, а верхній індекс 2 в квадратних дужках відноситься до оцього шару. Тепер. Один з недоліків обох - сигмоїди і гіперболічного тангенсу - це якщо z або дуже велике, або дуже мале, то Градієнт, або похідна, або нахил цих функцій стає дуже малим. Тож якщо z або дуже велике, або дуже мале, то нахил функції стає дуже близьким до 0. А це може сповільнити Градієнтний спуск. Тож одним з виборів, який дуже поширений в машинному навчанні, є так званий випрямлений лінійний вузол (ВЛВ). Функція ВЛВ виглядає отак. А її формула: a=max(0, z). Тож похідна =1 поки z - додатне і похідна або нахил =0 коли z - від'ємне. При реалізації, технічно, похідна, якщо z точно =0, є невизначеною. Але при програмній реалізації ми отримаємо, що z=0000000000000. Тобто дуже мале і нам непотрібно про це турбуватись на практиці. Ми можемо прирівняти похідну (коли z=0) або до 1, або до 0 і продовжити роботу. Тож той факт, що вона недиференційована... Ось що треба пам'ятати при виборі функцій активації. Якщо на виході в нас 0 або 1, 
тобто коли ми використовуємо двійкову класифікацію, то природно для вихідного шару взяти сигмоїду як функцію активації. А для всіх інших вузлів - ВЛВ або випрямлений лінійний вузол, який все частіше обирається функцією активації по замовчуванню. Тож якщо ти не впевнений/а 
що використати для прихованого шару я раджу функцію активації ВЛВ. Зараз її використовує більшість. Хоча інколи використовується і th як функція активації. Один з недоліків ВЛВ - це те, що похідна =0 коли z - від'ємне. На практиці це не заважає. Проте, є й інша версія ВЛВ. Вона називається нещільним ВЛВ. Я дам формулу на наступному слайді. Але вона не =0 при від'ємному z, а має отакий невеликий нахил, тому й називається нещільною. Як правило, як функція активації вона працює краще ніж ВЛВ, хоча на практиці використовується не так часто. Будь-яка з них повинна підійти. Якщо потрібно вибрати, то я, зазвичай, використовую ВЛВ. Перевагою обох - ВЛВ і нещільного ВЛВ - є те, що на великій частині області визначення z 
похідна функції активації, нахил функції активації, дуже відрізняється від 0. Тож на практиці, при використанні ВЛВ як функції активації наша НМ часто навчатиметься набагато швидше ніж при використанні th або сигмоїди як функції активації. Головна причина цього - менший вплив нахилу функції, тобто ефекту сповільнення навчання 
коли він наближений до 0. Я знаю, що на половині області визначення z нахил ВЛВ =0, але на практиці достатня кількість прихованих вузлів матимуть z>0. Тож навчання буде достатньо швидким 
для більшості тренувальних зразків. Давай швиденько резюмуємо за і проти різних функцій активації. Ось функція активації сигмоїда. Я раджу ніколи її не використовувати. Окрім вихідного шару при реалізації двійкової класифікації. 
Тобто майже ніколи її не використовувати. Причина того, що я її б майже ніколи 
не використовував, це те, що th набагато краще. Ось функція активації th. І ось вибір по замовчуванню, найбільш вживана функція активації, - ВЛВ. Ось вона. Тож якщо ти не впевнений/а що використати, використовуй її. І також не бійся спробувати нещільну ВЛВ. Що, можливо, буде =max(0.01*z, z). Зрозуміло? Тож a=max(0.01*z, z). Це і дає оцей невеличкий вигин функції. Можливо, тобі цікаво де взялось 0,01? Що ж, можна вибрати і інше значення для навчального алгоритму. Дехто каже, що інші працюють краще. Але я рідко бачу щоб це робили. Але якщо ти хочеш спробувати це для свого застосунку, 
будь ласка, не бійся. Ти зможеш побачити як це працює, наскільки добре і використовувати, якщо результат буде задовільний. Тож, надіюсь, я дав деяке розуміння як вибирати функції активації і використовувати їх в НМ. Як видно в ГН дуже часто доводиться щось вибирати, щоб написати код НМ: кількість прихованих вузлів, функції активації, як ініціалізувати коефіцієнти (про це ми поговоримо пізніше)... Багато різноманітних виборів. І, виявляється, що інколи складно виробити чіткі інструкції, щоб точно знати, що найкраще підійде для нашої задачі. Тож протягом цих курсів я намагатимусь давати приклади побачені в промисловості, щоб показати що більш, а що менш популярне. Але для твого застосунку, з його особливостями, насправді дуже складно знати наперед, що найкраще працюватиме. Тож більш-менш загальна порада - 
це, якщо ти не впевнений/а яку з цих функції вибрати - спробуй усі і оціни на приготованому валідаційному наборі або 
розробницькому наборі, про які ми поговоримо пізніше, і побачиш яка працюватиме краще, і вибереш її. Також думаю, що, тестуючи свій вибір в застосунку, ти зможеш краще в майбутньому вдосконалювати архітектуру своєї НМ згідно з особливостями задачі і розвитком алгоритмів. Якби я сказав тобі завжди використовувати ВЛВ як функцію активації і більш нічого, то могло б спрацювати, а могло б і ні, 
для будь-якої задачі над якою б ти не працював зараз, пізніше чи через багато років. Добре. Це був вибір функцій активації і ти побачив/ла найбільш поширені функції активації. Є одне питання, яке ти міг/ла б задати. Навіщо взагалі використовувати функції активації? Чому не можна обійтись без них? Тож давай поговоримо про це в наступному відео, в якому ти побачиш чому НМ дійсно потребують 
певної нелінійної функції активації.