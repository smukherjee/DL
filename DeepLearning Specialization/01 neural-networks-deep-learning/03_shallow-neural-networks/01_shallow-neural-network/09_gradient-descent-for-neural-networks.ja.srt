1
00:00:00,000 --> 00:00:03,840
この動画は楽しい動画になりそうです。

2
00:00:01,800 --> 00:00:06,240
この動画では、

3
00:00:03,840 --> 00:00:08,730
隠れ層が一つのニューラルネットワークのための

4
00:00:06,240 --> 00:00:10,530
最急降下法の実装方法を説明します。

5
00:00:08,730 --> 00:00:12,809
ここでは、

6
00:00:10,530 --> 00:00:14,639
最急降下法のバックプロパゲーションが動くようにするための

7
00:00:12,809 --> 00:00:17,039
実装に必要な等式をお伝えします。

8
00:00:14,639 --> 00:00:19,410
そしてこの後の動画で、

9
00:00:17,039 --> 00:00:21,510
どうしてこれらの等式が

10
00:00:19,410 --> 00:00:24,150
ニューラルネットワークのための

11
00:00:21,510 --> 00:00:26,070
勾配の計算に必要な等式として

12
00:00:24,150 --> 00:00:27,630
正しいのかということについて

13
00:00:26,070 --> 00:00:28,289
もう少し直感的な説明をしたいと思います。

14
00:00:27,630 --> 00:00:30,090
-

15
00:00:28,289 --> 00:00:32,520
隠れ層が一つの

16
00:00:30,090 --> 00:00:39,930
ニューラルネットワークは、

17
00:00:32,520 --> 00:00:44,760
パラメータw1,b1,w2,b2などを持っています。

18
00:00:39,930 --> 00:00:50,399
思い出してください nx個の...代わりに

19
00:00:44,760 --> 00:00:56,640
あー n[0]個の入力特徴量とn[1]個の隠れ層ユニット

20
00:00:50,399 --> 00:00:59,149
そして n[2]個の出力ユニットが 我々の例にはあり

21
00:00:56,640 --> 00:01:05,670
今のところ n[2]は１に等しい

22
00:00:59,149 --> 00:01:08,880
よって 行列w[1]は n[1]×n[0]行列に

23
00:01:05,670 --> 00:01:11,250
b[1]は n[1]次元ベクトルになり

24
00:01:08,880 --> 00:01:13,350
つまり n[1]×1次元行列に書くことができて

25
00:01:11,250 --> 00:01:16,500
実際は 列ベクトルだけど

26
00:01:13,350 --> 00:01:20,750
W[2]の次元は n[2]×n[1]で

27
00:01:16,500 --> 00:01:26,759
b[2]の次元は n[2]×1になる

28
00:01:20,750 --> 00:01:28,590
再び得たこの例が ここまで

29
00:01:26,759 --> 00:01:30,930
見てきたn[2]が1に等しい場合のもので

30
00:01:28,590 --> 00:01:36,930
それは 単一の隠れ層のみを持つ

31
00:01:30,930 --> 00:01:39,570
ニューラルネットワークにも

32
00:01:36,930 --> 00:01:41,340
コスト関数を持つ そして

33
00:01:39,570 --> 00:01:44,220
今のところ 以下の仮定をする

34
00:01:41,340 --> 00:01:48,659
2項分類を行うと。よって この場合

35
00:01:44,220 --> 00:01:51,740
ここに挙げたパラメータ群のコストは

36
00:01:48,659 --> 00:01:57,090
ロス関数の平均のm分の1になる

37
00:01:51,740 --> 00:01:59,969
そして

38
00:01:57,090 --> 00:02:03,420
ここで Lは ニューラルネットワークが y^ を予測した時のロスで

39
00:01:59,969 --> 00:02:06,240
これ(y^)は 本当は a[2]で

40
00:02:03,420 --> 00:02:07,649
ラベルは y に等しい

41
00:02:06,240 --> 00:02:09,629
そして 2項分類を行っているのなら

42
00:02:07,649 --> 00:02:12,510
ロス関数は 正に

43
00:02:09,629 --> 00:02:15,030
以前 ロジスティック・リグレッションに使ったものと同じになり得る

44
00:02:12,510 --> 00:02:18,420
よって アルゴリズムのパラメータを学習するのに

45
00:02:15,030 --> 00:02:21,450
勾配降下法を実行する必要がある

46
00:02:18,420 --> 00:02:23,189
ニューラルネットワークを学習させるのに

47
00:02:21,450 --> 00:02:25,379
重要なのは パラメータを

48
00:02:23,189 --> 00:02:26,129
ランダムに初期化し 全てを0にしないことだ

49
00:02:25,379 --> 00:02:28,140
0

50
00:02:26,129 --> 00:02:30,030
なぜ そうなのかは 後で見るが

51
00:02:28,140 --> 00:02:32,069
パラメータを何かに初期化した後

52
00:02:30,030 --> 00:02:34,140
勾配降下法の 各ループで

53
00:02:32,069 --> 00:02:36,780
予測値が計算される

54
00:02:34,140 --> 00:02:42,359
つまり まずは y^(i)を

55
00:02:36,780 --> 00:02:44,519
iが1からmまでについて 計算して そして

56
00:02:42,359 --> 00:02:49,440
微分値を計算する必要がある

57
00:02:44,519 --> 00:02:51,420
つまり dw[1] それは

58
00:02:49,440 --> 00:02:54,359
コスト関数の微分値で

59
00:02:51,420 --> 00:02:56,489
パラメータw[1]に対するものだ

60
00:02:54,359 --> 00:02:59,220
もう一つの変数も計算する必要がある

61
00:02:56,489 --> 00:03:00,870
それは db[1]で

62
00:02:59,220 --> 00:03:04,109
コスト関数の微分 もしくは 傾きで

63
00:03:00,870 --> 00:03:07,349
変数b[1]に対するものだ

64
00:03:04,109 --> 00:03:10,170
同様のことを

65
00:03:07,349 --> 00:03:12,629
パラメータw[2]とb[2]についても行う

66
00:03:10,170 --> 00:03:17,879
最終的に 勾配降下法の更新は

67
00:03:12,629 --> 00:03:22,709
w[1]を　w[1]から アルファ(つまり学習率)×dw[1] を引いた物にし

68
00:03:17,879 --> 00:03:26,129
b[1]を

69
00:03:22,709 --> 00:03:31,620
b[1]から 学習率×db[1] を引いた物にする

70
00:03:26,129 --> 00:03:34,739
同じことを w[2]とb[2]にも行う

71
00:03:31,620 --> 00:03:36,299
私は 時々:= を使ったり、時々=を使ったりするが

72
00:03:34,739 --> 00:03:38,489
どちらの表記でもいいだろう

73
00:03:36,299 --> 00:03:40,829
そして、これが

74
00:03:38,489 --> 00:03:42,510
勾配降下法の1反復だ

75
00:03:40,829 --> 00:03:44,280
そして パラメータが収束していることが分かるまで

76
00:03:42,510 --> 00:03:46,079
これを何回か繰り返すのだ

77
00:03:44,280 --> 00:03:48,150
前のビデオで

78
00:03:46,079 --> 00:03:50,099
予測値の計算方法や出力の計算方法について

79
00:03:48,150 --> 00:03:51,629
話した

80
00:03:50,099 --> 00:03:54,060
ベクトル化して計算する方法も見た

81
00:03:51,629 --> 00:03:56,269
つまり要点は

82
00:03:54,060 --> 00:04:00,180
偏微分値の計算方法を理解することだ

83
00:03:56,269 --> 00:04:04,079
dw[1], db[1]についてと

84
00:04:00,180 --> 00:04:06,780
同じく dw[2]とdb[2]についての微分値を

85
00:04:04,079 --> 00:04:09,419
私が行いたいのは、

86
00:04:06,780 --> 00:04:12,150
あなたに これらの微分値を計算するために

87
00:04:09,419 --> 00:04:14,699
必要となる等式を与えることだ

88
00:04:12,150 --> 00:04:17,430
次のビデオ オプションビデオ で説明するけども

89
00:04:14,699 --> 00:04:19,090
それは どのように それらの等式を導出するかを

90
00:04:17,430 --> 00:04:21,400
説明する素晴らしい回だよ

91
00:04:19,090 --> 00:04:26,169
さて 再びまとめると

92
00:04:21,400 --> 00:04:33,250
順伝播の等式

93
00:04:26,169 --> 00:04:37,900
Z[1] = w[1] X + b[1]　そして

94
00:04:33,250 --> 00:04:41,680
A[1]は z[1]の各要素を適用させた

95
00:04:37,900 --> 00:04:49,690
その層の活性化関数に等しい

96
00:04:41,680 --> 00:04:53,530
Z[2] = w[2] A[1] + b[2]

97
00:04:49,690 --> 00:04:55,180
そして 最終的に これが

98
00:04:53,530 --> 00:05:01,210
全ての学習セットを通じてベクトル化したものだ

99
00:04:55,180 --> 00:05:02,740
A[2]は Z[2]をg[2]に入れた物だ

100
00:05:01,210 --> 00:05:04,870
2項分類を行うことを前提とするならば

101
00:05:02,740 --> 00:05:06,610
この活性化関数は

102
00:05:04,870 --> 00:05:08,560
シグモイド関数にすべきだ

103
00:05:06,610 --> 00:05:11,080
この領域ではそうなる

104
00:05:08,560 --> 00:05:13,870
これが ニューラルネットワークの

105
00:05:11,080 --> 00:05:15,729
順伝播 もしくは 左から右へ進む計算だ

106
00:05:13,870 --> 00:05:18,430
次に 微分を計算しよう

107
00:05:15,729 --> 00:05:24,750
これは 誤差逆伝播ステップだ

108
00:05:18,430 --> 00:05:30,750
では計算しよう

109
00:05:24,750 --> 00:05:33,610
dz[2] = A[2] - 測定値Y

110
00:05:30,750 --> 00:05:36,580
ちょっと思い出して欲しいんだけど

111
00:05:33,610 --> 00:05:41,289
これら全ては サンプルに跨がってベクトル化したもので

112
00:05:36,580 --> 00:05:45,280
行列Yは このように1×M行列で

113
00:05:41,289 --> 00:05:51,370
全てのm個のサンプルを横方向にリストしたものだ

114
00:05:45,280 --> 00:05:55,330
そして dw[2]はこうなり

115
00:05:51,370 --> 00:05:58,870
実際には これらの最初の3つの等式は 

116
00:05:55,330 --> 00:06:00,900
ロジスティックリグレッションの勾配降下法のと とても 良く似ている

117
00:05:58,870 --> 00:06:00,900
カンマ

118
00:06:00,960 --> 00:06:12,610
axis=1 カンマ keepdims=True

119
00:06:07,419 --> 00:06:15,580
些細なことだけど この np.sum は

120
00:06:12,610 --> 00:06:18,070
Python numpy コマンドで

121
00:06:15,580 --> 00:06:21,100
行列の一つの次元に跨がって合計を取る

122
00:06:18,070 --> 00:06:24,810
この場合は 水平方向に合計を取る

123
00:06:21,100 --> 00:06:27,600
keepdims が行うのは

124
00:06:24,810 --> 00:06:31,230
Pythonが 奇妙なランク1配列を出力するのを 防ぐことだ

125
00:06:27,600 --> 00:06:34,650
ランク1配列とは (知っているだろうが)その次元が(n, )のものだ

126
00:06:31,230 --> 00:06:37,010
keepdims=True とすることで

127
00:06:34,650 --> 00:06:41,280
Pythonの

128
00:06:37,010 --> 00:06:44,580
db[2]ベクトルへの出力が

129
00:06:41,280 --> 00:06:47,820
(n, 1) になることを保証する

130
00:06:44,580 --> 00:06:50,130
技術的には(n[2], 1)

131
00:06:47,820 --> 00:06:53,520
この場合は 単に(1, 1) なので

132
00:06:50,130 --> 00:06:56,790
問題にならないが 後で

133
00:06:53,520 --> 00:06:58,500
それが 本当に問題になる場合を見るだろう

134
00:06:56,790 --> 00:07:01,320
ここまで やってきたことは ロジスティックリグレッションに

135
00:06:58,500 --> 00:07:03,919
非常に良く似ている　しかし

136
00:07:01,320 --> 00:07:14,370
誤差逆伝播を走らせ続けると

137
00:07:03,919 --> 00:07:19,380
これを計算するわけだが　dZ[2]×g[1]'(Z[1])

138
00:07:14,370 --> 00:07:20,880
この g[1]'の値 は

139
00:07:19,380 --> 00:07:22,919
隠れ層に どのような活性化関数を使ったとしても

140
00:07:20,880 --> 00:07:25,770
その微分値である

141
00:07:22,919 --> 00:07:27,030
出力層については

142
00:07:25,770 --> 00:07:29,400
2項分類を想定し、

143
00:07:27,030 --> 00:07:30,780
シグモイド関数を使うが

144
00:07:29,400 --> 00:07:34,620
それは、既にdZ[2]の式に織り込み済みだ

145
00:07:30,780 --> 00:07:39,090
そして 今度は

146
00:07:34,620 --> 00:07:43,050
要素に跨がった積で

147
00:07:39,090 --> 00:07:46,950
ここは (n[1], m) 行列だ

148
00:07:43,050 --> 00:07:48,990
そして ここは 要素に跨がった

149
00:07:46,950 --> 00:07:52,680
微分値で また

150
00:07:48,990 --> 00:07:54,720
(n[1], m) 行列だ そして そこにこれを掛けて

151
00:07:52,680 --> 00:07:59,669
2つの行列の要素に跨がったかけ算を行う

152
00:07:54,720 --> 00:08:08,490
そして 最終的には dw[1]はこれに等しくなり

153
00:07:59,669 --> 00:08:18,950
db[1]はこれに等しくなる

154
00:08:08,490 --> 00:08:21,900
np.sum(dZ[1], axis=1, keepdims=True)

155
00:08:18,950 --> 00:08:23,430
以前に

156
00:08:21,900 --> 00:08:27,210
keepdims は これまで n[2]が

157
00:08:23,430 --> 00:08:28,590
1に等しい場合、つまり1ｘ1の場合、一つの実数値の場合は

158
00:08:27,210 --> 00:08:35,729
あまり問題にならないとしたが

159
00:08:28,590 --> 00:08:38,370
ここでは、db[1] は (n[1], 1) ベクトルであり

160
00:08:35,729 --> 00:08:40,110
Pythonのnp.sumには

161
00:08:38,370 --> 00:08:43,110
このような次元の出力をしてほしい

162
00:08:40,110 --> 00:08:46,529
奇妙なライク1配列よりも

163
00:08:43,110 --> 00:08:48,360
それは 後の計算を

164
00:08:46,529 --> 00:08:50,580
めちゃくちゃにしてしまうから

165
00:08:48,360 --> 00:08:53,310
別に

166
00:08:50,580 --> 00:08:56,910
keepdimsパラメータを使わずに

167
00:08:53,310 --> 00:09:00,060
明示的に reshapeを呼んで

168
00:08:56,910 --> 00:09:04,400
np.sumの出力を dbがそうなるように

169
00:09:00,060 --> 00:09:08,310
この次元に整形する方法もある

170
00:09:04,400 --> 00:09:11,339
これが 順伝播であり 4つの等式が想定され

171
00:09:08,310 --> 00:09:14,310
誤差逆伝播では 6つの等式が想定される

172
00:09:11,339 --> 00:09:16,680
私は 今 単に

173
00:09:14,310 --> 00:09:18,870
これらの等式を書き下しただけだ

174
00:09:16,680 --> 00:09:22,050
次のオプションビデオでは

175
00:09:18,870 --> 00:09:23,940
どのように 誤差逆伝播アルゴリズム対する

176
00:09:22,050 --> 00:09:25,830
この6つの等式が導かれるのか 見てみる

177
00:09:23,940 --> 00:09:27,750
それを見るかどうかは あなたの自由ですが

178
00:09:25,830 --> 00:09:30,000
いずれにせよ もし あなたが

179
00:09:27,750 --> 00:09:32,730
これらのアルゴリズムを実装するのなら あなたは

180
00:09:30,000 --> 00:09:34,650
順伝播と逆伝播の正しい実装をすることになるし

181
00:09:32,730 --> 00:09:36,750
必要な微分値を計算できるようになるでしょう

182
00:09:34,650 --> 00:09:39,029
ニューラルネットワークの

183
00:09:36,750 --> 00:09:41,520
パラメータを学習するための

184
00:09:39,029 --> 00:09:43,680
勾配降下法を適用するため

185
00:09:41,520 --> 00:09:45,209
アルゴリズムを実装し、それを動作させることが可能だ

186
00:09:43,680 --> 00:09:47,130
微積分の深い理解無くても

187
00:09:45,209 --> 00:09:50,520
多くのディープラーニングの

188
00:09:47,130 --> 00:09:52,320
成功した実践者は そうしている

189
00:09:50,520 --> 00:09:54,180
しかし あなたが 次のビデオも見ることができると思うなら

190
00:09:52,320 --> 00:09:56,580
これらの等式の

191
00:09:54,180 --> 00:09:58,820
微分に関して もう少し多くの直感を得ることが

192
00:09:56,580 --> 00:09:58,820
できるだろう