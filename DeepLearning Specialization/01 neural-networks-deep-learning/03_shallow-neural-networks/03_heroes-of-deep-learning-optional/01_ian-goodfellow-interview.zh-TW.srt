1
00:00:02,550 --> 00:00:05,830
嗨, 伊恩，謝謝您今天的參與

2
00:00:05,830 --> 00:00:06,860
謝謝您的邀請

3
00:00:06,860 --> 00:00:08,775
Andrew. 我很高興能在這裡

4
00:00:08,775 --> 00:00:11,920
今天, 您是世界上最引人注目的深度學習研究人員之一

5
00:00:11,920 --> 00:00:14,450
跟大家分享一下您個人的故事

6
00:00:14,450 --> 00:00:16,810
您怎麼會從事這項工作？

7
00:00:16,810 --> 00:00:19,150
是的。這聽起來很棒

8
00:00:19,150 --> 00:00:24,287
我想我開始對機器學習有興趣是在剛遇見您之前

9
00:00:24,287 --> 00:00:29,705
在此之前我一直從事於神經科學, 我的大學導師

10
00:00:29,705 --> 00:00:34,600
Jerry Cain, 在史丹佛鼓勵我去上您的 AI 課程

11
00:00:34,600 --> 00:00:35,790
喔， 我並不知道

12
00:00:35,790 --> 00:00:39,885
所以在那之前我想 AI 是個好主意

13
00:00:39,885 --> 00:00:42,590
但在實用上, 主要

14
00:00:42,590 --> 00:00:44,483
的觀念會像是遊戲 AI

15
00:00:44,483 --> 00:00:47,375
人們有很多寫死的規則對於

16
00:00:47,375 --> 00:00:49,700
遊戲中非玩家角色說著

17
00:00:49,700 --> 00:00:52,085
不同的腳本在不同的時間點

18
00:00:52,085 --> 00:00:56,750
然後，當我上了您的 AI 入門課程， 涵蓋了像

19
00:00:56,750 --> 00:01:02,815
線性迴歸分析跟線性迴歸分析的誤差變異分解

20
00:01:02,815 --> 00:01:06,665
我開始意識到這是真的科學我真的可以

21
00:01:06,665 --> 00:01:10,970
有科學職涯在 AI 而不是神經科學

22
00:01:10,970 --> 00:01:12,730
好的, 然後發生了什麼事？

23
00:01:12,730 --> 00:01:15,290
後來我回來成為您的助教

24
00:01:15,290 --> 00:01:17,815
對了, 我的助教

25
00:01:17,815 --> 00:01:22,595
所以對我來說真的很大的轉捩點是在我當助教那個時候

26
00:01:22,595 --> 00:01:23,720
一位學生

27
00:01:23,720 --> 00:01:25,310
我的朋友 Ethan Dreifuss

28
00:01:25,310 --> 00:01:28,689
對於 Geoff Hinton 的深度信念網路論文有興趣

29
00:01:28,689 --> 00:01:29,022
瞭解

30
00:01:29,022 --> 00:01:35,660
我們兩個人建立了第一個 GPU CUDA環境的機器在

31
00:01:35,660 --> 00:01:43,280
史丹佛為了跑華生機器在我們寒假的空閒時間裡

32
00:01:43,280 --> 00:01:43,817
瞭解

33
00:01:43,817 --> 00:01:46,295
在那個時候, 我開始有

34
00:01:46,295 --> 00:01:50,720
很強的直覺認為深度學習會是未來要走的路

35
00:01:50,720 --> 00:01:53,660
我做過的很多其他的演算法

36
00:01:53,660 --> 00:01:56,285
像是支援向量機

37
00:01:56,285 --> 00:01:58,845
似乎不是正確的漸進

38
00:01:58,845 --> 00:02:01,400
您加入更多的資料它們會變慢

39
00:02:01,400 --> 00:02:03,476
或者相同的資料量

40
00:02:03,476 --> 00:02:08,240
很難透過改變設定來讓它們的表現好很多

41
00:02:08,240 --> 00:02:13,065
從那個時候起我開始盡可能專注在深度學習

42
00:02:13,065 --> 00:02:18,595
我記得 Richard Reyna 很老的 GPU 論文

43
00:02:18,595 --> 00:02:21,585
認可您在早期做了很多工作

44
00:02:21,585 --> 00:02:25,850
是的, 那篇論文使用一些我們建立的機器

45
00:02:25,850 --> 00:02:26,656
是的.是的.

46
00:02:26,656 --> 00:02:30,755
第一台機器我們建立在

47
00:02:30,755 --> 00:02:35,120
Ethan 媽媽家用我們自己的錢

48
00:02:35,120 --> 00:02:39,835
然後, 我們用史丹佛實驗室的錢建了第二跟第三個

49
00:02:39,835 --> 00:02:42,965
哇, 真棒, 我不知道這個故事

50
00:02:42,965 --> 00:02:45,830
在今天, 有一件

51
00:02:45,830 --> 00:02:48,365
事真的掀起

52
00:02:48,365 --> 00:02:51,645
深度網路世界的風暴是您發明了 GANs (生成對抗網路)

53
00:02:51,645 --> 00:02:54,085
所以您怎麼產生這種想法的?

54
00:02:54,085 --> 00:02:56,885
長久以來我一直在研究生成模型

55
00:02:56,885 --> 00:02:59,000
所以 GAN 是一種方式來做

56
00:02:59,000 --> 00:03:02,570
生成模型, 您有很多訓練資料您想要

57
00:03:02,570 --> 00:03:08,420
去學習去製造更多的資料來像是訓練資料， 但它們是想像的資料

58
00:03:08,420 --> 00:03:13,265
它們從未被用這種方式見過

59
00:03:13,265 --> 00:03:16,220
有好幾種其他方式來產生模型

60
00:03:16,220 --> 00:03:19,780
在我有 GAN 的主意之前好幾年

61
00:03:19,780 --> 00:03:24,860
在我使用所有這些方式在我的博士研究時

62
00:03:24,860 --> 00:03:29,000
我知道所有其他架構像是

63
00:03:29,000 --> 00:03:32,630
玻爾茲曼機跟稀疏編碼的優缺點

64
00:03:32,630 --> 00:03:35,955
跟所有其他已經好幾年很熱門的方向

65
00:03:35,955 --> 00:03:40,265
我在尋找一種方式來一次避免所有的缺點

66
00:03:40,265 --> 00:03:44,110
最後, 當我在酒吧跟我的朋友爭論有關生成模型的時候

67
00:03:44,110 --> 00:03:45,845
一些事情被點到了

68
00:03:45,845 --> 00:03:47,540
我開始談到您要做這個

69
00:03:47,540 --> 00:03:49,510
這個跟這個然後我發誓這可行

70
00:03:49,510 --> 00:03:52,890
我的朋友不相信這樣可行

71
00:03:52,890 --> 00:03:55,410
我應該是要寫深度學習教科書在那個時候

72
00:03:55,410 --> 00:03:55,790
瞭解

73
00:03:55,790 --> 00:03:57,620
但我強烈的相信這個可行

74
00:03:57,620 --> 00:03:59,870
我回到家, 在同一晚寫完程式, 而它的確可行

75
00:03:59,870 --> 00:04:02,920
所以您花了一個晚上來建置第一個 GAN ?

76
00:04:02,920 --> 00:04:06,050
我在半夜建置它

77
00:04:06,050 --> 00:04:09,530
在我從酒吧回來那一晚, 我的朋友的離別聚會

78
00:04:09,530 --> 00:04:10,086
瞭解

79
00:04:10,086 --> 00:04:11,784
而第一個版本就成功

80
00:04:11,784 --> 00:04:13,275
真的很幸運

81
00:04:13,275 --> 00:04:15,825
我並不用去搜尋超參數或是其他的

82
00:04:15,825 --> 00:04:17,840
在某個地方我讀過一個故事說

83
00:04:17,840 --> 00:04:21,851
您有一次瀕死經驗讓您更堅定承諾於 AI

84
00:04:21,851 --> 00:04:24,160
告訴我們這個故事

85
00:04:24,160 --> 00:04:30,215
是, 我實際上並不是瀕死而是我覺得如此

86
00:04:30,215 --> 00:04:33,170
我有很嚴重的頭痛, 一些

87
00:04:33,170 --> 00:04:37,571
醫生認為我可能腦內出血

88
00:04:37,571 --> 00:04:39,740
就在我等著

89
00:04:39,740 --> 00:04:43,180
我的 MRI 報告是否有腦內出血的結果時

90
00:04:43,180 --> 00:04:47,810
我發現我大部分的想法是在

91
00:04:47,810 --> 00:04:49,910
確認其他人最終會

92
00:04:49,910 --> 00:04:52,750
嘗試我那時候的的研究觀點

93
00:04:52,750 --> 00:04:53,224
瞭解

94
00:04:53,224 --> 00:04:55,820
現在回想起來, 其實是很傻的研究觀點

95
00:04:55,820 --> 00:04:56,553
瞭解

96
00:04:56,553 --> 00:04:58,700
但在那時候

97
00:04:58,700 --> 00:05:02,325
我覺得我人生最重要的優先事項

98
00:05:02,325 --> 00:05:05,780
是完成我機器學習的研究工作

99
00:05:05,780 --> 00:05:07,910
是的, 真棒

100
00:05:07,910 --> 00:05:10,055
當您覺得您可能很快死掉

101
00:05:10,055 --> 00:05:12,265
您只是想如何完成您的研究

102
00:05:12,265 --> 00:05:12,649
是的.是的.

103
00:05:12,649 --> 00:05:15,690
是的。這是承諾

104
00:05:15,690 --> 00:05:17,850
是的.是的.

105
00:05:17,850 --> 00:05:21,808
在今天, 您依然是很多 GAN 活動的核心

106
00:05:21,808 --> 00:05:24,560
也就是生成對抗網路

107
00:05:24,560 --> 00:05:27,710
告訴我們您如何看 GAN 的未來

108
00:05:27,710 --> 00:05:32,930
現在, GAN 被用在很多不同的東西上, 像是半監督式學習

109
00:05:32,930 --> 00:05:39,185
為其他模型產生訓練資料甚至模擬科學實驗

110
00:05:39,185 --> 00:05:43,850
理論上, 所有這些東西可以用其他生成模型產生

111
00:05:43,850 --> 00:05:47,695
我想 GAN 在一個很重要的十字路口上

112
00:05:47,695 --> 00:05:50,210
現在, 它們有時候作用得很好

113
00:05:50,210 --> 00:05:55,890
但這可能是一種藝術而不是科學來真的讓它表現

114
00:05:55,890 --> 00:05:59,870
它有點像深度學習在十年前

115
00:05:59,870 --> 00:06:01,430
那時候, 我們使用

116
00:06:01,430 --> 00:06:05,330
深度信念網路像是玻爾茲曼機當作建置礎石

117
00:06:05,330 --> 00:06:07,420
而他們是很挑剔的

118
00:06:07,420 --> 00:06:11,945
隨著時間推移, 我們換成像線性整流函數跟批次正常化

119
00:06:11,945 --> 00:06:14,635
而深度學習變得更可靠

120
00:06:14,635 --> 00:06:18,470
如果我們可以讓 GAN 變得像深度學習一樣可靠

121
00:06:18,470 --> 00:06:20,840
我想我們會看到 GAN 用在

122
00:06:20,840 --> 00:06:24,110
所有今天用到的地方而更加成功

123
00:06:24,110 --> 00:06:29,060
如果我們無法找出如何穩定 GAN 的方式

124
00:06:29,060 --> 00:06:32,960
我想它對深度學習歷史的貢獻

125
00:06:32,960 --> 00:06:35,060
是它會告訴人們如何做

126
00:06:35,060 --> 00:06:37,590
所有這些事情涉及生成模型

127
00:06:37,590 --> 00:06:41,505
最後我們會用其他種生成模型來取代它

128
00:06:41,505 --> 00:06:47,870
所以我也許花大約 40% 的時間在試著穩定 GAN

129
00:06:47,870 --> 00:06:50,780
瞭解, 很酷, 很多的人們

130
00:06:50,780 --> 00:06:53,765
參與深度學習已經 10 年了, 就像您

131
00:06:53,765 --> 00:06:54,963
變成是先鋒

132
00:06:54,963 --> 00:06:57,360
也許人們今天參與 GAN

133
00:06:57,360 --> 00:07:00,120
如果成功了, 就會是先鋒

134
00:07:00,120 --> 00:07:04,220
有很多人已經是 GAN 的先鋒

135
00:07:04,220 --> 00:07:09,105
我想如果您要瞭解目前為止 GAN 的歷史

136
00:07:09,105 --> 00:07:12,740
您真的需要提及其他團體像是 Indico

137
00:07:12,740 --> 00:07:17,280
跟臉書及柏克萊對於他們做過的不同事情

138
00:07:17,280 --> 00:07:19,735
除了您做的研究外

139
00:07:19,735 --> 00:07:24,300
您也合著了一本深度學習的書， 情況如何?

140
00:07:24,300 --> 00:07:26,897
是的, 跟 Yohua Bengio 及 Aaron Courville

141
00:07:26,897 --> 00:07:29,900
是我的博士聯合顧問

142
00:07:29,900 --> 00:07:35,465
我們寫了第一本現代版本深度學習的教科書

143
00:07:35,465 --> 00:07:38,615
一直很受歡迎

144
00:07:38,615 --> 00:07:42,920
無論是在英文版和中文版

145
00:07:42,920 --> 00:07:48,915
我想我們兩個語言加起來賣了約 7萬本

146
00:07:48,915 --> 00:07:54,730
有很多的學生反應他們從中學習了很多

147
00:07:54,730 --> 00:07:58,940
我們跟其他書的不同點是我們是從

148
00:07:58,940 --> 00:08:03,905
很注重在您需要在深度學習用得到的數學開始

149
00:08:03,905 --> 00:08:07,670
我想從您在史丹佛的課程學到的是

150
00:08:07,670 --> 00:08:11,570
線性代數跟機率是很重要的

151
00:08:11,570 --> 00:08:15,230
人們對於機器學習演算法很興奮

152
00:08:15,230 --> 00:08:18,500
但如果您要變成真正優秀的從業者

153
00:08:18,500 --> 00:08:26,055
您首先需要掌握基本的數學基礎這是整個方法的基石

154
00:08:26,055 --> 00:08:27,290
我們確定先給予

155
00:08:27,290 --> 00:08:31,345
很聚集的數學基礎在這本書的開端

156
00:08:31,345 --> 00:08:34,153
這樣,您不需要再去學整個線性代數

157
00:08:34,153 --> 00:08:35,900
您可以獲得

158
00:08:35,900 --> 00:08:37,770
很快的速成課程

159
00:08:37,770 --> 00:08:40,540
對於深度學習最有用的線性代數

160
00:08:40,540 --> 00:08:44,660
所以即使有人的數學基礎不太穩固，或好幾年沒碰過數學

161
00:08:44,660 --> 00:08:47,000
也是可以從您的書開始

162
00:08:47,000 --> 00:08:49,790
得到一些背景知識來進入深度學習?

163
00:08:49,790 --> 00:08:52,175
所有您需要知道的事實都在書中

164
00:08:52,175 --> 00:08:59,520
肯定需要花一些功夫來練習它們

165
00:08:59,520 --> 00:08:59,684
是的.是的. 是的.是的. 太棒了

166
00:08:59,684 --> 00:09:01,370
如果有人真的很怕數學

167
00:09:01,370 --> 00:09:03,700
這可能會是有點痛苦的經驗

168
00:09:03,700 --> 00:09:08,323
但如果您準備好來學經驗且您相信您可以掌握它

169
00:09:08,323 --> 00:09:11,360
我想您所需要的工具都在那裡

170
00:09:11,360 --> 00:09:15,470
當作一位長期在深度學習工作的人

171
00:09:15,470 --> 00:09:18,710
我很好奇, 如果您回頭看這些年

172
00:09:18,710 --> 00:09:21,050
告訴我們您如何看

173
00:09:21,050 --> 00:09:24,650
AI 跟深度學習這些年的演進

174
00:09:24,650 --> 00:09:28,595
十年前，我覺得這個社群

175
00:09:28,595 --> 00:09:31,580
最大的挑戰來自於如何讓

176
00:09:31,580 --> 00:09:34,715
機器學習或者 AI 真的可行

177
00:09:34,715 --> 00:09:39,440
我們有很棒的工具用在簡單任務上

178
00:09:39,440 --> 00:09:44,555
我們想在識別模式中如何萃取特徵

179
00:09:44,555 --> 00:09:47,000
設計這些的人們可以做很多

180
00:09:47,000 --> 00:09:51,965
工作來建立這些特徵然後交給電腦跑

181
00:09:51,965 --> 00:09:54,170
這是很棒的對於不同的事情

182
00:09:54,170 --> 00:09:56,750
像是當使用者點擊時的預測

183
00:09:56,750 --> 00:10:01,895
或者不同的基本科學分析

184
00:10:01,895 --> 00:10:07,505
但我們很難去做一些牽涉到百萬像素的圖像或者

185
00:10:07,505 --> 00:10:10,150
原始音訊波型這些

186
00:10:10,150 --> 00:10:13,950
系統需要從頭開始來建立它的理解

187
00:10:13,950 --> 00:10:18,880
我們也許在五年前終於跨越了一些障礙

188
00:10:18,880 --> 00:10:22,180
而現在, 我們在這一點上是

189
00:10:22,180 --> 00:10:26,268
有很多不同的路徑開放給想要進入 AI 的人

190
00:10:26,268 --> 00:10:31,060
也許他們面對最難的問題是他們要走哪條路

191
00:10:31,060 --> 00:10:35,500
您要做強化學習還是監督式學習?

192
00:10:35,500 --> 00:10:40,410
您要做非監督式學習或是監督式學習?

193
00:10:40,410 --> 00:10:44,333
您要確認機器學習演算法是公平的且

194
00:10:44,333 --> 00:10:48,460
不反應我們希望避免的偏差?

195
00:10:48,460 --> 00:10:54,565
您要確認 AI 圍繞的社會議題是好的

196
00:10:54,565 --> 00:10:58,535
就是確認 AI 對每個人都有益

197
00:10:58,535 --> 00:11:03,440
而不是引起社會動盪跟失業的問題?

198
00:11:03,440 --> 00:11:04,600
我認為現在

199
00:11:04,600 --> 00:11:08,025
真的有很驚人數量的不同事情可以做

200
00:11:08,025 --> 00:11:11,380
既要防止 AI 的負面影響，還要確保

201
00:11:11,380 --> 00:11:14,965
我們利用它帶給我們所有的好處

202
00:11:14,965 --> 00:11:19,800
所以在今天很多人想要進入 AI

203
00:11:19,800 --> 00:11:23,285
您會給這些人什麼建議?

204
00:11:23,285 --> 00:11:26,950
我想很多想進入 AI的人想說

205
00:11:26,950 --> 00:11:32,200
他們絕對需要博士學位或者類似的憑證

206
00:11:32,200 --> 00:11:35,155
我不覺得這是絕對必要的

207
00:11:35,155 --> 00:11:40,285
您可以得到關注的一種方式是寫一些好程式把他們放在 GitHub 上

208
00:11:40,285 --> 00:11:43,380
如果您有一個有趣的專案解決了

209
00:11:43,380 --> 00:11:47,320
一些在上層工作的人想要解決的問題

210
00:11:47,320 --> 00:11:49,840
當他們發現您在 GitHub 的資料庫

211
00:11:49,840 --> 00:11:53,450
他們會找到您邀請您去幫他們工作

212
00:11:53,450 --> 00:11:56,140
我雇用了很多人

213
00:11:56,140 --> 00:12:00,010
去年在 OpenAI , 今年在 Google

214
00:12:00,010 --> 00:12:02,755
我有興趣跟他們一起工作因為

215
00:12:02,755 --> 00:12:06,895
我看到了他們在開源論壇發佈的東西

216
00:12:06,895 --> 00:12:11,275
撰寫論文然後將它們放在 Archive 也是很好

217
00:12:11,275 --> 00:12:12,745
很多時候

218
00:12:12,745 --> 00:12:16,750
比較難達到您有閃亮到足以

219
00:12:16,750 --> 00:12:20,860
對於科學文獻來講新的學術貢獻

220
00:12:20,860 --> 00:12:27,885
但通常您可以早一點做到一些有用的軟體

221
00:12:27,885 --> 00:12:30,022
所以讀您們的書

222
00:12:30,022 --> 00:12:33,930
練習一些東西放在 GitHub 上或者也許 Archive 上

223
00:12:33,930 --> 00:12:36,100
我想如果您用讀書的方式來學習

224
00:12:36,100 --> 00:12:39,454
同時做一個專案也是很重要的

225
00:12:39,454 --> 00:12:42,730
也許選擇一些

226
00:12:42,730 --> 00:12:46,555
您已經感興趣的領域應用機器學習

227
00:12:46,555 --> 00:12:50,500
像是如果您是野外生物學家您像進入深度學習

228
00:12:50,500 --> 00:12:53,255
也許您可以用來辨識鳥類

229
00:12:53,255 --> 00:12:56,905
如果在您的生活中您沒有任何應用機器學習的想法

230
00:12:56,905 --> 00:13:01,600
您可以選擇像是街道住址號碼分類器

231
00:13:01,600 --> 00:13:05,580
所有個資料集已經都設置了讓您用起來很直接

232
00:13:05,580 --> 00:13:07,330
這樣, 您可以練習所有

233
00:13:07,330 --> 00:13:09,700
基本技巧當您讀這本書的時候或者當

234
00:13:09,700 --> 00:13:14,105
您看 Coursera 影片在解釋一些概念時

235
00:13:14,105 --> 00:13:15,670
在過去幾年

236
00:13:15,670 --> 00:13:20,045
我看到您也做一些工作在對抗例子上

237
00:13:20,045 --> 00:13:21,535
告訴我們吧

238
00:13:21,535 --> 00:13:24,490
我想對抗例子是

239
00:13:24,490 --> 00:13:29,835
新興的領域我稱為機器學習安全

240
00:13:29,835 --> 00:13:33,250
在過去, 我們看到電腦安全問題

241
00:13:33,250 --> 00:13:38,275
攻擊者可以騙電腦來跑一些錯誤的程式

242
00:13:38,275 --> 00:13:40,890
這稱為應用程式級安全

243
00:13:40,890 --> 00:13:46,300
然後有一種攻擊是人們可以騙電腦相信

244
00:13:46,300 --> 00:13:52,545
網路的訊息是從一個人其實並不是他們宣稱的那個人來的

245
00:13:52,545 --> 00:13:55,025
這是稱為網路級安全

246
00:13:55,025 --> 00:13:57,230
我們現在開始看到您可以騙

247
00:13:57,230 --> 00:13:59,920
機器學習演算法做他們不該做的事

248
00:13:59,920 --> 00:14:06,010
即使跑在機器學習演算法的程式是正確的程式

249
00:14:06,010 --> 00:14:07,960
即使跑在

250
00:14:07,960 --> 00:14:10,025
機器學習的程式認識

251
00:14:10,025 --> 00:14:13,605
真的從網路來的訊息的來源

252
00:14:13,605 --> 00:14:17,050
我想建立安全性在

253
00:14:17,050 --> 00:14:20,830
於新的科技在開發階段的一開始是很重要的

254
00:14:20,830 --> 00:14:27,065
我們發現先建立一個可行的系統以後再加上安全管制很難

255
00:14:27,065 --> 00:14:30,640
我對於這個觀念很興奮如果

256
00:14:30,640 --> 00:14:34,705
我們現在栽進去開始參與機器學習的安全性

257
00:14:34,705 --> 00:14:37,600
我們可以確定這些演算法從頭開始是安全

258
00:14:37,600 --> 00:14:41,650
與其過幾年後回溯再來修補它

259
00:14:41,650 --> 00:14:43,111
謝謝, 真棒

260
00:14:43,111 --> 00:14:46,090
有很多很迷人的您個人的故事

261
00:14:46,090 --> 00:14:47,470
儘管已經認識您好幾年

262
00:14:47,470 --> 00:14:49,935
我真的不知道, 謝謝您跟我們分享

263
00:14:49,935 --> 00:14:53,090
不客氣, 謝謝您邀請我, 真的很棒

264
00:14:53,090 --> 00:14:53,630
Okay?
Thank you.謝謝.

265
00:14:53,630 --> 00:14:55,010
不客氣