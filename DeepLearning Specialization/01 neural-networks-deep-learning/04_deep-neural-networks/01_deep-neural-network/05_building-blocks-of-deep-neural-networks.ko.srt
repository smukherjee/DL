1
00:00:00,000 --> 00:00:02,705
지난 몇 주 간 시청하신 강의와

2
00:00:02,705 --> 00:00:05,735
이번주 초에 시청하신 강의에서

3
00:00:05,735 --> 00:00:08,075
deep neural network를 구현할 때 필요한

4
00:00:08,075 --> 00:00:10,456
핵심 요소인 순전파와 역전파의

5
00:00:10,456 --> 00:00:14,090
기본 토대는 이미 배우셨습니다.

6
00:00:14,090 --> 00:00:18,303
이제 이러한 컴포넌트들을 어떻게 하나로 합쳐 <br />deep network를 구성하는지 보시게 될껍니다

7
00:00:18,303 --> 00:00:20,333
여기 몇 개의 레이어로 이루어진 네트워크가 있습니다

8
00:00:20,333 --> 00:00:27,290
이제 레이어 하나를 선택하여 거기서 일어나는 연산에 집중해봅시다

9
00:00:27,290 --> 00:00:33,410
레이어 L에는 WL과 BL같은 파라미터들이 있습니다

10
00:00:33,410 --> 00:00:35,645
그리고 순전파가 일어나면

11
00:00:35,645 --> 00:00:42,180
이전 레이어의 활성화 함수 출력인

12
00:00:42,180 --> 00:00:48,895
AL-1를 입력으로 하여 AL을 출력하게 됩니다

13
00:00:48,895 --> 00:01:00,105
지난 강의에서 저희는 ZL=WLxAL-1+BL를 계산한 후,

14
00:01:00,105 --> 00:01:08,025
AL=G(ZL)를 계산했습니다

15
00:01:08,025 --> 00:01:12,990
바로 이런 방법으로 입력 AL-1으로부터 출력 AL이 나오게 됩니다

16
00:01:12,990 --> 00:01:15,330
그리고 이 과정에서 계산된 ZL의 값도 캐싱해두면

17
00:01:15,330 --> 00:01:20,640
나중에 재사용이 필요할 때 매우 유용합니다

18
00:01:20,640 --> 00:01:24,060
그러니 이 캐시도 여기 포함하겠습니다

19
00:01:24,060 --> 00:01:28,170
왜냐하면 이렇게 저장해둔 ZL 값을

20
00:01:28,170 --> 00:01:31,290
나중에 역전파에서 유용하게 써먹을 수 있거든요

21
00:01:31,290 --> 00:01:32,745
backward 단계에서는,

22
00:01:32,745 --> 00:01:34,785
그러니까 역전파 단계에서는

23
00:01:34,785 --> 00:01:37,390
다시 같은 레이어 L에 집중해서

24
00:01:37,390 --> 00:01:41,635
dAL을 입력으로 받아 dAL-1을 출력하는

25
00:01:41,635 --> 00:01:51,840
함수를 구현하게 됩니다

26
00:01:51,840 --> 00:01:53,650
좀더 구체화해보면

27
00:01:53,650 --> 00:01:56,050
입력은 dAL과

28
00:01:56,050 --> 00:01:59,130
미리 계산해 저장해두었던

29
00:01:59,130 --> 00:02:04,040
ZL의 값이고,

30
00:02:04,040 --> 00:02:06,755
출력으로는 dAL-1뿐만 아니라

31
00:02:06,755 --> 00:02:09,940
경사 하강법 구현을 위해 필요한

32
00:02:09,940 --> 00:02:14,025
기울기도 올 수 있습니다

33
00:02:14,025 --> 00:02:19,772
이것이 우리가 forward function라고 부르는 순전파 단계와

34
00:02:19,772 --> 00:02:22,680
backward function이라고 부르는 역전파 단계를 구현하는

35
00:02:22,680 --> 00:02:24,970
기본 구조입니다

36
00:02:24,970 --> 00:02:27,535
요약하면 레이어 L에서는

37
00:02:27,535 --> 00:02:32,650
AL-1을 입력으로 받아 AL을 출력하는

38
00:02:32,650 --> 00:02:38,986
순전파 함수가 있는데,

39
00:02:38,986 --> 00:02:42,070
이 때 계산을 위해

40
00:02:42,070 --> 00:02:45,650
WL과 BL이 필요하고

41
00:02:45,650 --> 00:02:54,185
출력은 캐싱되는 ZL값입니다.

42
00:02:54,185 --> 00:02:56,357
그리고 역전파 함수에서는

43
00:02:56,357 --> 00:03:01,650
dAL을 입력으로 받아

44
00:03:01,650 --> 00:03:08,065
dAL-1을 출력합니다

45
00:03:08,065 --> 00:03:13,820
이 함수는 현재 레이어의 활성화 함수에 대한 미분 dAL이 주어졌을 때,

46
00:03:13,820 --> 00:03:16,950
이전 레이어의 미분은 어떻게 되는지,

47
00:03:16,950 --> 00:03:20,400
즉, AL-1을 얼마나 변화시키길 원하는지 말해줍니다.

48
00:03:20,400 --> 00:03:24,590
결국 이전 레이어의 활성화 함수에 대한 미분 dAL-1을 계산하는 것이죠

49
00:03:24,590 --> 00:03:29,870
이 박스 안에서도 WL과 BL이 필요합니다

50
00:03:29,870 --> 00:03:31,200
그리고 따라가다보면 결국

51
00:03:31,200 --> 00:03:34,144
dZL을 계산하게 됩니다

52
00:03:34,144 --> 00:03:36,400
그리고 역전파 함수를 나타내는 이 박스는

53
00:03:36,400 --> 00:03:43,515
dWL과 dBL도 출력합니다

54
00:03:43,515 --> 00:03:47,740
그런데 전 역전파를 나타내기 위해 때때로 빨간 화살표를 사용합니다

55
00:03:47,740 --> 00:03:51,668
그러니 여러분도 원하시면 이 화살표들을 빨간색으로 칠하세요

56
00:03:51,668 --> 00:03:55,305
어려분이 이 두 함수를 구현할 수 있다면,

57
00:03:55,305 --> 00:03:59,390
신경망의 가본 계산은 다음과 같을껍니다

58
00:03:59,390 --> 00:04:02,453
입력으로 feature A0를 넣으면,

59
00:04:02,453 --> 00:04:07,710
첫번째 레이어의 활성화 함수 값이 계산됩니다

60
00:04:07,710 --> 00:04:09,624
이걸 A1이라 부르죠

61
00:04:09,624 --> 00:04:14,535
이 계산을 위해 W1과 B1이 필요했고,

62
00:04:14,535 --> 00:04:21,090
Z1을 캐싱해둘껍니다

63
00:04:21,090 --> 00:04:22,475
그러면 이제 이것들을

64
00:04:22,475 --> 00:04:25,290
두번째 레이어에 집어넣습니다

65
00:04:25,290 --> 00:04:28,026
그리고 W2와 B2를 사용하여

66
00:04:28,026 --> 00:04:32,355
이 레이어의 활성화 함수 값인 A2를 계산합니다

67
00:04:32,355 --> 00:04:36,475
이런 식으로 계속하다보면 결국

68
00:04:36,475 --> 00:04:40,065
AL을 출력하게 되는데

69
00:04:40,065 --> 00:04:42,655
이건 Y_hat과 같습니다

70
00:04:42,655 --> 00:04:51,595
그리고 이 과정에서 모든 Z 값들을 캐싱해두었습니다

71
00:04:51,595 --> 00:04:55,161
이게 순전파 단계입니다

72
00:04:55,161 --> 00:04:57,585
이제 역전파 단계에서는

73
00:04:57,585 --> 00:05:03,220
역으로 거슬러올라가면서

74
00:05:03,220 --> 00:05:12,200
기울기들을 계산하게 됩니다

75
00:05:12,200 --> 00:05:17,350
dAL을 집어넣으면

76
00:05:17,350 --> 00:05:21,674
이 상자는 dAL-1을 주고

77
00:05:21,674 --> 00:05:30,424
이런 식으로 계속하다보면 종국에는 dA2와 dA1를 얻을 수 있습니다

78
00:05:30,424 --> 00:05:35,500
사실 dA0도 얻을 수 있긴 한데,

79
00:05:35,500 --> 00:05:40,250
이건 입력 feature에 대한 미분이므로,

80
00:05:40,250 --> 00:05:46,237
지도 신경망의 가중치 훈련에는 별 쓸모가 없습니다

81
00:05:46,237 --> 00:05:48,160
그러니 여기서 멈춰도 됩니다

82
00:05:48,160 --> 00:05:54,365
이 과정에서 역전파는 dWL과 dBL도 출력하게 됩니다

83
00:05:54,365 --> 00:05:58,800
파라미터 WL과 BL만 사용해서요

84
00:05:58,800 --> 00:06:04,810
이런 식으로 dW3와

85
00:06:04,810 --> 00:06:09,238
dB3 등을 출력합니다

86
00:06:09,238 --> 00:06:16,660
결국 여러분이 필요한 모든 미분값을 얻게 되는 것이죠

87
00:06:16,660 --> 00:06:21,235
이 구조에 내용을 좀 더 채우면

88
00:06:21,235 --> 00:06:28,500
이 상자들도 WL과

89
00:06:28,500 --> 00:06:34,560
BL을 사용하게 될껍니다. 그리고 나중에 보겠지만,

90
00:06:34,560 --> 00:06:37,460
이 상자 안에서 dZL도 계산됩니다

91
00:06:37,460 --> 00:06:43,305
신경망 학습의 한 이터레이션은 A0인 X로 시작하여

92
00:06:43,305 --> 00:06:46,725
다음과 같이 순전파 과정을 거쳐서

93
00:06:46,725 --> 00:06:50,715
Y_hat을 계산합니다. 그리고 이 값을 사용하여

94
00:06:50,715 --> 00:06:55,680
역전파를 합니다

95
00:06:55,680 --> 00:06:59,880
이제 필요한 모든 미분값들이 있으니

96
00:06:59,880 --> 00:07:08,830
각 레이어에서 W는 기존값에서 학습률과 dW의 곱을 뺀 값이 될껍니다

97
00:07:08,830 --> 00:07:12,570
마찬가지로 b도

98
00:07:12,570 --> 00:07:17,757
역전파를 통해 필요한 모든 미분값을 얻어 업데이트됩니다

99
00:07:17,757 --> 00:07:21,845
여기까지가 신경망 경사 하강법의 한 이터레이션입니다

100
00:07:21,845 --> 00:07:25,125
진도를 더 나가기 전에 구현 세부사항을 하나 더 살펴보죠

101
00:07:25,125 --> 00:07:29,665
개념적으로는 여기서 캐시를 역방향 함수에서 쓰기 위해

102
00:07:29,665 --> 00:07:34,361
Z 값을 저장해두는 것으로 생각하면 유용할껍니다

103
00:07:34,361 --> 00:07:35,635
하지만 이전 연습 문제에서 보신 것처럼

104
00:07:35,635 --> 00:07:38,496
이걸 구현할 때 캐시는

105
00:07:38,496 --> 00:07:40,840
파라미터 W1과

106
00:07:40,840 --> 00:07:43,730
B1을 역방향 함수에

107
00:07:43,730 --> 00:07:46,648
전달하는 편리한 방법이기도 합니다

108
00:07:46,648 --> 00:07:47,868
이전 연습문제에서,

109
00:07:47,868 --> 00:07:49,795
실제로 캐시에 Z를 저장하고,

110
00:07:49,795 --> 00:07:52,239
W와 B도 저장합니다

111
00:07:52,239 --> 00:07:57,715
따라서 그냥 Z2, W2, B2를 저장하는거죠

112
00:07:57,715 --> 00:07:59,860
구현 관점에서 보면,

113
00:07:59,860 --> 00:08:04,120
이건 파라미터들을 복사하여

114
00:08:04,120 --> 00:08:08,975
나중에 역전파 계산에서 사용할 수 있도록 하는 편리한 방법입니다

115
00:08:08,975 --> 00:08:15,446
이건 그냥 프로그래밍 연습문제에서 보게 될 구현 세부사항입니다

116
00:08:15,446 --> 00:08:20,061
지금까지 심층 신경망을 구현할 때 필요한 기본 구성 요소에 대해 살펴보았습니다

117
00:08:20,061 --> 00:08:21,640
각 레이어에는 순전파 단계가 있고

118
00:08:21,640 --> 00:08:24,260
대응하는 역전파 단계가 있으며,

119
00:08:24,260 --> 00:08:27,675
한 쪽에서 다른 쪽으로 정보를 복사하는 캐시가 있습니다

120
00:08:27,675 --> 00:08:28,930
다음 비디오에서는

121
00:08:28,930 --> 00:08:32,250
이제 이 구성 요소들을 어떻게 실제로 구현하는지 살펴보겠습니다

122
00:08:32,250 --> 00:08:33,500
다음 강의를 보시죠