1
00:00:00,000 --> 00:00:01,440
在上一节课中

2
00:00:01,440 --> 00:00:05,700
你已经看到如何通过向量化计算预测

3
00:00:05,700 --> 00:00:11,485
同时计算出整个训练集的激活值a

4
00:00:11,485 --> 00:00:15,030
在这个视频中你将看到如何使用向量化

5
00:00:15,030 --> 00:00:19,205
计算全部m个训练样本的梯度

6
00:00:19,205 --> 00:00:21,380
强调一下 是同时计算

7
00:00:21,380 --> 00:00:22,890
在本视频的末尾

8
00:00:22,890 --> 00:00:26,175
我们会将之前讲的结合起来

9
00:00:26,175 --> 00:00:29,730
展示如何非常高效地实现逻辑回归

10
00:00:29,730 --> 00:00:32,505
你也许记得在讲梯度计算时

11
00:00:32,505 --> 00:00:36,910
计算第一个样本的dz_(1)的步骤

12
00:00:36,910 --> 00:00:43,870
即dz^(1)=a^(1)-y^(1)

13
00:00:43,870 --> 00:00:52,134
dz^(2)=a^(2)-y^(2) 以此类推

14
00:00:52,134 --> 00:00:56,425
如此对所有m个训练样本进行同样的计算

15
00:00:56,425 --> 00:01:01,218
然后我们可以定义一个新变量

16
00:01:01,218 --> 00:01:08,595
dZ=[dz^(1) dz^(2) ... dz^(m)]

17
00:01:08,595 --> 00:01:13,910
注意 所有dz横向排列

18
00:01:13,910 --> 00:01:21,200
所以这是一个1乘m的矩阵
也可以说是一个m维行向量

19
00:01:21,200 --> 00:01:23,520
回想一下在之前的幻灯片中

20
00:01:23,520 --> 00:01:28,405
我们已经了解了如何计算A 即A=[a^(1)...a^(m)]

21
00:01:28,405 --> 00:01:36,735
我们也已经定义了Y 即Y=[y^(1) ... y^(m)]

22
00:01:36,735 --> 00:01:39,200
它们也是横向排列的

23
00:01:39,200 --> 00:01:42,780
那么基于这些定义

24
00:01:42,780 --> 00:01:46,770
你也许会发现 我们可以这样计算dZ

25
00:01:46,770 --> 00:01:52,750
dZ=A-Y 这等同于

26
00:01:52,750 --> 00:01:55,670
dZ=[a^(1)-y^(1), a^(2)-y^(2) ...]

27
00:01:55,670 --> 00:01:59,980
以此类推

28
00:01:59,980 --> 00:02:06,115
这里的第一个元素a^(1)-y^(1)就是dz^(1)

29
00:02:06,115 --> 00:02:11,670
第二个元素就是dz^(2) 等等

30
00:02:11,670 --> 00:02:13,965
所以仅需要一行代码

31
00:02:13,965 --> 00:02:20,095
你就可以同时完成这所有的计算

32
00:02:20,095 --> 00:02:24,010
在之前的实现中

33
00:02:24,010 --> 00:02:27,695
我们已经去掉了一个for循环 但是

34
00:02:27,695 --> 00:02:31,600
我们仍然有一个遍历训练集的循环

35
00:02:31,600 --> 00:02:35,440
我们将dw初始化为0向量

36
00:02:35,440 --> 00:02:38,905
但是我们还是需要遍历训练样本

37
00:02:38,905 --> 00:02:43,015
对第一个训练样本 计算dw+=x^(1)*dz^(1)

38
00:02:43,015 --> 00:02:50,440
对第二个训练样本 计算dw+=x^(2)*dz^(2) 等等

39
00:02:50,440 --> 00:02:56,980
我们重复m次 接着计算dw/=m 对于b也类似

40
00:02:56,980 --> 00:03:03,370
db被初始化为0向量 然后db+=dz^(1)

41
00:03:03,370 --> 00:03:09,120
db+=dz^(2) 这样重复到dz^(m)

42
00:03:09,120 --> 00:03:16,835
接着计算db/=m 这就是我们在之前的实现中做的

43
00:03:16,835 --> 00:03:18,700
我们已经去掉了一个for循环

44
00:03:18,700 --> 00:03:25,045
至少现在dw是个向量了 然后我们分别

45
00:03:25,045 --> 00:03:26,850
更新dw_1和dw_2等等

46
00:03:26,850 --> 00:03:29,860
我们去掉了这个for循环

47
00:03:29,860 --> 00:03:33,630
但还有个for循环用于遍历训练集中的m个样本

48
00:03:33,630 --> 00:03:36,290
让我们通过如下的操作把它们向量化

49
00:03:36,290 --> 00:03:38,380
我们要做的是

50
00:03:38,380 --> 00:03:42,745
对于计算db的向量化的实现

51
00:03:42,745 --> 00:03:47,940
只需要对这些dz求和 然后除以m

52
00:03:47,940 --> 00:03:51,580
即db等于1/m乘以

53
00:03:51,580 --> 00:03:56,530
dz^(i)到dz^(m)的和

54
00:03:56,530 --> 00:04:03,055
所有的dz组成了一个行向量

55
00:04:03,055 --> 00:04:04,765
在Python中

56
00:04:04,765 --> 00:04:08,155
你只需要使用

57
00:04:08,155 --> 00:04:12,210
1/m*np.sum(dZ)

58
00:04:12,210 --> 00:04:15,115
你只要把这个变量传给np.sum()函数

59
00:04:15,115 --> 00:04:19,195
就可以得到db

60
00:04:19,195 --> 00:04:22,330
那么dw呢

61
00:04:22,330 --> 00:04:26,375
这里先给出正确的公式
以确保我们做的是正确的

62
00:04:26,375 --> 00:04:28,164
dw的公式就是

63
00:04:28,164 --> 00:04:34,485
dw=1/m*X*dZ^T

64
00:04:34,485 --> 00:04:37,990
让我们看看这样写的原因

65
00:04:37,990 --> 00:04:41,806
这等于1/m乘以X矩阵

66
00:04:41,806 --> 00:04:48,325
X矩阵是由x^(1)到x^(m)按列堆叠起来的

67
00:04:48,325 --> 00:04:56,040
dZ的转置是从dz^(1)到dz^(m)

68
00:04:56,040 --> 00:05:00,900
如果你清楚这个矩阵乘以这个向量的结果

69
00:05:00,900 --> 00:05:05,585
这等于

70
00:05:05,585 --> 00:05:12,523
1/m*[x^(1)*dz^(1)+...+x^(m)*dz^(m)]

71
00:05:12,523 --> 00:05:21,405
这是一个n乘1的向量 也就是你得到的最终结果

72
00:05:21,405 --> 00:05:24,725
因为dw中包括了这些

73
00:05:24,725 --> 00:05:27,710
x^(i)*dz^(i) 然后把它们加起来

74
00:05:27,710 --> 00:05:32,300
这就是这个矩阵和向量的乘法所做的

75
00:05:32,300 --> 00:05:35,655
只要一行代码你就可以计算出dw

76
00:05:35,655 --> 00:05:40,010
计算导数的向量化实现方法如下

77
00:05:40,010 --> 00:05:44,540
你可以用这一行来实现db的计算

78
00:05:44,540 --> 00:05:50,540
用这一行来实现dw的计算
注意我们没有在训练集上使用for循环

79
00:05:50,540 --> 00:05:55,265
你现在就可以计算参数的更新了

80
00:05:55,265 --> 00:06:01,185
现在 我们总结一下
看看到底应该如何实现逻辑回归

81
00:06:01,185 --> 00:06:02,550
这是我们原来的

82
00:06:02,550 --> 00:06:07,866
非常低效 没有向量化的实现

83
00:06:07,866 --> 00:06:11,775
在前一个视频中 我们首先做的是去掉这一堆

84
00:06:11,775 --> 00:06:14,400
不用循环遍历

85
00:06:14,400 --> 00:06:15,755
dw_1 dw_2 等等

86
00:06:15,755 --> 00:06:23,595
我们把它替换为向量dw 即dw+=x^(i)*dz^(i)

87
00:06:23,595 --> 00:06:28,775
其中x^(i)是一个向量

88
00:06:28,775 --> 00:06:32,000
现在 我们不仅要摆脱下面的这个循环

89
00:06:32,000 --> 00:06:36,670
还要消灭掉这个for循环

90
00:06:36,670 --> 00:06:38,654
你要做的是

91
00:06:38,654 --> 00:06:42,925
通过前面幻灯片得到的结果

92
00:06:42,925 --> 00:06:46,085
你会得到

93
00:06:46,085 --> 00:06:57,625
Z=w^TX+b 写成代码就是Z=np.dot(w.T,X)+b

94
00:06:57,625 --> 00:07:07,315
然后 A=sigmoid(Z)

95
00:07:07,315 --> 00:07:12,710
你已经对所有i完成了这些计算

96
00:07:12,710 --> 00:07:14,715
在前面的幻灯片的下一步中

97
00:07:14,715 --> 00:07:21,070
我们知道你还应该计算dZ=A-Y

98
00:07:21,070 --> 00:07:24,460
就对所有i完成了这步计算

99
00:07:24,460 --> 00:07:31,495
最后我们得到 dw=1/m*XdZ^T

100
00:07:31,495 --> 00:07:39,700
db=1/m*np.sum(dZ)

101
00:07:39,700 --> 00:07:43,328
db=1/m*np.sum(dZ)

102
00:07:43,328 --> 00:07:49,120
至此你已经完成了前向传播和反向传播

103
00:07:49,120 --> 00:07:53,030
完成了对所有m个训练样本的预测和求导

104
00:07:53,030 --> 00:07:57,340
并且没有使用一个for循环

105
00:07:57,340 --> 00:08:00,835
然后更新梯度下降参数

106
00:08:00,835 --> 00:08:04,462
w的更新为w=w-alpha*dw

107
00:08:04,462 --> 00:08:12,020
b的更新为b=b-alpha*db 其中alpha是学习率

108
00:08:12,020 --> 00:08:17,341
有时候这里会有一个冒号 表示这是一个赋值操作

109
00:08:17,341 --> 00:08:21,675
但是我前后对这个符号的使用可能并不一致

110
00:08:21,675 --> 00:08:25,450
有了这些

111
00:08:25,450 --> 00:08:29,635
你就实现了逻辑回归梯度下降的一次迭代

112
00:08:29,635 --> 00:08:32,308
虽然我之前说过我们要尽量

113
00:08:32,308 --> 00:08:35,260
避免使用显式的for循环

114
00:08:35,260 --> 00:08:38,230
但如果要实现梯度下降的多次迭代

115
00:08:38,230 --> 00:08:42,880
那么你仍然需要使用for循环 去迭代指定的次数

116
00:08:42,880 --> 00:08:47,860
如果你想让梯度下降迭代1000次

117
00:08:47,860 --> 00:08:53,675
你可能还是需要一个for循环 来迭代指定的次数

118
00:08:53,675 --> 00:08:55,870
像这个最外面的for循环

119
00:08:55,870 --> 00:08:59,210
我认为没有办法能把它也去掉

120
00:08:59,210 --> 00:09:02,390
不过我还是觉得 不使用for循环

121
00:09:02,390 --> 00:09:07,117
就能实现梯度下降的一个迭代 这很cool

122
00:09:07,117 --> 00:09:09,880
现在你得到了一个高度向量化

123
00:09:09,880 --> 00:09:14,745
且非常高效的逻辑回归的梯度下降算法

124
00:09:14,745 --> 00:09:18,850
在下个视频中我还想讨论一些细节

125
00:09:18,850 --> 00:09:24,155
这里简单一提 这种技术称为广播

126
00:09:24,155 --> 00:09:28,240
广播是Python和Numpy提供的一种

127
00:09:28,240 --> 00:09:32,915
能够使特定代码更加高效的技术

128
00:09:32,915 --> 00:09:37,090
我们将在下个视频中学习关于广播的更多细节