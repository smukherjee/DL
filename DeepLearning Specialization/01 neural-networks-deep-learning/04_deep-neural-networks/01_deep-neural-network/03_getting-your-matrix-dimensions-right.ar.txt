عند تنفيذ شبكة عصبية عميقة، فإن إحدى
أدوات تصحيح الأخطاء التي غالبًا ما أستخدمها للتحقق من صحة تعليماتي البرمجية
تتمثل في إحضار ورقة، والعمل فقط من خلال الأبعاد
والمصفوفة التي أتعامل معها. إذًا، دعوني أريكم كيفية القيام بذلك،
لأنني آمل أن يُسهّل عليكم هذا الأمر تنفيذ شبكاتكم العميقة أيضًا. حرف L كبير = 5، مع العد
بسرعة، دون حساب طبقة الإدخال، توجد خمس طبقات هنا:
أربع طبقات مخفية وطبقة مخرجات واحدة. وبالتالي،
إذا نفذّت الانتشار الأمامي، فستكون الخطوة الأولى z1 = w1x + b1. إذًا دعونا نتجاهل شروط التحيز b في الوقت الحالي،
ونركز على المعلمات w. الآن، تحتوي هذه الطبقة المخفية الأولى على ثلاث
وحدات مخفية، لذا فإن هذه الطبقة هي الطبقة 0 والطبقة 1 والطبقة 2 والطبقة 3
والطبقة 4 والطبقة 5. باستخدام الترميز الذي حصلنا عليه من
الفيديو السابق، لدينا n1، وهو عدد الوحدات المخفية في الطبقة 1، يساوي 3. وهنا سيكون لدينا
n2 يساوي 5، وn3 يساوي 4، وn4 يساوي 2،
وn5 يساوي 1. وحتى الآن لم نشاهد سوى شبكات عصبية
ذات وحدة إخراج واحدة، ولكن في الدورات التدريبية اللاحقة، سوف نتحدث عن
شبكات عصبية ذات وحدات إخراج متعددة أيضًا. وأخيرًا، بالنسبة لطبقة الإدخال، لدينا أيضًا n0 = nx = 2. والآن، دعنا نفكر في
أبعاد z وw وx. z هو متجه عمليات التنشيط لهذه الطبقة المخفية الأولى،
لذا z سيكون 3 × 1، سيكون متجهًا ثلاثي الأبعاد. لذا سأقوم بكتابته n1 × متجه أحادي البعد، n1 × مصفوفة أحادية البعد،
إذًا 3 × 1 في هذه الحالة. الآن، ماذا عن خصائص الإدخال x وx؟
لدينا خاصيتا إدخال. إذًا x في هذا المثال 2 × 1، ولكن
بشكل عام، سيكون n0 × 1. إذًا، ما نحتاجه هو أن تكون
المصفوفة w1 شيئًا عندما نضرب n0 في متجه 1 إليه، نحصل
على n1 × متجه 1، أليس كذلك؟ بحيث يكون لديك نوع من
متجه ثلاثي الأبعاد يساوي شيئًا ما مضروبًا في متجه ثنائي الأبعاد. وهكذا من خلال
قواعد مضاعفة المصفوفة، هذا قد يكون مصفوفة 3 في 2. مفهوم، لأن مصفوفة 3 في 2
مضروبة في مصفوفة 2 في 1، أو مضروبة في متجه 2 في 1،
تمنحك متجه 3 في 1. وبشكل أعم، ستكون هذه عبارة
عن مصفوفة أبعاد n1 في n0. إذًا، ما توصلنا إليه هنا هو أن أبعاد w1 يجب أن تكون n1 في n0. وبشكل أعم، أبعاد wL يجب أن
تكون nL في nL ناقص 1. إذًا، على سبيل المثال، أبعاد w2، لهذا، يجب أن تكون 5 في 3، أو ستكون n2 في n1. لأننا سنقوم بحساب z2 مثل w2 في a1، ومرة أخرى، دعنا نتجاهل الانحياز في الوقت الحالي. وهذا سيكون 3 في 1، ونحن بحاجة إلى أن يكون هذا 5 في 1، ولذا هذا يجب أن يكون 5 في 3. وبالمثل، w3 هو في الحقيقة
بُعد الطبقة التالية، فاصلة، بُعد الطبقة
السابقة، لذلك سيكون هذا 4 في 5، وw4 سيكون 2 في 4، وw5 سيكون 1 في 2، تمام؟ إذًا، الصيغة العامة للتحقق
هي أنه عندما تقوم بتنفيذ مصفوفة
الطبقة L، يكون البُعد الخاص بتلك
المصفوفة nL في nL-1. الآن، دعونا نفكر في بُعد
هذا المتجه b. هذا سيكون متجه 3 في 1،
لذلك عليك أن تضيف ذلك إلى متجه 3 في1 آخر من أجل الحصول
على متجه 3 في 1 كناتج. أو في هذا المثال، نحتاج إلى إضافة هذا،
وهذا سيكون 5 في 1، لذلك سيكون هناك
متجه 5 في 1 آخر. لكي يكون مجموع هذين
الشيئين لدي في المربعات هو نفسه متجه 5 في 1. إذًا، القاعدة الأعم هي أنه
في المثال الموجود على اليسار، يكون b1 هو n1 في 1، تمام، وهذا 3 في 1، وفي المثال الثاني،
يكون هذا n2 في 1. وبالتالي فإن الحالة الأعم هي أن bL يجب أن يكون nL في 1 بُعدي. إذًا، آمل أن تساعدك هاتان المعادلتان
على التأكد من أن أبعاد المصفوفات الخاصة بك، وكذلك
المتجهات p، هي الأبعاد الصحيحة. وبالطبع، إذا كنت تقوم
بتطبيق الانتشار الخلفي، يجب أن تكون أبعاد dw
مماثلة لبُعد w. إذًا يجب أن يكون dw نفس البُعد مثل w، وأن يكون db نفس البُعد مثل b. الآن، المجموعة الرئيسية الأخرى من الكميات
التي أبعادها للتحقق هي z وx بالإضافة إلى L، التي
لم نتحدث عنها هنا كثيرًا. لكن لأن z من L يساوي g من a
من L، العنصر الذكي المطبّق، يجب أن يكون z وa لهما نفس
البُعد في هذه الأنواع من الشبكات. الآن، دعونا نرى ما يحدث عندما يكون
لديك تنفيذ متجه يظهر على عدة أمثلة في وقت واحد. حتى بالنسبة للتنفيذ المتجه، بالطبع، ستبقى أبعاد wb
وdw وdb كما هي. لكن أبعاد z وa
وx سوف تغيّر بعض الشيء
في التنفيذ المتجه. إذًا، في السابق، كان لدينا z1 = w1x + b1 حيث هذا كان n1 في 1، وهذا كان n1 في n0، وx كان n0 في 1، وb كان n1 في 1. الآن، في تنفيذ موجه، سوف يكون لديك z1 = w1x + b1. حيث يتم الآن الحصول على z1
من خلال أخذ z1 للأمثلة الفردية، لذلك
يوجد z11 وz12، حتى z1m، وتكديسها كما يلي،
وهذا يمنحك z1. إذًا، فالبُعد z1 هو أنه،
بدلاً من أن يكون n1 في 1، فإنه ينتهي به الحال ليكون n1 في m،
وm هو الحجم الذي تحاول ضبطه. تبقى أبعاد w1 كما هي،
لذا تظل n1 في n0. وx، بدلاً من أن يكون n0 في 1 الآن كل أمثلة التدريب الخاصة بك
مكدسة أفقيًا. إذًا، أصبح الآن n0 في m،
ومن ثم تلاحظ أنه عندما تأخذ مصفوفة n1 في n0 وتُضاعفها
بمقدار مصفوفة n0 في m. ومجموع ذلك في الواقع يعطيك مصفوفة
أبعاد n1 في m، كما هو متوقع. الآن، التفاصيل النهائية هي أن
b1 لا يزال n1 في 1، ولكن عند أخذ هذا وإضافته إلى b،
ثم من خلال توزيع بايثون، سوف يتكرر ذلك ويتحول إلى مصفوفة n1 في m،
ومن ثم يضيف العنصر الذكي. إذًا، في الشريحة السابقة، تحدثنا
عن أبعاد wb وdw وdb. هنا، ما نراه هو أنه في حين أن zL وكذلك aL ذات بُعد nL في 1، لدينا الآن بدلاً من ذلك
أن ZL وكذلك AL هما nL في m. وهناك حالة خاصة من هذا،
وهي عندما L تساوي 0، وفي هذه الحالة A0، التي تساوي فقط خصائص إدخال مجموعة التدريب الخاصة بك X، سوف تساوي n0 في m
كما هو متوقع. وبالطبع عند تنفيذ هذا
في الانتشار الخلفي، سنرى لاحقًا، أنك ستنتهي إلى
حساب dz بالإضافة إلى dA. وبالتالي سيكون لها بالطبع نفس البُعد مثل Z وA. إذًا آمل أن يساعد التدريب البسيط
الذي أجريناه في توضيح الأبعاد التي تمتلكها المصفوفات المختلفة
التي قد تعمل عليها. عند تنفيذ الانتشار الخلفي لشبكة
عصبية عميقة، ما دمت تعمل من خلال التعليمات البرمجية الخاصة بك،
تأكد من أن جميع أبعاد المصفوفات متناسقة. هذا سوف يساعد عادة، سوف يعمل بطريقة ما على القضاء على
بعض أسباب الأخطاء المحتملة. إذًا آمل أن يكون التدريب على فهم
أبعاد المصفوفات المختلفة التي سوف تعمل عليها مفيدًا. عندما تقوم بتنفيذ شبكة عصبية عميقة،
إذا حافظت على استقامة أبعاد هذه المصفوفات والمتجهات
المتنوعة التي تعمل عليها. آمل أن تساعدك على القضاء على
بعض أسباب الأخطاء المحتملة، فهي تساعدني بالفعل في جعل
تعليماتي البرمجية صحيحة. بعد ذلك، سوف نرى الآن بعض آليات كيفية إجراء انتشار أمامي في شبكة عصبية. لكن لماذا تكون الشبكات العصبية
العميقة فعالة للغاية، ولماذا يكون أداؤها أفضل من
التمثيل السطحي؟ دعونا نستهلك بضع دقائق في
الفيديو التالي لمناقشة ذلك.