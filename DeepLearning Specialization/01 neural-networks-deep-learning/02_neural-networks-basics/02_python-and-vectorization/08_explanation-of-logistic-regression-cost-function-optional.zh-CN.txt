在较早的视频中
我已经写下了 逻辑回归的成本函数的公式 在这个选看的视频里
我想向大家简短解释一下 为什么我们在逻辑回归里使用这个代价函数 我们快速地复习一下，逻辑回归中 我们已经预测了ŷ=sigmoid(W^T X + b) 其中的sigmoid函数就是这个 同时，我们想将ŷ解释为p(y=1|x)，在x的条件下，y=1的概率 因此我们希望算法输出ŷ 表示当给定输入特征x的时候y=1的概率 换句话说，如果y等于1 那么p(y|x)就等于ŷ 相反地，当y=0时 p(y|x)就等于1-ŷ 因此，如果ŷ表示当y=1的概率 那么1-ŷ就表示y=0的概率 现在把最后两个方程
复制到下一张幻灯片里 我把两个方程写在这， 它们给出了，在y=0和y=1两种情况下，
p(y|x)的定义 接下来，把两个方程归纳为一个方程 需要指出的是
y只能等于0或1，因为是二元分类函数 因此这里只有y=0或1这两种可能性 有人拿到这两个方程，然后归纳为下面这样 我先写出这个方程
然后我们再解释为什么有人这样写 方程是：p(y|x)=ŷ^y * (1-ŷ)^(1-y) 所以上面的两个方程就只用一行就概括了 让我解释一下原因 在第一种情况下，假设y=1 如果y=1，那么这一项就会变成了ŷ 因为这是ŷ的一次幂 这一项最后会变成(1-ŷ)的(1-1)次幂
也就是0次幂 但是，任何数的零次幂等于1
所以这一项可以省略 所以，当y=1时，这个方程就是p(y|x)=ŷ 这完全就是我们想要的方程 那么，第二种情况呢
如果y=0呢？ 如果y=0
那么上面这个方程就是p(y|x)=ŷ^0 ... 但是，任何数的零次幂等于1 所以这就等于1*(1-ŷ)^(1-y) 所以1-y就是1-0，这项就是1 这项等于1*(1-ŷ)=1-ŷ 所以我们得到
当y=0时，P(y|x)=1-ŷ 这完全就是我们上面想要的 所以，刚才展示的这个公式的确是 p(y|x)的正确定义 最后，
因为对数函数是一个绝对的单调递增函数 最大化log(p(y|x))会得出
和最大化p(y|x)相似的结果 如果计算log(p(y|x)) 也就是log((ŷ^y)*(1-ŷ)^(1-y)) 简化为ylog(ŷ)+(1-y)log(1-ŷ) 对吗 其实，这就是我们之前找到的 损失函数函数的相反数 这里有一个负号
，因为通常在训练一个学习算法的时候 我们想要让概率变大 而在逻辑回归中，我们这样来表示 我们想要最小化L(ŷ,y)这个损失函数 因此，最小化损失函数相当于
最大化概率的对数 这就是损失函数在单一样本上的例子 那代价函数呢 整体的代价函数，在所有样本的训练集里是怎样呢 让我们搞明白 把训练集里全部标签出现的概率 用一种不是很正式地方法写下来 如果假设取出的训练样本相互独立
或者说服从IID 也就是独立同分布
(I.I.D: Independent and Identically Distributed) 那么这些样本的概率就是各项概率的乘积 在给定X(i)
从i=1到m时
p(y(i))的乘积 如果想要采用最大似然估计 那么要就找到一些参数，使其能够最大化 观察点和训练集中的样本的概率 但最大化这个式子本身
和最大化它的对数效果相同 所以在方程两边取对数 依据乘积的对数等于对数的和的性质 训练集标签的概率的对数就可以表达为 在给定X(i)时
从i=1到m的log(p(y(i)))的和 在前一张幻灯片中，我们已经知道 这就是-L(ŷ(i),y(i)) 在统计学上
有个原理叫做最大似然估计原理 意思是选择能使之最大化的参数 换句话说，选这个参数来使这个式子最大化 这项-sum(L(ŷ^i, y^i)，从i=1到m
<i><i> 可以直接将负号移到累加号的外边 这样就解释了我们为逻辑回归设置的 代价函数J(w,b)也就是对L(ŷ(i),y(i))的求和 因为我们现在要最小化代价函数
而不是最大化似然值 所以要去掉这个负号 最后为了方便起见
我们使这些数值处于更好的尺度上 就在前面添加一个缩放系数1/m 综上，通过最小化代价函数J(w,b) 实际上对逻辑回归模型进行了最大似然估计 这是基于训练样本是i.i.d. 或者说是独立同分布的假设之上的 谢谢你看这个视频
即使这是选看的 希望它帮助你理解
为什么我们将这个代价函数用于 逻辑回归的实际学习算法 我希望你继续完成练习， 本周编程练习和问答题 祝在接下来的测验和编程练习中好运
GTC字幕组 翻译