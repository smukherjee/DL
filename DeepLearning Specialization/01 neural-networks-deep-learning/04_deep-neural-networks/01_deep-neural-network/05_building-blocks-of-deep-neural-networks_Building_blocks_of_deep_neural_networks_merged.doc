<html xmlns:o="urn:schemas-microsoft-com:office:office" xmlns:w10="urn:schemas-microsoft-com:office:word" xmlns:m="http://schemas.microsoft.com/office/omml/2004/12/core" xmlns:ve="http://schemas.openxmlformats.org/markup-compatibility/2006" xmlns:o12="http://schemas.microsoft.com/office/2004/7/core" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:v="urn:schemas-microsoft-com:vml" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/3/wordprocessingDrawing" xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/3/main" xmlns="http://www.w3.org/TR/REC-html40">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <head>
    <style>style</style>
  </head>
  <body>
    <div class="Section1">
      <p>
        <b>
          <u>
            <span style="font-size:20.0pt">Building blocks of deep neural networks</span>
          </u>
        </b>
      </p>
      <p>
(DESCRIPTION)        <br/>
Text, Deep Neural Networks. Building blocks of deep neural networks. Website, deep learning, dot, A.I.        <br/>
      </p>
      <p>
(SPEECH)        <br/>
In the earlier videos from this week, as well as from the videos from the past several weeks, you've already seen the basic building blocks of forward propagation and back propagation, the key components you need to implement a deep neural network.        <br/>
      </p>
      <p>
Let's see how you can put these components together to build your deep net.        <br/>
      </p>
      <p>
(DESCRIPTION)        <br/>
New slide, Forward and backward functions.        <br/>
      </p>
      <p>
(SPEECH)        <br/>
Here's a network of a few layers.        <br/>
      </p>
      <p>
Let's pick one layer.        <br/>
      </p>
      <p>
And look into the computations focusing on just that layer for now.        <br/>
      </p>
      <p>
So for layer L, you have some parameters wl and bl and for the forward prop, you will input the activations a l-1 from your previous layer and output a l.        <br/>
      </p>
      <p>
So the way we did this previously was you compute z l = w l times al - 1 + b l.        <br/>
      </p>
      <p>
And then al = g of z l.        <br/>
      </p>
      <p>
All right.        <br/>
      </p>
      <p>
So, that's how you go from the input al minus one to the output al.        <br/>
      </p>
      <p>
And, it turns out that for later use it'll be useful to also cache the value zl.        <br/>
      </p>
      <p>
So, let me include this on cache as well because storing the value zl would be useful for backward, for the back propagation step later.        <br/>
      </p>
      <p>
And then for the backward step or for the back propagation step, again, focusing on computation for this layer l, you're going to implement a function that inputs da(l).        <br/>
      </p>
      <p>
And outputs da(l-1), and just to flesh out the details, the input is actually da(l), as well as the cache so you have available to you the value of zl that you computed and then in addition, outputing da(l) minus 1 you bring the output or the gradients you want in order to implement gradient descent for learning, okay?        <br/>
      </p>
      <p>
So this is the basic structure of how you implement this forward step, what we call the forward function as well as this backward step, which we'll call backward function.        <br/>
      </p>
      <p>
So just to summarize, in layer l, you're going to have the forward step or the forward prop of the forward function.        <br/>
      </p>
      <p>
Input al- 1 and output, al, and in order to make this computation you need to use wl and bl.        <br/>
      </p>
      <p>
And also output a cache, which contains zl, right?        <br/>
      </p>
      <p>
And then the backward function, using the back prop step, will be another function that now inputs da(l) and outputs da(l-1).        <br/>
      </p>
      <p>
So it tells you, given the derivatives respect to these activations, that's da(l), what are the derivatives?        <br/>
      </p>
      <p>
How much do I wish?        <br/>
      </p>
      <p>
You know, al- 1 changes the computed derivatives respect to deactivations from a previous layer.        <br/>
      </p>
      <p>
Within this box, right?        <br/>
      </p>
      <p>
You need to use wl and bl, and it turns out along the way you end up computing dzl, and then this box, this backward function can also output dwl and dbl, but I was sometimes using red arrows to denote the backward iteration.        <br/>
      </p>
      <p>
So if you prefer, we could draw these arrows in red.        <br/>
      </p>
      <p>
So if you can implement these two functions then the basic computation of the neural network will be as follows.        <br/>
      </p>
      <p>
You're going to take the input features a0, feed that in, and that would compute the activations of the first layer, let's call that a1 and to do that, you need a w1 and b1 and then will also, you know, cache away z1, right?        <br/>
      </p>
      <p>
Now having done that, you feed that to the second layer and then using w2 and b2, you're going to compute deactivations in the next layer a2 and so on.        <br/>
      </p>
      <p>
Until eventually, you end up outputting a l which is equal to y hat.        <br/>
      </p>
      <p>
And along the way, we cached all of these values z.        <br/>
      </p>
      <p>
So that's the forward propagation step.        <br/>
      </p>
      <p>
Now, for the back propagation step, what we're going to do will be a backward sequence of iterations in which you are going backwards and computing gradients like so.        <br/>
      </p>
      <p>
So what you're going to feed in here, da(l) and then this box will give us da(l- 1) and so on until we get da(2) da(1).        <br/>
      </p>
      <p>
You could actually get one more output to compute da(0) but this is derivative with respect to your input features, which is not useful at least for training the weights of these supervised neural networks.        <br/>
      </p>
      <p>
So you could just stop it there. But along the way, back prop also ends up outputting dwl, dbl.        <br/>
      </p>
      <p>
I just used the prompt as wl and bl.        <br/>
      </p>
      <p>
This would output dw3, db3 and so on.        <br/>
      </p>
      <p>
So you end up computing all the derivatives you need.        <br/>
      </p>
      <p>
And so just to maybe fill in the structure of this a little bit more, these boxes will use those parameters as well.        <br/>
      </p>
      <p>
wl, bl and it turns out that we'll see later that inside these boxes we end up computing the dz's as well.        <br/>
      </p>
      <p>
So one iteration of training through a neural network involves: starting with a(0) which is x and going through forward prop as follows.        <br/>
      </p>
      <p>
Computing y hat and then using that to compute this and then back prop, right, doing that and now you have all these derivative terms and so, you know, w would get updated as w1 = the learning rate times dw, right?        <br/>
      </p>
      <p>
For each of the layers and similarly for b rate.        <br/>
      </p>
      <p>
Now the computed back prop have all these derivatives.        <br/>
      </p>
      <p>
So that's one iteration of gradient descent for your neural network.        <br/>
      </p>
      <p>
Now before moving on, just one more informational detail.        <br/>
      </p>
      <p>
Conceptually, it will be useful to think of the cache here as storing the value of z for the backward functions.        <br/>
      </p>
      <p>
But when you implement this, and you see this in the programming exercise, When you implement this, you find that the cache may be a convenient way to get to this value of the parameters of w1, b1, into the backward function as well. So for this exercise you actually store in your cache to z as well as w and b. So this stores z2, w2, b2. But from an implementation standpoint, I just find it a convenient way to just get the parameters, copy to where you need to use them later when you're computing back propagation.        <br/>
      </p>
      <p>
So that's just an implementational detail that you see when you do the programming exercise.        <br/>
      </p>
      <p>
So you've now seen what are the basic building blocks for implementing a deep neural network.        <br/>
      </p>
      <p>
In each layer there's a forward propagation step and there's a corresponding backward propagation step.        <br/>
      </p>
      <p>
And has a cache to pass information from one to the other.        <br/>
      </p>
      <p>
In the next video, we'll talk about how you can actually implement these building blocks.        <br/>
      </p>
      <p>
Let's go on to the next video.        <br/>
      </p>
    </div>
  </body>
</html>
