これまではベクトル化によって
コードの実行速度をかなり速くできることを見てきました このビデオではロジステック回帰で ベクトル化する方法を話します トレーニングデータ全体を
処理できるようにします ここでは１つもfor loop を書かずに トレーニングデータ全体での勾配降下法の実行を
１度の実行で完了できるように実装します このテクニックや １つのfor loop さえも
使っていないような ニューラルネットワークを語るのは
すごく興奮します では始めましょう
ロジステック回帰の誤差順伝播を検討しましょう m のトレーニングデータがあるとして １つ目のデータで予測するために この計算が必要です この馴染みの式を使って
z を計算します 次に活性化を計算します １つ目のデータで
yハットを計算します 次に２つ目のデータで
予測するために この計算が必要です ３つ目のデータでの予測には これが必要で
どんどん続きます m 個のデータがあれば これをm 回実行する必要があります 誤差順伝播法を実行するためには m 個のトレーニングデータの予測を
計算する必要があります for loop なしで 行う方法があります どうするか見てみましょう 最初に行列X を定義して
トレーニングデータの入力を 別々の列に
このように積み重ねます これは行列です nxとm の行列です Python numpy での
shape の形で書いていますが これはXはnxとmの次元を持つ列を
意味しているだけです まず最初に実行するのは
z(1)、z(2)、z(3)を どう１回で 計算するかです 実際 １行のコードでです まず1 と m の行列を作ります これは列ベクトルでz(1)、z(2)などと z(m)まで ずっと計算します 実はこれは wの転置に行列Xと
ベクトルbを足したものです bが続きます この続いていくもの このb、b、b、b、b は 1 とm のベクトルまたは 1 とm の行列
m要素の行ベクトルです みなさんの行列積の詳しさによりますが wの転置には
x(1)、x(2)から x(m)までありますよね wの転置したものは
列ベクトルになります こんな列ベクトルになります この最初の項を評価すると
wの転置にx(1)をかけ wの転置にx(2)をかけ
これが続いて wの転置にx(m)となります
次の項のb を 続けると b を全要素に
足すことになります さらにもう１つの
1とmのベクトルができることになります １つ目の要素がこれで ２つ目の要素がこれで
これが続いて m 個目の要素まで行きます この定義を見ると この１つ目の要素は
z(1)の定義と全く同一です ２つ目の要素そして次も
同じ定義が続きます X が得られるのと同じで トレーニングデータを使って 次々に水平に重ねていくと Z をこのように
定義することになります 小文字のz を使って
水平に重ねます 小文字のx を別のトレーニングデータに
水平に重ねると 大文字の変数X が得られます これと同じように
小文字のz の変数を 水平に重ねていくと 大文字の先に出てきた変数 Zになります 実は この部分を実装するには numpy の命令は
Z=np.dot(w.T,X)+bです wの転置とXで
これにb を加えます Python には
見えにくい点があり b は実数で
1と1の行列です 普通の実数ですが このベクトルに実数を足すと Python は自動で
実数b を1 x m の列ベクトルに拡張します この演算が
ちょっと神秘的に見えた場合には これはブロードキャストと
呼ばれています 今は これは気にしないでください 次のビデオで
もっと解説します ここで覚えてもらいたいのは
このたった１行のコードです 大文字のZ を計算でき 大文字のZ は1 x m の行列になっています
小文字のz をすべて含んでいます z(1)からz(m)までです これが大文字のZ です
ではa たちの値はどうでしょうか 次に行うのは a(1)からa(2)
そしてa(m)までを １度で計算を実行するような方法を 探します 小文字のx を水平に重ねることで 大文字のX になったのと同じように 小文字のa を重ねていくと 新しい変数の大文字Aを 定義することになります この後のプログラムアサインメントでは ベクトルになったシグモイド関数を
実装する方法が出てきます シグモイド関数とは この大文字のZ を入力とし
非常に効率的に大文字のA を出力するものです プログラムアサインメントで
詳しく紹介しています まとめると このスライドで見てきたのは
トレーニングデータのm に対して 小文字のz と小文字のa を
１つずつ計算するためのループが必要になる替わりに この１行のコードを実装することで 全てのz を
一度で計算できます さらにこの１行のコードで 小文字のシグマを 計算する適切な実装で
小文字のa 全てを１度で計算します これがベクトル化実装で 全ての トレーニングデータm に対して
誤差順伝播法行う方法です まとめると
ここでは ベクトル化して
活性化関数の小文字のa を 非常に効率的に
計算する方法を見てきました 次は勾配を計算し
誤差逆伝播法を 非常に効率的に計算する方法を 使えるようになります どうするのか
次のビデオで見てみましょう