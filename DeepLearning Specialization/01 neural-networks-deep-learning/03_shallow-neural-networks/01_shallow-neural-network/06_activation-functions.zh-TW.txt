當你構建神經網路時,
一個你要選擇的是,使用什麼 啟動函數在隱藏層中, 以及什麼是
神經網路的輸出單位。 到目前為止,我們只使用
S型函數 sigmoid 啟動函數。 但有時其他選擇
可以作得更好。 讓我們來看看一些其他選項。 在神經網路正向傳播的步驟中, 我們有兩個步驟
我們在這裡使用 sigmoid 函數。 因此,sigmoid 被稱為
啟動函數 這是您熟悉的S型
sigmoid 函數 等於 1 / ( 1 + e^-z ) 所以在更一般的情況下,
我們可以有不同的函數, g(z) ,我寫在這裡, g 可以是一個非線性函數
但未必是 sigmoid 函數。 例如,sigmoid 函數
在0 和 1 之間, 啟動函數,幾乎總是
比 sigmoid 函數效果更好的 的是 tanh 函數
或者稱為雙曲線正切函數. 所以這是 z ,這是 a,
這是一個 a = tanh(z), 這介於 +1 和 -1 之間。 tanh 函數的公式為 e^z 減去 e^-z ,除以他們的總和。 實際在數學上是
sigmoid 函數的移位版本。 所以,作為一個 sigmoid 函數,就像這樣,
但移位,所以 它現在跨越 (0 , 0 ) 和重新調整
,所以它介於 +1 和 -1 之間。 實際上對於隱藏單元
如果您讓函數 g(z) 等於 tanh(z), 這個幾乎總是比 sigmoid 函數更好 因為在 +1, -1 之間,啟動的平均值 從隱藏層出來時,會更趨近于 0 的平均值 就像有時,當訓練一個學習演算法時 您可能選擇資料中間, 使得資料具有 0 均值,使用一個 tanh ,
而不是一個sigmoid函數。 它有集中您的資料的效果,所以 您的資料的平均值是
接近 0 ,而不是 0.5。 實際上,讓下一層的學習更容易一點。 我們將在第二個課程談到這些
當我們談到優化 演算法。 但一個要點是,我 幾乎不再用S型 Sigmoid 啟動函數. tanh 函數幾乎總是更優越. 一個例外是在輸出層,因為如果 y 是 0 或 1, 那麼它 y-hat 比較合理,
 是一個數目 介於 0 和 1,而不是 -1 和 +1 之間。 因此,一個例外,
我會用 Sigmoid 啟動函數是當您
正在使用二元分類, 在這種情況下,您可以使用 sigmoid
在輸出層的啟動功能。 因此, 這裡 g(z2) 等於 sigmoid of z2。 所以,你在這個例子中看到的
是在那裡,你可能會有一個 tanh 啟動函數在隱藏層,
和 sigmoid 在輸出層。 因此,啟動函數可以不同,
在不同的層。 有時要表示不同的啟動函數,在 不同的層,我們會使用這些
方括弧上標, 以表示 g[1] 或許與 g[2] 不同。 再次,上標方括弧 1 指的是在這一層,並且 上標方括弧 2
指的是輸出層。 現在,一個缺點對於兩者, sigmoid 函數和 tanh 函數是,如果 z 是
要麼很大,要麼很小, 它的梯度或導數或
此函數的斜率變得非常小。 因此,如果 z 是非常大或 z 非常小, 函數的斜率非常接近 0。 所以這會讓梯度下降法慢下來. 所以另一個很受歡迎的選擇在 機器學習,是什麼稱為'線性整流單元'。 因此, ReLU 函數像如下所示。 公式為 a = max(0,z)。 因此,導數是1,只要 z 是正數。 導數值或者說這個函數的斜率為 0,
當 z 為負時。 如果你使用這個, 技術上,導數在 z 正好為 0 時,沒有很好地定義。 但是,當你建置在電腦裡, 你得到的準確答案是
z 等於 00000000000000。 它非常小,所以你不需要去擔心它。 你可以假裝導數,當 z 等於 0 時, 你可以假裝它是1或0,
那你的程式沒問題的. 因此,事實上,它是不能微分的,所以 這裡有一些準則來選擇啟動函數。 如果輸出為 0, 1 值,
如果您使用的是二元分類, sigmoid 啟動函數是一個
在輸出層非常自然的選擇。 然後,對於所有其他單位, ReLU, 線性整流函數 (Rectified Linear Unit) 日益成為公認的啟動函數選擇。 所以,如果你不確定用什麼
在你的隱藏層 我會直接使用 ReLU 啟動函數. 這是你看到大多數的人使用的。 雖然有時人們也使用 tanh 啟動函數。 ReLU 的一個缺點是
導數等於零, 當 z 為負數時。 在實務上,這沒問題的。 還有另一個版本的 ReLU, 稱為Leaky ReLU。 會在下一張投影片, 給您公式. 但是,當 z 為負時它不是 0, 它只是有一個輕微的斜坡,
因此,這被稱為 leaky ReLU。 它通常比 ReLU 啟動函數好, 在實作上比較少用到. 任何一個都是好的,雖然, 如果你必須挑一個,
我通常只使用 ReLU。 ReLU 和 leaky ReLU 的優勢是,為 在 Z 的很多領域空間,
啟動函數的導數, 啟動函數的斜率
非常不同於 0。 所以在實踐中,
使用 ReLU 啟動函數, 你的神經網路經常學習的快多了,
比使用 tanh 或 sigmoid 啟動函數。 主要原因是
少了這些效果,當函數的斜率 逼近 0,這減慢了學習速度。 我知道,一半 z 的範圍
,ReLU 的斜率為 0, 但在實踐中,足夠的隱藏單位
具有 z 大於 0。 因此,大多數學習示例
學習仍然可以相當快。 因此,讓我們快速回顧一下
不同啟動函數的優點和缺點。 這是 S型 Sigmoid 啟動函數 我會說永遠不要用這個
除了輸出層, 如果您正在做二元分類,
或者幾乎從不用這個. 我幾乎從不使用這是因為, tanh 是 非常嚴整優越 tanh 啟動函數像這樣 然後是公認用的, 最常用的啟動函數是 ReLU,是這樣。 所以,如果你不確定還有什麼用的,
使用這個,並 高興的話也可以嘗試 leaky ReLU. 可能是 (0.01 z, z)。 對？因此, a = max(0.01z, z) 所以給你這些函數中的一些彎曲。 你可能會說為什麼是常數 0.01? 你也可以讓它成為另一個學習演算法的參數 有些人說這樣做會更好, 但我很少看到人們這樣做。 但是,如果你想嘗試在你的
應用程式,請隨時嘗試。 你可以看到它是如何做的,
以及它是如何做的好, 堅持它,如果它
給你一個好的結果。 我希望這給你一種感覺,一些
啟動函數的選擇,你 可在神經網路中使用。 我們會看到在深度學習的話題是
你經常有很多 不同的選擇,在你如何
為您的神經網路編寫程式 從隱藏單位的數量,
到選擇啟動函數, 如何初始化的方式,
我們稍後會看到。 很多這樣的選擇。 事實證明,它有時
很難得到良好的指南, 究竟什麼最適合你的問題。 因此,在這些課程,我
繼續給你一些感覺有關 我在行業中看到的
什麼或多或少比較流行的。 但對於您的應用程式,
與應用程式的特點, 其實很難事先確切知道,
什麼將會最好。 所以一條常見的建議是
如果你不確定這些 啟動函數工作最佳,
嘗試所有的,並評估在 保留驗證集,或開發集,
我們將稍後討論, 哪一種效果最好,然後就用它. 我認為通過測試這些
不同的選擇在您的應用, 你會更好地在面向未來,你的
神經網路架構 你問題的特質,
以及演算法的演變。 而不是如果我告訴你
始終使用 ReLU 啟動和 別用別的了. 那只是可能或可能不適用,
無論你最終解決什麼問題 在不久的將來,或
在遙遠的未來。 好吧,這是有關於
啟動函數的選擇,和 你見過的一些最流行的啟動函數. 還有一個問題
有時候你可以問, 這就是為什麼你甚至需要
使用啟動函數? 為什麼不使用呢? 因此,讓我們來在下一個影片中談談, 為什麼神經網路需要非線性啟動函數.