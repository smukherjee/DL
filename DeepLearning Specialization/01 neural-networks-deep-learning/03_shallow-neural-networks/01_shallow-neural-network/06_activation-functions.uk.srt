1
00:00:00,870 --> 00:00:04,550
Коли ми будуємо нейронну мережу (НМ), то неодмінно мусимо вибрати

2
00:00:04,550 --> 00:00:07,570
функції активації і для прихованих шарів,

3
00:00:07,570 --> 00:00:10,950
і для вихідних вузлів НМ.

4
00:00:10,950 --> 00:00:14,360
Досі ми використовували лише сигмоїду як функцію активації.

5
00:00:14,360 --> 00:00:17,980
Проте, інколи інші функції можуть працювати набагато краще.

6
00:00:17,980 --> 00:00:19,970
Давай розглянемо деякі опції.

7
00:00:19,970 --> 00:00:23,840
В кроці прямого поширення НМ

8
00:00:23,840 --> 00:00:28,279
ми маємо оці 3 кроки, в яких використовуємо сигмоїду.

9
00:00:28,279 --> 00:00:32,460
Ця сигмоїда називається функцією активації.

10
00:00:32,460 --> 00:00:35,871
Ось вже знайома сигмоїдальна функція.

11
00:00:35,871 --> 00:00:39,980
Вона =1/(1+e⁻ᶻ).

12
00:00:39,980 --> 00:00:43,910
Тож в більш загальному випадку ми можемо мати іншу функцію

13
00:00:45,050 --> 00:00:50,100
g(z), яку я запишу отут,

14
00:00:51,300 --> 00:00:57,300
де g може бути нелінійною функцією і не сигмоїдою.

15
00:00:57,300 --> 00:01:02,530
Приклад. Сигмоїда приймає значення від 0 до 1. А,

16
00:01:02,530 --> 00:01:07,040
в більшості випадків, кращою функцією активації ніж сигмоїда

17
00:01:07,040 --> 00:01:12,220
є функція th - функція гіперболічного тангенсу.

18
00:01:12,220 --> 00:01:18,909
Тобто, оце - z, оце - a, оце - a=th(z)

19
00:01:18,909 --> 00:01:24,390
з областю значень від -1 до +1.

20
00:01:24,390 --> 00:01:29,745
Формула функції th

21
00:01:29,745 --> 00:01:35,920
=(eᶻ-e⁻ᶻ)/(eᶻ+e⁻ᶻ).

22
00:01:35,920 --> 00:01:42,829
І, насправді, математично, є зсунутою версією сигмоїди.

23
00:01:42,829 --> 00:01:47,399
Тобто дуже подібна на сигмоїду, але дещо зміщена

24
00:01:47,399 --> 00:01:54,180
(і тепер перетинає точку (0, 0)) і розтягнута 
(тобто має область значень між -1 і +1)

25
00:01:54,180 --> 00:01:59,406
Виявляється, що для прихованих вузлів, 
якщо дозволити функції g(z)

26
00:01:59,406 --> 00:02:06,775
дорівнювати th(z),

27
00:02:06,775 --> 00:02:11,490
то вона майже завжди буде працювати краще ніж сигмоїда.

28
00:02:11,490 --> 00:02:16,540
Тому що при області значень між -1 і +1 
середнє значення активації,

29
00:02:16,540 --> 00:02:20,360
що виводиться з прихованих вузлів, буде близьким до 0.

30
00:02:20,360 --> 00:02:24,468
І, отже, інколи, при тренуванні навчального алгоритму,

31
00:02:24,468 --> 00:02:26,364
ми можемо відцентрувати дані

32
00:02:26,364 --> 00:02:31,029
відносно 0-ого середнього з допомогою th, а не сигмоїди.

33
00:02:31,029 --> 00:02:34,500
Тобто вона може типу центрувати дані, тож

34
00:02:34,500 --> 00:02:39,420
середнє значення даних буде ближче до 0, замість 0,5.

35
00:02:39,420 --> 00:02:43,200
А це робить навчання наступного шару дещо легшим.

36
00:02:43,200 --> 00:02:46,900
Ми більше поговоримо про це в 2-ому курсі, 
коли говоритимемо про оптимізацію

37
00:02:46,900 --> 00:02:47,990
алгоритмів.

38
00:02:47,990 --> 00:02:50,480
Але зауважу, що я

39
00:02:50,480 --> 00:02:54,500
майже вже не використовую сигмоїду як функцію активації.

40
00:02:54,500 --> 00:02:58,410
Функція th майже завжди є функцією вибору.

41
00:02:58,410 --> 00:03:04,880
Одне з виключень - вихідний шар. 
Бо якщо y може бути лише 0 або 1,

42
00:03:04,880 --> 00:03:10,070
то має сенс, щоб ŷ приймала значення

43
00:03:10,070 --> 00:03:14,830
(тобто ми так хочемо) між 0 і 1, замість між -1 і 1.

44
00:03:14,830 --> 00:03:19,380
Тож винятком, в якому я використав би сигмоїду

45
00:03:19,380 --> 00:03:23,610
як функцію активації - це двійкова класифікація.

46
00:03:23,610 --> 00:03:28,760
В цьому випадку можна використовувати сигмоїду 
як функцію активації для вихідного шару.

47
00:03:28,760 --> 00:03:34,828
Тож g(z[2]) тут =σ(z[2]).

48
00:03:34,828 --> 00:03:40,236
І, отже, на цьому прикладі видно, що th може бути

49
00:03:40,236 --> 00:03:47,102
функцією активації для прихованого шару, 
а сигмоїда - для вихідного шару.

50
00:03:47,102 --> 00:03:50,982
Тож функції активації можуть бути різними в різних шарах.

51
00:03:50,982 --> 00:03:55,112
Тож занотуй, що іноді функції активації різні в

52
00:03:55,112 --> 00:03:59,907
різних шарах. Ми будемо використовувати 
верхні індекси в квадратних дужках, щоб

53
00:03:59,907 --> 00:04:05,270
позначати, що g[1] може відрізнятись від g[2].

54
00:04:05,270 --> 00:04:09,360
І знову ж, верхній індекс 1 в квадратних дужках 
відноситься до оцього шару, а

55
00:04:09,360 --> 00:04:12,280
верхній індекс 2 в квадратних дужках відноситься до оцього шару.

56
00:04:13,830 --> 00:04:17,050
Тепер. Один з недоліків обох - сигмоїди і

57
00:04:17,050 --> 00:04:21,920
гіперболічного тангенсу - це якщо z або дуже велике, або дуже мале,

58
00:04:21,920 --> 00:04:26,380
то Градієнт, або похідна, або нахил цих функцій стає дуже малим.

59
00:04:26,380 --> 00:04:29,550
Тож якщо z або дуже велике, або дуже мале,

60
00:04:29,550 --> 00:04:33,740
то нахил функції стає дуже близьким до 0.

61
00:04:33,740 --> 00:04:35,600
А це може сповільнити Градієнтний спуск.

62
00:04:36,630 --> 00:04:39,850
Тож одним з виборів, який дуже поширений в

63
00:04:39,850 --> 00:04:44,650
машинному навчанні, є так званий випрямлений лінійний вузол (ВЛВ).

64
00:04:44,650 --> 00:04:47,925
Функція ВЛВ виглядає отак.

65
00:04:50,463 --> 00:04:57,020
А її формула: a=max(0, z).

66
00:04:57,020 --> 00:05:01,755
Тож похідна =1 поки z - додатне

67
00:05:01,755 --> 00:05:05,785
і похідна або нахил =0 коли z - від'ємне.

68
00:05:05,785 --> 00:05:06,855
При реалізації, технічно,

69
00:05:06,855 --> 00:05:11,425
похідна, якщо z точно =0, є невизначеною.

70
00:05:11,425 --> 00:05:13,175
Але при програмній реалізації

71
00:05:13,175 --> 00:05:17,947
ми отримаємо, що z=0000000000000.

72
00:05:17,947 --> 00:05:22,697
Тобто дуже мале і нам непотрібно про це турбуватись на практиці.

73
00:05:22,697 --> 00:05:26,729
Ми можемо прирівняти похідну (коли z=0)

74
00:05:26,729 --> 00:05:31,775
або до 1, або до 0 і продовжити роботу.

75
00:05:31,775 --> 00:05:35,640
Тож той факт, що вона недиференційована...

76
00:05:35,640 --> 00:05:39,670
Ось що треба пам'ятати при виборі функцій активації.

77
00:05:39,670 --> 00:05:44,940
Якщо на виході в нас 0 або 1, 
тобто коли ми використовуємо двійкову класифікацію,

78
00:05:44,940 --> 00:05:49,390
то природно для вихідного шару взяти сигмоїду як функцію активації.

79
00:05:49,390 --> 00:05:54,272
А для всіх інших вузлів - ВЛВ

80
00:05:54,272 --> 00:05:58,374
або випрямлений лінійний вузол,

81
00:06:02,348 --> 00:06:06,689
який все частіше обирається функцією активації по замовчуванню.

82
00:06:06,689 --> 00:06:11,000
Тож якщо ти не впевнений/а 
що використати для прихованого шару

83
00:06:11,000 --> 00:06:14,681
я раджу функцію активації ВЛВ.

84
00:06:14,681 --> 00:06:16,820
Зараз її використовує більшість.

85
00:06:16,820 --> 00:06:21,460
Хоча інколи використовується і th як функція активації.

86
00:06:21,460 --> 00:06:25,852
Один з недоліків ВЛВ - це те, що похідна =0

87
00:06:25,852 --> 00:06:27,152
коли z - від'ємне.

88
00:06:27,152 --> 00:06:29,136
На практиці це не заважає.

89
00:06:29,136 --> 00:06:33,323
Проте, є й інша версія ВЛВ. Вона називається нещільним ВЛВ.

90
00:06:33,323 --> 00:06:35,174
Я дам формулу на наступному слайді.

91
00:06:35,174 --> 00:06:38,581
Але вона не =0 при від'ємному z,

92
00:06:38,581 --> 00:06:43,890
а має отакий невеликий нахил, тому й називається нещільною.

93
00:06:45,870 --> 00:06:49,850
Як правило, як функція активації вона працює краще ніж ВЛВ,

94
00:06:49,850 --> 00:06:53,790
хоча на практиці використовується не так часто.

95
00:06:53,790 --> 00:06:55,130
Будь-яка з них повинна підійти.

96
00:06:55,130 --> 00:06:59,140
Якщо потрібно вибрати, то я, зазвичай, використовую ВЛВ.

97
00:06:59,140 --> 00:07:03,255
Перевагою обох - ВЛВ і нещільного ВЛВ - є те,

98
00:07:03,255 --> 00:07:07,765
що на великій частині області визначення z 
похідна функції активації,

99
00:07:07,765 --> 00:07:12,061
нахил функції активації, дуже відрізняється від 0.

100
00:07:12,061 --> 00:07:15,472
Тож на практиці, при використанні ВЛВ як функції активації

101
00:07:15,472 --> 00:07:19,855
наша НМ часто навчатиметься набагато швидше ніж при використанні th або

102
00:07:19,855 --> 00:07:21,959
сигмоїди як функції активації.

103
00:07:21,959 --> 00:07:26,148
Головна причина цього - менший вплив нахилу

104
00:07:26,148 --> 00:07:30,015
функції, тобто ефекту сповільнення навчання 
коли він наближений до 0.

105
00:07:30,015 --> 00:07:34,425
Я знаю, що на половині області визначення z нахил ВЛВ =0,

106
00:07:34,425 --> 00:07:39,585
але на практиці достатня кількість прихованих вузлів матимуть z>0.

107
00:07:39,585 --> 00:07:43,040
Тож навчання буде достатньо швидким 
для більшості тренувальних зразків.

108
00:07:43,040 --> 00:07:47,490
Давай швиденько резюмуємо за і проти різних функцій активації.

109
00:07:47,490 --> 00:07:49,160
Ось функція активації сигмоїда.

110
00:07:49,160 --> 00:07:53,360
Я раджу ніколи її не використовувати. Окрім вихідного шару

111
00:07:53,360 --> 00:07:56,740
при реалізації двійкової класифікації. 
Тобто майже ніколи її не використовувати.

112
00:07:57,950 --> 00:08:01,860
Причина того, що я її б майже ніколи 
не використовував, це те, що th

113
00:08:01,860 --> 00:08:04,330
набагато краще.

114
00:08:04,330 --> 00:08:06,532
Ось функція активації th.

115
00:08:11,012 --> 00:08:12,476
І ось вибір по замовчуванню,

116
00:08:12,476 --> 00:08:17,190
найбільш вживана функція активації, - ВЛВ. Ось вона.

117
00:08:18,400 --> 00:08:22,790
Тож якщо ти не впевнений/а що використати, використовуй її.

118
00:08:22,790 --> 00:08:27,668
І також не бійся спробувати нещільну ВЛВ.

119
00:08:27,668 --> 00:08:34,540
Що, можливо, буде =max(0.01*z, z).

120
00:08:34,540 --> 00:08:39,140
Зрозуміло? Тож a=max(0.01*z, z).

121
00:08:39,140 --> 00:08:43,200
Це і дає оцей невеличкий вигин функції.

122
00:08:43,200 --> 00:08:48,210
Можливо, тобі цікаво де взялось 0,01?

123
00:08:48,210 --> 00:08:53,366
Що ж, можна вибрати і інше значення для навчального алгоритму.

124
00:08:53,366 --> 00:08:55,320
Дехто каже, що інші працюють краще.

125
00:08:55,320 --> 00:08:56,790
Але я рідко бачу щоб це робили.

126
00:08:58,380 --> 00:09:01,910
Але якщо ти хочеш спробувати це для свого застосунку, 
будь ласка, не бійся.

127
00:09:01,910 --> 00:09:05,460
Ти зможеш побачити як це працює, наскільки добре

128
00:09:05,460 --> 00:09:07,980
і використовувати, якщо результат буде задовільний.

129
00:09:07,980 --> 00:09:11,828
Тож, надіюсь, я дав деяке розуміння як вибирати функції активації

130
00:09:11,828 --> 00:09:13,389
і використовувати їх в НМ.

131
00:09:13,389 --> 00:09:17,276
Як видно в ГН дуже часто доводиться

132
00:09:17,276 --> 00:09:20,299
щось вибирати, щоб написати код НМ:

133
00:09:20,299 --> 00:09:24,024
кількість прихованих вузлів, функції активації,

134
00:09:24,024 --> 00:09:26,752
як ініціалізувати коефіцієнти (про це ми поговоримо пізніше)...

135
00:09:26,752 --> 00:09:28,310
Багато різноманітних виборів.

136
00:09:28,310 --> 00:09:32,630
І, виявляється, що інколи складно виробити чіткі інструкції,

137
00:09:32,630 --> 00:09:35,280
щоб точно знати, що найкраще підійде для нашої задачі.

138
00:09:35,280 --> 00:09:38,410
Тож протягом цих курсів я намагатимусь давати приклади

139
00:09:38,410 --> 00:09:41,890
побачені в промисловості, щоб показати що більш, а що менш популярне.

140
00:09:41,890 --> 00:09:45,215
Але для твого застосунку, з його особливостями,

141
00:09:45,215 --> 00:09:49,330
насправді дуже складно знати наперед, що найкраще працюватиме.

142
00:09:49,330 --> 00:09:52,480
Тож більш-менш загальна порада - 
це, якщо ти не впевнений/а яку з цих

143
00:09:52,480 --> 00:09:57,220
функції вибрати - спробуй усі і оціни

144
00:09:57,220 --> 00:10:02,220
на приготованому валідаційному наборі або 
розробницькому наборі, про які ми поговоримо пізніше, і

145
00:10:02,220 --> 00:10:05,780
побачиш яка працюватиме краще, і вибереш її.

146
00:10:05,780 --> 00:10:10,303
Також думаю, що, тестуючи свій вибір в застосунку,

147
00:10:10,303 --> 00:10:15,189
ти зможеш краще в майбутньому вдосконалювати архітектуру своєї НМ

148
00:10:15,189 --> 00:10:20,030
згідно з особливостями задачі і розвитком алгоритмів.

149
00:10:20,030 --> 00:10:24,694
Якби я сказав тобі завжди використовувати ВЛВ як функцію активації

150
00:10:24,694 --> 00:10:26,720
і більш нічого,

151
00:10:26,720 --> 00:10:30,877
то могло б спрацювати, а могло б і ні, 
для будь-якої задачі над якою б ти не працював

152
00:10:30,877 --> 00:10:33,925
зараз, пізніше чи через багато років.

153
00:10:33,925 --> 00:10:37,420
Добре. Це був вибір функцій активації і

154
00:10:37,420 --> 00:10:39,890
ти побачив/ла найбільш поширені функції активації.

155
00:10:39,890 --> 00:10:43,120
Є одне питання, яке ти міг/ла б задати.

156
00:10:43,120 --> 00:10:46,490
Навіщо взагалі використовувати функції активації?

157
00:10:46,490 --> 00:10:48,260
Чому не можна обійтись без них?

158
00:10:48,260 --> 00:10:51,510
Тож давай поговоримо про це в наступному відео, в якому

159
00:10:51,510 --> 00:10:56,230
ти побачиш чому НМ дійсно потребують 
певної нелінійної функції активації.