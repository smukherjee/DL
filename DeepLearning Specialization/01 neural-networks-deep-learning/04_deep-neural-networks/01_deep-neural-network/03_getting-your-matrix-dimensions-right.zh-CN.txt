当采用深度神经网络时<br />我常用的一个用于检查代码正确性的调试工具就是 拿出一张纸 然后检查我使用的矩阵的维度 我将告诉你如何做<br />因为我希望这能够帮你更容易的使用深度网络 我将告诉你如何做<br />因为我希望这能够帮你更加容易的使用深度网络 大写L等于5 快点数 不要数输入层 这儿有5层网络 其中 四层是隐藏层以及一层输出层 如果你采用前向传播 那么 第一步将是z1 = w1 * x + b1 现在我们忽略偏置b 只关注参数w 现在第一级隐藏层有3个隐藏单元 这是0层 1层 2层 3层 4层和5层 所以利用前期视频中的注释 我们有n1 也就是第一层中隐藏单元数 等于3 然后n2=5 n3=4 n4=2以及n5=1 然后n2=5 n3=4 n4=2以及n5=1 到现在为止 我们只看具有一个输出单元的神经网络 在后续课程中 我们也会谈到具有多个输出单元的神经网络 最后 对于输入层 我们有n0=nx=2 现在 让我们思考z w以及x的维度 z是第一级隐藏层的激活矢量 所以z的维度是3×1 是一个3维矢量 我将写作n1个一维矢量 也就是n1×1矩阵 此处为3×1矩阵 那么关于输入特征x呢? 我们有两个输入 所以x也就是2×1矩阵 更一般地 写作n0×1矩阵 所以我们需要的是<br />对于矩阵w 应该满足乘以n0×1矢量得到n1×1矢量 是这样吧? 所以我们有一个3维矢量等于 某个矩阵乘以一个2维矢量 运用矩阵乘法定理 得到一个3×2矩阵 好的 因为3×2矩阵乘以2×1矩阵 或者2×1矢量 等于一个3×1矢量 更一般地说 会得到一个n1×n0的矩阵 所以结果是 w1的维度为n1×n0 所以结果是 w1的维度为n1×n0 更一般地说 wL的维度为nL×n(L-1) 举个例子 w2的维度为 在这里 是5×3 或者是n2×n1 我们将计算z2 等于w2 乘以 a1 这里再一次忽略偏置b 因为a1的维度为3×1 我们需要z2的维度为5×1 所以 w2的维度为5×3 相似地 w3的维度就是下一层的维度 乘以上一层的维度 所以就是4×5 w4的维度就是2×4 w5的维度就是1×2 对吧? 所以一般检测公式是 L层的矩阵维度是nL×n(L-1) L层的矩阵维度是nL×n(L-1) 现在 让我们思考矢量b的维度 它的维度为3×1 那样你才可以加上另一个3×1矢量 从而获得3×1的输出矢量 或者在这里 我们需要加上这个 它的维度为5×1 那么这需要另一个5×1矢量 为了让被框出的这两个矩阵的和的维度 和它本身一样是5×1 所以更一般的规则是 如左侧的例子 b1的维度是n1×1 也就是3×1 在第二个例子中 维度为n2×1 那么 更一般的情况是 bL的维度应该为nL×1 希望这两个等式能帮助你们再次检查矩阵w 以及矢量b的维度 确保都是正确的维度 当然 如果你用反向传播 那么dw的维度应该和w的维度一致 所以dw和w的维度一样 且db和b的维度一样 现在需要检查维度的其他关键数组是z z x以及aL 这里我们不会过多谈及 因为zL等于gL作用于aL 那么z和a在这类网络中有着相同的维度 现在让我们看看当我们一次性对多个例子采用矢量化会发生什么 现在让我们看看当我们一次性对多个例子采用矢量化会发生什么 即使是矢量化的实现 显然易见 wb dw以及db的维度保持一致 但是z a以及x的维度 在矢量化过程中 将会发生一点变化 如上所述 z1 = w1 * x + b1 z1 = w1 * x + b1 z1的维度为n1×1 w1的维度为n1×n0 x1的维度为n0×1 b1的维度为n1×1 现在 在矢量化过程中 我们有Z1 = W1 * X + b1 Z1通过提取一系列的z1获得 在此例中 是z11 z12 直到z1m 直到z1m 把它们堆叠在一起从而获得Z1 所以Z1的维度不是n1×1 而是n1×m 且m的大小由我们训练集决定 W1的维度保持一致 仍然是n1×n0 对于X 其维度并不是n0×1 现在是所有的训练样本水平堆叠 所以维度是n0×m 我们注意到当我们 用一个n1×n0矩阵乘以一个n0×m矩阵 实际上会得到一个维度为n1×m的矩阵 现在 最后的细节是b1的维度仍然是n1×1 但是如果你用这个b 那么通过Python自适应特性 它将会被重复以扩展成n1×m矩阵 然后添加此扩展元素 在上一张幻灯片中 我们谈到w b dw以及db的维度 这里 我们看到zL以及aL的维度都是nL×1 zL以及aL的维度都是nL×1 在此 大写ZL和AL的维度都是nL×m 大写ZL和AL的维度都是nL×m 特别地 当L为0时 此时A0 它的维度等于训练集输入特征X 所以为n0×m 当然 当我们在反向传播中应用它时 在后续中将会提到 我们最终会计算dZ和dA 显而易见 它们的维度和Z以及A保持一致 它们的维度和Z以及A保持一致 所以我希望这些小练习能够帮助区分 实际使用中不同矩阵的维度 当我们在深度神经网络中采用反向传播时 请检查一遍你的代码
确保所有的矩阵维度保持一致 这会减少你代码中的错误 这会减少你代码中的错误 我希望这些检查矩阵维度的练习对你有用 我希望这些检查矩阵维度的练习对你有用 当我们用到深度神经网络时 如果你能一直保持这些不同矩阵以及向量维度的一致性 这将会帮助你减少bug 这对我确实很有效 所以接下来 我们都已经知道<br />神经网络中的前向传播的机制 所以接下来 我们都已经知道<br />神经网络中的前向传播的机制 但是为什么深度神经网络如此有效? 为什么它们比浅层网络效果要好? 让我们花点时间在下部视频中探讨它
GTC字幕组翻译