在上段影片中您看到如何計算
神經網路的預估 給予一個訓練例子 在這段影片，您看到如何用向量化
跨越多重訓練例子 而結果會是很類似
您在羅吉斯迴歸分析看到的 將不同訓練例子
疊在矩陣不同的列上 您可以用
從前面影片得到方程式 只要小小修改
就可以用在神經網路 同時計算所有的例子 讓我們來看看
如何運作的細節 這是我們從前面影片得到四個方程式
您如何來計算 z[1] a[1], z[2] 跟 a[2] 而這告訴您
給予一個輸入特徵 x 您可以用它來產生 a[2] = y-hat 
對於單一訓練例子 如果您有 m 個訓練例子
您需要重複這個程序 第一個訓練例子 x上標 (1) 來計算 y-hat (1), 您
第一個訓練例子的預估 x(2) 用它來產生
預估 y-hat(2) 等等直到 x(m) 
產生預估 y-hat(m) 用啟動
函數的記號 我將它寫成 a[2](1) 這是 a[2](2) 跟 a[2](m) 這個記號 a[2](i) 這圓括號我指的是
訓練例子 i 這方括號 2
指的是第二層 所以這是方括號
跟圓括號的作用 建議如果您有
非向量化的建置 想要計算
所有訓練例子的預估 您需要做 for i = 1 to m 然後基本上建置
這四個方程式 您需要使 z[1](i) = W[1] x(i) + b[1] a[1](i) = S形函數 of z[1](i) z[2](i) = w[2]a[1](i) + b[2], z[2](i) 等於 w[2]a[1](i) + b[2} a[2](i) 等於S形函數 of z[2](i) 基本上將上面這四個方程式
加上上標圓括號 到所有變數
根據相對的訓練例子 所以加這個上標圓括號
 i 到 x, z 跟 a 如果您想要計算所有的
您 m 個訓練例子的輸出 我們想做的是向量化這整個
計算，來去掉 for 迴圈 順便說一下, 如果我用了很多基本的 線性代數, 是因為
能夠 正確的建置它是
在深度學習時很重要的事 我們實際上
在這門課程很小心選擇符號 讓向量化
步驟盡可能容易些 我希望透過這些
本質會真正幫助您 更快的正確建置
的這些可行的演算法 讓我複製這整個
程式到下一張投影片 然後我們來看如何做向量化 這是我們從
前面投影片複製來的用 迴圈經過所有 m 個訓練例子 記得我們定義
矩陣 X 等於 將我們訓練例子
用列疊起來 拿這些訓練例子
用列疊起來 這變成 n 或者說 nx 乘 m 維度矩陣 我將直接給您答案
告訴您需要建置什麼 來完成取代這個迴圈的向量化 實際上您需要做的是計算 Z[1] = W[1] X + b[1] A[1] = S形函數 of Z[1] Z[2] = W[2]乘 A[1] + b[2] 然後 A[2] = S形函數 of Z[2] 如果您要比喻的話
我們從小寫向量 x 到大寫 X 矩陣
用小寫 x 疊在不同的列上 如果您對 z 
做同樣的事 如果您拿 z[1](1), z[1](2), 等等 這些都是列向量
直到 z[1](m) 這個是
將第一個跟所有這些 m 個用列疊起來 這會給您矩陣 Z[1] 同樣的您看
這個量 拿 a[1](1), a[1](2) 等等到 a[1](m) 然後用列疊起來 這個就像我們從
小寫 x 到大寫 X, 小寫 z 到大寫 Z 這個從小寫 a 
一個向量到這個大寫 A[1] 就在那裡
同樣的 Z[2] 跟 A[2] 它們也是
用這些向量 水平疊以來 拿這些向量
水平疊起來 為了得到 Z[2] 跟 A[2] 這個記號的一個屬性
可能可以幫助 您的是想像
這個矩陣 Z 跟 A 我們水平的
索引整個訓練例子 所以這是為什麼水平索引
相對於不同的訓練例子 當您從左邊掃到右邊您
正好也掃過所有的訓練集 而垂直的這個垂直索引
相對於 神經網路不同節點 舉個例子，這個點
這個在最左上的點 在矩陣上是
相對於 第一個訓練例子的第一個隱藏單元的啟動值 下面一項是相對於 第一個訓練例子第二個隱藏單元的啟動值 然後是第一個訓練例子第三個隱藏單元等等 所以當您向下掃這是您
索引的隱藏單元數字 而如果您往水平移動
您將從第一個隱藏單元 第一個訓練例子第一個隱藏單元 第二個訓練例子
第三個訓練例子 等等直到這一個節點相對於 最後訓練例子的的第一個隱藏單元
第 m 個訓練例子 矩陣水平
掃過每一個訓練例子 垂直方向不同
的矩陣索引 相對於不同的隱藏單元 類似的直觀
對於矩陣 Z 跟 X 也是一樣 水平相對於
不同的訓練例子 垂直相對於
不同的輸入特徵 其實是不同的
輸入層節點在神經網路上 所以用這些方程式, 您知道
如何建置您的網路 向量化, 也就是
用向量化來橫跨多重例子 在下一段影片, 我想要
給一點理由為什麼 這種向量化
是正確的建置 實際上會是
類似於您看過的羅吉斯迴歸分析的理由 讓我們進入下一段影片