في آخر فيديو، وصفنا المقصود بالشبكة العصبية العميقة
ذات الطبقة L، وتحدثنا أيضًا عن الرمز الذي نستخدمه
لوصف هذه الشبكات. في هذا الفيديو، ترون
كيف يمكنكم تنفيذ الانتشار الأمامي، في شبكة عميقة. كالعادة، دعونا أولاً نتناول كيف سيبدو الانتشار الأمامي
لمثال تدريبي واحد x، وبعد ذلك، سنتحدث عن
الإصدار الذي تم تحويله إلى متجه، حيث تريدون تنفيذ
الانتشار الأمامي في مجموعة التدريب الكاملة
في الوقت ذاته. ولكن بإعطاء مثال تدريبي واحد x، إليكم كيفية حساب حالات
التنشيط للطبقة الأولى. لذلك فإنه لهذه الطبقة الأولى، تحسبون z1 يساوي w1 مضروبًا في x زائد b1. لذلك فإن w1 وb1 هما المعلمتان اللتان
تؤثران على حالات التنشيط في الطبقة الأولى. وهذه الطبقة الأولى للشبكة العصبية، وبعد ذلك تحسبون حالات التنشيط
لهذه الطبقة لتساوي g من z1. تعتمد دالة التنشيط g
على الطبقة التي تكون موجودًا فيها وربما المؤشر المعيّن كدالة
تنشيط من الطبقة الأولى. لذلك إذا قمتم بذلك، فأنتم الآن
قد حسبتم حالات التنشيط للطبقة الأولى. وماذا عن الطبقة الثانية؟ لنفترض هذه الطبقة. حسنًا، قد تحسبون حينها z2 تساوي w2 a1 زائد b2. إذًا، فإن تنشيط الطبقة الثانية هو
المصفوفة y مضروبة في مخرجات الطبقة الأولى. حسنًا، فإنها هذه القيمة، زائد متجه الانحياز للطبقة الثانية. ثم a2 تساوي دالة التنشيط
المطبقة على z2. حسنًا؟ هذا كل شيء للطبقة الثانية، وهكذا باستمرار. حتى تصلوا إلى الطبقة العلوية،
وهي الطبقة الرابعة. حيث سيكون لديكم z4 يساوي المعلمات لهذه الطبقة مضروبة في
حالات التنشيط من الطبقة السابقة، زائد متجه الانحياز هذا. وبالمثل، a4 يساوي g من z4. وهكذا يمكنكم حساب
الإخراج التقديري y hat. لذلك، فهناك شيء يمكنكم ملاحظته، x هنا يساوي أيضًا a0، لأن متجه ميزة الإدخال x
يمثل أيضًا حالات تنشيط الطبقة صفر. لذلك فإننا نقوم بكشط x. وعندما أقوم بشطب x ووضع a0 هنا، فإن جميع هذه المعادلات
تبدو مشابهة إجمالاً. القاعدة العامة أن zl يساوي wl مضروبًا في a من l ناقص 1 زائد bl. وهو واحد هنا، إذًا، حالات تنشيط هذه الطبقة هي دالة التنشيط المطبقة
على قيم z. فهذه هي معادلة
الانتشار الأمامي العامة. حيث قمنا بكل ذلك
لمثال تدريبي واحد. ماذا عن القيام بذلك بطريقة تنطوي على التحويل لمتجه
لمجموعة التدريب الكاملة في نفس الوقت؟ تبدو المعادلات متشابهة للغاية كما كانت من قبل. بالنسبة للطبقة الأولى، سيكون لديكم
Z1 بحرف كبير يساوي w1 مضروبًا في X بحرف كبير زائد b1. وبعد ذلك، A1 يساوي g من Z1. يجب أن نضع في اعتبارنا أن X تساوي A0. وهذه هي الأمثلة التدريبية
مكدسة في أعمدة مختلفة. يمكنكم أخذ ذلك، دعوني أقم بكشط X، ويمكن وضع A0 هناك. إذًا للطبقة التالية، يبدو الأمر مشابهًا، Z2 يساوي w2 A1 زائد b2 وA2 يساوي g من Z2. نأخذ فقط هذه المتجهات
z أو a وما إلى ذلك، ونعمل على تكديسها. هذا هو المتجه z
للمثال التدريبي الأول، المتجه z
للمثال التدريبي الثاني، وهكذا، وصولاً إلى
المثال التدريبي nth، حيث نقوم بتجميع ذلك في أعمدة
ونطلق عليها Z بحرف كبير. وبالمثل، بالنسبة لـ A بحرف كبير، مثل X بحرف كبير. جميع الأمثلة التدريبية هي
متجهات أعمدة مكدسة من اليسار إلى اليمين. في هذه العملية، تنتهون بـ
y hat والتي تكون مساوية لـ g من Z4، وهذا أيضًا يساوي A4. هذه التوقعات في كل
أمثلة التدريب الخاصة بكم مكدسة أفقيًا. لذلك فقط لتلخيص ما يتعلق بالرمز، سأقوم بتعديل ذلك بالأعلى هنا. يتيح لنا الرمز استبدال z وa بحرف صغير
بنظيراتهما من الحروف الكبيرة، هل يبدو هذا بالفعل كحرف Z كبير. هذا يمنحكم الإصدار المحول إلى موجه من الانتشار الأمامي الذي تقومون بتنفيذه
في مجموعة التدريب بالكامل في كل مرة، حيث A0 هي X. الآن إذا نظرتم إلى هذا
التنفيذ الخاص بالتحويل إلى متجه، فيبدو أنه ستكون
هناك حلقة For هنا. لذلك فإن l يساوي ‎1-4. بالنسبة لـ L يساوي 1 من خلال حرف L كبير. يتعين عليكم
حساب حالات التنشيط للطبقة الأولى، ثم للطبقة الثانية، وبعد ذلك للطبقة الثالثة، وبعد ذلك الطبقة الرابعة. لذلك، يبدو أن هناك حلقة For هنا. أعلم أنه عند تطبيق
الشبكات العصبية، فإننا نرغب عادة في التخلص
من حلقات For الواضحة. ولكن هذا من الأوضاع التي لا أعتقد أن هناك أي طريقة لتنفيذ هذا
بدون حلقة For واضحة. لذلك عند تنفيذ الانتشار الأمامي، فسيكون من المقبول تمامًا وجود حلقة For
لحساب حالات التنشيط للطبقة الأولى، ثم للطبقة الثانية، وبعد ذلك للطبقة الثالثة،
وبعد ذلك الطبقة الرابعة. لا أحد يعلم، ولا أعتقد أن
هناك أي طريقة للقيام بذلك بدون حلقة For تنتقل
من واحد إلى حرف L كبير، من واحد حتى العدد الإجمالي
للطبقات في الشبكة العصبية. لذلك، في هذا الوضع، سيكون من
المقبول تمامًا وجود حلقة For واضحة. حسنًا، هذا كل شيء للرمز
للشبكات العصبية العميقة، وكذلك كيفية تنفيذ الانتشار الأمامي
في هذه الشبكات. إذا كانت الأجزاء التي شاهدناها حتى الآن
تبدو مألوفة نوعًا ما بالنسبة لكم، فذلك لأن ما نشاهده هو أخذ
جزء مماثل جدًا لما شاهدتموه في الشبكة العصبية مع طبقة مخفية واحدة
وتكرار ذلك مرات أخرى. وتبيّن الآن أننا نقوم
بتطبيق شبكة عصبية عميقة، ومن طرق زيادة احتمال
التمتع بتطبيق خالٍ من الأخطاء هي التفكير بطريقة منظمة للغاية وبعناية فيما يتعلق بأبعاد المصفوفة
التي تعملون من خلالها. لذلك عندما أحاول تصحيح أخطاء تعليماتي البرمجية، فإنني غالبًا أسحب ورقة، وأفكر بعناية بخصوص، أبعاد المصفوفة التي أتعامل معها. لنتعرف على كيف يمكنكم القيام
بذلك في الفيديو التالي.