1
00:00:00,060 --> 00:00:04,380
여러분이 신경망을 효과적으로 발전시키키

2
00:00:02,669 --> 00:00:06,870
위해서는, 파라미터
뿐만 아니라

3
00:00:04,380 --> 00:00:09,269
하이퍼 파라미터 또한 잘 조직화

4
00:00:06,870 --> 00:00:11,759
시켜야합니다. 그러면
하이퍼 파라미터s는

5
00:00:09,269 --> 00:00:15,170
무엇일까요? 한번
살펴보겠습니다.

6
00:00:11,759 --> 00:00:17,820
여러분 모델의 파라미터는 W와 B입니다.

7
00:00:15,170 --> 00:00:21,720
여러분의 알고리즘에게 알려줘야하는

8
00:00:17,820 --> 00:00:26,220
것들이 있는데요, 러닝속도 a

9
00:00:21,720 --> 00:00:28,920
와 같은 것 말이죠. a의

10
00:00:26,220 --> 00:00:32,329
값을 잘 지정해서

11
00:00:28,920 --> 00:00:34,890
결과적으로 이 a값이
여러분의 파라미터가

12
00:00:32,329 --> 00:00:38,190
어떻게 진화하는지 또는

13
00:00:34,890 --> 00:00:40,170
기울기 강하의
iterarion의 수가 필요할 수 있죠,

14
00:00:38,190 --> 00:00:42,629
러닝 알고리즘에서 진행되는 iteration
말이죠

15
00:00:40,170 --> 00:00:47,340
또는 숨겨진 레이어의 개수들을

16
00:00:42,629 --> 00:00:50,629
알려줘야 할 수도 있습니다. 이것을

17
00:00:47,340 --> 00:00:56,039
대문자 L 또는 숨겨진
유닛의 개수라고 하겠습니다.

18
00:00:50,629 --> 00:00:59,670
여기에서처럼 0, 1, 그리고 2 등등 말이죠

19
00:00:56,039 --> 00:01:03,329
또한, activation
function을 어떤 것을

20
00:00:59,670 --> 00:01:05,610
고를지에 대한 선택권도 있습니다.

21
00:01:03,329 --> 00:01:06,869
즉, ReLu 함수를
이용할지, tahn h 또는

22
00:01:05,610 --> 00:01:11,760
시그모이드 함수를 쓸지 말이죠.

23
00:01:06,869 --> 00:01:13,590
특히 숨겨진 레이어에서 말이죠.

24
00:01:11,760 --> 00:01:15,990
이런 모든 것들이 여러분이

25
00:01:13,590 --> 00:01:19,640
러닝 알고리즘에 제공해야 하는 것들인데요.

26
00:01:15,990 --> 00:01:22,200
이런 파라미터들이
ultimate 파라미터인

27
00:01:19,640 --> 00:01:25,640
W와 B를 컨트롤합니다. 그렇기
때문에

28
00:01:22,200 --> 00:01:29,340
여기 밑에있는 모든 것들을 하이퍼 파라미터라고 합니다.

29
00:01:25,640 --> 00:01:30,750
여기 알파와 같은

30
00:01:29,340 --> 00:01:32,369
학습속도, iteration 숫자,

31
00:01:30,750 --> 00:01:36,000
숨겨진 레이어의 수, 등등

32
00:01:32,369 --> 00:01:39,290
이런 파라미터들은 W와 B를 컨트롤합니다.

33
00:01:36,000 --> 00:01:41,970
그러므로 이것들을 하이퍼 파라미터라고 합니다.

34
00:01:39,290 --> 00:01:44,250
이런 하이퍼 파라미터들이

35
00:01:41,970 --> 00:01:46,950
최종적으로 W와 B와 같은

36
00:01:44,250 --> 00:01:50,100
파라미터의 값을 결정짓기 때문에,

37
00:01:46,950 --> 00:01:53,520
딥러닝에서는 여러 개의

38
00:01:50,100 --> 00:01:55,470
다양한 하이퍼 파라미터가 상존하는데요,

39
00:01:53,520 --> 00:01:57,899
나중에서 다루는 코스에서는

40
00:01:55,470 --> 00:02:02,990
모멘텀 요서, 그리고 the 미니 배치 사이즈,

41
00:01:57,899 --> 00:02:02,990
또, 다양한 일반화와
같은 유형의

42
00:02:05,150 --> 00:02:13,020
하이퍼 파라미터를

43
00:02:07,220 --> 00:02:14,700
보도록 하겠습니다. 여기
밑에

44
00:02:13,020 --> 00:02:16,020
있는 용어들이 잘 이해되지 않으시면,

45
00:02:14,700 --> 00:02:18,810
너무 걱정하지 마십시요, 이
내용은

46
00:02:16,020 --> 00:02:21,870
2번째 코스에서 다루도록 하겠습니다.

47
00:02:18,810 --> 00:02:24,120
딥러닝 분야에서는 하이퍼 파라미터의

48
00:02:21,870 --> 00:02:26,370
종류가 머신 러닝 초기 시점과 비교 했을 때,

49
00:02:24,120 --> 00:02:28,890
굉장히 많습니다. 저는
러닝속도인

50
00:02:26,370 --> 00:02:31,050
알파 하이퍼 파라미터를 부르는데

51
00:02:28,890 --> 00:02:33,480
일관되게 하겠습니다.

52
00:02:31,050 --> 00:02:35,250
파라미터 라고 부르지 않고요.
이전에,

53
00:02:33,480 --> 00:02:37,920
머신 러닝 초반부에는 하이퍼 파라미터의

54
00:02:35,250 --> 00:02:39,600
개수가 그리 많지 않았기 때문에

55
00:02:37,920 --> 00:02:42,120
종사자들은 이 내용과 관련해서 조금

56
00:02:39,600 --> 00:02:44,610
천천히 받아들였습니다. 그래서
그냥

57
00:02:42,120 --> 00:02:47,580
파라미터라고 불렀는데요. 엄밀히
이야기하면

58
00:02:44,610 --> 00:02:50,280
알파는 리얼 파라미터가 맞긴 맞습니다.

59
00:02:47,580 --> 00:02:51,570
파라미터를 결정하는 파라미터죠.

60
00:02:50,280 --> 00:02:54,180
알파와 같은 거나,

61
00:02:51,570 --> 00:02:55,769
iteration의 수 등등 의 하이퍼 파라미터 말이죠.

62
00:02:54,180 --> 00:02:57,810
그러므로 여러분 개인 어플의

63
00:02:55,769 --> 00:02:59,940
깊은 네트워크를 트레이닝하는 경우에

64
00:02:57,810 --> 00:03:01,560
여러분의 하이퍼 파라미터를 사용할 수 있는

65
00:02:59,940 --> 00:03:04,440
다양한 설정값이 있기 때문에,
단순히

66
00:03:01,560 --> 00:03:07,230
시도를 해봐야 알 수 있습니다.
그렇기

67
00:03:04,440 --> 00:03:09,840
때문에 오늘날의 Applied
Deep Learning 절차는

68
00:03:07,230 --> 00:03:12,150
여러분이 어떤 아이디어가 있으면

69
00:03:09,840 --> 00:03:13,549
예를 들어, 학습
속도에 대한 최적값을

70
00:03:12,150 --> 00:03:16,739
생각하고 있으면, 알파의
값이

71
00:03:13,549 --> 00:03:20,670
0.01이라고 할 수 있겠죠. 이 값을 시도해보고 싶을 수 있습니다.

72
00:03:16,739 --> 00:03:22,530
그러면 도입해서, 시도해보고,

73
00:03:20,670 --> 00:03:23,910
어떻게 작동하는지 볼 수 있습니다.

74
00:03:22,530 --> 00:03:25,890
그리고 그 결과를 바탕으로, 그
값을

75
00:03:23,910 --> 00:03:28,620
바꾸고 싶을 수 있습니다.

76
00:03:25,890 --> 00:03:30,930
학습속도를 0.05로
증가시키고 싶을 수 있겠죠.

77
00:03:28,620 --> 00:03:32,970
만약 러닝 속도를 어떤 값으로

78
00:03:30,930 --> 00:03:35,010
해야 최고일지 잘 모르겠으면,

79
00:03:32,970 --> 00:03:37,680
하나의 알파 값을 시도해보고

80
00:03:35,010 --> 00:03:39,690
j 비용함수가
이렇게 내려가는 것을

81
00:03:37,680 --> 00:03:41,820
직접 본 후에, 조금
더 큰

82
00:03:39,690 --> 00:03:43,650
값의 알파를 러닝 속도로 지정할 수 있겠죠.

83
00:03:41,820 --> 00:03:45,060
그렇게 했는데 비용함수가 폭발적으로

84
00:03:43,650 --> 00:03:47,250
변화를 일으켜 그래프가 갈라지게 되면,

85
00:03:45,060 --> 00:03:49,709
다른 버전을 시도해서 빨리 내려가는 것을 볼 수도 있겠죠.

86
00:03:47,250 --> 00:03:51,780
이 경우 큰 값의 반대이기 때문에

87
00:03:49,709 --> 00:03:53,670
또 다른 버전을 시도해서 지켜봅니다.

88
00:03:51,780 --> 00:03:55,530
비용함수가 이러는 것을 보게 되겠죠.

89
00:03:53,670 --> 00:03:57,840
값을 시도해본 다음에,

90
00:03:55,530 --> 00:04:00,870
여기 이 알파의 값이 꽤 빠른

91
00:03:57,840 --> 00:04:02,790
러닝 속도를 주고,

92
00:04:00,870 --> 00:04:04,290
더 낮은 j 비용함수로
수렴하게 만들어주기

93
00:04:02,790 --> 00:04:06,720
때문에 여기 이 알파 값을 이용할 것

94
00:04:04,290 --> 00:04:08,040
이라고 이야기할 수 있겠죠

95
00:04:06,720 --> 00:04:10,170
이전 슬라이드에서 봤듯이,여러가지 종류의 hybrid 파라미터가

96
00:04:08,040 --> 00:04:11,489
있습니다. 그리고
여러분이

97
00:04:10,170 --> 00:04:13,830
새로운 어플을 시작하는

98
00:04:11,489 --> 00:04:15,450
경우에, 저는 사실

99
00:04:13,830 --> 00:04:17,940
가장 좋은 하이퍼 파라미터 의 값이

100
00:04:15,450 --> 00:04:20,580
어떤 것인지 미리 그 값을 알아내는 것이

101
00:04:17,940 --> 00:04:22,169
매우 어렵다고 생각합니다. 그러므로

102
00:04:20,580 --> 00:04:24,570
어떤 일이 벌어지냐면, 여러분이

103
00:04:22,169 --> 00:04:26,970
다양한 값들을 시도해보는 방법 밖에는 없습니다.

104
00:04:24,570 --> 00:04:28,440
이 동그란 싸이클을 돌면서, 예를
들어,

105
00:04:26,970 --> 00:04:31,140
5개의 숨겨진 레이어를 시도해보고, 이 만큼의

106
00:04:28,440 --> 00:04:34,140
숨겨진 레이어를 시도해보고, 이것을
도입해서

107
00:04:31,140 --> 00:04:36,180
잘 되는지 확인하고, 시도를
반복하는 것입니다.이 슬라이드의 제목은
"apply

108
00:04:34,140 --> 00:04:38,340
deep learning is very

109
00:04:36,180 --> 00:04:40,740
empirical process" 인데요,

110
00:04:38,340 --> 00:04:42,419
empirical process라고 하는 것은 여러 가지

111
00:04:40,740 --> 00:04:45,330
다른 것들을 시도해보고 어떤 것이 잘

112
00:04:42,419 --> 00:04:47,190
작동하는지 확인하는 것을 멋스럽게 표현한 것입니다.제가 추가적으로 본 효과는

113
00:04:45,330 --> 00:04:48,810
오늘날의 딥러닝이 컴퓨터 비전,

114
00:04:47,190 --> 00:04:51,990
음성인식, 자연언어처리와
같은

115
00:04:48,810 --> 00:04:53,789
수 많은 다양한 문제에 적용되고

116
00:04:51,990 --> 00:04:55,500
있다는 점입니다. 또한,

117
00:04:53,789 --> 00:04:59,250
구조적 데이터 어플인

118
00:04:55,500 --> 00:05:02,430
온라인 광고 또는 웹서칭 또는

119
00:04:59,250 --> 00:05:05,640
제품 추천 등등과 같은 것에 적용될 수도 있죠

120
00:05:02,430 --> 00:05:08,190
제가 봤던 것으로 비추어 보았을 때,

121
00:05:05,640 --> 00:05:10,080
한가지 특정 분야의 리서치 연구원들이

122
00:05:08,190 --> 00:05:12,060
다른 분야로 가려고 하는데

123
00:05:10,080 --> 00:05:14,400
가끔은 하이퍼 파라미터에 대한 직관은

124
00:05:12,060 --> 00:05:16,590
이어서 연계되는 경우도 있고,

125
00:05:14,400 --> 00:05:17,849
다른 때에는 안 그런 경우도 있습니다.그러므로 저는

126
00:05:16,590 --> 00:05:20,970
특히 새로운 문제를 시작하는 사람들에게

127
00:05:17,849 --> 00:05:23,550
여러가지 범위의 값들을

128
00:05:20,970 --> 00:05:25,500
먼저 시도해보고 어떤 것이 잘 되는지 확인하라고말씀 드립니다.

129
00:05:23,550 --> 00:05:27,930
다음 코스에서는 시스템적인 방법을 보겠습니다.

130
00:05:25,500 --> 00:05:30,780
여러가지 범위의 값들을 시도하는

131
00:05:27,930 --> 00:05:32,070
시스템적인 방법을 살펴볼 텐데요,

132
00:05:30,780 --> 00:05:33,570
둘째로, 여러분이
한가지 어플에

133
00:05:32,070 --> 00:05:35,220
오랜 기간 작업을 하더라도,

134
00:05:33,570 --> 00:05:37,979
온라인 광고와 같은 분야에서 오랫동안

135
00:05:35,220 --> 00:05:39,930
작업할 수도 있겠죠. 이런
작업을 하면서

136
00:05:37,979 --> 00:05:41,580
가장 최적의 값이 변할 수도 있는데요

137
00:05:39,930 --> 00:05:43,830
러닝속도나,

138
00:05:41,580 --> 00:05:46,440
숨겨진 유닛의 총 개수 등의 값들이 업무를 진행하는 단계에서변할 수도 있습니다.

139
00:05:43,830 --> 00:05:49,229
만약 오늘 가장 최적의 하이퍼 파라미터의 값으로

140
00:05:46,440 --> 00:05:51,750
시스템을 튜닝한다고 하더라도,

141
00:05:49,229 --> 00:05:53,430
가장 최적의 값이 지금으로부터

142
00:05:51,750 --> 00:05:55,650
1년후에는 변할 수도 있습니다.

143
00:05:53,430 --> 00:05:57,840
컴퓨터 인프라가 변해서, 예를
들어,

144
00:05:55,650 --> 00:05:59,789
CPU 나 GPU와 같은 것이

145
00:05:57,840 --> 00:06:01,560
완전히 변해서 그럴 수 있습니다. 그러나

146
00:05:59,789 --> 00:06:03,659
한가지 적용할 수 있는 경험의 근거한 규칙은

147
00:06:01,560 --> 00:06:05,070
간간히 몇 개월마다

148
00:06:03,659 --> 00:06:06,659
여러분이 어떤 특정 문제를

149
00:06:05,070 --> 00:06:09,030
장기간 작업한다고 하면, 또는

150
00:06:06,659 --> 00:06:10,800
수년 간 작업하는 경우도 말이죠,

151
00:06:09,030 --> 00:06:12,570
일단 먼저 몇개의 값을 하이퍼 파라미터 값을 시도해보고

152
00:06:10,800 --> 00:06:15,150
더 좋은 값의 하이퍼 파라미터가 있는지도

153
00:06:12,570 --> 00:06:17,280
확인해봅니다. 이렇게
계속 진행을

154
00:06:15,150 --> 00:06:18,779
하다보면 하이퍼 파라미터에 대한 직관적인 이해도가

155
00:06:17,280 --> 00:06:19,870
천천히 늘어나고 어떤 것이 본인 문제에 가장 적합한지

156
00:06:18,779 --> 00:06:21,820
배울 수 있습니다.

157
00:06:19,870 --> 00:06:24,010
저는 이렇게 계속해서

158
00:06:21,820 --> 00:06:25,510
반복적으로 여러가지 하이퍼 파라미터값을

159
00:06:24,010 --> 00:06:27,940
시도해봐야 하는 이런 부분이

160
00:06:25,510 --> 00:06:30,160
매우 불만족스러운 부분일 수도 있다고 생각하는데요,

161
00:06:27,940 --> 00:06:32,200
이 부문은 어쩌면 리서치 부분의

162
00:06:30,160 --> 00:06:33,850
딥러닝에서는 아직도 발전하고 있는 분야이기 때문에

163
00:06:32,200 --> 00:06:36,190
어느 정도 시간이 지나면, 어떤

164
00:06:33,850 --> 00:06:38,350
하이퍼 파라미터가 가장 좋은 값인지 더 나은

165
00:06:36,190 --> 00:06:41,260
가이드를 해줄 수도 있을 것입니다.

166
00:06:38,350 --> 00:06:43,630
하지만 동시에 CPU 와 GPU와 네트워크, 그리고

167
00:06:41,260 --> 00:06:45,910
데이터 세트들이 항상 변하기 때문에

168
00:06:43,630 --> 00:06:47,680
특정 가이드라인이 한동안은 수렴하지 않을

169
00:06:45,910 --> 00:06:49,360
가능성도 있습니다. 그렇기
때문에

170
00:06:47,680 --> 00:06:50,860
계속 다른 값들을 시도해보고

171
00:06:49,360 --> 00:06:52,479
cross-validation set와 같은 것을 통해

172
00:06:50,860 --> 00:06:54,100
평가하고 그렇게 해서

173
00:06:52,479 --> 00:06:56,350
여러분의 문제에 가장 잘 맞는

174
00:06:54,100 --> 00:06:58,870
값을 고르도록 합니다. 이번에

175
00:06:56,350 --> 00:07:01,030
다룬 내용은 하이퍼 파라미터에 관련 내용이 였는데요.

176
00:06:58,870 --> 00:07:03,280
두번째 코스에서는 시스템적으로

177
00:07:01,030 --> 00:07:06,040
어떻게 하이퍼 파라미터의 공간을 탐구하는

178
00:07:03,280 --> 00:07:07,570
방법을 몇 가지 알아보겠습니다.

179
00:07:06,040 --> 00:07:09,430
그렇지만 이제 여러분은 이미

180
00:07:07,570 --> 00:07:11,470
연습문제를 진행하기 위한 충분한 도구를

181
00:07:09,430 --> 00:07:14,050
가지고 있습니다. 연습문제를

182
00:07:11,470 --> 00:07:16,150
진행하기 앞서서 한가지의 아이디어를

183
00:07:14,050 --> 00:07:18,660
더 공유할 텐데요. 바로
딥러닝이

184
00:07:16,150 --> 00:07:18,660
어떻게 인간의 두뇌와 관련 있는지.