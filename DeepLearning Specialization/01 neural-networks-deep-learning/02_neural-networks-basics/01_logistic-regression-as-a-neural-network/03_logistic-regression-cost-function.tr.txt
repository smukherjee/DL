Bir önceki videoda, lojistik bağlanım(regresyon) modelini gördünüz. Lojistik regresyon modelinin W ve B parametrelerini eğitmek için, Bir maliyet fonksiyonu tanımlamanız gerekir. Lojistik regresyonu eğitmek için kullanabileceğiniz maliyet fonksiyonuna bir göz atalım. Tekrarlamak için, önceki slayttan bulmamız gereken şey bu. Yani çıkış y-şapka(hat) w'nin sigmoidi Z'nin bir sigmoidinin burada tanımlandığı gibi x artı b'yi aktarın. Yani, modelinizdeki parametreleri öğrenmeniz için size m eğitim örneklerinden oluşan bir eğitim seti verilir ve W ve B parametrelerini bulmak istemeniz doğal görünür, böylece en azından eğitim setinde çıktılara sahip olursunuz. Eğitim setinde sahip olduğunuz tahminler, -bizim sadece y-şapka (i) olarak yazdığımız- eğitim setindeki kesin referans etiketleri y_i'ye yakın olacak. Üstteki denklem için biraz daha ayrıntı eklemek için, y-şapka'nın yukarıda tanımlandığı gibi olduğunu söyledik, alıştırma örneği olarak x ve elbette her bir alıştırma örneği için, bu üstyazıları indekslemek ve örnekleri ayırt etmek için parantez içeren parantezlerle kullanırız. Y-şapka (i) olan eğitim örneğindeki (i) tahmininiz sigmoid fonksiyonunu alarak ve onu W transpoze X'e uygulayarak elde edilir, (i) alıştırma örneğinin girdisi artı V ve ayrıca Z (i)'yi aşağıda belirtildiği gibi tanımlayabilirsiniz. Z (i), W aktarılmış x (i) artı b'ye eşittir. Bu ders boyunca, üstyazı (i)'nin datayı işaret ettiği bu notasyon evrişimini kullanacağız. X veya Y veya Z veya i'ninci alıştırma örneğiyle ilişkili başka bir şey, i'ninci örnekle ilişkilidir. Üstyazı (i)'nin anlamı budur. Şimdi, algoritmamızın ne kadar iyi çalıştığını ölçmek için hangi kayıp fonksiyonunu ya da hata fonksiyonunu kullanabileceğimizi görelim. Yapabileceğiniz bir şey, algoritmanız y-şapkayı ve doğru etiketi Y olarak çıkardığında belki kare hatası veya bir yarım kare hatası olduğunda kaybı tanımlamaktır. Bunu yapabileceğiniz ortaya çıkar, ama lojistik regresyonda insanlar genellikle bunu yapmazlar çünkü parametreleri öğrendiğinizde, daha sonra konuştuğumuz optimizasyon probleminin konveks olmadığına karar verirsiniz. Yani, birden çok yerel optima ile optimizasyon problemi ile sonuçlanırsınız. Yani dereceli alçalma küresel optimumu bulamayabilir. Son birkaç yorumu anlamadıysanız. Bunun için endişelenmeyin, daha sonraki videoda bundan bahsedeceğiz. Ancak, uzaklaşmanın sezgisi, L olarak adlandırılan bu kayıp fonksiyonu sizin gerçek etiket y olduğunda çıktı y-şapka'nın ne kadar iyi olduğunu ölçmek için tanımlamanız gereken bir fonksiyondur. Kare hatası makul bir seçim gibi görünse de Gradyan düşümünü iyi çalıştırmaz. Yani lojistik regresyonda, aslında bize dışbükey olan bir optimizasyon sorunu verecek, kare hatasıyla benzer bir rol oynayan farklı bir kayıp işlevini tanımlayacağız. Ve böylece daha sonraki videoların optimize edilmesini daha kolay hale geldiğini göreceğiz. Yani, lojistik regresyonda kullandığımız şey aslında sıradaki kayıp fonksiyonudur, şu an burada olduğu gibi negatif y log y-şapka artı bir eksi y log, bir eksi y-şapka'dır. Bu kayıp fonksiyonunun neden mantıklı olduğuna dair bazı sezgiler var. Kare hatası kullanırsak, karenin hatalarının olabildiğince küçük olmasını istediğinizi unutmayın. Ve bu lojistik regresyon kaybı fonksiyonuyla, bunun da mümkün olduğunca küçük olmasınıisteyeceğiz. Bunun neden mantıklı olduğunu anlamak için, iki örneğe bakalım, İlk durumda, Y'nin bire eşit olduğunu varsayalım, o zaman kayıp fonksiyonu y-şapka virgül y, bu negatif işaretin yazılması için uygundur. Eğer y bire eşitse negatif log y-şapkadır. Çünkü eğer y bire eşitse, o zaman ikinci terim bir eksi Y sıfıra eşittir. Yani, eğer y eşittirse, negatif log y-şapka'nın mümkün olduğunca büyük olmasını istersiniz. Öyleyse, y-şapka'nın büyük olmasını istiyorsunuz, mümkün olduğunca büyük olmasını ve bu y-şapka'nın büyük olmasını istediğiniz anlamına gelir. Ama y-şapka, sigmoid fonksiyonu olduğu için, asla birinden büyük olamaz. Demek ki, eğer y bire eşitse, Y-şapkanın mümkün olduğunca büyük olmasını istiyorsunuz. Ama asla birinden daha büyük olamaz, o yüzden de y-şapkasının da bire yakın olmasını istiyoruz. Diğer durum da y eşittir sıfırdır. Y, sıfıra eşitse, kayıp fonksiyonundaki bu ilk terim sıfıra eşittir, çünkü y sıfır ve daha sonra ikinci terim, kayıp fonksiyonunu tanımlar. Böylece kayıp negatif log bir eksi y-hat olur. Ve eğer öğrenme sürecinizde kayıp fonksiyonunu küçük yapmaya çalışırsanız, Bunun nedeni, bir eksi y-hatın büyük olmasını istemeniz. Ve orada negatif bir işaret olduğu için ve benzer bir sebepten dolayı bu kayıp fonksiyonunun y-şapkayı olabildiğince küçük yapmaya çalıştığı sonucuna varabilirsiniz. Ve yine y-şapka sıfır ile bir arasında olması gerektiğinden. Bu, y sıfıra eşitse, kayıp fonksiyonunuzun, y-şapkayı mümkün olduğunca sıfıra yakın hale getirmek için parametreleri zorlayacağını söyler. Şimdi, Rafidah'ın etkisiyle çok fazla fonksiyonlar var, eğer y bire eşitse, y-şapkası büyük yapmaya çalışıyoruz ve eğer Y eşitse, y-şapkası küçük yapmaya çalışıyoruz. Burayı sadece yeşil olarak verdik Bu kayıp fonksiyonu için gayri resmi bir gerekçe sunacak daha sonra isteğe bağlı bir video ile, lojistik regresyonda neden bu özel formla kayıp işlevini kullanmak istediğimize dair daha resmi bir gerekçe sunacaktır. Son olarak, kayıp fonksiyonu tek bir eğitim örneğine göre tanımlandı. Tek bir eğitim örneğinde ne kadar iyi yaptığınızı ölçer. Şimdi maliyet fonksiyonu olarak adlandırılan bir şey tanımlayacağım, tüm eğitim setini ne kadar iyi yaptığınızı ölçen bir şey. Yani, W ve B parametrelerinize uygulanan maliyet fonksiyonu J alıştırma örneklerinin her birine uygulanan kayıp fonksiyonunun
toplamının m'lerinden biriyle ortalama olacak ve dönecek. Burada y-şapka elbette belirli bir W ve B parametre seti kullanan lojistik regresyon algoritmanızın tahmini çıktısıdır, Ve sadece bunu genişletmek için, bu eksi bir bölü m'ye eşittir toplam i eşittir birden m ye kadar kayıp fonksiyonunun tanımıdır. Yani bu y (i) Log y-şapka (i) artı bir eksi (i) bir eksi y-şapkadır (i). Sanırım buraya köşeli parantezler koyabilirim. Yani eksi işareti her şeyin dışında. Bu yüzden kullanacağım terminoloji, kayıp fonksiyonunun sadece tek bir eğitim örneğine uygulanmasıdır. Ve maliyet fonksiyonu parametrelerinizin maliyetidir. Bu nedenle lojistik regresyon modelinizi eğitirken Altta yazılı olan genel maliyet fanksiyonu J'yi en aza indiren W ve B parametrelerini bulmaya çalışacağız. Yani, az önce lojistik regresyon algoritması kurulumunu, eğitim örneği için kayıp fonksiyonunu ve, algoritmanızın parametreleri için genel maliyet fonksiyonunu gördünüz. Lojistik regresyonun çok küçük bir sinir ağı olarak görülebileceği ortaya çıkıyor. Bir sonraki videoda bunun üzerine gideceğiz, böylece sinir ağlarının ne yaptığına dair sezgiler kazanmaya başlayabilirsiniz. Bu yüzden, bir sonraki videoya, lojistik regresyonun çok küçük bir sinir ağı olarak nasıl görüntüleneceğine bakalım.