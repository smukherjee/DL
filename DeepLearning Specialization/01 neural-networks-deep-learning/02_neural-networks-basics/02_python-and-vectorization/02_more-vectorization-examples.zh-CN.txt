在上一节视频中<br />我们通过一些例子说明了如何通过内置函数 和避免使用显式的for循环来实现向量化 这可以有效地提高代码的运行速度 让我们再看几个例子 需要记住的经验之谈是 当你在编写神经网络或逻辑回归时 都要尽可能避免使用显式的for循环 虽然有时候无法完全避免使用for循环 但如果你能使用内置函数 或者找到其他方式 来计算你想要的答案 这通常会比直接使用for循环更快 我们再来看一个例子 比如你想要计算向量u 它是矩阵A和 向量v的乘积 根据矩阵乘法的定义 u_i等于A_ij*v_j 对j求和 对吧 这就是u_i的定义 非向量化的实现方法是 u=np.zeros(n, 1) u=np.zeros(n, 1) 然后对i循环 接着对j循环 然后 u[i]+=A[i][j]*v[j] 这是一个两层的for循环 分别对i和j进行循环 这是一种非向量化的实现方式 向量化的实现方式是
u=np.dot(A,v) 右边这种 向量化的实现方式 消除了两层for循环 运行速度要快得多 我们再来看一个例子 假设在内存中已经有了一个向量v 你想要对向量v中的每一个元素进行指数运算 计算u等于e^v_1 e^v_2直到 e^v_n这样一个向量 这是一种非向量化的实现方式 即首先初始化u为一个零向量 然后用一个for循环每次计算一个元素 但实际上 Python和NumPy中有许多内置函数 只需调用一个函数 就可以完成这些向量计算 所以我的实现方式是<br />import numpy as np 然后只需调用 u=np.exp(v) 注意之前的这个显式的for循环 在这里变成了一行代码 输入为向量v 输出为向量u 右边的实现方式避免了使用显式的for循环 而且比直接使用for循环更快 实际上 Numpy中有很多支持向量值的函数 比如np.log(v)会对向量中每个元素进行对数计算 np.abs()可以计算绝对值 np.maximum(v,0)会将v中的每个元素 与0相比求最大值 v**2 计算向量v中每个元素的平方 1/v计算每个元素的倒数 等等 所以每当你想要写for循环的时候 先看看是否可以通过调用 NumPy的内置函数来避免for循环 接下来 让我们将这些知识 运用到逻辑回归的梯度下降算法实现中 看看我们是否可以至少摆脱两个for循环中的一个 这是我们计算逻辑回归中的导数的代码 这里有两个for循环 一个在这里 另一个在这里 在这个例子中 n_x等于2 但如果你的特征数量超过2个 那就需要在for循环中处理 dw_1 dw_2 dw_3等等 相当于这里有一个<br />for j=1...n_x 在循环中更新dw_j 接下来我们要消灭这第二个for循环 这就是我们要在这一页做的 我们的做法是 不再将dw_1 dw_2等 显式地初始化为零 移除这一段 然后令dw成为一个向量 令dw=np.zeros((n_x,1)) 即n_x乘1的向量 接着在这里 不再使用针对单个元素的for循环 而是使用这个针对向量值的运算 dw+=x^(i)dz^(i) dw+=x^(i)dz^(i) 最后 不再使用这一段 而是使用dw/=m<br />现在 我们从两个for循环简化为只有一个for循环 我们保留了这层针对各个训练样本的for循环 我希望本节视频能给你一种向量化的概念 通过移除一个for循环 你的代码会运行地更快 但实际上我们还可以做得更好 在下节视频中 我们会进一步讨论如何对逻辑回归进行向量化 你会惊讶地看到一种没有for循环的实现方式 不需要对于训练样本的for循环 你的代码几乎可以同时处理 整个训练集 我们下个视频再见