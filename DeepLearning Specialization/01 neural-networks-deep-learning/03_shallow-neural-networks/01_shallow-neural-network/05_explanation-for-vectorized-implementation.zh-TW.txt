在先前的影片中, 我們看到您個訓練例子水平疊成了矩陣 X 您可以推導出向量化正向傳播在您的神經網路 讓我們給多一點理由為什麼這個我寫的方程式 是正確的方式來建置向量化跨越多重例子 讓我們先談一下部分的正向傳播計算在一些例子 假設對於第一個訓練例子 您的計算為 這個項目 x1 加 b1, 然後第二個訓練例子 您計算這個 x2 加 b1 第三個訓練例子 您計算為這個 x3 加 b1 所以為了簡化解釋在這個投影片, 我先忽略 b 假設, 簡化這個證明一點點設 b 等於 0 但以下我們將進行的議論還是可行 當 b 不為 0 時只要一點點改變 這只是為了要簡化這張投影片的描述而已 w1 會是某個矩陣, 是吧 ？ 我有一些行在這個矩陣 如果您看這個 x1 的計算 您有的是 w1 乘 x1 給您一些列向量您可以這樣畫 同樣地, 如果您看向量 x2 您有 w1 乘 x2 給您另一個列向量, 是吧? 這會給您 z[1](2) 最後您看 x3 您有 w1 乘 x3 給您第三個列向量, 也就是 z[1](3) 如果您看這個訓練集大寫 X 是我們用所有訓練例子疊起來 這個矩陣大寫 X 是拿向量 x1 跟 x2 還有 x3垂直疊起來 這是假設總共有三個訓練例子 如果有更多, 就像這樣繼續垂直疊起來 如果您拿這個矩陣 X 跟 W 相乘您會得到 如果您回想矩陣相乘如何作用 您會得到第一列是 相等於我用紫色畫的這些值 第二列會是跟這四個值一樣 第三列會是這些橘色的值 最終會是這樣 但當然這只是等於 z[1](1) 表示成 列向量, 接著是 z[1](2) 表示成列向量, 接著是 z[1](3) 也是列向量 而這前提是您有三個訓練例子 您有更多例子, 那您會有更多的列 這只是我們的矩陣大寫 Z1 我希望這給您一些理由為什麼我們用 前面的 W1 乘 Xi 等於 Z[1](i), 當我們只看單一訓練例子時 當您拿不同的訓練例子把它們疊在不同的列時 那這相對的結果是 z 也是疊在列上 我不證明但您可以說服您自己如果您用 Python 傳播 如果您加入 這些 b 的值, 這些值還是正確的 實際上會發生的是您用 Python 傳播 您最終會有一個個 bi 在這個矩陣的每個列上 在這個投影片, 我只說明了 z1 等於 w1x 加 b1 是 正確的向量化在 前面的投影片四個步驟中的第一個步驟 但事實上同樣的分析讓您 明白其他的步驟也是可以 用相同的邏輯來作用, 如果您將輸入疊成列在方程式中 您會得到相對的輸出也是疊在列上 最後, 讓我們回顧一下我們在這段影片中談過的 如果者是您的神經網路 我們說這是您要做的如果您在建置正向傳播 一次一個訓練例子從 i 等於 1 到 m, 然後我們說 讓我們將訓練例子疊在列上像這樣, 對於每個 Z1, A1, Z2, A2 讓我們疊在相對應的列上 這是 A1 的例子, 但對於 Z1, A1, Z2 跟 A2 也是相同 然後我們在前面的投影片說明了 這一行程式讓您向量化同時處理所有 m 個例子 同樣的原因 您也可以說明其他行的程式也是 正確的向量化對於這四行程式 只是要提醒一下 因為 X 也是等於 A0, 因為記得 輸入特徵向量 X 等於 A0, 而 xi 等於 a[0](i) 那這實際上也一定的對稱性對於 這些方程式, 第一個方程式也可以 寫成 Z[1] 等於 W[1] A[0] 加 b[1] 所以, 您看這一對方程式跟這一對 方程式實際上很類似, 只是所有指標前進 1 這也說明了神經網路的不同層 大約做同樣的事或者說做同樣的計算一再重複 這裡我們的是兩層神經網路而當我們用 更深入的神經網路在下一週的影片 您看到即使更深入的深度學習基本上也是用 這兩個步驟只是比我們這裡做的更多次 所以這是您如何向量化您的神經網路跨多重的訓練例子 下一步是, 我們目前使用 S型函數在我們的神經網路 實際上這並不是最好的選擇 在下一段影片, 我們更進一步 我們如何使用, 被稱為 啟動函數而S型函數只是其中一種選擇