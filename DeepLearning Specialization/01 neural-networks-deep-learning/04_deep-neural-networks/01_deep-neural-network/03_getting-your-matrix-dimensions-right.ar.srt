1
00:00:00,028 --> 00:00:04,605
عند تنفيذ شبكة عصبية عميقة، فإن إحدى
أدوات تصحيح الأخطاء التي غالبًا ما أستخدمها

2
00:00:04,605 --> 00:00:08,118
للتحقق من صحة تعليماتي البرمجية
تتمثل في إحضار ورقة،

3
00:00:08,118 --> 00:00:11,727
والعمل فقط من خلال الأبعاد
والمصفوفة التي أتعامل معها.

4
00:00:11,727 --> 00:00:15,895
إذًا، دعوني أريكم كيفية القيام بذلك،
لأنني آمل أن يُسهّل عليكم هذا الأمر

5
00:00:15,895 --> 00:00:18,275
تنفيذ شبكاتكم العميقة أيضًا.

6
00:00:18,275 --> 00:00:23,174
حرف L كبير = 5، مع العد
بسرعة، دون حساب طبقة الإدخال،

7
00:00:23,174 --> 00:00:27,390
توجد خمس طبقات هنا:
أربع طبقات مخفية وطبقة مخرجات واحدة.

8
00:00:27,390 --> 00:00:34,878
وبالتالي،
إذا نفذّت الانتشار الأمامي،

9
00:00:34,878 --> 00:00:41,408
فستكون الخطوة الأولى z1 = w1x + b1.

10
00:00:41,408 --> 00:00:48,144
إذًا دعونا نتجاهل شروط التحيز b في الوقت الحالي،
ونركز على المعلمات w.

11
00:00:48,144 --> 00:00:54,501
الآن، تحتوي هذه الطبقة المخفية الأولى على ثلاث
وحدات مخفية، لذا فإن هذه الطبقة هي الطبقة 0

12
00:00:54,501 --> 00:00:59,517
والطبقة 1 والطبقة 2 والطبقة 3
والطبقة 4 والطبقة 5.

13
00:00:59,517 --> 00:01:05,741
باستخدام الترميز الذي حصلنا عليه من
الفيديو السابق، لدينا n1،

14
00:01:05,741 --> 00:01:11,265
وهو عدد الوحدات المخفية في الطبقة 1، يساوي 3.

15
00:01:11,265 --> 00:01:16,202
وهنا سيكون لدينا
n2 يساوي 5،

16
00:01:16,202 --> 00:01:23,018
وn3 يساوي 4، وn4 يساوي 2،
وn5 يساوي 1.

17
00:01:23,018 --> 00:01:27,715
وحتى الآن لم نشاهد سوى شبكات عصبية
ذات وحدة إخراج واحدة، ولكن في

18
00:01:27,715 --> 00:01:32,497
الدورات التدريبية اللاحقة، سوف نتحدث عن
شبكات عصبية ذات وحدات إخراج متعددة أيضًا.

19
00:01:32,497 --> 00:01:36,989
وأخيرًا، بالنسبة لطبقة الإدخال،

20
00:01:36,989 --> 00:01:40,443
لدينا أيضًا n0 = nx = 2.

21
00:01:40,443 --> 00:01:45,860
والآن، دعنا نفكر في
أبعاد z وw وx.

22
00:01:45,860 --> 00:01:49,120
z هو متجه عمليات التنشيط

23
00:01:49,120 --> 00:01:54,244
لهذه الطبقة المخفية الأولى،
لذا z سيكون 3 × 1،

24
00:01:54,244 --> 00:01:58,675
سيكون متجهًا ثلاثي الأبعاد.

25
00:01:58,675 --> 00:02:03,093
لذا سأقوم بكتابته n1 × متجه أحادي البعد،

26
00:02:03,093 --> 00:02:08,546
n1 × مصفوفة أحادية البعد،
إذًا 3 × 1 في هذه الحالة.

27
00:02:08,546 --> 00:02:12,319
الآن، ماذا عن خصائص الإدخال x وx؟
لدينا خاصيتا إدخال.

28
00:02:12,319 --> 00:02:18,622
إذًا x في هذا المثال 2 × 1، ولكن
بشكل عام، سيكون n0 × 1.

29
00:02:18,622 --> 00:02:24,082
إذًا، ما نحتاجه هو أن تكون
المصفوفة w1 شيئًا عندما

30
00:02:24,082 --> 00:02:30,181
نضرب n0 في متجه 1 إليه، نحصل
على n1 × متجه 1، أليس كذلك؟

31
00:02:30,181 --> 00:02:34,747
بحيث يكون لديك نوع من
متجه ثلاثي الأبعاد يساوي

32
00:02:34,747 --> 00:02:38,600
شيئًا ما مضروبًا في متجه ثنائي الأبعاد.

33
00:02:38,600 --> 00:02:42,993
وهكذا من خلال
قواعد مضاعفة المصفوفة،

34
00:02:42,993 --> 00:02:46,041
هذا قد يكون مصفوفة 3 في 2.

35
00:02:46,041 --> 00:02:51,138
مفهوم، لأن مصفوفة 3 في 2
مضروبة في مصفوفة 2 في 1،

36
00:02:51,138 --> 00:02:56,249
أو مضروبة في متجه 2 في 1،
تمنحك متجه 3 في 1.

37
00:02:56,249 --> 00:03:02,771
وبشكل أعم، ستكون هذه عبارة
عن مصفوفة أبعاد n1 في n0.

38
00:03:02,771 --> 00:03:07,167
إذًا، ما توصلنا إليه هنا هو أن

39
00:03:07,167 --> 00:03:12,665
أبعاد w1 يجب أن تكون n1 في n0.

40
00:03:12,665 --> 00:03:20,191
وبشكل أعم، أبعاد wL يجب أن
تكون nL في nL ناقص 1.

41
00:03:20,191 --> 00:03:26,021
إذًا، على سبيل المثال، أبعاد w2،

42
00:03:26,021 --> 00:03:31,508
لهذا، يجب أن تكون 5 في 3،

43
00:03:31,508 --> 00:03:35,119
أو ستكون n2 في n1.

44
00:03:35,119 --> 00:03:40,036
لأننا سنقوم بحساب

45
00:03:40,036 --> 00:03:45,132
z2 مثل w2 في a1، ومرة أخرى،

46
00:03:45,132 --> 00:03:50,059
دعنا نتجاهل الانحياز في الوقت الحالي.

47
00:03:50,059 --> 00:03:54,584
وهذا سيكون 3 في 1،

48
00:03:54,584 --> 00:03:59,432
ونحن بحاجة إلى أن يكون هذا 5 في 1، ولذا

49
00:03:59,432 --> 00:04:03,169
هذا يجب أن يكون 5 في 3.

50
00:04:03,169 --> 00:04:10,273
وبالمثل، w3 هو في الحقيقة
بُعد الطبقة التالية،

51
00:04:10,273 --> 00:04:15,501
فاصلة، بُعد الطبقة
السابقة،

52
00:04:15,501 --> 00:04:19,266
لذلك سيكون هذا 4 في 5، وw4

53
00:04:22,055 --> 00:04:27,489
سيكون 2 في 4،

54
00:04:27,489 --> 00:04:34,405
وw5 سيكون 1 في 2، تمام؟

55
00:04:34,405 --> 00:04:38,730
إذًا، الصيغة العامة للتحقق
هي أنه عندما تقوم

56
00:04:38,730 --> 00:04:43,416
بتنفيذ مصفوفة
الطبقة L،

57
00:04:43,416 --> 00:04:48,475
يكون البُعد الخاص بتلك
المصفوفة nL في nL-1.

58
00:04:48,475 --> 00:04:55,362
الآن، دعونا نفكر في بُعد
هذا المتجه b.

59
00:04:55,362 --> 00:05:01,017
هذا سيكون متجه 3 في 1،
لذلك عليك أن تضيف ذلك إلى

60
00:05:01,017 --> 00:05:06,008
متجه 3 في1 آخر من أجل الحصول
على متجه 3 في 1 كناتج.

61
00:05:06,008 --> 00:05:11,287
أو في هذا المثال، نحتاج إلى إضافة هذا،
وهذا سيكون 5 في 1،

62
00:05:11,287 --> 00:05:14,823
لذلك سيكون هناك
متجه 5 في 1 آخر.

63
00:05:14,823 --> 00:05:19,122
لكي يكون مجموع هذين
الشيئين لدي في

64
00:05:19,122 --> 00:05:22,767
المربعات هو نفسه متجه 5 في 1.

65
00:05:22,767 --> 00:05:30,090
إذًا، القاعدة الأعم هي أنه
في المثال الموجود على اليسار،

66
00:05:30,090 --> 00:05:35,470
يكون b1 هو n1 في 1، تمام، وهذا 3 في 1،

67
00:05:35,470 --> 00:05:41,156
وفي المثال الثاني،
يكون هذا n2 في 1.

68
00:05:41,156 --> 00:05:45,891
وبالتالي فإن الحالة الأعم هي أن

69
00:05:45,891 --> 00:05:50,637
bL يجب أن يكون nL في 1 بُعدي.

70
00:05:50,637 --> 00:05:56,402
إذًا، آمل أن تساعدك هاتان المعادلتان
على التأكد من أن أبعاد

71
00:05:56,402 --> 00:06:02,091
المصفوفات الخاصة بك، وكذلك
المتجهات p، هي الأبعاد الصحيحة.

72
00:06:02,091 --> 00:06:06,206
وبالطبع، إذا كنت تقوم
بتطبيق الانتشار الخلفي،

73
00:06:06,206 --> 00:06:10,657
يجب أن تكون أبعاد dw
مماثلة لبُعد w.

74
00:06:10,657 --> 00:06:16,373
إذًا يجب أن يكون dw نفس البُعد مثل w،

75
00:06:16,373 --> 00:06:22,276
وأن يكون db نفس البُعد مثل b.

76
00:06:22,276 --> 00:06:28,399
الآن، المجموعة الرئيسية الأخرى من الكميات
التي أبعادها للتحقق هي z

77
00:06:28,399 --> 00:06:33,658
وx بالإضافة إلى L، التي
لم نتحدث عنها هنا كثيرًا.

78
00:06:33,658 --> 00:06:39,856
لكن لأن z من L يساوي g من a
من L، العنصر الذكي المطبّق،

79
00:06:39,856 --> 00:06:46,914
يجب أن يكون z وa لهما نفس
البُعد في هذه الأنواع من الشبكات.

80
00:06:46,914 --> 00:06:51,582
الآن، دعونا نرى ما يحدث عندما يكون
لديك تنفيذ متجه يظهر على

81
00:06:51,582 --> 00:06:53,258
عدة أمثلة في وقت واحد.

82
00:06:53,258 --> 00:06:56,092
حتى بالنسبة للتنفيذ المتجه،

83
00:06:56,092 --> 00:07:00,687
بالطبع، ستبقى أبعاد wb
وdw وdb كما هي.

84
00:07:00,687 --> 00:07:04,929
لكن أبعاد z وa
وx سوف

85
00:07:04,929 --> 00:07:09,771
تغيّر بعض الشيء
في التنفيذ المتجه.

86
00:07:09,771 --> 00:07:13,420
إذًا، في السابق،

87
00:07:13,420 --> 00:07:18,372
كان لدينا z1 = w1x + b1

88
00:07:18,372 --> 00:07:23,845
حيث هذا كان n1 في 1،

89
00:07:23,845 --> 00:07:28,276
وهذا كان n1 في n0،

90
00:07:28,276 --> 00:07:35,846
وx كان n0 في 1، وb كان n1 في 1.

91
00:07:35,846 --> 00:07:40,979
الآن، في تنفيذ

92
00:07:40,979 --> 00:07:46,398
موجه، سوف

93
00:07:46,398 --> 00:07:53,536
يكون لديك z1 = w1x + b1.

94
00:07:53,536 --> 00:07:58,023
حيث يتم الآن الحصول على z1
من خلال أخذ z1

95
00:07:58,023 --> 00:08:03,575
للأمثلة الفردية، لذلك
يوجد z11 وz12،

96
00:08:03,575 --> 00:08:10,207
حتى z1m، وتكديسها كما يلي،
وهذا يمنحك z1.

97
00:08:10,207 --> 00:08:15,042
إذًا، فالبُعد z1 هو أنه،
بدلاً من أن يكون n1 في 1،

98
00:08:15,042 --> 00:08:20,285
فإنه ينتهي به الحال ليكون n1 في m،
وm هو الحجم الذي تحاول ضبطه.

99
00:08:20,285 --> 00:08:26,140
تبقى أبعاد w1 كما هي،
لذا تظل n1 في n0.

100
00:08:26,140 --> 00:08:29,201
وx، بدلاً من أن يكون n0 في 1 الآن

101
00:08:29,201 --> 00:08:33,431
كل أمثلة التدريب الخاصة بك
مكدسة أفقيًا.

102
00:08:33,431 --> 00:08:38,565
إذًا، أصبح الآن n0 في m،
ومن ثم تلاحظ أنه عندما تأخذ

103
00:08:38,565 --> 00:08:43,833
مصفوفة n1 في n0 وتُضاعفها
بمقدار مصفوفة n0 في m.

104
00:08:43,833 --> 00:08:50,160
ومجموع ذلك في الواقع يعطيك مصفوفة
أبعاد n1 في m، كما هو متوقع.

105
00:08:50,160 --> 00:08:55,030
الآن، التفاصيل النهائية هي أن
b1 لا يزال n1 في 1، ولكن

106
00:08:55,030 --> 00:09:01,147
عند أخذ هذا وإضافته إلى b،
ثم من خلال توزيع بايثون،

107
00:09:01,147 --> 00:09:08,218
سوف يتكرر ذلك ويتحول إلى مصفوفة n1 في m،
ومن ثم يضيف العنصر الذكي.

108
00:09:08,218 --> 00:09:14,977
إذًا، في الشريحة السابقة، تحدثنا
عن أبعاد wb وdw وdb.

109
00:09:14,977 --> 00:09:21,143
هنا، ما نراه هو أنه في حين أن zL

110
00:09:21,143 --> 00:09:26,922
وكذلك aL ذات بُعد nL في 1،

111
00:09:26,922 --> 00:09:34,650
لدينا الآن بدلاً من ذلك
أن ZL وكذلك AL هما nL في m.

112
00:09:34,650 --> 00:09:40,410
وهناك حالة خاصة من هذا،
وهي عندما L تساوي 0،

113
00:09:40,410 --> 00:09:45,188
وفي هذه الحالة A0، التي تساوي فقط

114
00:09:45,188 --> 00:09:49,543
خصائص إدخال مجموعة التدريب الخاصة بك X،

115
00:09:49,543 --> 00:09:54,616
سوف تساوي n0 في m
كما هو متوقع.

116
00:09:54,616 --> 00:10:01,259
وبالطبع عند تنفيذ هذا
في الانتشار الخلفي،

117
00:10:01,259 --> 00:10:06,749
سنرى لاحقًا، أنك ستنتهي إلى
حساب dz بالإضافة إلى dA.

118
00:10:06,749 --> 00:10:11,327
وبالتالي سيكون لها بالطبع

119
00:10:11,327 --> 00:10:15,736
نفس البُعد مثل Z وA.

120
00:10:15,736 --> 00:10:19,467
إذًا آمل أن يساعد التدريب البسيط
الذي أجريناه في توضيح الأبعاد التي

121
00:10:19,467 --> 00:10:21,685
تمتلكها المصفوفات المختلفة
التي قد تعمل عليها.

122
00:10:21,685 --> 00:10:25,947
عند تنفيذ الانتشار الخلفي لشبكة
عصبية عميقة، ما دمت تعمل

123
00:10:25,947 --> 00:10:30,350
من خلال التعليمات البرمجية الخاصة بك،
تأكد من أن جميع أبعاد المصفوفات متناسقة.

124
00:10:30,350 --> 00:10:31,825
هذا سوف يساعد عادة،

125
00:10:31,825 --> 00:10:35,908
سوف يعمل بطريقة ما على القضاء على
بعض أسباب الأخطاء المحتملة.

126
00:10:35,908 --> 00:10:40,325
إذًا آمل أن يكون التدريب على فهم
أبعاد المصفوفات المختلفة التي سوف

127
00:10:40,325 --> 00:10:41,979
تعمل عليها مفيدًا.

128
00:10:41,979 --> 00:10:44,788
عندما تقوم بتنفيذ شبكة عصبية عميقة،
إذا حافظت على استقامة

129
00:10:44,788 --> 00:10:48,241
أبعاد هذه المصفوفات والمتجهات
المتنوعة التي تعمل عليها.

130
00:10:48,241 --> 00:10:52,162
آمل أن تساعدك على القضاء على
بعض أسباب الأخطاء المحتملة،

131
00:10:52,162 --> 00:10:54,467
فهي تساعدني بالفعل في جعل
تعليماتي البرمجية صحيحة.

132
00:10:54,467 --> 00:10:58,882
بعد ذلك، سوف نرى الآن بعض آليات كيفية إجراء انتشار

133
00:10:58,882 --> 00:11:01,227
أمامي في شبكة عصبية.

134
00:11:01,227 --> 00:11:04,163
لكن لماذا تكون الشبكات العصبية
العميقة فعالة للغاية،

135
00:11:04,163 --> 00:11:07,243
ولماذا يكون أداؤها أفضل من
التمثيل السطحي؟

136
00:11:07,243 --> 00:11:09,939
دعونا نستهلك بضع دقائق في
الفيديو التالي لمناقشة ذلك.