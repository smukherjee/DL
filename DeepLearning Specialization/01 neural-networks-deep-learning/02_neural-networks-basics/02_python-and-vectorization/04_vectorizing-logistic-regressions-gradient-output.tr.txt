Bir önceki videoda tahminlerini hesaplamak için vektörleştirmeyi nasıl kullanabileceğinizi gördünüz. Küçük a aynı zamanda O eğitim setine de girer. Bu videoda, tüm M eğitim örnekleri için eğim (gradyan) hesaplamaları gerçekleştirmede vektörleştirmeyi nasıl kullanabileceğinizi görüyorsunuz. Yine, aynı zamanda tüm sıralama. Ve sonra bu videonun sonunda, hepsini bir araya getireceğiz ve lojistik regresyonun (s biçimli bağlanım) çok verimli bir şekilde nasıl uygulanabileceğini göstereceğiz. Gradyan hesaplaması için bunu hatırlayabilirsiniz, ilk örnek için dz1'i hesaplamıştık. dz1 = a1 - y1 ve dz2 = a2 - y2 ve tüm M eğitim örnekleri için böyle devam eder. Yapacağımız şey yeni bir değişken tanımlamaktır, dZ = dz1, dz2, ..dzm. Tüm d küçük harf z (dz) değişkenleri yatay olarak yığılmıştır. Yani bu 1xm'lik matris veya m boyutlu bir sıra vektör olacaktır. Şimdi, bir önceki slayttan hatırlayın, Büyük A'nın nasıl hesaplanacağını göstermiştik, a1'den am'e ve büyük Y'yi y1'den ym'e bulmuştuk. Yatay olarak yığıldığını zaten biliyorsunuz. Dolayısıyla, bu tanımlara dayanarak, dz'nin sadece A - Y olarak hesaplanabileceğini göreceksiniz çünkü ilk elemanı a1 - y1 ve ikinci elemanı a2 - y2 diye devam eden matrise eşit olacak. Yani, ilk eleman olan a1 - y1 tam olarak dz1'in tanımıdır. İkinci eleman ise dz2'nin tanımıdır ve böyle devam eder. Yani, sadece bir kod satırı ile tüm bunları aynı anda hesaplayabilirsiniz. Şimdi, önceki uygulamada, tam bir döngüden kurtulduk ama hala eğitim örnekleri üzerinde döngü için bu ikinci vardı. Bu yüzden dw'yi sıfırlar vektörü için sıfırdan başlattık. Fakat yine de, dw + = x1 çarpı dz1'e sahip olduğumuz eğitim örnekleri üzerinde, ilk eğitim örneğinde dw + = x2 çarpı dz2, öyle devam ediyor. m kere yaparız yani dw / = m olur. Aynı db gibi, değil mi? db'de sıfırdan başlatıldı ve sonra db + = dz1. db + = dz2 ve aynı şekilde devam ediyor. db + = dzm ve en son db / = m. Yani önceki uygulamada sahip olduğumuz şey buydu. Tam bir döngüden zaten kurtulduk. Yani, en azından şu anda dw bir vektördür. Ve biz onu d1, dw2 vb şeklinde ayrıştırdık. Yani, bundan zaten kurtulmuştuk fakat eğitim setindeki M örnekleri üzerinde hala tam bir döngü var. Hadi bu işlemleri yapalım ve onları düzenleyelim. Yapabileceğimiz şey, db'nin uygulanmasının vektörize edilmesi için temel olarak toplanması, bu dz'lerin tümünün daha sonra m ile bölünmesidir. db temel olarak 1 üzeri m'dir. i = 1'den m'e kadar dzi'lerin toplamı ve bütün dz'ler bu satır vektöründedir. Python'da nasıl uygulayacağını biliyorsun, 1 üzeri m çarpı np çarpı dz'lerin toplamı. Yani, bu değişkeni al ve np ile çarp. Sonuç olarak bu denklem size db'yi verecektir. Dw ya da yazdığın başka bir şey için kanıtlanabilir doğru denklemler, yapılacak doğru şeydir. Dw matrisi, bir üzeri m çarpı X matrisi çarpı dz'nin tanspozesi haline dönüyor. Bu denklik, 1 üzeri m çarpı X matrisi, Eşitlik, 1 üzeri m çarpı X matrisi (x1'den xm'e kadar sütunlarda yığılı olan) ve sonra dz'nin transpozesi (dz1'den dzm'e aşağı doğru). Böylece, bu matris ve bu vektörün ne zaman çalıştığını anlarsanız 1 üzeri m çarpı x1, dz1 artı ...... artı xm, dzm'e dönüşür. yani, bu nx1'lik bir vektördür ve aslında dw ile biter. çünkü biliyorsunuz ki dw bunları alıyordu xi, dzi ve bunların toplamları. Yani tam olarak bu matris vektör çarpımı yapıyor. Ve tekrardan belirtmek gerekirse dw'yi tek satır kod ile hesaplayabilirsiniz. Bu nedenle, türev hesaplamalarının vektörize uygulaması sadece bu, db uygulaması için bu satırı dw uygulaması için bu satırı kullanın ve eğitim seti üzerindeki tüm döngüde istediğiniz parametreyi güncelleyerek hesaplama yapabilirsiniz. Şimdi, hep birlikte lojistik regresyonunu gerçekte nasıl uygulayacağınıza bir bakalım. Bu bizim orijinal, oldukça verimsiz vektörize olmamış uygulamamız. Burası önceki videoda ilk kurtulduğumuz kısımdı değil mi? Yani, dw1 dw2 ve benzerleri üzerinden döngü yapmak yerine, bunu dw + = xi çarpı dzi olan bir vektör değeri ile değiştirdik. Ama şimdi, sadece satırın altındaki bir döngüden değil, aynı zamanda bu tam döngüden de kurtulabileceğimizi göreceğiz. Yani, işte böyle yapıyorsun. Önceki slaytlardan elde ettiğimiz bilgileri kullanarak, şunu söyleyebiliriz ki, büyük Z w'nun transpozesi çarpı X artı b' ye eşittir ve koda, Z eşittir np çarpı w'nun transpozesi çarpı X artı b ve sonra A eşittir Z'nin sigmoidi. Yani, şimdi tüm bunları ve tüm bunları, i'nin tüm değerleri için hesapladınız. Bir önceki slaytta, dz'yi A - Y ile hesaplayabileceğinizi söylemiştik. Yani, şimdi tüm bunları i'nin bütün değerleri için hesapladınız. Son olarak dw eşittir 1/m çarpı X, çarpı dz'nin transpozesi ve db eşittir 1/m çarpı np çarpı dz'lerin toplamı Yani, sadece ileri yayılım ve geri yayılım yaptınız, gerçekten tam bir döngü kullanmadan tüm M eğitim örneklerinde tahmin hesaplamalarını ve türev hesaplamalarını yapıyorsunuz. Böylece degrede iniş güncellemesinde W'nun, w eksi öğrenme oranı çarpı dw (yukarıda hesaplanan) olarak güncelleneceğini biliyor olacaksınız. Ve b ise b eksi öğrenme oranı çarpı db olarak güncellenecek. (burayı anlamadım) bazen görev olarak göstermek için kolonları yerleştirirdim ama sanırım o notasyonla tamamen tutarlı olmadım. Ancak bununla birlikte, lojistik regresyon için tek bir yükseliş gradyanı gerçekleştirdiniz. Şimdi, biliyorum her zaman açık tam döngülerden kurtulmamız gerektiğini söylüyordum ama ama bir gradyan kökenli olarak çoklu ayarlamayı uygulamak istiyorsanız, yineleme sayısı üzerinde hala bir döngüye ihtiyacınız var. Yani, degrade inişinin bin iterasyonunu istiyorsanız, yineleme sayısı üzerinden hala bir döngüye ihtiyaç duyabilirsiniz. Bunun gibi en dışta tam bir döngü var, o yüzden bu for döngüsünden kurtulmanın bir yolu olduğunu düşünmüyorum. Ancak, inanılmaz derecede güzel olduğunu düşünüyorum çünkü bir for döngüsü kullanmadan en az bir degrade iniş yinelemesi gerçekleştirebilirsiniz. Yani, şu anda lojistik regresyon için son derece vektörize ve yüksek verimli bir degrade iniş uygulamanız var. Bir sonraki videoda konuşmak istediğim bir detay daha var. Tanımına burada kısaca değineceğim tekniğe broadcasting (yayın) deniyor. Broadcasting, Python ve numpy'nin kodunuzun belirli bölümlerini daha verimli hale getirmek için kullanmanıza izin verdiği bir teknik olarak ortaya çıkmaktadır. Öyleyse, bir sonraki videoda broadcasting hakkında daha fazla ayrıntı görelim.