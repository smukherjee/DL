Bir önceki videoda eğitim örneklerinin nasıl x matrisinde yatay olarak yığıldığını gördük, siz de sinir ağlarında yayılım için vektörize edilmiş uygulamalar türetebilirsiniz. hadi yazdığımız denklemlerin vektörize edilen örneklerin neden doğru uygulama olduğunu biraz daha açıklayalım. Bunun için yayılım hesaplarını örneklerini biraz inceleyelim. İlk alıştırma örneğini x1 + b1 hesaplayarak, daha sonra ikinci eğitim örneğini x2 + b1 hesaplayarak, sonra da üçüncü eğitim örneğini x3 + b1 hesaplayarak elde edebiliriz. Bu slayttaki açıklamayı basitleştirmek için b'yi göz ardı edelim. Bu nedenle b'yi 0'a eşitleyelim. Zaten bizim oluşturduğumuz argüman b 0 değilken bile az miktarda değişir. Slayttaki açıklamayı biraz daha basitleştirsek. Şimdi w1 matris olacak değil mi? Şimdi bu matriste bazı satırtarlar var. x1'in hesaplamasına bakacak olursak, Elimizde olan şey w1 ile x1'in çarpımı 
şu şekilde çizebileceğiniz bir 
sütun vektörü vermesidir. Ve benzer olarak, x2 vektörüne bakarsak elimizdeki w1 * x2 bize başka sütun vektörleri veriyor, değil mi? Bu da bize z12'yi verir. Sonuç olarak, eğer x3'e bakarsak, elimizdeki w1 * x3 bize başka bir üçüncü sütun vektörünü yani z13'ü verecek. Şimdi, eğitim kümesini (büyük)X olarak oluşturalım ve tüm eğitim örneklerini yığma yoluyla birleştirelim. Şimdi (büyük)X matrisi x1 vektöründen ve x2 ve x3 matrislerini dik olarak yığarak oluşturuldu. Bu durum 3 tane eğitim örneği olursa geçerli. Eğer daha fazla örnek olursa, örnekler bunun gibi yatay olarak yığılacak. Ama x matrisini w ile çarparsak matris çarpımı prensibinden bildiğiniz üzere birinci sütun mor ile çizdiğim değerlerle eşit olacak. İkinci sütun yeşil ile çizdiğim ve son olarak üçüncü sütun sarı ile çizdiğim değerlere eşit olacak. Fakat tabi ki birinci sütun z11, diğer sütun z12 ve son sütun z13 olarak ifade edilir. Ama bu durum 3 eğitim örneğiniz varsa geçerlidir. Eğer daha fazla örnek ile çalışırsanız daha fazla sütununuz olur. Yani bu sadece bizim oluşturduğumuz (büyük)Z1 matrisi. Umuyorum ki bu örnek size tek bir eğitim örneğine baktığımız zaman w1 * xi'nin neden z1i'ye eşit olduğunu açıklar. Diğer eğitim örneklerine baktığımız da ve onları farklı sütunlara yığdığımızda z'den elde edeceğimiz sonucun yine sütunlarda saklı olduğunu göreceğiz. Size gösteremiyorum; ama emin olabilirsiniz ki Python yayınlarına baktığınız zaman b'nin bu değerlerinin doğru olduğunu göreceksiniz. Aslında python yayınını bitirdiğinizde matrisin her bir sütununa bi'yi ekledğimiz sonucuna varırsınız. Bu slaytta ben sadece z1'in w1x + b1'e eşit olmasının daha önceki slaytta incelediğimiz 4 adımlı doğru vektörleştirmenin ilk adımı olduğunu açıkladım. Ama benzer bir analizle, diğer adımların da çok benzer bir mantık kullanarak çalıştığını göstermenize izin verdiği ortaya çıkıyor. Mesela girdileri<br />sütunlara yığarsanız denklemden sonra sütunlarda da yığılmış olan çıktıların karşılıklarını elde edersiniz. Son olarak bu videoda konuştuğumuz her şeyi gözden geçirelim. Eğer bu sizin sinir ağınız olsaydı söyleyeceğimiz şey <br />yayılım için uygulamanız gereken, her seferinde 1 eğitim örneği için i'yi 1'den m'e kadar sırayla eşitlemek. Sonrasında z1, a1, z2 ve a2'de uygun sütunlara yığdığımız gibi eğitim örneklerini de sütunlara yığarız. Bu a1 için bir örnek; ancak z1, a1, z2 ve a2 için de doğru. Ardından bir önceki slaytta gösterdiğimiz şey bu satırın size tüm m örneklerini aynı anda vektörize etmenize olanak sağlamasıydı. Ve benzer bir mantık ile ortaya çıkıyor ki, diğer tüm satırlar 4 satırın doğru bir şekilde vektörize edilmiş hali olduğunu gösterebiliriz. Ve bir hatırlatıcı olarak x aynı zamanda a0'a eşit çünkü hatırlayın girdi öznitelikli x vektörü a0'a eşit, bu yüzden xi a0i'ye eşit. Aslında buradaki eşitliklerde bazı simetriklikler var. Mesela bu ilk denklem aynı zamanda z1 = w1 * a0 + b1 olarak da yazılabilir. Bu durumda bu ikili denklem ve bu ikili denklem çok benzer görünüyor, sadece indeksi 1 arttırılmış. Yani bu tür bir sinir ağ göstermektedir ki farklı katmanlar kabaca aynı işi yapıyor <br />ya da aynı işlemi defalarca<br />tekrar ediyor. Ve burada 2-katmanlı sinir<br />ağımız var ve bir sonraki haftada sinir ağlarında daha derinlemesine ileleyeceğiz. Gördüğünüz üzere daha derin sinir ağlarında bile temelde bu 2 adımı alıyor ve bunu defalarca tekrarlıyor. Bu olay sizin sinir ağınızın nasıl birçok eğiti örneğinde vektörize edileceğini gösteriyor. Şimdiye kadar sinir ağımızda sigmoid fonksiyonu kullandık. Bunun aslında en iyi seçim olmadığını gösteriyor. Bir sonraki videoda daha derine inip sizin nasıl farklı bir etkileşim fonksiyonunu kullanacağınızı inceleyeceğiz, mesela sigmoid fonksiyonu bir seçenek olabilir.