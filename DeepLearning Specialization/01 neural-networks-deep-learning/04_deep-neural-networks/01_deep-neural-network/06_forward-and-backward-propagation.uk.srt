1
00:00:00,000 --> 00:00:01,350
У попередньому відео

2
00:00:01,350 --> 00:00:05,065
ти бачив/ла структурні блоки реалізації глибокої нейронної мережі (НМ):

3
00:00:05,065 --> 00:00:07,696
крок прямого поширення для кожного шару і

4
00:00:07,696 --> 00:00:09,975
і відповідний крок зворотного поширення.

5
00:00:09,975 --> 00:00:12,370
Давай подивимось як їх реалізувати на практиці.

6
00:00:12,370 --> 00:00:14,230
Почнемо з прямого поширення.

7
00:00:14,230 --> 00:00:20,045
Згадаймо, що тут вводиться a[l-1], а виводиться a[l]

8
00:00:20,045 --> 00:00:21,795
і кеш z[l].

9
00:00:21,795 --> 00:00:24,615
І ми казали, що з точки зору реалізації

10
00:00:24,615 --> 00:00:28,145
добре б кешувати також і w[l] з b[l],

11
00:00:28,145 --> 00:00:31,585
просто щоб простіше обчислювати функції в програмувальній вправі.

12
00:00:31,585 --> 00:00:35,438
А рівняння для цього нам вже знайомі.

13
00:00:35,438 --> 00:00:46,813
Для реалізації прямої функції вони ось такі: z[l]=w[l]a[l-1]+b[l]

14
00:00:46,813 --> 00:00:53,620
і a[l]= активаційній функції g від z.

15
00:00:53,620 --> 00:00:57,250
І якщо ми хочемо векторизувати реалізацію,

16
00:00:57,250 --> 00:01:06,218
то це буде Z[l]=W[l]A[l-1]+b

17
00:01:06,218 --> 00:01:09,930
(b буде додаватись з допомогою посіву Python)

18
00:01:09,930 --> 00:01:15,296
і A[l]= g поелементно від Z[l].

19
00:01:15,296 --> 00:01:20,128
Згадай, що в Діаграмі [обчислень] прямого кроку

20
00:01:20,128 --> 00:01:22,565
ми мали отакий ланцюг квадратів спрямованих вперед,

21
00:01:22,565 --> 00:01:26,775
які ми ініціалізували введенням A[0],

22
00:01:26,775 --> 00:01:29,305
що =X.

23
00:01:29,305 --> 00:01:31,440
Тож це ми ініціалізували.

24
00:01:31,440 --> 00:01:33,380
А чим є вхідні дані в 1-ий квадрат?

25
00:01:33,380 --> 00:01:40,207
Це - a[0], тобто вхідні ознаки або одного тренувального зразка,

26
00:01:40,207 --> 00:01:42,255
якщо ми обробляємо 1 зразок,

27
00:01:42,255 --> 00:01:45,800
або велике A[0], тобто увесь тренувальний набір,

28
00:01:45,800 --> 00:01:48,215
якщо ми обробляємо увесь тренувальний набір.

29
00:01:48,215 --> 00:01:51,990
Тож це - вхідні дані для 1-ої прямої функції в ланцюгу.

30
00:01:51,990 --> 00:01:53,970
А потім просте повторення дозволяє нам

31
00:01:53,970 --> 00:01:57,085
обчислити пряме поширення зліва-направо.

32
00:01:57,085 --> 00:02:00,110
Далі давай поговоримо про крок зворотного поширення.

33
00:02:00,110 --> 00:02:03,585
Тут наша ціль - ввести da[l],

34
00:02:03,585 --> 00:02:08,040
а вивести - da[l-1], dw[l] і db[l].

35
00:02:08,040 --> 00:02:16,240
Давай запишемо кроки, які нам потрібні для обчислення цих величин. dz[l]=da[l]

36
00:02:16,240 --> 00:02:23,866
помножене поелементно на похідну g[l]'(z[l]).

37
00:02:23,866 --> 00:02:27,405
Потім обчислюємо похідні

38
00:02:27,405 --> 00:02:34,420
dw[l]=dz[l]a[l-1].

39
00:02:34,420 --> 00:02:37,445
Я не зберігаю це в кеші, але, виявляється, що

40
00:02:37,445 --> 00:02:39,114
воно теж потрібно. [?]

41
00:02:39,114 --> 00:02:47,560
Далі db[l]=dz[l]. І, нарешті,

42
00:02:47,560 --> 00:02:59,089
da[l-1]=w[l]ᵀdz[l]. Правильно?

43
00:02:59,089 --> 00:03:02,310
Я не хочу заглиблюватись в деталі цього виведення,

44
00:03:02,310 --> 00:03:06,605
але, виявляється, якщо взяти оце визначення da і підставити сюди,

45
00:03:06,605 --> 00:03:10,260
то ми отримаємо ту ж формулу, що мали на попередньому тижні.

46
00:03:10,260 --> 00:03:16,617
Тобто будемо обчислювати dz[l] як функцію від попереднього dz[l]. 
Фактично,

47
00:03:16,617 --> 00:03:18,135
якщо ми підставимо оце сюди,

48
00:03:18,135 --> 00:03:33,817
то отримаємо dz[l]=w[l+1]ᵀdz[l+1]g[l]'(z[l]).

49
00:03:33,817 --> 00:03:36,165
Я розумію, що здається, що тут багато алгебри.

50
00:03:36,165 --> 00:03:38,360
Насправді, ти можеш самостійно перевірити,

51
00:03:38,360 --> 00:03:40,820
що це - рівняння, яке ми записали для

52
00:03:40,820 --> 00:03:43,055
зворотного поширення на минулому тижні. Коли

53
00:03:43,055 --> 00:03:45,930
ми реалізуємо НМ з єдиним прихованим шаром.

54
00:03:45,930 --> 00:03:48,602
І просто нагадаю, що це - поелементне множення.

55
00:03:48,602 --> 00:03:54,950
Тож все, що нам потрібно - це ось ці 4 рівняння. 
І ми зможемо реалізувати зворотну функцію.

56
00:03:54,950 --> 00:03:58,735
І, врешті-решт, запишемо векторизовану версію.

57
00:03:58,735 --> 00:04:04,135
Перший рядок буде dZ[l]=dA[l]

58
00:04:04,135 --> 00:04:11,045
помножене поелементно на g[l]'(z[l]).

59
00:04:11,045 --> 00:04:13,060
Ну тут нічого незвичайного.

60
00:04:13,060 --> 00:04:23,715
dW[l] стане 1/m помножене на dZ[l] помножене на A[l-1]ᵀ,

61
00:04:23,715 --> 00:04:30,844
а db[l] стане 1/m помножене на np.sum(dZ[l],

62
00:04:30,844 --> 00:04:37,970
потім axis=1, keepdims=True).

63
00:04:37,970 --> 00:04:44,095
Ми говорили про використання np.sum() 
для обчислення db на минулому тижні.

64
00:04:44,095 --> 00:04:56,155
І, нарешті, dA[l-1]=W[l]ᵀdZ[l].

65
00:04:56,155 --> 00:05:02,435
Тож це дозволяє нам ввести оцю величину - da - сюди

66
00:05:02,435 --> 00:05:07,950
і вивести dW[l], db[l] -

67
00:05:07,950 --> 00:05:10,100
похідні, які нам потрібні -

68
00:05:10,100 --> 00:05:16,022
і вивести da[l-1] і dA[l-1]. Правильно? Ось так.

69
00:05:16,022 --> 00:05:18,905
Тож отак ми реалізуємо зворотну функцію.

70
00:05:18,905 --> 00:05:20,560
І щоб резюмувати.

71
00:05:20,560 --> 00:05:23,585
Беремо вхідне X.

72
00:05:23,585 --> 00:05:25,060
Швидше за все, ми маємо 1-ий шар,

73
00:05:25,060 --> 00:05:28,445
припустимо, з ВЛВ [ReLU] в ролі активаційної функції.

74
00:05:28,445 --> 00:05:30,570
Потім йдемо до 2-ого шару,

75
00:05:30,570 --> 00:05:33,340
який, припустимо, використовує ще одну ВЛВ [ReLU] 
в ролі активаційної функції.

76
00:05:33,340 --> 00:05:35,175
Йдемо до 3-ого шару,

77
00:05:35,175 --> 00:05:39,595
який, припустимо, має сигмоїду в ролі активаційної функції, 
припустимо, ми робимо двійкову класифікацію.

78
00:05:39,595 --> 00:05:41,875
І отримуємо ŷ.

79
00:05:41,875 --> 00:05:43,890
А потім, використовуючи ŷ,

80
00:05:43,890 --> 00:05:46,265
ми можемо обчислити Втрату.

81
00:05:46,265 --> 00:05:49,685
А це дозволяє нам почати зворотну ітерацію.

82
00:05:49,685 --> 00:05:51,775
Я намалюю спочатку стрілки, добре?

83
00:05:51,775 --> 00:05:54,320
Просто не хочу дуже часто змінювати колір курсору.

84
00:05:54,320 --> 00:06:03,430
І тут ми матимемо зворотне поширення, що обчислює похідні

85
00:06:03,430 --> 00:06:16,063
dW[3], db[3], dW[2], db[2], dW[1], db[1].

86
00:06:16,063 --> 00:06:18,865
І по дорозі ми обчислюватимемо...

87
00:06:18,865 --> 00:06:24,820
Ага. Буде передаватись кеш z[1], z[2], z[3].

88
00:06:24,820 --> 00:06:32,250
А тут ми передаємо назад da[2] і da[1].

89
00:06:32,250 --> 00:06:34,730
Тут може бути обчислено і da[0],

90
00:06:34,730 --> 00:06:35,880
але ми його не використовуватимемо.

91
00:06:35,880 --> 00:06:37,935
Тож можемо просто його відкинути. Правильно?

92
00:06:37,935 --> 00:06:40,785
Тож отак ми реалізуємо пряме і зворотне поширення для

93
00:06:40,785 --> 00:06:44,040
3-шарової НМ. Тепер

94
00:06:44,040 --> 00:06:46,140
остання деталь, яку я не

95
00:06:46,140 --> 00:06:48,735
згадав. Пряму рекурсію

96
00:06:48,735 --> 00:06:52,420
ми ініціалізуватимемо вхідними даними X.

97
00:06:52,420 --> 00:06:54,090
А як щодо зворотної рекурсії?

98
00:06:54,090 --> 00:06:59,235
Що ж, виявляється, що da[l]

99
00:06:59,235 --> 00:07:01,065
при використанні логістичної регресії (ЛР),

100
00:07:01,065 --> 00:07:02,977
при виконанні двійкової класифікації,

101
00:07:02,977 --> 00:07:09,685
=-(y/a)+(1-y)/(1-a).

102
00:07:09,685 --> 00:07:12,575
Тож, виявляється, що похідна функції Втрати

103
00:07:12,575 --> 00:07:14,180
по відношенню до вихідних даних,

104
00:07:14,180 --> 00:07:17,475
по відношенню до ŷ, може бути представлена отак.

105
00:07:17,475 --> 00:07:19,105
Якщо ти знайомий/а з диф.численням,

106
00:07:19,105 --> 00:07:21,328
якщо візьмеш функцію Втрати L

107
00:07:21,328 --> 00:07:24,150
і візьмеш похідні по ŷ або a,

108
00:07:24,150 --> 00:07:26,505
то побачиш, що отримаєш оцю формулу.

109
00:07:26,505 --> 00:07:31,350
Тож це - формула, яку ми повинні використовувати 
для останнього шару da[велике L].

110
00:07:31,350 --> 00:07:35,715
І, звичайно, якщо виконувати векторизовану реалізацію,

111
00:07:35,715 --> 00:07:38,351
то ініціалізація зворотної рекурсії

112
00:07:38,351 --> 00:07:43,706
буде не оцим, а - dA[L],

113
00:07:43,706 --> 00:07:48,465
що буде тим же, але для окремого зразка.

114
00:07:48,465 --> 00:07:54,011
-y⁽¹⁾/a⁽¹⁾ для 1-ого тренувального зразка плюс (1-y⁽¹⁾)

115
00:07:54,011 --> 00:07:55,285
для 1-ого тренувального зразка

116
00:07:55,285 --> 00:07:58,153
поділити на (1-a⁽¹⁾) для 1-ого тренувального зразка.

117
00:07:58,153 --> 00:08:05,185
І т.д. до останнього тренувального зразка m.

118
00:08:05,185 --> 00:08:09,423
Тож отак ми реалізуємо векторизовану версію.

119
00:08:09,423 --> 00:08:13,055
Отак ми ініціалізуємо векторизовану версію зворотного поширення.

120
00:08:13,055 --> 00:08:16,100
Отож, тепер ти побачив/ла основні структурні блоки

121
00:08:16,100 --> 00:08:20,030
як прямого, так і зворотного поширення.

122
00:08:20,030 --> 00:08:22,340
Тепер, якщо ти реалізуєш ці рівняння,

123
00:08:22,340 --> 00:08:24,530
то отримаєш правильну реалізацію

124
00:08:24,530 --> 00:08:27,640
прямого і зворотного поширень і отримаєш необхідні похідні.

125
00:08:27,640 --> 00:08:29,660
Можливо, ти думаєш, що тут дуже багато рівнянь

126
00:08:29,660 --> 00:08:32,080
і дещо збентежений/а, бо не впевнений/а, 
що повністю зрозумів/ла як це працює.

127
00:08:32,080 --> 00:08:34,645
Тож якщо ти так себе почуваєш, я раджу,

128
00:08:34,645 --> 00:08:37,205
почати робити програмувальну вправу.

129
00:08:37,205 --> 00:08:40,175
Ти зможеш реалізувати це самостійно.

130
00:08:40,175 --> 00:08:42,020
Там все буде більш конкретно.

131
00:08:42,020 --> 00:08:43,805
Я розумію, що рівнянь дуже багато

132
00:08:43,805 --> 00:08:46,265
і, можливо, деякі рівняння не повністю зрозумілі,

133
00:08:46,265 --> 00:08:49,055
але якщо ти пропрацюєш диф.числення

134
00:08:49,055 --> 00:08:50,905
і лінійну алгебру (що не так вже й легко)...

135
00:08:50,905 --> 00:08:52,430
Спробуй,

136
00:08:52,430 --> 00:08:56,390
хоч це й справді одне з найбільш 
складних виведень в машинному навчанні (МН).

137
00:08:56,390 --> 00:08:57,950
Дійсно, рівняння наростають лавиноподібно.

138
00:08:57,950 --> 00:09:02,685
Принаймі, диф.рівняння для обчислення похідних, 
особливо при зворотному поширенні.

139
00:09:02,685 --> 00:09:04,750
Проте, знову ж, якщо тобі це видається абстрактним,

140
00:09:04,750 --> 00:09:06,400
дещо містичним,

141
00:09:06,400 --> 00:09:09,108
то я раджу зробити програмувальну вправу.

142
00:09:09,108 --> 00:09:11,465
Тоді воно набуде конкретики.

143
00:09:11,465 --> 00:09:14,120
Також мушу сказати, що навіть зараз,

144
00:09:14,120 --> 00:09:16,805
коли я реалізую навчальний алгоритм, інколи

145
00:09:16,805 --> 00:09:18,000
навіть я дивуюсь,

146
00:09:18,000 --> 00:09:21,170
коли моя реалізація навчального алгоритму працює. 
Це все тому,

147
00:09:21,170 --> 00:09:25,670
що складність МН більше залежить від даних ніж від рядків коду.

148
00:09:25,670 --> 00:09:27,095
Тож інколи здається,

149
00:09:27,095 --> 00:09:28,685
що ти реалізуєш кілька рядків коду,

150
00:09:28,685 --> 00:09:30,110
не повністю розумієш, що вони роблять,

151
00:09:30,110 --> 00:09:31,625
але вони мало не магічно працюють.

152
00:09:31,625 --> 00:09:35,296
Це тому, що більшість магії знаходиться 
не в шматку коду, який ми пишемо,

153
00:09:35,296 --> 00:09:37,090
бо він дуже часто не такий вже й великий.

154
00:09:37,090 --> 00:09:38,705
Він не те щоб простий,

155
00:09:38,705 --> 00:09:40,730
але це не десятки тисяч

156
00:09:40,730 --> 00:09:42,115
чи сотні тисяч рядків коду.

157
00:09:42,115 --> 00:09:44,750
Просто ми вводимо так багато даних, що інколи

158
00:09:44,750 --> 00:09:46,850
навіть я, який вже довго працює в МН,

159
00:09:46,850 --> 00:09:49,400
інколи і я дещо дивуюсь, коли

160
00:09:49,400 --> 00:09:53,274
мій навчальний алгоритм працює. 
Бо більша частина складності навчального алгоритму

161
00:09:53,274 --> 00:09:55,970
залежить від даних, а не

162
00:09:55,970 --> 00:10:01,020
від необхідності писати тисячі-тисячі рядків коду.

163
00:10:01,020 --> 00:10:05,935
Добре. Тож отак ми реалізуємо глибоку НМ.

164
00:10:05,935 --> 00:10:10,325
І, знову ж, конкретики додасться, 
коли ти зробиш програмувальну вправу.

165
00:10:10,325 --> 00:10:14,220
Перед тим як піти далі, я хочу обговорити в наступному відео

166
00:10:14,220 --> 00:10:17,480
гіперпараметри і параметри.

167
00:10:17,480 --> 00:10:19,681
Виявляється, що при тренуванні глибоких НМ,

168
00:10:19,681 --> 00:10:22,225
уміння добре налаштувати гіперпараметри

169
00:10:22,225 --> 00:10:25,400
допоможе тобі більш ефективно розробляти НМ.

170
00:10:25,400 --> 00:10:29,000
В наступному відео давай поговоримо що це значить.