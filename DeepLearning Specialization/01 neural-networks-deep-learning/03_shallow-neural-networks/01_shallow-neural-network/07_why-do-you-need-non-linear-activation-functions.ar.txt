لماذا تحتاج الشبكة العصبية
إلى دالة تنشيط غير خطية؟ تبين لنا أنه كي تقوم شبكتكم العصبية
بحساب دوال مثيرة للاهتمام، عليكم اختيار دالة تنشيط
غير خطية، دعونا نرى السبب. حسنًا، هذه معادلات الانتشار
الأمامي للشبكة العصبية. لماذا لا نتخلص من هذا؟ نتخلص من الدالة g؟ ونعين a1 يساوي z1. أو بدلاً من ذلك، يمكنكم قول إن
g(z)‎ يساوي z، صحيح؟ في بعض الأحيان، تسمى هذه
الدالة الخطية التنشيطية. وربما يكون الاسم الأفضل لها هو
دالة التنشيط المتطابقة لأنها تقوم بإخراج
ما تم إدخاله فحسب. ولهذا الغرض، ماذا لو كان
a(2)‎ يساوي z(2)‎؟ ويتبين أنه إذا قمتم بذلك،
فإن هذا النموذج يحسب y أو y-hat كمعادلة خطية
لميزات الإدخال، x، لأخذ أول معادلتين. إذا كان لديكم a(1)‎ = Z(1) = W(1)x + b، و حينها يكون a(2) = z (2)‎ = W(2)a(1) + b. حينها إذا أخذنا تعريف a1 هذا ووضعناه في هذه المعادلة، فإننا نجد أن a2 = w2(w1x + b1)‎، ونرفع ذلك قليلاً. حسنًا؟ فإن هذا يكون
a1 + b2، ولذلك يتم تبسيط ذلك إلى: ‎(W2w1)x + (w2b1 + b2). لذلك، هذا فقط، لنطلق على ذلك w' b'. وهذا يساوي w' x + b'‎. إذا كنتم تريدون استخدام
دوال التنشيط الخطي أو كما نسميها دوال
التنشيط المتطابقة، فإن الشبكة العصبية ستخرج فقط
دالة خطية من المدخلات. وسنتحدث عن الشبكات العميقة لاحقًا،
الشبكات العصبية مع الكثير من الطبقات، الكثير من الطبقات المخفية. وتبين أنه إذا كنتم تستخدمون دالة تنشيط خطية
أو بدلاً من ذلك، إذا لم تكن لديكم دالة تنشيط،
فبصرف النظر عن عدد الطبقات الموجودة لدى شبكتكم العصبية، كل ما تقوم به الشبكة
هو مجرد حساب دالة تنشيط خطية. لذا ربما من الأفضل
ألا تكون لديكم أي طبقات مخفية. بعض من الحالات التي تم ذكرها بإيجاز،
فقد تبيّن أنه إذا كانت لديكم دالة تنشيط خطية هنا
ودالة سينية هنا، فإن هذا النموذج ليس أكثر تعبيرًا من نموذج
الانحدار اللوجستي الاعتيادي دون أي طبقة مخفية. لذا لن أحاول إثبات ذلك ولكن
تستطيعون محاولة القيام بذلك إذا كنتم تريدون. ولكن النقطة المهمة هي أن
الطبقة المخفية الخطية هي بدون أي قيمة لأن تكوين دالتين خطيتين هو
دالة خطية بحد ذاته. لذا، ما لم تقوموا بطرح [غير مسموع] غير خطي هنا،
فأنتم لا تحسبون دوال أكثر إثارة للاهتمام تعمل
حتى إذا تعمقتم في الشبكة. هناك مكان واحد فقط يمكنكم
استخدام دالة تنشيط خطية فيه، g(x) = z. وذلك إذا كنتم تقومون
بالتعلم الآلي على مشكلة انحدار. لذا، إذا كان y عددًا حقيقيًا. على سبيل المثال، إذا كنتم
في محاولة للتنبؤ بأسعار المنازل، إذًا، y ليس 0، 1، ولكنه عدد حقيقي،
يتراوح بين - لا أعلم - 0 دولار هو سعر المنزل حتى
القيمة التي تصل إليها المنازل الباهظة، أعتقد أنه ربما تكون المنازل
بملايين الدولارات، لذا، مهما كانت تكلفة المنازل في مجموعة البيانات الخاصة بكم. ولكن إذا كان y يأخذ هذه القيم الحقيقية، فإنه قد يكون مقبولاً أن
تكون هناك دالة تنشيط خطية هنا حتى يكون الإخراج y hat أيضًا عددًا حقيقيًا يتراوح
ما بين سالب اللانهاية إلى موجب اللانهاية. ولكن، الوحدات المخفية يجب
ألا تستخدم دوال التنشيط. ويمكنها استخدام ReLU أو tanh أو
Leaky ReLU أو ربما شيء آخر. لذا يكون المكان الوحيد الذي يمكنكم
استخدام دالة تنشيط خطية خلاله، هو عادة طبقة الإخراج. ولكن بخلاف ذلك، فإن استخدام
دالة تنشيط خطية في الطبقة المخفية باستثناء بعض الظروف الخاصة جدًا
فيما يتعلق بالضغط الذي سنتحدث عنه حول استخدام دالة التنشيط الخطية،
هو أمر نادر للغاية. وبالتأكيد، إذا كنا فعلاً
نتوقع أسعار المنازل كما رأيتم في فيديو الأسبوع الأول لأن
أسعار المنازل ليست سالبة كلها، وربما حتى ذلك الحين، يمكنكم استخدام
دالة تنشيط قيمة حتى تكون مخرجاتكم y-hats
كلها أكبر من أو تساوي الصفر. لذا آمل أن يمنحكم هذا إحساس بأهمية
وجود الدالة التنشيطية غير الخطية كجزء هام
من الشبكات العصبية. بعد ذلك، سنبدأ بالحديث عن
الانحدار المتدرج، وأن نفعل ذلك، للإعداد
لمناقشتنا عن الانحدار المتدرج، في الفيديو التالي، أريد أن أوضح لكم كيفية
تقدير، كيفية حساب الانحدار أو المشتقات الخاصة
بدوال التنشيط الفردية. لننتقل الى الفيديو التالي.