<html xmlns:o="urn:schemas-microsoft-com:office:office" xmlns:w10="urn:schemas-microsoft-com:office:word" xmlns:m="http://schemas.microsoft.com/office/omml/2004/12/core" xmlns:ve="http://schemas.openxmlformats.org/markup-compatibility/2006" xmlns:o12="http://schemas.microsoft.com/office/2004/7/core" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:v="urn:schemas-microsoft-com:vml" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/3/wordprocessingDrawing" xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/3/main" xmlns="http://www.w3.org/TR/REC-html40">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <head>
    <style>style</style>
  </head>
  <body>
    <div class="Section1">
      <p>
        <b>
          <u>
            <span style="font-size:20.0pt">More Vectorization Examples</span>
          </u>
        </b>
      </p>
      <p>
(DESCRIPTION)        <br/>
Text, deep learning dot AI. Basics of neural network programming. More vectorization examples        <br/>
      </p>
      <p>
(SPEECH)        <br/>
In the previous video you saw a few examples of how vectorization, by using built in functions and by avoiding explicit for loops, allows you to speed up your code significantly.        <br/>
      </p>
      <p>
Let's look at a few more examples.        <br/>
      </p>
      <p>
(DESCRIPTION)        <br/>
Text, neural network programming guideline        <br/>
      </p>
      <p>
(SPEECH)        <br/>
The rule of thumb to keep in mind is, when you're programming your new networks, or when you're programming just a regression, whenever possible avoid explicit for-loops.        <br/>
      </p>
      <p>
And it's not always possible to never use a for-loop, but when you can use a built in function or find some other way to compute whatever you need, you'll often go faster than if you have an explicit for-loop.        <br/>
      </p>
      <p>
Let's look at another example.        <br/>
      </p>
      <p>
If ever you want to compute a vector u as the product of the matrix A, and another vector v, then the definition of our matrix multiply is that your Ui is equal to sum over j,, Aij, Vj.        <br/>
      </p>
      <p>
That's how you define Ui.        <br/>
      </p>
      <p>
And so the non-vectorized implementation of this would be to set u equals NP.zeros, it would be n by 1.        <br/>
      </p>
      <p>
For i, and so on.        <br/>
      </p>
      <p>
For j, and so on..        <br/>
      </p>
      <p>
And then u[i] plus equals a[i][j] times v[j].        <br/>
      </p>
      <p>
So now, this is two for-loops, looping over both i and j.        <br/>
      </p>
      <p>
(DESCRIPTION)        <br/>
Second for loop is nested inside the first for loop        <br/>
      </p>
      <p>
(SPEECH)        <br/>
So, that's a non-vectorized version, the vectorized implementation which is to say u equals np dot (A,v).        <br/>
      </p>
      <p>
And the implementation on the right, the vectorized version, now eliminates two different for-loops, and it's going to be way faster.        <br/>
      </p>
      <p>
Let's go through one more example.        <br/>
      </p>
      <p>
Let's        <br/>
      </p>
      <p>
(DESCRIPTION)        <br/>
Text, vectors and matrix valued functions        <br/>
      </p>
      <p>
(SPEECH)        <br/>
say you already have a vector, v, in memory and you want to apply the exponential operation on every element of this vector v.        <br/>
      </p>
      <p>
So you can put u equals the vector, that's e to the v1, e to the v2, and so on, down to e to the vn.        <br/>
      </p>
      <p>
So        <br/>
      </p>
      <p>
(DESCRIPTION)        <br/>
Code that initializes u to zeros, then explicitly loops over it to calculate each element based on the corresponding element of v        <br/>
      </p>
      <p>
(SPEECH)        <br/>
this would be a non-vectorized implementation, which is at first you initialize u to the vector of zeros.        <br/>
      </p>
      <p>
And then you have a for-loop that computes the elements one at a time.        <br/>
      </p>
      <p>
But it turns out that Python and NumPy have many built-in functions that allow you to compute these vectors with just a single call to a single function.        <br/>
      </p>
      <p>
So what I would do to implement this is import numpy as np, and then what you just call u = np.exp(v).        <br/>
      </p>
      <p>
And so, notice that, whereas previously you had that explicit for-loop, with just one line of code here, just v as an input vector u as an output vector, you've gotten rid of the explicit for-loop, and the implementation on the right will be much faster that the one needing an explicit for-loop.        <br/>
      </p>
      <p>
In fact, the NumPy library has many of the vector value functions.        <br/>
      </p>
      <p>
So np.log (v) will compute the element-wise log, np.abs computes the absolute value, np.maximum computes the element-wise maximum to take the max of every element of v with 0.        <br/>
      </p>
      <p>
v**2 just takes the element-wise square of each element of v.        <br/>
      </p>
      <p>
One over v takes the element-wise inverse, and so on.        <br/>
      </p>
      <p>
So, whenever you are tempted to write a for-loop take a look, and see if there's a way to call a NumPy built-in function to do it without that for-loop.        <br/>
      </p>
      <p>
So,        <br/>
      </p>
      <p>
(DESCRIPTION)        <br/>
Text, logistic regression derivatives        <br/>
      </p>
      <p>
(SPEECH)        <br/>
let's take all of these learnings and apply it to our logisti regression gradient descent implementation, and see if we can at least get rid of one of the two for-loops we had.        <br/>
      </p>
      <p>
So        <br/>
      </p>
      <p>
(DESCRIPTION)        <br/>
Num Py code that initializes all its variables to zero, then explicitly for loops over . Within the body of the loop, operations are written out repeatedly for each of the features of a vector, constituting an implicit inner loop        <br/>
      </p>
      <p>
(SPEECH)        <br/>
here's our code for computing the derivatives for logistic regression, and we had two for-loops.        <br/>
      </p>
      <p>
One was this one up here, and the second one was this one.        <br/>
      </p>
      <p>
So in our example we had nx equals 2, but if you had more features than just 2 features then you'd need have a for-loop over dw1, dw2, dw3, and so on.        <br/>
      </p>
      <p>
So its as if there's actually a 4j equals 1, 2, and x.        <br/>
      </p>
      <p>
dWj gets updated.        <br/>
      </p>
      <p>
So we'd like to eliminate this second for-loop.        <br/>
      </p>
      <p>
That's what we'll do on this slide.        <br/>
      </p>
      <p>
So the way we'll do so is that instead of explicitly initializing dw1, dw2, and so on to zeros, we're going to get rid of this and instead make dw a vector.        <br/>
      </p>
      <p>
(DESCRIPTION)        <br/>
Crossing out variable initializations at top        <br/>
      </p>
      <p>
(SPEECH)        <br/>
So we're going to set dw equals np.zeros, and let's make this a nx by 1, dimensional vector.        <br/>
      </p>
      <p>
Then, here, instead of this for loop over the individual components, we'll just use this vector value operation, dw plus equals xi times dz(i).        <br/>
      </p>
      <p>
And        <br/>
      </p>
      <p>
(DESCRIPTION)        <br/>
Crossing out inner implicit loop        <br/>
      </p>
      <p>
(SPEECH)        <br/>
then finally, instead of this, we will just have dw divides equals m.        <br/>
      </p>
      <p>
So now we've gone from having two for-loops to just one for-loop.        <br/>
      </p>
      <p>
We still have this one for-loop that loops over the individual training examples.        <br/>
      </p>
      <p>
So I hope this video gave you a sense of vectorization.        <br/>
      </p>
      <p>
And by getting rid of one for-loop your code will already run faster.        <br/>
      </p>
      <p>
But it turns out we could do even better.        <br/>
      </p>
      <p>
So the next video will talk about how to vectorize logistic aggression even further.        <br/>
      </p>
      <p>
And you see a pretty surprising result, that without using any for-loops, without needing a for-loop over the training examples, you could write code to process the entire training sets.        <br/>
      </p>
      <p>
So, pretty much all at the same time.        <br/>
      </p>
      <p>
So, let's see that in the next video.        <br/>
      </p>
    </div>
  </body>
</html>
