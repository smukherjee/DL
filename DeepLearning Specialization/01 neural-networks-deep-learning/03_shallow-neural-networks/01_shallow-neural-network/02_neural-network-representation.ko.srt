1
00:00:00,000 --> 00:00:03,116
여러분은 제가 신경망을 그린 적을 몇번 보셨을텐데요, 

2
00:00:03,116 --> 00:00:05,712
이번 비디오에서는 이런 그림이 정확히 의미하는 것이 
무엇인지 살펴보겠습니다. 다시 말해, 

3
00:00:05,712 --> 00:00:06,728
달리 말하면,

4
00:00:06,728 --> 00:00:11,235
저희가 그려왔던 이런 신경망들이 
어떤 것을 대표하는지 보겠다는 것입니다.

5
00:00:11,235 --> 00:00:15,014
신경망에서 신걸 숨겨진 층이라고 하는 것의 

6
00:00:15,014 --> 00:00:17,290
경우를 먼저 집중적으로 보겠습니다.

7
00:00:17,290 --> 00:00:19,667
이것은 신경망의 그림입니다.

8
00:00:19,667 --> 00:00:22,986
여기 이 그림의 일부분을 
명칭을 부여하겠습니다.

9
00:00:22,986 --> 00:00:27,447
x1, x2, x3와 같이 입력 특성이
이렇게 세로로 쌓여있는데요,

10
00:00:27,447 --> 00:00:30,694
이것은 그리고 신경망의 
입력값 층입니다.

11
00:00:30,694 --> 00:00:35,764
그럼 놀랍지 않게, 이것은 
신경망의 입력 값을 보유하고 있겠죠. 

12
00:00:35,764 --> 00:00:37,990
그리고 또한, 동그란 층을 추가적으로 가지고 있을 텐데요, 

13
00:00:37,990 --> 00:00:41,663
이것은 숨겨진 유니으로, 
신경망의 숨겨진 층이라고 합니다.

14
00:00:41,663 --> 00:00:45,414
여기서 의미하는 수겨진이 무엇을 뜻하는지는
조금 있다 설명드리겠습니다.

15
00:00:45,414 --> 00:00:49,509
여기 마지막 층은, 이 경우에는 
1개의 노드로 구성되어 있는데요,

16
00:00:49,509 --> 00:00:53,894
여기 이렇게 1개의 노드로 이루어진 층을
결과값층, 아웃풋 레이어라고 합니다.

17
00:00:53,894 --> 00:00:56,059
그리고 ŷ의 값을 생성하는 역학을 합니다.

18
00:00:56,059 --> 00:00:59,932
지도학습의 신경망에서는 

19
00:00:59,932 --> 00:01:05,237
트레이닝세트가 x의 값과 목표 y결과값을 
모두 가지고 있습니다.

20
00:01:05,237 --> 00:01:09,239
그렇기 때문에 숨겨진 층이라는 단어는 
트레이닝 세트에서

21
00:01:09,239 --> 00:01:12,702
여기 가운데 노드들의 실제값이 보이지 않는 경우를 말합니다.

22
00:01:12,702 --> 00:01:15,185
즉, 트레이닝세트에서 보여야 할 것들이 보이지 않는 것을 말합니다.

23
00:01:15,185 --> 00:01:16,640
입력값은 보이고, 

24
00:01:16,640 --> 00:01:18,094
또, 결과값도 보이나,

25
00:01:18,094 --> 00:01:20,992
숨겨진 층에 있는 것들은 트레이닝 세트에서 보이지 않습니다.

26
00:01:20,992 --> 00:01:25,542
그러면 hidden이라는 이름이 
무엇일 뜻하는지 어느 정도 
설명이 됐을 텐데요,

27
00:01:25,542 --> 00:01:28,088
트레이닝세트에서 안보이는 경우를 뜻하니 말이죠. 

28
00:01:28,088 --> 00:01:30,262
새로운 표기를 조금 더 소개해보겠습니다.

29
00:01:30,262 --> 00:01:35,542
이전에는 입력값을 x로 표현했는데요, 

30
00:01:35,542 --> 00:01:37,226
입력특성 값들에 대한 또 다른 표기법으로는, 

31
00:01:37,226 --> 00:01:41,987
a 위첨자 괄호 0입니다.

32
00:01:41,987 --> 00:01:44,934
a는 여기서 activation을 상징합니다.

33
00:01:44,934 --> 00:01:47,733
서로 다른 신경망들의 층이 

34
00:01:47,733 --> 00:01:51,651
다음 이어지는 신경망 층들로 전달하는 
값을 뜻합니다.

35
00:01:51,651 --> 00:01:55,998
그러므로 입력 층이 x 값을
숨겨진 층으로 전달하고, 

36
00:01:55,998 --> 00:02:01,110
이것을 activation, 입력 층 A 위첨자 0으로
나타낼텐데요, 

37
00:02:01,110 --> 00:02:05,990
다음 층에서, 숨겨진 층은 activation을 
생성할 텐데요,

38
00:02:05,990 --> 00:02:09,601
이것은 a 위첨자 대괄호 1로 표현하겠습니다.

39
00:02:09,601 --> 00:02:13,306
그럼 특히, 여기 이 첫번째 유닛, 
또는 첫번째 노드

40
00:02:13,306 --> 00:02:17,824
a 위첨자 대괄호 1와 같은 값을 생성하는데요, 

41
00:02:17,824 --> 00:02:20,735
두번째 값은, 

42
00:02:20,735 --> 00:02:23,311
여기 이렇게 위첨자2의 값으로 생성됩니다.

43
00:02:23,311 --> 00:02:26,488
그러면, 여기 위 첨자 대괄호 1의 값은 

44
00:02:26,488 --> 00:02:30,120
4차원 벡터인데요, 

45
00:02:30,120 --> 00:02:34,707
파이썬에서 구현할 것입니다. 4 X 1 
common 벡터 매트릭스 인데요. 이렇게 생겼습니다.

46
00:02:34,707 --> 00:02:39,205
이것은 4차원인데요, 그 이유는 
이 경우 4개의 노드가 있고, 또는 

47
00:02:39,205 --> 00:02:42,684
4개의 유닛이 있고, 
또는 숨겨진 층에 4개의 숨겨진 유닛이 있기 때문입니다.

48
00:02:42,684 --> 00:02:46,302
마지막으로 오픈 layer가 A2 값을 재생성합니다.

49
00:02:46,302 --> 00:02:47,948
이 값은 그냥 실수입니다.

50
00:02:47,948 --> 00:02:51,658
그러면 ŷ은 a2의 값을 가질텐데요, 

51
00:02:51,658 --> 00:02:55,885
이것은 ŷ 이 a였던 로지스틱 회귀분석때이 
경우와 비슷한데요, 

52
00:02:55,885 --> 00:03:00,349
이때는 1개의 결과값 층밖에 없었습니다.

53
00:03:00,349 --> 00:03:03,583
그렇기 때문에 위첨자 대괄호를 사용하지 않았습니다.

54
00:03:03,583 --> 00:03:07,916
이제 새로운 신경망에서는 
어떤 층에서 왔는지를 나타내기 위해서 

55
00:03:07,916 --> 00:03:11,653
위첨자를 사용할 것입니다.

56
00:03:11,653 --> 00:03:15,468
신경망의 표기법이 웃긴 것 중 한가지는

57
00:03:15,468 --> 00:03:20,194
여기에서 본 네트워크가 two layer 신경망이라는 것입니다. 

58
00:03:20,194 --> 00:03:23,541
신경망에서 그렇게 불리는 이유는 

59
00:03:23,541 --> 00:03:25,321
입력 층을 세지 않기 때문입니다.

60
00:03:25,321 --> 00:03:28,858
그러므로 숨겨진 층 이 1번 층이고, 
결과값 층이 2번 층입니다.

61
00:03:28,858 --> 00:03:32,661
그러면 표기법에서
입력값 층을 0번 층이라고하고, 

62
00:03:32,661 --> 00:03:35,887
엄밀히 이야기하면 3개의 층이 있다고
할 수 있겠죠. 

63
00:03:35,887 --> 00:03:39,649
그 이유는 입력 층이 있고, 
숨겨진 층이 있고, 그리고 결과값 층이 있기 
때문입니다.

64
00:03:39,649 --> 00:03:43,357
하지만 공식적인 표기로는, 
리서치 페이퍼 논문이나 다른 코스에서

65
00:03:43,357 --> 00:03:47,489
보면, 사람들이 이렇게 생긴 신경망을 

66
00:03:47,489 --> 00:03:51,602
two layer 신경망이라고 부르는 것을 볼 수 있을 것입니다.
입력 층을 공식적인 층으로 세지 않기 때문이죠. 

67
00:03:51,602 --> 00:03:55,912
마지막으로, 나중에 보겠지만
숨겨진 층과

68
00:03:55,912 --> 00:03:59,670
결과값 층은 각각 연관된 매개 변수가 있습니다.

69
00:03:59,670 --> 00:04:03,447
숨겨진 층은 연계된 매개 변수 
w와 b가 있을 것입니다.

70
00:04:03,447 --> 00:04:08,218
그리고 저는 위점차 대괄호 1로

71
00:04:08,218 --> 00:04:12,395
숨겨진 층을 가진 첫번째 층과 연계되는 것을
나타낼 것입니다. 

72
00:04:12,395 --> 00:04:15,416
나중에 보겠지만 w는 
4 X 3 매트릭스 입니다.

73
00:04:15,416 --> 00:04:18,016
b는 4 X 1 벡터이구요. 

74
00:04:18,016 --> 00:04:21,754
4라는 첫번째 좌표는 4개의 숨겨진 층과 층이 있는

75
00:04:21,754 --> 00:04:24,503
노드가 있다는 것을 나타내구요, 

76
00:04:24,503 --> 00:04:28,120
그리고 3이라는 좌표 숫자는
우리가 입력 특성이 3개를 가지고 있다는 것입니다.

77
00:04:28,120 --> 00:04:31,980
이런 매트릭스의 다이멘션에 대한 내용은 
나중에 또 더 이야기하겠습니다.

78
00:04:31,980 --> 00:04:33,844
더 이야기하는 시점에 이해가 많이 되실 수 있습니다.

79
00:04:33,844 --> 00:04:37,813
그리고 비슷하게 결과값도 

80
00:04:37,813 --> 00:04:41,663
w 위첨가 대괄호 2 와
b 위첨자 대괄호 2와 연관되어 있을 수 있는데요, 

81
00:04:41,663 --> 00:04:45,747
각각의 dimension은 1 X 4, 그리고
1 X 1입니다.

82
00:04:45,747 --> 00:04:49,297
그리고 여기서 1 X 4인 이유는, 
숨겨진 층이 4개 있기 때문입니다.

83
00:04:49,297 --> 00:04:51,177
그리고 결과값은 1개의 유닛이 있습니다.

84
00:04:51,177 --> 00:04:56,378
여기에서 나오는 다이멘션에 대한 내용은
나중에 이어지는 비디오 강의에서 배우겠습니다.

85
00:04:56,378 --> 00:04:59,839
이제 여러분은 two layered 신경망이 
어떻게 생겼는지 보았는데요, 

86
00:04:59,839 --> 00:05:03,108
이것은 1개의 숨겨진 층으로 이루어진 것을 보았습니다.

87
00:05:03,108 --> 00:05:04,260
다음 비디오에서는

88
00:05:04,260 --> 00:05:08,513
이런 신경망이 정확히 어떤 것을 계산하는지 알아보겠습니다.

89
00:05:08,513 --> 00:05:11,223
즉, 이 신경망이 x를 어떻게 입력하고, 

90
00:05:11,223 --> 00:05:14,169
Ŷ 결과값을 산출하기까지 어떻게 이어지는지 알아보겠습니다.