Son videoda, tek bir eğitim örneği verilen bir sinir ağındaki tahminin nasıl hesaplanacağını gördünüz. Bu videoda, birden fazla eğitim örneğinde vector nasıl vektörelleştirileceğini görüyorsunuz. Ve sonuç lojistik regresyon için gördüğünüz ile oldukça benzer olacaktır. Matrisin farklı sütunlarındaki farklı antrenmanları örnekleyerek, önceki videodan elde ettiğiniz denklemleri alabileceksiniz. Ve çok az modifikasyonla, nöral ağın tüm örneklerin çıktılarını hemen hemen aynı anda hesaplaması için bunları değiştirin. Öyleyse bunu nasıl yapacağımızın detaylarına geçelim. Bunlar, bir önceki videodan aldığımız dört denklem idi hesapladığınız z1 a1 z2 ve a2 'dir. Ve size giriş özelliği verilmiş x'e nasıl geri döneceğinizi söyler, bunları tek bir eğitim örneği için a^[2] = y^ oluşturmak için kullanabilirsiniz. Şimdi, m tane eğitim örneğiniz var ise, bu işlemi ilk eğitim örneği için tekrar etmeniz gerekiyor. x^(1) y^(1)'i hesaplamak için training ilk eğitim örneğinizde bir tahmin yapar. Sonra x^(2), y^ (2) tahmini üretmek için kullanılır. Ve böylece x ^(m) 'ye kadar bir tahmin y^ (m) üretir. Ve böylece tüm bu aktivasyonda fonksiyon gösterimde de bunu a^[2](1) olarak yazacağım. Ve bu a^[2](2), ve a(2)(m), böylece Bu gösterim a[2](i). Parantez i ,i örneklerinin öğrenilmesini ifade eder, ve [2] ,katman 2 'yi ifade eder. Yani bu nasıl köşeli ayraç ve parantez ayracı belirteçlerinin çalıştığıdır. Ve düşünün ki, eğer unvectorized implementatione sahipseniz ve tüm tahminleri hesaplamak istiyorsanız i= 1 için m yapmak gerekir Sonra temel olarak bu dört denklemi uygulayın Z^[1](i) = =W^(1)*x^(i) +b^(1), a^[1](i) =sigma (Z^[1](1)) Z^[2](i) = W^[2]*a^[1](i) +b^[2] a^[2](i) = sigma (Z^[2](i)) Yani temel olarak bu dört denklem (i) üzeri ekleyerek (i) üzeri eğitim örneğine bağlı olan tüm değişkenlere eklemektir. a 'ya üstel [z](i) ekleyin, eğer tüm eğitim çıktılarını training antrenman örneklerindeki örneklerle hesaplamak istiyorsanız. Bundan kurtulmak için yapacağımız şey, bütün hesaplamayı vektörize etmektir. Ve bu arada,lineer algebranın özünü anlatıyor gibi görüyorsa olsam da , bunu doğru uygulamak deep derin öğrenme döneminde önemlidir Ve aslında bu ders için notasyonu çok dikkatli bir şekilde seçtik ve bu vektörü mümkün olduğunca kolay hale getirdik. Bu yüzden, bu konunun özünün üzerinden geçmenin aslında bu algoritmaların doğru uygulamalarını daha hızlı gerçekleştirebilmenizi umuyoruz. Pekala, bu kod bloğunu bir sonraki slayta kopyalamama izin verin ve sonra bunu nasıl düzenleyeceğimizi görelim. Yani, burada önceki slaytlardan elde ettiklerimiz m döngülerimizin üzerinden geçmesi içindir. Bu yüzden,hatırlayın bu sütunlarda yığılmış eğitim örneklerimize eşit olacak şekilde x matrisini tanımladık. Bu yüzden eğitim örneklerini alın ve columns bunları sütunlara yerleştirin. Yani bu bir n olur veya (nx,m) boyutlu matrix Can alıcı nokta vereceğim ve bu döngü için vektörize edilmiş bir uygulama yapmak için ne yapmanız gerektiğini söyleyeceğim. Yapmanız gereken şey hesaplama yapmaktır. Z^[1] = W^[1]*X+b^[1] A^[1] =sigma(Z^[1]) Sonra, Z^[2] =W^[2]* A^[1] +b^[2] ve sonra, A^[2] =sigma (Z^[2]) Yani eğer benzetmeyi istiyorsan, o zaman küçük harf vektör eksenlerinden, küçük harf eksenlerini farklı sütunlarda biriktirerek sadece büyük harf X matrisine gittik. Aynı şeyi z için yaparsanız, örneğin eğer Z^[1](1),Z^[1](2),ve daha fazlası Z^[1](m) ' e kadar bunlar sutün vektörüdür. Yani, bu ilk miktar, bunların tüm tüm m 'ler, ve bunları sütunlara yerleştirir. Sonra sadece matris Z^[1] 'i verir. Ve benzer bir şekilde, bu miktarda ve a^[1](1),a^[1](2),ve daha fazlası ve a^[1](m) ,ve bunları sütunda yığarız. Daha sonra, küçük harf x’den büyük harf X’e gittiğimiz gibi, ve küçük harf z’den büyük harf Z’e Bu, küçük harf a'dan büyük A^[1] 'a gider, ve benzer şekilde Z^[2] ve A^[2] Ayrıca, bu vektörleri alarak ve yatay olarak istifleyerek de elde edilirler. Ve bu vektörleri alarak ve onları yatay olarak istifleyerek, Z^[2] ve E^[2] 'yi elde etmek için Bu gösterimlerden biri yardımcı olabilir. Bu konuda düşünmeniz gereken şey bu matrisler Z ve A olduğunu söylemektir. eğitim örnekleri arasında yatay olarak indexliyoruz. Bu yüzden yatay indeks farklı eğitim örneğine karşılık gelir, Soldan sağa süpürdüğünüzde eğitim hücrelerini tarayabilirsiniz. Ve bu dikey indeks diğer düğümlerle de , sinir ağında uyumludur. Örneğin, bu düğüm, top bu değer en tepede, ortalamanın en sol üst köşesinde bu değer aktivasyona karşılık gelir ilk yönlendirme ünitesinin ilk eğitim örneğindeki aktivasyonuna karşılık gelir. Bir değer aşağı ilk eğitim örneğindeki ikinci gizli ünitedeki aktivasyonuna karşılık gelir, ilk eğitim örneği için, Daha sonra üçüncü başlık birimi ilk eğitim örneği vb. Bu yüzden,bunu tarandığınız zaman , gizli birimler numarasına indekslemenizdir. Eğer yatay hareket ederseniz, o zaman hidden ilk gizli üniteden gidersiniz. Ve ilk eğitim örneği, şimdi ilk gizli birim ve İkinci eğitim örneği, third üçüncü eğitim örneği. Ve buradaki bu düğüm, çıkış katmanının ilk saklı birimine ve m. eğitim örneğine denk gelmektedir. Tamam, yatay olarak, matris A, farklı eğitim örneklerinden geçer. Ve dikey olarak matrisin A'daki farklı indeksleri farklı gizli birimlere karşılık gelir. Ve benzer bir öngörü, matrisi Z'nin yanı sıra X için farklı eğitim örneklerine yatay olarak karşılık gelen X için de geçerlidir. Ve dikey olarak farklı giriş özelliklerine karşılık gelir. sinir ağının giriş katmanından gerçekten farklı olan Bu denklemler dolayısıyla,vectorization ile artık ağınızda nasıl uygulanacağını biliyorsunuz. yani çoklu örneklerde vektörizasyon. Bir sonraki videoda size, bu tür bir vektörleştirmenin neden doğru bir uygulaması olduğuna dair biraz daha fazla haklı göstermek istiyorum. Gerekçenin ortaya çıkması,neye gördüğünüze benzediği Bir sonraki videoya geçelim.