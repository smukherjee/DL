1
00:00:02,550 --> 00:00:05,830
مرحبًا يا إيان. شكرًا جزيلاً لانضمامك إلينا اليوم.

2
00:00:05,830 --> 00:00:06,860
شكرًا لك لدعوتي يا

3
00:00:06,860 --> 00:00:08,775
أندرو، يسعدني أن أكون هنا.

4
00:00:08,775 --> 00:00:11,920
اليوم، أنت واحد من أبرز الباحثين في مجال التعلم العميق في العالم.

5
00:00:11,920 --> 00:00:14,450
دعنا نتشارك القليل حول قصتك الشخصية.

6
00:00:14,450 --> 00:00:16,810
إذًا، كيف انتهي بك الأمر للقيام بهذا العمل الذي تؤديه الآن؟

7
00:00:16,810 --> 00:00:19,150
نعم. هذا يبدو رائعًا.

8
00:00:19,150 --> 00:00:24,287
أعتقد أنني في البداية أصبحت مهتمًا بالتعلم الآلي قبل أن ألتقي بك، في الواقع.

9
00:00:24,287 --> 00:00:29,705
لقد كنت أعمل في علم الأعصاب وقد شجعني مستشاري في المرحلة الجامعية،

10
00:00:29,705 --> 00:00:34,600
جيري كاين، في جامعة ستانفورد على أخذ صف "مقدمة إلى الذكاء الاصطناعي" الذي تقدّمه.

11
00:00:34,600 --> 00:00:35,790
أوه، لم أكن أعرف ذلك. حسنًا.

12
00:00:35,790 --> 00:00:39,885
إذًا، كنت أعتقد دائمًا أن الذكاء الاصطناعي فكرة جيدة،

13
00:00:39,885 --> 00:00:42,590
لكن في الواقع، الفكرة الأساسية، كما أعتقد،

14
00:00:42,590 --> 00:00:44,483
وراء ما يحدث كانت مثل الذكاء الاصطناعي في الألعاب،

15
00:00:44,483 --> 00:00:47,375
حيث يوجد لدى الناس الكثير من قواعد التعليمات البرمجية المضمنة

16
00:00:47,375 --> 00:00:49,700
لشخصيات غير اللاعبين في الألعاب كي تقول

17
00:00:49,700 --> 00:00:52,085
خطوطًا برمجية نصية مختلفة في مراحل زمنية مختلفة.

18
00:00:52,085 --> 00:00:56,750
وبعد ذلك، عندما أخذت صف "مقدمة إلى الذكاء الاصطناعي" الذي تقدمه وغطيت موضوعات مثل

19
00:00:56,750 --> 00:01:02,815
الانحدار الخطي وتحليل التباين لخطأ الانحدار الخطي،

20
00:01:02,815 --> 00:01:06,665
بدأت أدرك أن هذا علم حقيقي وأنني يمكنني في الواقع

21
00:01:06,665 --> 00:01:10,970
أن أكتسب مسيرة مهنية علمية في الذكاء الاصطناعي بدلاً من علم الأعصاب.

22
00:01:10,970 --> 00:01:12,730
فهمت، رائع. وماذا حدث بعد ذلك؟

23
00:01:12,730 --> 00:01:15,290
حسنًا، لقد عدت وقدمت المساعدة كمساعد مدرس لدورتك التدريبية لاحقًا.

24
00:01:15,290 --> 00:01:17,815
رائع، فهمت. كمساعد مدرس.

25
00:01:17,815 --> 00:01:22,595
إذًا، كانت نقطة تحول كبيرة بالنسبة لي عندما كنت أقدم المساعدة لهذه الدورة التدريبية،

26
00:01:22,595 --> 00:01:23,720
أحد الطلاب،

27
00:01:23,720 --> 00:01:25,310
صديقي إيثان دريفوس،

28
00:01:25,310 --> 00:01:28,689
أصبح مهتمًا بأبحاث جيف هينتون عن التعلم العميق.

29
00:01:28,689 --> 00:01:29,022
فهمت.

30
00:01:29,022 --> 00:01:35,660
وانتهى الأمر بكلينا إلى إنشاء واحدة من أولى الآلات القائمةعلى GPU CUDA في

31
00:01:35,660 --> 00:01:43,280
ستانفورد من أجل تشغيل آلات بولتزمان في وقت فراغنا خلال العطلة الشتوية.

32
00:01:43,280 --> 00:01:43,817
فهمت.

33
00:01:43,817 --> 00:01:46,295
وعند هذه المرحلة، بدأت أشعر

34
00:01:46,295 --> 00:01:50,720
بحدس قوي جدًا بأن التعلم العميق هو طريق التقدم نحو المستقبل،

35
00:01:50,720 --> 00:01:53,660
وأن الكثير من الخوارزميات الأخرى التي كنت أعمل عليها،

36
00:01:53,660 --> 00:01:56,285
مثل آلات متجهات الدعم،

37
00:01:56,285 --> 00:01:58,845
لم يبدُ أنها تمتلك خطوط المقاربة الصحيحة،

38
00:01:58,845 --> 00:02:01,400
وأنك تضيف المزيد من بيانات التدريب ومن ثم تصبح أبطأ،

39
00:02:01,400 --> 00:02:03,476
أو لنفس القدر من بيانات التدريب،

40
00:02:03,476 --> 00:02:08,240
من الصعب جعلها تحقق أداءً أفضل بكثير من خلال تغيير الإعدادات الأخرى.

41
00:02:08,240 --> 00:02:13,065
عند هذه المرحلة، بدأت في التركيز على التعلم العميق قدر الإمكان.

42
00:02:13,065 --> 00:02:18,595
وأذكر أن أبحاث ريتشارد رينا القديمة جدًا عن وحدة معالجة الرسومات

43
00:02:18,595 --> 00:02:21,585
قد أقرّت لك بالفضل في إنجاز المزيد من العمل المبكر.

44
00:02:21,585 --> 00:02:25,850
صحيح. تمت كتابة ذلك باستخدام بعض الآلات التي صممناها.

45
00:02:25,850 --> 00:02:26,656
صحيح.

46
00:02:26,656 --> 00:02:30,755
كانت الآلة الأولى التي صنعتها مجرد شيء صممته أنا وإيثان في

47
00:02:30,755 --> 00:02:35,120
منزل والدة إيثان من أموالنا الخاصة،

48
00:02:35,120 --> 00:02:39,835
وبعد ذلك، استخدمنا أموال المختبر لبناء أول آلتين أو ثلاثة لمختبر ستانفورد.

49
00:02:39,835 --> 00:02:42,965
هذا رائع. لم أكن أعلم بهذه القصة أبدًا. هذا جيد.

50
00:02:42,965 --> 00:02:45,830
ومن ثم، اليوم، فإن أحد

51
00:02:45,830 --> 00:02:48,365
الأشياء التي ساهمت حقًا في جعل

52
00:02:48,365 --> 00:02:51,645
التعلم العميق أشهر من نار على علم هو اختراع شبكات التنافس الإنتاجي.

53
00:02:51,645 --> 00:02:54,085
إذًا كيف أتيت بهذا؟

54
00:02:54,085 --> 00:02:56,885
لقد كنت أدرس النماذج الإنتاجية لوقت طويل،

55
00:02:56,885 --> 00:02:59,000
لذا فإن شبكات التنافس الإنتاجي وسيلة للقيام

56
00:02:59,000 --> 00:03:02,570
بالنمذجة الإنتاجية حيث يكون لديك الكثير من بيانات التدريب وترغب

57
00:03:02,570 --> 00:03:08,420
في تعلم كيفية إنتاج المزيد من الأمثلة التي تمثل بيانات التداول، لكنها تكون تصوّرية.

58
00:03:08,420 --> 00:03:13,265
لم يسبق رؤيتها في هذا الشكل بالضبط من قبل.

59
00:03:13,265 --> 00:03:16,220
كانت هناك عدة طرق أخرى لعمل نماذج إنتاجية، وقد كانت

60
00:03:16,220 --> 00:03:19,780
شائعة لعدة سنوات قبل أن أتوصل إلى فكرة شبكات التنافس الإنتاجي.

61
00:03:19,780 --> 00:03:24,860
وبعد أن كنت أعمل على كل تلك الأساليب الأخرى طوال معظم تحضيري للدكتوراة،

62
00:03:24,860 --> 00:03:29,000
عرفت الكثير عن مزايا وعيوب جميع إطارات العمل الأخرى مثل

63
00:03:29,000 --> 00:03:32,630
آلات بولتزمان والترميز المتناثر

64
00:03:32,630 --> 00:03:35,955
وجميع الأساليب الأخرى التي كانت شائعة بالفعل لسنوات.

65
00:03:35,955 --> 00:03:40,265
كنت أبحث عن شيء يتلافى كل هذه العيوب في الوقت نفسه.

66
00:03:40,265 --> 00:03:44,110
وفي النهاية، بينما كنت أتجادل حول النماذج الإنتاجية مع أصدقائي في الحانة،

67
00:03:44,110 --> 00:03:45,845
تفتق ذهني عن فكرة رائعة،

68
00:03:45,845 --> 00:03:47,540
وبدأت أخبرهم أنه يجب عليهم أن يفعلوا

69
00:03:47,540 --> 00:03:49,510
ذلك وهذا وتلك وأقسمت أن الأمر سينجح.

70
00:03:49,510 --> 00:03:52,890
ولم يصدقني أصدقائي بأنه سينجح.

71
00:03:52,890 --> 00:03:55,410
كان من المفترض أن أكتب كتابًا عن التعلم العميق في ذلك الوقت،

72
00:03:55,410 --> 00:03:55,790
فهمت.

73
00:03:55,790 --> 00:03:57,620
لكن كان لديّ اعتقاد راسخ بما فيه الكفاية بأنه سينجح، لذلك

74
00:03:57,620 --> 00:03:59,870
رجعت إلى البيت وأجريت الترميز في الليلة نفسها، ومن ثم نجح الأمر.

75
00:03:59,870 --> 00:04:02,920
إذًا، استغرق الأمر منك ليلة واحدة لتنفيذ أول نسخة من شبكات التنافس الإنتاجي؟

76
00:04:02,920 --> 00:04:06,050
قمت بتنفيذها بحلول منتصف الليل تقريبًا

77
00:04:06,050 --> 00:04:09,530
بعد العودة إلى المنزل من البار حيث كان صديقي يقيم حفلة وداع.

78
00:04:09,530 --> 00:04:10,086
فهمت.

79
00:04:10,086 --> 00:04:11,784
والنسخة الأولى منها نجحت،

80
00:04:11,784 --> 00:04:13,275
وهذا في منتهى الحظ.

81
00:04:13,275 --> 00:04:15,825
لم أضطر إلى البحث عن معامِلات hyperparameter أو أي شيء.

82
00:04:15,825 --> 00:04:17,840
كانت هناك قصة، قرأتها في مكان ما،

83
00:04:17,840 --> 00:04:21,851
أنك مررتَ بتجربة اقتراب من الموت والتي أكدت من جديد التزامك نحو الذكاء الاصطناعي.

84
00:04:21,851 --> 00:04:24,160
أخبرني عن تلك القصة.

85
00:04:24,160 --> 00:04:30,215
إذًا، صحيح. لم أكن في الواقع بالقرب من الموت ولكنني اعتقدت لفترة وجيزة أنني كذلك.

86
00:04:30,215 --> 00:04:33,170
كنت أعاني من صداع مزمن للغاية، ورأى بعض

87
00:04:33,170 --> 00:04:37,571
الأطباء أنني ربما أعاني من نزيف في الدماغ.

88
00:04:37,571 --> 00:04:39,740
وخلال الفترة التي كنت أنتظر فيها

89
00:04:39,740 --> 00:04:43,180
نتائج التصوير بالرنين المغناطيسي لمعرفة ما إذا كان لديّ نزيف في الدماغ أم لا،

90
00:04:43,180 --> 00:04:47,810
أدركت أن معظم الأفكار التي واجهتها كانت بخصوص الحرص

91
00:04:47,810 --> 00:04:49,910
على أن يتوصل الآخرون في نهاية المطاف إلى

92
00:04:49,910 --> 00:04:52,750
أفكار البحث التي كانت لديّ في ذلك الوقت.

93
00:04:52,750 --> 00:04:53,224
فهمت، فهمت.

94
00:04:53,224 --> 00:04:55,820
بالنظر إلى الماضي، كانت جميعها أفكارًا بحثية سخيفة جدًا.

95
00:04:55,820 --> 00:04:56,553
فهمت.

96
00:04:56,553 --> 00:04:58,700
لكن عند هذه النقطة،

97
00:04:58,700 --> 00:05:02,325
أدركت أن هذه كانت في الواقع واحدة من أهم أولوياتي في الحياة،

98
00:05:02,325 --> 00:05:05,780
وذلك أن أنجز عملي البحثي في مجال التعلّم الآلي.

99
00:05:05,780 --> 00:05:07,910
فهمت. بلى. هذا شيء عظيم،

100
00:05:07,910 --> 00:05:10,055
إنك عندما اعتقدت أنك قد تموت قريبًا،

101
00:05:10,055 --> 00:05:12,265
لم تفكر إلّا في كيفية إنجاز أبحاثك.

102
00:05:12,265 --> 00:05:12,649
صحيح.

103
00:05:12,649 --> 00:05:15,690
صحيح، هذا ينم عن التزام.

104
00:05:15,690 --> 00:05:17,850
صحيح.

105
00:05:17,850 --> 00:05:21,808
صحيح. إذًا، ما زلتَ اليوم وسط معمعة من الأنشطة مع شبكات GAN،

106
00:05:21,808 --> 00:05:24,560
مع شبكات التنافس الإنتاجي.

107
00:05:24,560 --> 00:05:27,710
إذا أخبرني كيف ترى مستقبل شبكات التنافس الإنتاجي.

108
00:05:27,710 --> 00:05:32,930
في الوقت الحالي، تُستخدم شبكات التنافس الإنتاجي في العديد من الأشياء المختلفة، مثل التعلم تحت إشراف متوسط،

109
00:05:32,930 --> 00:05:39,185
وتوليد بيانات التدريب للنماذج الأخرى وحتى محاكاة التجارب العلمية.

110
00:05:39,185 --> 00:05:43,850
من حيث المبدأ، يمكن القيام بكل هذه الأشياء بواسطة أنواع أخرى من النماذج الإنتاجية.

111
00:05:43,850 --> 00:05:47,695
لذلك أعتقد أن شبكات التنافس الإنتاجي في مفترق طرق مهم في الوقت الحالي.

112
00:05:47,695 --> 00:05:50,210
في الوقت الحالي، إنها تعمل جيدًا في بعض الأحيان،

113
00:05:50,210 --> 00:05:55,890
ولكنه قد يكون فنًا أكثر منه علمًا أن تحقق ذلك الأداء الأمثل منها.

114
00:05:55,890 --> 00:05:59,870
يتفاوت شعور الناس حول التعلم العميق بشكل عام خلال آخر 10 سنوات.

115
00:05:59,870 --> 00:06:01,430
وفي ذلك الوقت، كنا نستخدم

116
00:06:01,430 --> 00:06:05,330
شبكات تعلم عميق مع آلات بولتزمان ككتل إنشاء،

117
00:06:05,330 --> 00:06:07,420
وقد كان التعامل معها في منتهى الصعوبة.

118
00:06:07,420 --> 00:06:11,945
وبمرور الوقت، تحولنا إلى أشياء مثل الوحدات الخطية المعدّلة والتطبيع الدُفعي،

119
00:06:11,945 --> 00:06:14,635
وأصبح التعلم العميق أكثر موثوقية.

120
00:06:14,635 --> 00:06:18,470
إذا استطعنا جعل شبكات التنافس الإنتاجي موثوقًا بها مثلما حدث مع التعلم العميق،

121
00:06:18,470 --> 00:06:20,840
فعندئذٍ أعتقد أننا سنستمر في رؤية شبكات التنافس الإنتاجي مستخدمةً في

122
00:06:20,840 --> 00:06:24,110
جميع الأماكن التي تستخدمها اليوم مع مزيد من النجاح.

123
00:06:24,110 --> 00:06:29,060
إذا لم نتمكن من معرفة كيفية تحقيق الاستقرار في شبكات التنافس الإنتاجي،

124
00:06:29,060 --> 00:06:32,960
فعندئذٍ أعتقد أن مساهمتها الرئيسية في تاريخ التعلم العميق تتمثل في

125
00:06:32,960 --> 00:06:35,060
أنها سوف تبيّن للناس كيفية

126
00:06:35,060 --> 00:06:37,590
القيام بكل هذه المهام التي تنطوي على نمذجة إنشائية،

127
00:06:37,590 --> 00:06:41,505
وفي النهاية، يتم استبدالها بأشكال أخرى من النماذج الإنتاجية.

128
00:06:41,505 --> 00:06:47,870
لذا أقضي ربما حوالي 40% من وقتي حاليًا في العمل على تحقيق الاستقرار في شبكات التنافس الإنتاجي.

129
00:06:47,870 --> 00:06:50,780
فهمت. رائع. حسنًا. وكما أن الكثير من الأشخاص

130
00:06:50,780 --> 00:06:53,765
الذين انضموا إلى التعلم العميق قبل 10 سنوات، مثلك،

131
00:06:53,765 --> 00:06:54,963
انتهى بهم الحال إلى أن يكونوا روادًا،

132
00:06:54,963 --> 00:06:57,360
ربما الأشخاص الذين ينضمون إلى شبكات التنافس الإنتاجي اليوم،

133
00:06:57,360 --> 00:07:00,120
إذا نجح الأمر، قد ينتهي بهم الأمر كروّاد أوائل.

134
00:07:00,120 --> 00:07:04,220
بلى. الكثير من الأشخاص هم بالفعل روّاد أوائل في شبكات التنافس الإنتاجي،

135
00:07:04,220 --> 00:07:09,105
وأعتقد أنه إذا أردت تقديم أي نوع من تاريخ شبكات التنافس الإنتاجي حتى الآن،

136
00:07:09,105 --> 00:07:12,740
فستحتاج حقًا إلى ذكر مجموعات أخرى مثل Indico

137
00:07:12,740 --> 00:07:17,280
وفيسبوك وBerkeley لجميع الأشياء المختلفة التي ساهموا بها.

138
00:07:17,280 --> 00:07:19,735
بالإضافة إلى كل أبحاثك،

139
00:07:19,735 --> 00:07:24,300
اشتركت أيضًا في تأليف كتاب عن التعلم العميق. ماذا عن ذلك؟

140
00:07:24,300 --> 00:07:26,897
هذا صحيح، مع يوشوا بينجيو وآرون كورفيل،

141
00:07:26,897 --> 00:07:29,900
وهما مستشاران مشاركان لرسالتي في الدكتوراة.

142
00:07:29,900 --> 00:07:35,465
لقد كتبنا أول كتاب مدرسي عن النسخة الحديثة للتعلم العميق،

143
00:07:35,465 --> 00:07:38,615
وقد حظي بشعبية كبيرة،

144
00:07:38,615 --> 00:07:42,920
سواءً بالطبعة الإنجليزية أو الطبعة الصينية.

145
00:07:42,920 --> 00:07:48,915
لقد بعنا ما يقرب من 70000 نسخة، على ما أعتقد، بين هاتين اللغتين.

146
00:07:48,915 --> 00:07:54,730
ولقد تلقيت الكثير من التعليقات من الطلاب الذين قالوا إنهم تعلموا الكثير منه.

147
00:07:54,730 --> 00:07:58,940
من الأشياء التي فعلناها بشكل مختلف قليلاً عن بعض الكتب الأخرى هو أننا نبدأ

148
00:07:58,940 --> 00:08:03,905
بمقدمة مركزة جدًا على نوع الرياضيات التي تحتاج إلى ممارستها في التعلم العميق.

149
00:08:03,905 --> 00:08:07,670
أعتقد أن من الأمور التي استفدت من دوراتك التدريبية في ستانفورد هو

150
00:08:07,670 --> 00:08:11,570
هو أن الجبر الخطي والاحتمال مهمان للغاية،

151
00:08:11,570 --> 00:08:15,230
وأن الناس متحمسون لخوارزميات التعلم الآلي،

152
00:08:15,230 --> 00:08:18,500
ولكن إذا كنت تريد أن تصبح ممارسًا بارعًا حقًا،

153
00:08:18,500 --> 00:08:26,055
فعليك أن تتقن الرياضيات الأساسية التي يقوم عليها النهج بأكمله في المقام الأول.

154
00:08:26,055 --> 00:08:27,290
لذلك نحن نحرص على تقديم

155
00:08:27,290 --> 00:08:31,345
عرض تقديمي شديد التركيز لأساسيات الرياضيات في بداية الكتاب.

156
00:08:31,345 --> 00:08:34,153
بهذه الطريقة، لا تحتاج إلى المضي قدمًا وتعلّم كل ما يتعلق بالجبر الخطي،

157
00:08:34,153 --> 00:08:35,900
حيث يمكنك الحصول على

158
00:08:35,900 --> 00:08:37,770
دورة تدريبية سريعة جدًا في أقسام

159
00:08:37,770 --> 00:08:40,540
الجبر الخطي التي تعد مفيدة للغاية للتعلم العميق.

160
00:08:40,540 --> 00:08:44,660
إذًا، حتى الشخص الذي لا يتقن الرياضيات أو لم يشاهد الرياضيات

161
00:08:44,660 --> 00:08:47,000
لبضع سنوات سيكون قادرًا على البدء من مقدمة كتابك

162
00:08:47,000 --> 00:08:49,790
واكتساب تلك الخلفية والدخول إلى التعلم العميق؟

163
00:08:49,790 --> 00:08:52,175
كل المعلومات التي قد تحتاج إلى معرفتها موجودة بتلك المقدمة.

164
00:08:52,175 --> 00:08:59,520
من المؤكد أن الأمر سيستغرق بعض الجهد المركز لممارسة الاستفادة منها.

165
00:08:59,520 --> 00:08:59,684
نعم، نعم، عظيم.

166
00:08:59,684 --> 00:09:01,370
إذا كان هناك شخص ما لا يحب الرياضيات،

167
00:09:01,370 --> 00:09:03,700
فقد تكون تجربة مؤلمة قليلاً.

168
00:09:03,700 --> 00:09:08,323
لكن إذا كنت مستعدًا لتجربة التعلم وتعتقد أنه يمكنك إتقانها،

169
00:09:08,323 --> 00:09:11,360
أعتقد أن جميع الأدوات التي تحتاجها موجودة هناك.

170
00:09:11,360 --> 00:09:15,470
كشخص عمل في مجال التعلم العميق لفترة طويلة،

171
00:09:15,470 --> 00:09:18,710
سأكون فضوليًا، إذا نظرتَ إلى الوراء على مر السنين.

172
00:09:18,710 --> 00:09:21,050
أخبرني قليلاً عن كيفية تفكيرك

173
00:09:21,050 --> 00:09:24,650
في الذكاء الاصطناعي وتطور التعلم العميق على مر السنين.

174
00:09:24,650 --> 00:09:28,595
قبل عشر سنوات، انتابني شعور، كأحد المجتمعات،

175
00:09:28,595 --> 00:09:31,580
أن التحدي الأكبر في التعلم الآلي كان التوصل

176
00:09:31,580 --> 00:09:34,715
إلى طريقة لتشغيله للمهام المرتبطة بالذكاء الاصطناعي دون قيود.

177
00:09:34,715 --> 00:09:39,440
كانت لدينا أدوات جيدة حقًا استطعنا استخدامها في مهام أبسط،

178
00:09:39,440 --> 00:09:44,555
حيث أردنا التعرف على أنماط في كيفية استخراج الخصائص،

179
00:09:44,555 --> 00:09:47,000
حيث يمكن للمصمم البشري القيام بالكثير من

180
00:09:47,000 --> 00:09:51,965
العمل عن طريق إنشاء هذه الخصائص ثم إدخالها في الحاسوب.

181
00:09:51,965 --> 00:09:54,170
الآن، كان هذا جيدًا لأشياء مختلفة

182
00:09:54,170 --> 00:09:56,750
مثل التنبؤ بالإعلانات التي قد ينقر عليها المستخدم

183
00:09:56,750 --> 00:10:01,895
أو أنواع مختلفة من التحليلات العلمية الأساسية.

184
00:10:01,895 --> 00:10:07,505
لكننا ناضلنا فعلاً لعمل أي شيء يتضمن الملايين من وحدات البيكسل في صورة أو

185
00:10:07,505 --> 00:10:10,150
شكل موجة صوتية أولية حيث كان

186
00:10:10,150 --> 00:10:13,950
على النظام أن يبني كل فهمه من الصفر.

187
00:10:13,950 --> 00:10:18,880
لقد تجاوزنا العقبة بالكامل في نهاية المطاف، ربما قبل خمس سنوات.

188
00:10:18,880 --> 00:10:22,180
والآن، نحن في مرحلة يوجد بها

189
00:10:22,180 --> 00:10:26,268
العديد من المسارات المختلفة المفتوحة لأي شخص يرغب في الانخراط في الذكاء الاصطناعي،

190
00:10:26,268 --> 00:10:31,060
وربما تكون المشكلة الأصعب التي يواجهها هي اختيار المسار الذي يريد الخوض فيه.

191
00:10:31,060 --> 00:10:35,500
هل تريد أن تنجز عملاً في مجال التعلم المعزَّز بالإضافة إلى أعمال في مجال التعلم الخاضع للإشراف؟

192
00:10:35,500 --> 00:10:40,410
هل ترغب في إنجاز عمل في مجال التعليم غير الخاضع للإشراف بالإضافة إلى أعمال في مجال التعلم الخاضع للإشراف؟

193
00:10:40,410 --> 00:10:44,333
هل تريد التأكد من أن خوارزميات التعلم الآلي محايدة

194
00:10:44,333 --> 00:10:48,460
ولا تعكس التحيزات التي نفضل تجنّبها؟

195
00:10:48,460 --> 00:10:54,565
هل تريد التأكد من أن القضايا المجتمعية التي تحيط بالذكاء الاصطناعي تسير على ما يرام،

196
00:10:54,565 --> 00:10:58,535
وأننا قادرون على التأكد من أن الذكاء الاصطناعي يفيد الجميع

197
00:10:58,535 --> 00:11:03,440
بدلاً من التسبب في اضطرابات اجتماعية ومشاكل في فقدان الوظائف؟

198
00:11:03,440 --> 00:11:04,600
أظن أنه في الوقت الحالي،

199
00:11:04,600 --> 00:11:08,025
هناك كم هائل من الأشياء المختلفة التي يمكن القيام بها،

200
00:11:08,025 --> 00:11:11,380
لمنع حدوث سلبيات من الذكاء الاصطناعي مع التأكد

201
00:11:11,380 --> 00:11:14,965
من أننا نستغل جميع الإيجابيات التي يقدمها لنا.

202
00:11:14,965 --> 00:11:19,800
واليوم، هناك الكثير من الأشخاص الذين يريدون دخول مجال الذكاء الاصطناعي.

203
00:11:19,800 --> 00:11:23,285
إذًا، ما النصيحة التي قد تقدمها لشخص مثل هذا؟

204
00:11:23,285 --> 00:11:26,950
أعتقد أن الكثير من الأشخاص الذين يريدون دخول مجال الذكاء الاصطناعي يعتقدون في البداية

205
00:11:26,950 --> 00:11:32,200
أنهم بحاجة إلى الحصول على شهادة دكتوراة أو أي نوع آخر من وثائق الاعتماد من هذا القبيل.

206
00:11:32,200 --> 00:11:35,155
لا أعتقد في الواقع أن هذا قد أصبح مطلبًا بعد الآن.

207
00:11:35,155 --> 00:11:40,285
من الطرق التي يمكنك بها الحصول على كثير من الاهتمام كتابة تعليمات برمجية جيدة ووضعها على GitHub.

208
00:11:40,285 --> 00:11:43,380
إذا كان لديك مشروع مثير للاهتمام يحل

209
00:11:43,380 --> 00:11:47,320
إحدى المشكلات التي يريد حلها شخصٌ ما يعمل على المستوى الأعلى،

210
00:11:47,320 --> 00:11:49,840
بمجرد عثوره على مستودع GitHub الخاص بك،

211
00:11:49,840 --> 00:11:53,450
فسوف يبحث عنك ويطلب منك العمل لديه.

212
00:11:53,450 --> 00:11:56,140
الكثير من الأشخاص الذين قمت بتوظيفهم

213
00:11:56,140 --> 00:12:00,010
أو تشغيلهم في OpenAI العام الماضي أو في Google هذا العام،

214
00:12:00,010 --> 00:12:02,755
أصبحت في البداية مهتمًا بالعمل معهم بسبب

215
00:12:02,755 --> 00:12:06,895
شيء رأيته قد تم إصداره في منتدى مفتوح المصدر على الإنترنت.

216
00:12:06,895 --> 00:12:11,275
يمكن أن تكون كتابة الأبحاث ونشرها على Archive أمرًا جيدًا أيضًا.

217
00:12:11,275 --> 00:12:12,745
في كثير من الأحيان،

218
00:12:12,745 --> 00:12:16,750
يصعب الوصول إلى المرحلة التي تُنتج فيها شيئًا مصقولاً بما يكفي ليكون حقًا

219
00:12:16,750 --> 00:12:20,860
مساهمة أكاديمية جديدة في المصنفات العلمية،

220
00:12:20,860 --> 00:12:27,885
لكن يمكنك في كثير من الأحيان الوصول إلى نقطة امتلاك منتج برمجي مفيد قبل ذلك بكثير.

221
00:12:27,885 --> 00:12:30,022
إذًا، عليه أن يقرأ كتابك

222
00:12:30,022 --> 00:12:33,930
ويتدرب على المواد التعليمية وينشر على GitHub وربما على Archive.

223
00:12:33,930 --> 00:12:36,100
أعتقد أنه إذا تعلمت من خلال قراءة الكتاب،

224
00:12:36,100 --> 00:12:39,454
فمن المهم أيضًا العمل على مشروع في الوقت نفسه،

225
00:12:39,454 --> 00:12:42,730
وذلك باختيار طريقة ما

226
00:12:42,730 --> 00:12:46,555
لتطبيق التعلم الآلي على مجال تهتم به بالفعل.

227
00:12:46,555 --> 00:12:50,500
على سبيل المثال، إذا كنت عالم أحياء ميدانيًا وتريد دخول مجال التعلم العميق،

228
00:12:50,500 --> 00:12:53,255
فربما يمكنك استخدامه للتعرف على الطيور،

229
00:12:53,255 --> 00:12:56,905
أو إذا لم تكن لديك فكرة عن الطريقة التي تفضلها لاستخدام التعلم الآلي في حياتك الخاصة،

230
00:12:56,905 --> 00:13:01,600
يمكنك اختيار شيء مثل إنشاء تصنيف لأرقام المنازل في التجوّل الافتراضي،

231
00:13:01,600 --> 00:13:05,580
حيث يتم إعداد جميع مجموعات البيانات لجعله بسيطًا جدًا بالنسبة لك.

232
00:13:05,580 --> 00:13:07,330
وبهذه الطريقة، يمكنك ممارسة جميع

233
00:13:07,330 --> 00:13:09,700
المهارات الأساسية أثناء قراءة الكتاب أو أثناء

234
00:13:09,700 --> 00:13:14,105
مشاهدة مقاطع فيديو كورسيرا التي تشرح لك المفاهيم.

235
00:13:14,105 --> 00:13:15,670
إذًا، على مدار العامين الماضيين،

236
00:13:15,670 --> 00:13:20,045
رأيت أيضًا أنك تقوم بعمل آخر على النماذج التنافسية.

237
00:13:20,045 --> 00:13:21,535
أخبرنا قليلاً عن ذلك.

238
00:13:21,535 --> 00:13:24,490
صحيح. أعتقد أن النماذج التنافسية

239
00:13:24,490 --> 00:13:29,835
بداية مجال جديد أسميه أمان التعلم الآلي.

240
00:13:29,835 --> 00:13:33,250
في الماضي، رأينا مشكلات أمان الكمبيوتر

241
00:13:33,250 --> 00:13:38,275
حيث يمكن للمهاجمين خداع أي حاسوب بتشغيل التعليمات البرمجية الخاطئة.

242
00:13:38,275 --> 00:13:40,890
هذا ما يسمى بالأمن على مستوى التطبيق.

243
00:13:40,890 --> 00:13:46,300
وكانت هناك هجمات يستطيع من خلالها الأشخاص خداع حاسوب بإيهامه بأن

244
00:13:46,300 --> 00:13:52,545
الرسائل على شبكة ما تأتي من شخص ليس في الواقع من يظنونه.

245
00:13:52,545 --> 00:13:55,025
هذا ما يسمى بالأمن على مستوى الشبكة.

246
00:13:55,025 --> 00:13:57,230
الآن، بدأنا نرى أنه بإمكانك أيضًا خداع

247
00:13:57,230 --> 00:13:59,920
خوارزميات التعلم الآلي بتنفيذ أشياء لا ينبغي لها فعلها،

248
00:13:59,920 --> 00:14:06,010
حتى إذا كان البرنامج الذي يُشغّل خوارزمية التعلم الآلي يُشغّل التعليمات البرمجية الصحيحة،

249
00:14:06,010 --> 00:14:07,960
وحتى إذا كان البرنامج الذي يُشغّل

250
00:14:07,960 --> 00:14:10,025
خوارزمية التعلم الآلي يعرف

251
00:14:10,025 --> 00:14:13,605
جميع مُرسِلي الرسائل الفعليين على الشبكة.

252
00:14:13,605 --> 00:14:17,050
وأعتقد أنه من المهم تأسيس الأمن

253
00:14:17,050 --> 00:14:20,830
في تكنولوجيا جديدة بالقرب من بداية تطورها.

254
00:14:20,830 --> 00:14:27,065
لقد وجدنا أنه من الصعب جدًا بناء نظام عامل أولاً ثم إضافة الأمن لاحقًا.

255
00:14:27,065 --> 00:14:30,640
لذلك أنا متحمس جدًا لفكرة أننا إذا

256
00:14:30,640 --> 00:14:34,705
بذلنا جهدًا وبدأنا في توقع المشكلات الأمنية في التعلم الآلي الآن،

257
00:14:34,705 --> 00:14:37,600
يمكننا التأكد من أمان هذه الخوارزميات منذ

258
00:14:37,600 --> 00:14:41,650
البداية بدلاً من محاولة تصحيحها في سنوات لاحقة بأثر رجعي.

259
00:14:41,650 --> 00:14:43,111
شكرًا لك. كان ذلك رائعًا.

260
00:14:43,111 --> 00:14:46,090
هناك الكثير من الأمور التي أظنها رائعة في قصتك،

261
00:14:46,090 --> 00:14:47,470
وعلى الرغم من معرفتك لسنوات،

262
00:14:47,470 --> 00:14:49,935
لم أكن أعلمها في الواقع، لذا أشكرك على مشاركة كل ذلك.

263
00:14:49,935 --> 00:14:53,090
أوه، على الرحب والسعة. شكرًا لك لدعوتي. لقد كانت فرصة رائعة.

264
00:14:53,090 --> 00:14:53,630
حسنًا. شكرًا جزيلاً.

265
00:14:53,630 --> 00:14:55,010
على الرحب والسعة.