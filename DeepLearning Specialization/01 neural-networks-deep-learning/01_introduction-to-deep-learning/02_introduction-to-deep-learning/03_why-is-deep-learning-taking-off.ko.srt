1
00:00:00,329 --> 00:00:05,700
딥러닝의 기술적인 측면이,

2
00:00:03,060 --> 00:00:07,470
여러분의 딥러닝 네트워크 개념이

3
00:00:05,700 --> 00:00:09,870
수십 년 동안 존재해왔다고 하면

4
00:00:07,470 --> 00:00:12,090
왜 지금이 되어서야 뜨고 있는 것일까요?

5
00:00:09,870 --> 00:00:14,130
이번 강의에서는 딥러닝 산업 상승의 

6
00:00:12,090 --> 00:00:16,170
주요 원인이 되는 내용을 다루어 볼 텐데요

7
00:00:14,130 --> 00:00:18,090
이런 내용을 통해

8
00:00:16,170 --> 00:00:20,850
여러분들이 종사하고 있는 기업 내에서 

9
00:00:18,090 --> 00:00:22,439
가장 좋은 기회를 누리시고 실제로 적용하는 계기가 되실 겁니다.

10
00:00:20,850 --> 00:00:24,240
지난 몇 년 동안 많은 사람들이 

11
00:00:22,439 --> 00:00:26,820
저에게 물어봤었는데요,

12
00:00:24,240 --> 00:00:28,949
"앤드류, 딥러닝이
갑자기 이렇게 잘되는 이유가 뭐야?"

13
00:00:26,820 --> 00:00:31,109
라고 말이죠, 이런
질문을 받을 때면

14
00:00:28,949 --> 00:00:33,210
이런 그림을 그려주곤 하죠.

15
00:00:31,109 --> 00:00:36,180
이런 형체를 가로줄에 그리고,

16
00:00:33,210 --> 00:00:39,270
수행 업무의 데이터 양을 

17
00:00:36,180 --> 00:00:42,570
나타내고, 세로줄에는

18
00:00:39,270 --> 00:00:44,430
방금 설명한 교육 알고리즘의 성능을

19
00:00:42,570 --> 00:00:48,180
구체적으로 spam
classifier(스팸 차단기)나

20
00:00:44,430 --> 00:00:51,960
또는 ad click
predictor, 

21
00:00:48,180 --> 00:00:53,969
또는 신경망의 정확도 즉, 

22
00:00:51,960 --> 00:00:56,399
자율주행 자 산업에서 

23
00:00:53,969 --> 00:00:58,440
다른 자동차의 위치를 파악하는 정확도를 나타낼 수 있습니다.

24
00:00:56,399 --> 00:01:00,270
여러분이 전형적인 교육 

25
00:00:58,440 --> 00:01:02,460
알고리즘의 성능을 그래프에 그리면

26
00:01:00,270 --> 00:01:04,710
support vector machine (지지 벡터 기계)이나

27
00:01:02,460 --> 00:01:07,619
logistic regression (로지스틱 회귀)같은 내용을
데이터양의 함수로 표현하게 되면

28
00:01:04,710 --> 00:01:09,720
이러한 커브모양의

29
00:01:07,619 --> 00:01:11,670
함수가 나올 수 있는데요

30
00:01:09,720 --> 00:01:14,280
데이터를 추가할수록 성능이 일시적으로 

31
00:01:11,670 --> 00:01:16,200
향상되어 보이는 효과가 있는데요

32
00:01:14,280 --> 00:01:18,630
여러분이 알고 있는 성능의 개념은 

33
00:01:16,200 --> 00:01:21,180
수평적으로 안정을 찾아가는 형태죠.

34
00:01:18,630 --> 00:01:25,320
만약 가로선이 이렇게 된다고 해봅시다.

35
00:01:21,180 --> 00:01:28,140
방대한 양의 데이터를 어떻게 활용할지 

36
00:01:25,320 --> 00:01:30,689
몰랐다고 생각하면, 지난 10년간

37
00:01:28,140 --> 00:01:32,850
저희 사회에서 벌어진 현상은

38
00:01:30,689 --> 00:01:34,820
많은 문제들에 대하여

39
00:01:32,850 --> 00:01:38,610
소량의 데이터를 보유하고 있다가,

40
00:01:34,820 --> 00:01:40,979
꽤 많은 다량의 데이터를 보유하게 되었다고 

41
00:01:38,610 --> 00:01:43,979
볼 수 있죠, 이러한
모든 현상은,

42
00:01:40,979 --> 00:01:46,979
사회가 디지털화되면서 가능해진 것입니다.

43
00:01:43,979 --> 00:01:48,720
수많은 인간 활동에 있어서,

44
00:01:46,979 --> 00:01:51,180
디지털 영역이 상당부분 차지하고 있는데요,

45
00:01:48,720 --> 00:01:54,320
컴퓨터와 인터넷, 핸드폰에
많은 시간을 소비하면서

46
00:01:51,180 --> 00:01:57,960
어플이나 웹사이트를 접속하는 시간이 늘어났습니다.

47
00:01:54,320 --> 00:02:00,360
이런 현상으로 인해, 새로
만들어지는 데이터의 양이 늘고, 

48
00:01:57,960 --> 00:02:02,369
핸드폰 안에 장착되는 저렴한 카메라, 

49
00:02:00,360 --> 00:02:05,909
가속도계 등

50
00:02:02,369 --> 00:02:07,890
IoT의 센서들을 통해

51
00:02:05,909 --> 00:02:11,129
더 많은 양의 데이터를 수집하게 되었습니다.

52
00:02:07,890 --> 00:02:12,870
과거 20년 동안
수많은 양의 데이터를 다루게 되었는데요

53
00:02:11,129 --> 00:02:13,560
많은 어플을 통해

54
00:02:12,870 --> 00:02:16,319
더 많은 양의 데이터를 축적하고

55
00:02:13,560 --> 00:02:17,550
전통적인 트레이닝 

56
00:02:16,319 --> 00:02:20,520
알고리즘과 비교하여 더 많은 양을 

57
00:02:17,550 --> 00:02:22,560
효과적으로 다루면서,

58
00:02:20,520 --> 00:02:26,310
새로운 네트워크의 장점을 누릴 수 있게 되었습니다.

59
00:02:22,560 --> 00:02:28,470
조금한 신경망을 트레이닝 시키는 경우,

60
00:02:26,310 --> 00:02:31,349
그 성능은 이렇게 보여질 수 있습니다.

61
00:02:28,470 --> 00:02:34,590
medium-sized
인터넷이라도 불리는
조금 더 큰 버전의

62
00:02:31,349 --> 00:02:36,330
인터넷을 트레이닝 시키면

63
00:02:34,590 --> 00:02:39,900
조금 더 나은 그래프가
이렇게 나오는데요,

64
00:02:36,330 --> 00:02:42,180
매우 큰 신경망을
교육시키는 경우에는

65
00:02:39,900 --> 00:02:44,580
그 형태 그대로
유지하면서

66
00:02:42,180 --> 00:02:46,890
지속적으로 성능이
향상되는 경향이 있습니다.

67
00:02:44,580 --> 00:02:49,410
첫 번째로, 

68
00:02:46,890 --> 00:02:52,620
고성능의 프로그램에는

69
00:02:49,410 --> 00:02:54,420
2가지가 필요합니다.

70
00:02:52,620 --> 00:02:57,360
먼저 큰 신경망을
트레이닝 할 수 있어야 합니다. 

71
00:02:54,420 --> 00:02:59,670
매우 방대한 양의
데이터를 이용할 수 있으려면

72
00:02:57,360 --> 00:03:02,010
말이죠. 두 번째로, 

73
00:02:59,670 --> 00:03:05,430
x축에서 여기에 위치해야 하는데요

74
00:03:02,010 --> 00:03:07,799
그렇게 하기 위해선
많은 양의 데이터를 필요로 합니다.

75
00:03:05,430 --> 00:03:10,860
그렇기 때문에 scale이 딥러닝의 

76
00:03:07,799 --> 00:03:12,900
진행속도를 유지시켰다고
보고 있는데요, scale이라고 하면

77
00:03:10,860 --> 00:03:15,150
신경망의 크기를
이야기하기도 하고,

78
00:03:12,900 --> 00:03:17,069
새로운 네트워크, 숨겨진 다량의 유닛,

79
00:03:15,150 --> 00:03:21,480
많은 양의 변수, 연결 요소들,

80
00:03:17,069 --> 00:03:23,910
그리고 데이터의 scale을 이야기하기도 합니다.

81
00:03:21,480 --> 00:03:25,440
실제로 가장 믿을
수 있는 방법 중 한가지는

82
00:03:23,910 --> 00:03:27,390
신경망의 성능을
향상 시키기 위해선

83
00:03:25,440 --> 00:03:29,940
더 큰 네트워크를
트레이닝 시키거나

84
00:03:27,390 --> 00:03:31,829
데이터를 더 구현시키는
방법입니다.

85
00:03:29,940 --> 00:03:33,359
이러한 시도들은
특정 시점까지만 효과가 있는데요,

86
00:03:31,829 --> 00:03:35,640
그 이유는 어느
시점 이후 데이터가 고갈되기 때문입니다.

87
00:03:33,359 --> 00:03:37,769
아니면 결국에는
여러분의 네트워크가 

88
00:03:35,640 --> 00:03:40,200
너무 커져서 트레이닝
시키는데 너무 많은 시간이 

89
00:03:37,769 --> 00:03:42,690
소요될 수 있습니다. 그렇지만 scale을 향상 시키는 것 만으로

90
00:03:40,200 --> 00:03:45,810
러닝의 세계에서는
많은 길을 걸어왔습니다.

91
00:03:42,690 --> 00:03:48,060
해당 도표를 조금
더 

92
00:03:45,810 --> 00:03:49,920
기술적으로 정확도를
높이고, 

93
00:03:48,060 --> 00:03:53,040
제가 쓴 것들을
더하기 위해서, x축에 위치한

94
00:03:49,920 --> 00:03:57,900
데이터의 양, 엄밀히 이야기하면 이건

95
00:03:53,040 --> 00:04:00,180
레이블 되어 있는
데이터의 양인데요,

96
00:03:57,900 --> 00:04:03,630
레이블 데이터라고
하면, 트레이닝 예시를 말합니다.

97
00:04:00,180 --> 00:04:05,910
x입력 값과 y레이블 트레이닝 예시에 대해서 

98
00:04:03,630 --> 00:04:07,709
추가로 표기법을
도입했습니다.

99
00:04:05,910 --> 00:04:10,769
나중에 학습할 비디오에서
사용할 것입니다.

100
00:04:07,709 --> 00:04:12,540
이제부터는 소문자
알파벳으로 사용하도록 하겠습니다.

101
00:04:10,769 --> 00:04:13,739
트레이닝 세트의
사이즈를 나타낼 때나

102
00:04:12,540 --> 00:04:15,690
트레이닝 예시의
개수를 나타낼 때 

103
00:04:13,739 --> 00:04:18,989
소문자 m을 사용하고, 

104
00:04:15,690 --> 00:04:20,310
가로축에 표기될
것입니다.

105
00:04:18,989 --> 00:04:23,340
해당 표 관련해서, 몇
개 더 이야기할 텐데요,  

106
00:04:20,310 --> 00:04:26,970
더 작은 트레이닝
세트의 지도에서는 

107
00:04:23,340 --> 00:04:29,700
알고리즘의 상대적
순서가 

108
00:04:26,970 --> 00:04:31,590
잘 정의되어 있지
않기 때문에 

109
00:04:29,700 --> 00:04:34,500
트레이닝 세트가
많이 없다고 하면

110
00:04:31,590 --> 00:04:36,510
본인 고유 스킬,

111
00:04:34,500 --> 00:04:39,090
엔지니어링 특성에
좌우할 수 있습니다.

112
00:04:36,510 --> 00:04:41,910
그렇기 때문에 

113
00:04:39,090 --> 00:04:44,070
누군가가 SVM 을 트레이닝 시킬 경우,

114
00:04:41,910 --> 00:04:46,320
hand
engineering을
쓰기 원한다면 

115
00:04:44,070 --> 00:04:48,300
본인이 더 큰 트레이닝
세트를 다루고

116
00:04:46,320 --> 00:04:50,730
지금 것은 작은
트레이닝이라고 여기면

117
00:04:48,300 --> 00:04:53,130
SEM이 더 효과적일
수 있습니다.

118
00:04:50,730 --> 00:04:55,020
이 영역에서 왼쪽부분은 

119
00:04:53,130 --> 00:04:57,090
상대적 순서가,

120
00:04:55,020 --> 00:04:59,550
유전자 알고리즘끼리의
배열이

121
00:04:57,090 --> 00:05:01,919
잘 정의되어 있지
않고, 그 성능은 

122
00:04:59,550 --> 00:05:03,389
개인의 엔진 특성
스킬에 따라

123
00:05:01,919 --> 00:05:05,970
그리고 알고리즘의
모바일 정보에 따라 차이가 날 수 있습니다.

124
00:05:03,389 --> 00:05:08,850
그리고 오로지 

125
00:05:05,970 --> 00:05:12,000
많은 양의 트레이닝세트가
있는 빅데이터 체제에서만 

126
00:05:08,850 --> 00:05:14,669
대문자 M, 즉 오른쪽부분에서만

127
00:05:12,000 --> 00:05:17,639
꾸준히 큰 신경망의 접근이

128
00:05:14,669 --> 00:05:19,560
우위적으로 점유하고 있는
것을 볼 수 있습니다.

129
00:05:17,639 --> 00:05:21,600
만약 여러분 친구가 신경망이
왜 

130
00:05:19,560 --> 00:05:23,700
뜨고 있는지 물어본다면,
저 같은 경우에

131
00:05:21,600 --> 00:05:26,729
이 그림을 그릴 것 같습니다.

132
00:05:23,700 --> 00:05:28,890
저는 딥러닝이 급부상하던

133
00:05:26,729 --> 00:05:29,310
초기 시점에는

134
00:05:28,890 --> 00:05:32,070
scaled
data와
scale computation이 중심적인

135
00:05:29,310 --> 00:05:34,919
역할을 했다고 봅니다.

136
00:05:32,070 --> 00:05:36,330
아주 큰 신경망 네트워크를

137
00:05:34,919 --> 00:05:39,479
트레이닝 하는 방법 말이죠.

138
00:05:36,330 --> 00:05:41,850
CPU나 GPU에서 말이죠.

139
00:05:39,479 --> 00:05:43,590
이를 통해 딥러닝 분야는

140
00:05:41,850 --> 00:05:45,800
최근 몇 년간 많은 발전을
이루었고,

141
00:05:43,590 --> 00:05:48,360
아주 큰 알고리즘 

142
00:05:45,800 --> 00:05:50,539
혁신을 이루었습니다.

143
00:05:48,360 --> 00:05:53,700
그래서 제가 과소평가하고
싶지 않은 부분은,

144
00:05:50,539 --> 00:05:56,940
알고리즘의 혁신 중 상당
부분은

145
00:05:53,700 --> 00:06:01,139
신경망을 더 빨리 운영되게

146
00:05:56,940 --> 00:06:03,510
만들고 이 중 가장 구체적인
예로

147
00:06:01,139 --> 00:06:05,310
가장 큰 네트워크의 변화를
가지고 온

148
00:06:03,510 --> 00:06:08,729
부분은 바로 시그모이드
함수에서

149
00:06:05,310 --> 00:06:12,330
이렇게 보이는 함수인데요,

150
00:06:08,729 --> 00:06:14,760
이렇게 생긴 함수에서,
ReLU(렐류) 함수로

151
00:06:12,330 --> 00:06:18,479
변경되었다는 점입니다.

152
00:06:14,760 --> 00:06:20,190
이전 비디오에서 잠깐 다루었던
내용이죠. 

153
00:06:18,479 --> 00:06:22,260
제가 이야기하고 있는 내용이
잘 이해 가지 않으시면

154
00:06:20,190 --> 00:06:24,389
너무 걱정하지 마십시오.

155
00:06:22,260 --> 00:06:26,010
시그모이드 함수의 문제는
나중에 알고 보니

156
00:06:24,389 --> 00:06:27,870
머신러닝에 적용시키는 경우

157
00:06:26,010 --> 00:06:29,520
이 부분들이

158
00:06:27,870 --> 00:06:30,280
함수의 기울기가 

159
00:06:29,520 --> 00:06:32,920
거의

160
00:06:30,280 --> 00:06:35,350
0에 가까운 값이 되는데요,

161
00:06:32,920 --> 00:06:37,060
그러므로 러닝의 속도가
매우 느려집니다.

162
00:06:35,350 --> 00:06:39,639
그 이유는
gradient descent (기울기 강하)를 도입하는 경우

163
00:06:37,060 --> 00:06:41,470
기울기가 0이면 개체가
매우 

164
00:06:39,639 --> 00:06:44,740
느린 속도로 변하기 때문입니다.

165
00:06:41,470 --> 00:06:46,450
반면에, 액티베이션 함수로

166
00:06:44,740 --> 00:06:48,600
바꾸는 경우엔 신경망이

167
00:06:46,450 --> 00:06:52,060
여기 보이는 함수를 적용시켜

168
00:06:48,600 --> 00:06:54,970
Relu(렐류)라고 하는 함수,
rectified linear unit,

169
00:06:52,060 --> 00:06:57,070
R-E-L-U,
기울기는 

170
00:06:54,970 --> 00:07:00,220
모든 양수에 대해 1이되고, 

171
00:06:57,070 --> 00:07:03,100
그렇기 때문에 기울기가

172
00:07:00,220 --> 00:07:04,750
0으로 줄어드는 확률은 급격히
감소됩니다.

173
00:07:03,100 --> 00:07:07,300
그래서, 여기 이 부분에서
기울기는

174
00:07:04,750 --> 00:07:09,520
값이 0이지만, 

175
00:07:07,300 --> 00:07:12,580
시그모이드 함수에서,

176
00:07:09,520 --> 00:07:14,410
ReLU 함수로 변경하면서, 

177
00:07:12,580 --> 00:07:16,960
gradient
descent (기울기
강하)라는 알고리즘을 

178
00:07:14,410 --> 00:07:19,169
생성시켰습니다. 더 빨리
작동하는데요,

179
00:07:16,960 --> 00:07:22,030
이런 사례와 같이, 일종의
알고리즘

180
00:07:19,169 --> 00:07:23,860
발명을 통해 결과적으로

181
00:07:22,030 --> 00:07:27,520
알고리즘의 혁신을 일으키고,

182
00:07:23,860 --> 00:07:29,080
계산을 하는 과정에 큰
도움을 주었습니다.

183
00:07:27,520 --> 00:07:31,240
이제까지 이런
example들이 꽤 많았는데요

184
00:07:29,080 --> 00:07:33,340
알고리즘에 변형을 주어서

185
00:07:31,240 --> 00:07:35,140
코딩이 더 빨리 작동할
수 있도록 환경을 제공해주고

186
00:07:33,340 --> 00:07:37,479
결과적으로 더 큰 신경망을

187
00:07:35,140 --> 00:07:39,520
트레이닝 시킬 수 있도록
해줍니다.

188
00:07:37,479 --> 00:07:42,250
진행하는 과정에 많은 양의
데이터로 

189
00:07:39,520 --> 00:07:45,810
이루어진 네트워크가 있어도
말이죠.

190
00:07:42,250 --> 00:07:48,610
빠른 계산이 중요한 또
다른 이유는, 

191
00:07:45,810 --> 00:07:51,070
네트워크를 트레이닝 시키는

192
00:07:48,610 --> 00:07:53,710
과정이 매우 반복적이고

193
00:07:51,070 --> 00:07:56,350
자주 신경망 구조에 대한

194
00:07:53,710 --> 00:07:58,020
발상이나 아이디어가 있는
경우,

195
00:07:56,350 --> 00:08:01,060
이러한 발상을 코드화시켜,

196
00:07:58,020 --> 00:08:02,830
그 발상을 바탕으로 실험을
하고, 

197
00:08:01,060 --> 00:08:05,050
실험을 통해 신경망이 얼마나

198
00:08:02,830 --> 00:08:07,510
성능을 발휘하는지 확인하고,

199
00:08:05,050 --> 00:08:10,030
다시 돌아가서 네트워크의
상세

200
00:08:07,510 --> 00:08:12,930
내용을 바꾸고 

201
00:08:10,030 --> 00:08:15,880
이 원형 프로세스를 계속
반복합니다.

202
00:08:12,930 --> 00:08:18,550
새로운 네트워크가 트레이닝
시키는데 

203
00:08:15,880 --> 00:08:21,400
오래 시간이 소요되는 경우엔

204
00:08:18,550 --> 00:08:24,039
이 원형 cycle이 진행되는
시간이 늘어나며

205
00:08:21,400 --> 00:08:26,740
생산성은 큰 차이를 불러옵니다.

206
00:08:24,039 --> 00:08:29,560
효과적인 신경망을 만드는
환경이 

207
00:08:26,740 --> 00:08:34,169
변할 수 있는 것이죠. 아이디어를 기반으로

208
00:08:29,560 --> 00:08:36,370
10분안에 작동하는 것을 볼
수도 있고, 하루가 걸릴 수도 있지만

209
00:08:34,169 --> 00:08:39,490
반대로 한 달간 신경망을
트레이닝 

210
00:08:36,370 --> 00:08:40,590
시키는 경우도 발생할 수

211
00:08:39,490 --> 00:08:42,570
있습니다.

212
00:08:40,590 --> 00:08:44,670
이유인 즉 슨, 시도해보고

213
00:08:42,570 --> 00:08:47,250
10분안에 바로 결과가 나오거나,
하루 만에 결과가 나오거나

214
00:08:44,670 --> 00:08:49,170
할 수 있기 때문입니다.
여러 가지 아이디어를 시도해보면서

215
00:08:47,250 --> 00:08:50,610
본인의 네트워크를 통해

216
00:08:49,170 --> 00:08:53,720
어플이 잘 작동하는지 확인할
수 있습니다.

217
00:08:50,610 --> 00:08:57,900
그러므로, 빠른 산출은

218
00:08:53,720 --> 00:08:59,730
실험한 내용에 대한

219
00:08:57,900 --> 00:09:02,610
결과를 빨리 확인하는데

220
00:08:59,730 --> 00:09:05,400
큰 도움을 주었습니다.

221
00:09:02,610 --> 00:09:07,550
신경망 분야 종사자들이나

222
00:09:05,400 --> 00:09:10,650
리서치 부문 담당자들에게도

223
00:09:07,550 --> 00:09:13,320
도움을 주면서 반복업무를

224
00:09:10,650 --> 00:09:16,589
더 빨리 가능케 하고 아이디어를
더 신속히

225
00:09:13,320 --> 00:09:18,570
개선시킬 수 있게 되면서

226
00:09:16,589 --> 00:09:21,029
전체적인 딥러닝 커뮤니티에게는

227
00:09:18,570 --> 00:09:23,370
아주 요긴한 것임이 드러났습니다.

228
00:09:21,029 --> 00:09:25,620
새로운 알고리즘을 개발하는
것,

229
00:09:23,370 --> 00:09:28,920
멈추지 않고 지속적으로

230
00:09:25,620 --> 00:09:30,990
발전을 시키는 부분에서
말이죠.

231
00:09:28,920 --> 00:09:33,570
이런 점들이 딥러닝을

232
00:09:30,990 --> 00:09:36,000
급부상하게 만드는 것입니다.

233
00:09:33,570 --> 00:09:38,490
좋은 소식은 이런 강력한
요소들이 지속적으로 

234
00:09:36,000 --> 00:09:41,130
딥러닝을 더 좋게 만들고
있다는 것입니다.

235
00:09:38,490 --> 00:09:43,800
데이터를 예로 들겠습니다.
사회는 더 많은 데이터를 뱉어내고 있습니다.

236
00:09:41,130 --> 00:09:45,660
또는 산출 부분, 

237
00:09:43,800 --> 00:09:48,300
특화된 하드웨어의 부상,

238
00:09:45,660 --> 00:09:50,940
GPU나 더 빠른 네트워킹

239
00:09:48,300 --> 00:09:53,250
등의 다양한 하드웨어 말이죠.

240
00:09:50,940 --> 00:09:55,140
우리가 아주 큰 신경망을
만들 수 있는 능력이

241
00:09:53,250 --> 00:09:57,320
산출하는 부분에 있어서

242
00:09:55,140 --> 00:10:00,360
더욱 좋아질 것이라고 자신합니다.

243
00:09:57,320 --> 00:10:02,880
알고리즘을 말해보죠. 

244
00:10:00,360 --> 00:10:05,070
리서치 커뮤니티는 계속해서

245
00:10:02,880 --> 00:10:07,680
알고리즘 측면에서 봤을
때 

246
00:10:05,070 --> 00:10:09,839
혁신을 하는데 있어 환상적인
성과를 보이고 있고,

247
00:10:07,680 --> 00:10:11,370
긍정적인 시각을 유지할
수 있다고 봅니다.

248
00:10:09,839 --> 00:10:13,650
딥러닝은 계속해서 

249
00:10:11,370 --> 00:10:14,120
앞으로 몇 년 간 발전을

250
00:10:13,650 --> 00:10:17,100
이룰 것 입니다.

251
00:10:14,120 --> 00:10:18,540
이제 마지막 비디오로 넘어가겠습니다.

252
00:10:17,100 --> 00:10:20,280
다음 비디오에선

253
00:10:18,540 --> 00:10:22,610
이번 코스에서 어떤 것들을
배우는지

254
00:10:20,280 --> 00:10:22,610
더 자세히 말씀 드리겠습니다.