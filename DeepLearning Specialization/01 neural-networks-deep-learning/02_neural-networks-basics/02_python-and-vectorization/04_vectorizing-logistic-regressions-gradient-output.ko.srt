1
00:00:00,000 --> 00:00:01,440
이전 강의에서 여러분에게

2
00:00:01,440 --> 00:00:05,700
이번 비디오에서는, vectorization을 이용하여, 예측수치를 산출하는 방법을 배웠는데요, 

3
00:00:05,700 --> 00:00:11,485
전체 트레이닝 세트 O에 대해서 소문자 a들을 상대로 말이죠. 

4
00:00:11,485 --> 00:00:15,030
이번 비디오에서는 vectorization을 이용해서 

5
00:00:15,030 --> 00:00:19,205
어떻게 M 트레이닝 샘플들 데 대해 기울기를 산출할 수 있는지 보겠습니다.

6
00:00:19,205 --> 00:00:21,380
다시 한번, 모두 한꺼번에 말이죠. 

7
00:00:21,380 --> 00:00:22,890
그리고 비디오 마지막 부분에서는, 

8
00:00:22,890 --> 00:00:26,175
모두 취합하여, 매우 효율적인 로지스틱 회귀분석의 

9
00:00:26,175 --> 00:00:29,730
도입을 가능케 할지 한번 보여드리겠습니다.

10
00:00:29,730 --> 00:00:32,505
기울기 강하 산출에는 기억하시겠지만, 

11
00:00:32,505 --> 00:00:36,910
첫 번째 예시에 대해서 dz1을 계산하고, 

12
00:00:36,910 --> 00:00:43,870
이 값은 a1 빼기 y1 그리고 dz2는 

13
00:00:43,870 --> 00:00:52,134
a2 빼기 y2 등등 말이죠. 

14
00:00:52,134 --> 00:00:56,425
모들 m 트레이닝 예시에 대해서 말이죠. 

15
00:00:56,425 --> 00:01:01,218
이제 저희는 새로운 변수를 정의할 것입니다.

16
00:01:01,218 --> 00:01:08,595
dZ는 dz1, dz2, dz2이 될 텐데요, 

17
00:01:08,595 --> 00:01:13,910
다시 한번, d 소문자 z의 변수는 가로로 쌓입니다.

18
00:01:13,910 --> 00:01:21,200
그러면 이것은 1xm 매트릭스 또는 m차원의 벡터입니다.

19
00:01:21,200 --> 00:01:23,520
이전 슬라이드에서 아시겠지만, 

20
00:01:23,520 --> 00:01:28,405
대문자 A를 구하는 방법은 알아냈는데요, a1에서 am까지

21
00:01:28,405 --> 00:01:36,735
말이죠. 그리고 여기 대문자 Y는 y1에서 ym까지의 값입니다.

22
00:01:36,735 --> 00:01:39,200
이것 또한, 가로로 쌓인 것이죠. 

23
00:01:39,200 --> 00:01:42,780
이러한 정의를 바탕으로, 

24
00:01:42,780 --> 00:01:46,770
직접 보실 수도 있겠지만, dz은 

25
00:01:46,770 --> 00:01:52,750
A 빼기 Y로 계산되는데요, a1 - y1이기 때문에 

26
00:01:52,750 --> 00:01:55,670
이것은 first element가 되고, a2 - y2는

27
00:01:55,670 --> 00:01:59,980
second element이렇게 이어집니다. 

28
00:01:59,980 --> 00:02:06,115
그렇기 때문에 first element인 a1-y1은 dz1의 정의와 똑같습니다.

29
00:02:06,115 --> 00:02:11,670
second element 는 dz2와 동일하구요, 

30
00:02:11,670 --> 00:02:13,965
그렇게 해서 한 줄의 코드로, 

31
00:02:13,965 --> 00:02:20,095
모든 값을 한번에 산출해낼 수 있습니다.

32
00:02:20,095 --> 00:02:24,010
이전의 도입에서 보면, 

33
00:02:24,010 --> 00:02:27,695
1개의 for loop를 이미 없앴는데요, 

34
00:02:27,695 --> 00:02:31,600
여기 두 번째 for loop은 트레이닝 예시에 대해 아직 있었습니다.

35
00:02:31,600 --> 00:02:35,440
그래서 dw를 0으로 초기화하고, 0의 벡터로 말이죠. 

36
00:02:35,440 --> 00:02:38,905
하지만 아직도 트레이닝 예시 에 대해 loop over해야 합니다.

37
00:02:38,905 --> 00:02:43,015
dw 플러스는 x1 곱하기 dz1 인데요, 

38
00:02:43,015 --> 00:02:50,440
첫 번째 트레이닝 예시에 대해 말이죠, dw 플러스는 x2 dz2 등등으로 이어집니다.

39
00:02:50,440 --> 00:02:56,980
이것을 M번 반복하여 dw 나누기=M과 같이 됩니다. B도 비슷하게 말이죠. 맞죠?

40
00:02:56,980 --> 00:03:03,370
db는 0으로 포기화됐고, db 플러스는 dz1, 

41
00:03:03,370 --> 00:03:09,120
db 플러스는 dz2 이렇게 말이죠, 

42
00:03:09,120 --> 00:03:16,835
dz(m)까지 이어지는 데요, db /=M이 됩니다. 이것이 이전 도입에 있었던 내용인데요, 

43
00:03:16,835 --> 00:03:18,700
1개의 for loop은 이미 없앤 상태입니다.

44
00:03:18,700 --> 00:03:25,045
이제 적어도 dw은 벡터이고 개별적으로 dw1, 

45
00:03:25,045 --> 00:03:26,850
dw2, 등등을 업데이트 해줬습니다.

46
00:03:26,850 --> 00:03:29,860
이제 이미 없앴긴 했지만, 

47
00:03:29,860 --> 00:03:33,630
아직도 트레이닝세트에서 m 개의 예시에 대해서는 for loop이 있었습니다.

48
00:03:33,630 --> 00:03:36,290
그러면 여기 절차를 이용해서 vectorize 시켜 보겠습니다.

49
00:03:36,290 --> 00:03:38,380
저희가 할 수 있는 것은 이렇습니다.

50
00:03:38,380 --> 00:03:42,745
db의 vectorize 도입을 위해서는, 저희는 기본적으로 이것을 더하는 것입니다.

51
00:03:42,745 --> 00:03:47,940
여기 모든 dz들을 더한 후에, m으로 나누어 주는 것이죠, 

52
00:03:47,940 --> 00:03:51,580
db는 1 나누기 

53
00:03:51,580 --> 00:03:56,530
그리고 합의 공식은 i=1 에서 m까지 적용되는데요, 그 공식은 dzi 입니다. 

54
00:03:56,530 --> 00:04:03,055
여기서 모든 dz는 row vector입니다. 그러므로 파이썬에서는, 

55
00:04:03,055 --> 00:04:04,765
여러분이 할 것은, 

56
00:04:04,765 --> 00:04:08,155
1/m np. 

57
00:04:08,155 --> 00:04:12,210
dz의 합을 도입시킵니다.

58
00:04:12,210 --> 00:04:15,115
그러면 여기 변수를 가지고 np.sum 함수라고 부르십시오.

59
00:04:15,115 --> 00:04:19,195
그러면 db를 줄 것입니다.

60
00:04:19,195 --> 00:04:22,330
dw는 어떨까요? 올바른 

61
00:04:22,330 --> 00:04:26,375
공식을 적을 테니 나중에 확인하셔도 됩니다. 

62
00:04:26,375 --> 00:04:28,164
1 나누기 m, 

63
00:04:28,164 --> 00:04:34,485
곱하기 X매트릭스 곱하기 dz transpose입니다.

64
00:04:34,485 --> 00:04:37,990
왜 이런 건지 조금 보자면, 

65
00:04:37,990 --> 00:04:41,806
이건은 1 나누기 m, 그 다음에 매트릭스 x들인데요, 

66
00:04:41,806 --> 00:04:48,325
x1에서 xm까지 세로로 쌓이고, dz transpose는

67
00:04:48,325 --> 00:04:56,040
이렇게 dz1에서 dzm까지 이어질 것입니다. 

68
00:04:56,040 --> 00:05:00,900
그러면 여기 이 매트릭스와 여기 매트릭스를 곱하면 되는 값을 보자면, 

69
00:05:00,900 --> 00:05:05,585
1 나누기 M 곱하기 x1dz1 더하기... 

70
00:05:05,585 --> 00:05:12,523
더하기 xm dzm이 됩니다.

71
00:05:12,523 --> 00:05:21,405
그러면 이것은 n x 1벡터인데요, 이것이 남게 됩니다.

72
00:05:21,405 --> 00:05:24,725
dw와 같이 말이죠, 그 이유는 dw는 

73
00:05:24,725 --> 00:05:27,710
xi dzi와 같은 값을 더해서 생기는 것인데요, 

74
00:05:27,710 --> 00:05:32,300
여기 이 매트릭스를 곱하면서 그런 작용이 일어납니다.

75
00:05:32,300 --> 00:05:35,655
그러면 가시, 한 줄의 코드를 이용해서, dw를 계산할 수 있습니다.

76
00:05:35,655 --> 00:05:40,010
그래서 derivative 계산에 대한 vectorized 도입은 이렇습니다, 

77
00:05:40,010 --> 00:05:44,540
여기 이 라인을 이용해서 db를 도입하고, 

78
00:05:44,540 --> 00:05:50,540
여기 이 라인을 이용해서 dw를 도입합니다. 그리고 아시겠지만, 트레이닝 세트에
거쳐서 for loop를 이용해

79
00:05:50,540 --> 00:05:55,265
이제는 파라미터에서 원하는 업데이트를 계산할 수 있습니다.

80
00:05:55,265 --> 00:06:01,185
이제, 모든 것을 취합해서 로지스틱 회귀분석을 어떻게 도입할지 보겠습니다.

81
00:06:01,185 --> 00:06:02,550
이것이 원본인데요, 

82
00:06:02,550 --> 00:06:07,866
매우 비효율적인 non-vectorized 도입 방식입니다.

83
00:06:07,866 --> 00:06:11,775
이전 비디오에서 처음으로 한 것은 여기 이 볼륨을 없앴습니다. 맞죠? 

84
00:06:11,775 --> 00:06:14,400
그러면 dw1, dw2 등에 

85
00:06:14,400 --> 00:06:15,755
looping over하는 대신에, 

86
00:06:15,755 --> 00:06:23,595
이 값을 벡터 값인 dw, 그 값이 dw+=xi인 값으로 대체했습니다.

87
00:06:23,595 --> 00:06:28,775
이제는 이 값이 벡터 곱하기 dz(i)죠.

88
00:06:28,775 --> 00:06:32,000
하지만 이제, 어떤 줄에서의 for loop를 없애는 것뿐만 아니라

89
00:06:32,000 --> 00:06:36,670
여기 전체 for loop를 없앨 수 있다는 것을 알죠. 

90
00:06:36,670 --> 00:06:38,654
이렇게 하는 것입니다.

91
00:06:38,654 --> 00:06:42,925
이전 슬라이드에 있는 내용을 사용하면, 

92
00:06:42,925 --> 00:06:46,085
대문자 Z가, 

93
00:06:46,085 --> 00:06:57,625
w transpose X 더하기 B이고, 사용하는 코드는 

94
00:06:57,625 --> 00:07:07,315
np w transpose X 더하기 B입니다. 그리고 a는 대문자 Z의 시그마입니다.

95
00:07:07,315 --> 00:07:12,710
그럼 여러분은 이제 여기 모든 값을 구했고, 모든 i값에 대해 여기 있는 값들을 구했습니다.

96
00:07:12,710 --> 00:07:14,715
다음으로, 이전 슬라이드에서는, 

97
00:07:14,715 --> 00:07:21,070
dz는 A -Y로 계산한다고 했죠. 

98
00:07:21,070 --> 00:07:24,460
그리고 여기 있는 모든 것을 i의 모든 값에 대해 구했습니다.

99
00:07:24,460 --> 00:07:31,495
마지막으로, dw=1 나누기 m 곱하기 

100
00:07:31,495 --> 00:07:39,700
dz transpose t 그리고 db는 1 나누기 m 그리고 

101
00:07:39,700 --> 00:07:43,328
sum (dz) 입니다.

102
00:07:43,328 --> 00:07:49,120
그러면 여러분은 전 방향전파 과 후 방향전파를 했고, 

103
00:07:49,120 --> 00:07:53,030
예측 값과 derivative를 계산함으로써, 모든 M 트레이닝 샘플에 대해서 말이죠, 

104
00:07:53,030 --> 00:07:57,340
for loop사용 없이 말입니다.

105
00:07:57,340 --> 00:08:00,835
그러면, 기울기 강하 업데이트는 , 

106
00:08:00,835 --> 00:08:04,462
w 는 w 빼기 러닝속도 곱하기 dw 

107
00:08:04,462 --> 00:08:12,020
dw는 위에서 계산했고요, b는 b 빼기 러닝속도 곱하기 db로 업데이트 됩니다.

108
00:08:12,020 --> 00:08:17,341
특정 방법 이 있는 걸 볼 수 있습니다.

109
00:08:17,341 --> 00:08:21,675
저도 생각해보면 이 표기법을 그리 일관되게 쓰진 않았네요. 

110
00:08:21,675 --> 00:08:25,450
이렇게 하여, 로지스틱 회귀분석에서 

111
00:08:25,450 --> 00:08:29,635
single iteration의 기울기 강하를 방금 도입했습니다.

112
00:08:29,635 --> 00:08:32,308
제가 일정한 for loop을 없앨 수 있을 때

113
00:08:32,308 --> 00:08:35,260
무조건 없애야 한다고 했는데요, 

114
00:08:35,260 --> 00:08:38,230
만약 여러분이 복수의 iteration을 통해

115
00:08:38,230 --> 00:08:42,880
도입하고 싶은 경우 몇몇의 iteration에서 for loop이 필요할 수 있습니다.

116
00:08:42,880 --> 00:08:47,860
여러분이 만약 기울기 강하의 1000개의 iteration을 갖고 싶으면, 

117
00:08:47,860 --> 00:08:53,675
이 숫자까지 for loop이 필요할 것입니다.

118
00:08:53,675 --> 00:08:55,870
이렇게 가장 바깥 쪽의 for loop이 있는 경우엔, 

119
00:08:55,870 --> 00:08:59,210
여러분이 아마 for loop를 없앨 수 있는 방법이 없을 것입니다.

120
00:08:59,210 --> 00:09:02,390
하지만 제 생각에는 적어도 

121
00:09:02,390 --> 00:09:07,117
한번의 기울기 강하의 iteration을 for loop없이 도입할 수 있다는 것은 꽤 멋진 것 같습니다.

122
00:09:07,117 --> 00:09:09,880
자 이게 본 강의 내용의 전부인데요, 
이제 여러분은 vectorize되고,

123
00:09:09,880 --> 00:09:14,745
매우 효율적인 로지스틱 회귀분석 기울기 강하 도입을 압니다.

124
00:09:14,745 --> 00:09:18,850
한가지 더 내용이 있는데요, 이것은 다음 비디오에서 이야기하겠습니다.

125
00:09:18,850 --> 00:09:24,155
여기에서 앞서 언급한 broadcasting에 대한 내용인데요, 

126
00:09:24,155 --> 00:09:28,240
파이썬 넘파이가 특정 부분의 코드에서 더 효율적으로 될 수 있도록

127
00:09:28,240 --> 00:09:32,915
가능케 하는 기술이 바로 broadcasting 입니다.

128
00:09:32,915 --> 00:09:37,090
broadcasting에 대한 더 자세한 내용은 다음 비디오를 통해 보겠습니다.