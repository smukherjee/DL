1
00:00:00,870 --> 00:00:04,550
जब आप बनाते हैं आपका न्यूरल नेटवर्क, 
एक विकल्प जो आपके पास है कि क्या

2
00:00:04,550 --> 00:00:07,570
ऐक्टिवेशन फ़ंक्शन इस्तेमाल में लेना चाहिए हिडन लेयर्स के लिए,

3
00:00:07,570 --> 00:00:10,950
और क्या हैं आउट्पुट यूनिट्स आपके न्यूरल नेटवर्क में.

4
00:00:10,950 --> 00:00:14,360
अभी तक, हम सिर्फ़ इस्तेमाल करते रहे हैं 
सिग्मोईड ऐक्टिवेशन फ़ंक्शन.

5
00:00:14,360 --> 00:00:17,980
लेकिन कभी-कभी अन्य विकल्प बेहतर काम कर सकते हैं.

6
00:00:17,980 --> 00:00:19,970
चलिए देखते हैं कुछ अन्य विकल्प.

7
00:00:19,970 --> 00:00:23,840
फ़ॉर्वर्ड प्रॉपगेशन सटेप्स में न्यूरल नेटवर्क के,

8
00:00:23,840 --> 00:00:28,279
हमारे पास हैं ये तीन स्टेप्स जहाँ हम लेते हैं 
सिग्मोईड फ़ंक्शन यहाँ.

9
00:00:28,279 --> 00:00:32,460
तो उस सिग्मोईड फ़ंक्शन को कहते हैं एक ऐक्टिवेशन फ़ंक्शन.

10
00:00:32,460 --> 00:00:35,871
और यहाँ है परिचित सिग्मोईड फ़ंक्शन,

11
00:00:35,871 --> 00:00:39,980
a बराबर है एक बटा एक जमा e की पावर नेगेटिव z.

12
00:00:39,980 --> 00:00:43,910
अधिक सामान्य केस में हमारे पास 
हो सकता है एक भिन्न फ़ंक्शन,

13
00:00:45,050 --> 00:00:50,100
g ऑफ़ z, जो मैं लिखूँगा यहाँ,

14
00:00:51,300 --> 00:00:57,300
जहाँ g हो सकता है एक नॉन-लिनीअर फ़ंक्शन 
जो शायद न हो सिग्मोईड फ़ंक्शन.

15
00:00:57,300 --> 00:01:02,530
तो उदाहरण के लिए, सिग्मोईड फ़ंक्शन होता है 
ज़ीरो और एक के बीच, और

16
00:01:02,530 --> 00:01:07,040
ऐक्टिवेशन फ़ंक्शन जो लगभग हमेशा बेहतर 
काम करता हैं तुलना में सिग्मोईड फ़ंक्शन के

17
00:01:07,040 --> 00:01:12,220
है tanh फ़ंक्शन या हायपर्बालिक टैंजेंट फ़ंक्शन.

18
00:01:12,220 --> 00:01:18,909
तो यह है z, यह है a, यह है a बराबर tanh(z),

19
00:01:18,909 --> 00:01:24,390
और यह होता है प्लस 1 और माइनस 1 के बीच.

20
00:01:24,390 --> 00:01:29,745
tanh फ़ंक्शन का फ़ॉर्म्युला है e की पावर

21
00:01:29,745 --> 00:01:35,920
z माइनस e की पावर नेगेटिव z बटा उनका योग.

22
00:01:35,920 --> 00:01:42,829
और यह है वास्तव में गणितीय रूप में एक 
स्थानांतरित वर्ज़न सिग्मोईड फ़ंक्शन का.

23
00:01:42,829 --> 00:01:47,399
तो, एक सिग्मोईड फ़ंक्शन जैसे लेकिन स्थानांतरित ताकि

24
00:01:47,399 --> 00:01:54,180
यह अब क्रॉस करता है ज़ीरो,ज़ीरो पोईंट और 
री-स्केल होता है, तो जाता है माइनस 1 से प्लस 1 तक.

25
00:01:54,180 --> 00:01:59,406
और ऐसा होता है कि हिडन यूनिट्स के लिए, 
यदि आप रखते हैं फ़ंक्शन g

26
00:01:59,406 --> 00:02:06,775
ऑफ़ ज बराबर tanh(z) के,

27
00:02:06,775 --> 00:02:11,490
यह हमेशा बेहतर 
काम करता है तुलना में सिग्मोईड

28
00:02:11,490 --> 00:02:16,540
फ़ंक्शन के क्योंकि माइनस 1 और प्लस 1 के बीच की वैल्यूज़, 
औसत ऐक्टिवेशन्स का

29
00:02:16,540 --> 00:02:20,360
जो आता है आपके नेट से, और वे है क़रीब 0 औसत के.

30
00:02:20,360 --> 00:02:24,468
और इसलिए कभी-कभी जब आप
 ट्रेन करते हैं एक लर्निंग अल्गोरिद्म,

31
00:02:24,468 --> 00:02:26,364
आप डेटा को केंद्रित कर सकते हैं और

32
00:02:26,364 --> 00:02:31,029
कर सकते हैं आपका डेटा 0 औसत के क़रीब इस्तेमाल
 करके एक tanh बजाय सिग्मोईड फ़ंक्शन के.

33
00:02:31,029 --> 00:02:34,500
इसका एक तरह से प्रभाव है केंद्रित करना आपका डेटा ताकि

34
00:02:34,500 --> 00:02:39,420
आपके डेटा का औसत है क़रीब 0 के बजाय शायद 0.5 के.

35
00:02:39,420 --> 00:02:43,200
और यह वास्तव में बनाता है लर्निंग 
अगली लेयर के लिए थोड़ा आसान.

36
00:02:43,200 --> 00:02:46,900
हम बात करेंगे थोड़ा और इस बारे में दूसरे कोर्स में 
जब हम बात करेंगे ऑप्टिमायज़ेशन

37
00:02:46,900 --> 00:02:47,990
अल्गोरिद्म्स की भी.

38
00:02:47,990 --> 00:02:50,480
लेकिन एक चीज़ समझने की है कि मैं

39
00:02:50,480 --> 00:02:54,500
लगभग कभी भी इस्तेमाल नहीं करता 
सिग्मोईड ऐक्टिवेशन फ़ंक्शन अब.

40
00:02:54,500 --> 00:02:58,410
tanh फ़ंक्शन है लगभग हमेशा बेहतर.

41
00:02:58,410 --> 00:03:04,880
एक अपवाद है आउट्पुट लेयर के लिए, 
क्योंकि यदि y है 0 या

42
00:03:04,880 --> 00:03:10,070
1 तो बेहतर होगा y हैट के होना एक नम्बर,

43
00:03:10,070 --> 00:03:14,830
वह जिसकी आउट्पुट है 0 और 1 के बीच 
बजाय माइनस 1 और 1 के बीच.

44
00:03:14,830 --> 00:03:19,380
तो एक अपवाद जहाँ मैं इस्तेमाल करूँगा सिग्मोईड

45
00:03:19,380 --> 00:03:23,610
ऐक्टिवेशन फ़ंक्शन है जब आप कर रहे हैं बाइनेरी क्लासिफ़िकेशन,

46
00:03:23,610 --> 00:03:28,760
जिस केस में आप शायद लें सिग्मोईड
 ऐक्टिवेशन फ़ंक्शन आउट्पुट लेयर के लिए.

47
00:03:28,760 --> 00:03:34,828
तो g ऑफ़ z यहाँ है बराबर सिग्मोईड z 2.

48
00:03:34,828 --> 00:03:40,236
और इसलिए जो आप देखते हैं इस उदाहरण में है 
जहां आपके पास है शायद एक tanh

49
00:03:40,236 --> 00:03:47,102
ऐक्टिवेशन फ़ंक्शन हिडन लेयर के लिए, 
और सिग्मोईड आउट्पुट लेयर के लिए.

50
00:03:47,102 --> 00:03:50,982
तो ऐक्टिवेशन फ़ंक्शन्स भिन्न हो सकते हैं भिन्न लेयर्स के लिए.

51
00:03:50,982 --> 00:03:55,112
और कभी-कभी बताने के लिए कि ऐक्टिवेशन फ़ंक्शंज़ हैं भिन्न

52
00:03:55,112 --> 00:03:59,907
भिन्न लेयर्स के लिए, हम शायद इस्तेमाल करें ये
 वर्ग कोष्ठक सूपर्स्क्रिप्ट भी

53
00:03:59,907 --> 00:04:05,270
इंगित करने के लिए कि g वर्ग कोष्ठक एक 
हो सकता है भिन्न g वर्ग कोष्ठक दो से.

54
00:04:05,270 --> 00:04:09,360
और फिर से, वर्ग कोष्ठक एक सूपरस्क्रिप्ट 
संदर्भित करता है इस लेयर को और

55
00:04:09,360 --> 00:04:12,280
सुपरस्क्रिप्ट वर्ग कोष्ठक दो संदर्भित करता है आउट्पुट लेयर को.

56
00:04:13,830 --> 00:04:17,050
अब एक नकारात्मक पहलू दोनो सिग्मोईड फ़ंक्शन और

57
00:04:17,050 --> 00:04:21,920
tanh फ़ंक्शन का है कि यदि z है या बहुत बड़ा या बहुत छोटा,

58
00:04:21,920 --> 00:04:26,380
तब ग्रेडीयंट या डेरिवेटिव या स्लोप 
इस फ़ंक्शन की हो जाती है बहुत कम.

59
00:04:26,380 --> 00:04:29,550
तो यदि z है बहुत बड़ा या z है बहुत छोटा,

60
00:04:29,550 --> 00:04:33,740
इस फ़ंक्शन की स्लोप हो जाती लगभग 0.

61
00:04:33,740 --> 00:04:35,600
और इसलिए यह धीमा कर सकता है ग्रेडीयंट डिसेंट.

62
00:04:36,630 --> 00:04:39,850
तो एक अन्य विकल्प जो कि बहुत लोकप्रिय है

63
00:04:39,850 --> 00:04:44,650
मशीन लर्निंग में है जिसे कहते हैं रेक्टिफ़ाई लिनीअर यूनिट.

64
00:04:44,650 --> 00:04:47,925
तब वैल्यू फ़ंक्शन दिखता है ऐसा.

65
00:04:50,463 --> 00:04:57,020
और फार्मूला है a = मैक्स (0, z).

66
00:04:57,020 --> 00:05:01,755
डेरिवेटिव है 1 जब तक z पॉज़िटिव है.

67
00:05:01,755 --> 00:05:05,785
डेरिवेटिव का या स्लोप है 0 जब z नेगेटिव है.

68
00:05:05,785 --> 00:05:06,855
यदि आप इम्प्लमेंट कर रहे हैं इसे,

69
00:05:06,855 --> 00:05:11,425
तकनीकी रूप से डेरिवेटिव जब Z है पूर्णत: 0, 
सही ढंग से परिभाषित नहीं है

70
00:05:11,425 --> 00:05:13,175
लेकिन जब आप इम्प्लमेंट करते हैं इसे कम्प्यूटर में,

71
00:05:13,175 --> 00:05:17,947
उत्तर जो आपको पूरा मिलता है वह है z बराबर है 0000000000000.

72
00:05:17,947 --> 00:05:22,697
यह बहुत कम है और इसलिए अभ्यास
 में इसकी चिंता करने की जरूरत नहीं है.

73
00:05:22,697 --> 00:05:26,729
आप सोच सकते हैं डेरिवेटिव जब z है बराबर 0 के,

74
00:05:26,729 --> 00:05:31,775
आप ले सकते हैं इसे या तो 1 या 0 और 
तब एक तरह से आपका काम चल जाता है.

75
00:05:31,775 --> 00:05:35,640
तो तथ्य कि इसे ड़िफ़्फ़ेरेंशिएट नहीं कर सकते और तथ्य कि, तो

76
00:05:35,640 --> 00:05:39,670
यहाँ हैं कुछ अनुभवसिद्ध नियम 
चुनने के लिए ऐक्टिवेशन फ़ंक्शन्सज़.

77
00:05:39,670 --> 00:05:44,940
यदि आपकी आउट्पुट है 0,1 वैल्यू, 
यदि आप कर रहे हैं बाइनेरी क्लासिफ़िकेशन,

78
00:05:44,940 --> 00:05:49,390
तब सिग्मोईड ऐक्टिवेशन फ़ंक्शन है 
स्वभाविक विकल्प आउट्पुट लेयर के लिए.

79
00:05:49,390 --> 00:05:54,272
और फ़िर अन्य सभी यूनिट्स के लिए ReLU,

80
00:05:54,272 --> 00:05:58,374
या रेक्टिफ़ायड लिनीअर यूनिट,

81
00:06:02,348 --> 00:06:06,689
है अधिकांश डिफ़ॉल्ट विकल्प ऐक्टिवेशन फ़ंक्शन का.

82
00:06:06,689 --> 00:06:11,000
तो यदि आप सुनिश्चित नहीं है 
क्या इस्तेमाल करना है हिडन लेयर के लिए,

83
00:06:11,000 --> 00:06:14,681
मैं बस इस्तेमाल करूँगा ReLU ऐक्टिवेशन

84
00:06:14,681 --> 00:06:16,820
यह आप ज़्यादातर लोगों को करते हुए देखते हैं आजकल.

85
00:06:16,820 --> 00:06:21,460
हालाँकि कभी-कभी लोग इस्तेमाल करते हैं 
tanh ऐक्टिवेशन भी.

86
00:06:21,460 --> 00:06:25,852
एक नुक़सान ReLU का है कि डेरिवेटिव है बराबर ज़ीरो के,

87
00:06:25,852 --> 00:06:27,152
जब z है नेगेटिव.

88
00:06:27,152 --> 00:06:29,136
अधिकतर, यह सही काम करता है.

89
00:06:29,136 --> 00:06:33,323
लेकिन एक और वर्ज़न है ReLU का जिसे कहते 
हैं लीकी ReLU.

90
00:06:33,323 --> 00:06:35,174
मैं आपको दूँगा फ़ॉर्म्युला अगली स्लाइड पर.

91
00:06:35,174 --> 00:06:38,581
लेकिन बजाय इसके होने के ज़ीरो जब z है नेगेटिव,

92
00:06:38,581 --> 00:06:43,890
यह लेता है थोड़ा स्लोप ऐसे, तो इसे कहते हैं लीकी ReLU.

93
00:06:45,870 --> 00:06:49,850
यह अक्सर बेहतर काम करता है ReLU ऐक्टिवेशन फ़ंक्शन से,

94
00:06:49,850 --> 00:06:53,790
हालाँकि उतना इस्तेमाल नहीं होता अभ्यास में.

95
00:06:53,790 --> 00:06:55,130
दोनो में से कोई भी ठीक होना चाहिए, हालाँकि.

96
00:06:55,130 --> 00:06:59,140
यदि आपको लेना होता एक, मैं अक्सर लेता हूँ ReLU.

97
00:06:59,140 --> 00:07:03,255
लाभ दोनो ReLU और लीकी ReLU का है कि

98
00:07:03,255 --> 00:07:07,765
z की बहुत सी स्पेस में, डेरिवेटिव ऐक्टिवेशन फ़ंक्शन का,

99
00:07:07,765 --> 00:07:12,061
स्लोप ऐक्टिवेशन फ़ंक्शन की है बहुत भिन्न 0 से.

100
00:07:12,061 --> 00:07:15,472
तो अभ्यास में, इस्तेमाल करना ReLU ऐक्टिवेशन फ़ंक्शन,

101
00:07:15,472 --> 00:07:19,855
आपका न्यूरल नेटवर्क अक्सर लर्न करेगा बहुत जल्दी तुलना में 
इस्तेमाल करने से tanh या

102
00:07:19,855 --> 00:07:21,959
सिग्मोईड ऐक्टिवेशन फ़ंक्शन.

103
00:07:21,959 --> 00:07:26,148
और मुख्य कारण है कि वहाँ इसका प्रभाव कम है स्लोप का

104
00:07:26,148 --> 00:07:30,015
फ़ंक्शन के ज़ीरो होने पर जिससे धीरे हो जाती है लर्निंग.

105
00:07:30,015 --> 00:07:34,425
और मैं जानता हूँ कि रेंज के लिए z की, स्लोप ReLU की है 0,

106
00:07:34,425 --> 00:07:39,585
लेकिन व्यावहारिक रूप में, 
बहुत सी हिडन यूनिट्स में होगा z बड़ा 0 से.

107
00:07:39,585 --> 00:07:43,040
तो लर्निंग अभी भी हो सकती है तेज़ी से 
अधिकांश ट्रेनिंग इग्ज़ाम्पल्ज़ के लिए.

108
00:07:43,040 --> 00:07:47,490
तो चलिए जल्दी से दोहराते हैं लाभ और हानियाँ 
विभिन्न ऐक्टिवेशन फ़ंक्शंज़ की.

109
00:07:47,490 --> 00:07:49,160
यहाँ है सिग्मोईड ऐक्टिवेशन फ़ंक्शन.

110
00:07:49,160 --> 00:07:53,360
मैं कहूँगा कभी इस्तेमाल न करें इसे, सिवाय आउट्पुट लेयर के.

111
00:07:53,360 --> 00:07:56,740
यदि आप कर रहे हैं बाइनेरी वर्गीकरण, 
या शायद लगभग कभी न इस्तेमाल करें इसे.

112
00:07:57,950 --> 00:08:01,860
और कारण कि मैं यह लगभग कभी इस्तेमाल 
नहीं करता इसका क्योंकि tanh

113
00:08:01,860 --> 00:08:04,330
है बहुत अधिक बेहतर.

114
00:08:04,330 --> 00:08:06,532
तो tanh ऐक्टिवेशन फ़ंक्शन है यह.

115
00:08:11,012 --> 00:08:12,476
और फिर डिफ़ॉल्ट,

116
00:08:12,476 --> 00:08:17,190
सबसे अधिक इस्तेमाल किया जाने वाला 
ऐक्टिवेशन फ़ंक्शन है ReLU, जो है यह.

117
00:08:18,400 --> 00:08:22,790
तो यदि आप सुनिश्चित नहीं है 
क्या इस्तेमाल करना है इसे करें, और

118
00:08:22,790 --> 00:08:27,668
और शायद बेझिझक इस्तेमाल करें leaky ReLU.

119
00:08:27,668 --> 00:08:34,540
जहाँ यह शायद है (0.01 z, z).

120
00:08:34,540 --> 00:08:39,140
ठीक है? तो a है मैक्स ऑफ़ 0.01 गुणा z और

121
00:08:39,140 --> 00:08:43,200
z का, तो वह देता है आपको ये मोड़ फ़ंक्शन में.

122
00:08:43,200 --> 00:08:48,210
और आप शायद कहें, क्यों वह कॉन्स्टंट 0.01?

123
00:08:48,210 --> 00:08:53,366
ख़ैर, आप बना सकते है उसे एक 
अन्य पेरामिटर लर्निंग अल्गोरिद्म का.

124
00:08:53,366 --> 00:08:55,320
कुछ लोगों का कहना है कि वह काम करता है 
और भी बेहतर

125
00:08:55,320 --> 00:08:56,790
लेकिन मैं शायद ही लोगों को देखता हूँ 
ऐसा करते हुए.

126
00:08:58,380 --> 00:09:01,910
लेकिन यदि आपका परीक्षण करने का मन है उसे 
आपकी ऐप्लिकेशन में, बेझिझक करें वैसा.

127
00:09:01,910 --> 00:09:05,460
और आप देख सकते हैं यह कैसे काम करता है,
 कितना बेहतर करता है, और

128
00:09:05,460 --> 00:09:07,980
रखें इसे यदि यह देता है आपको एक बढ़िया परिणाम.

129
00:09:07,980 --> 00:09:11,828
तो मुझे आशा है कि यह आपको देता है एक ज्ञान 
 विकल्पों का ऐक्टिवेशन फ़ंक्शंज़ के आप

130
00:09:11,828 --> 00:09:13,389
इस्तेमाल कर सकते हैं आपके न्यूरल नेटवर्क में.

131
00:09:13,389 --> 00:09:17,276
एक विषय जो हम देखेंगे डीप लर्निंग में है कि 
आपके पास अक्सर होते हैं बहुत से

132
00:09:17,276 --> 00:09:20,299
विभिन्न विकल्प कि कैसे आप कोड करते हैं 
आपका न्यूरल नेटवर्क.

133
00:09:20,299 --> 00:09:24,024
हिडन यूनिट्स से लेकर, ऐक्टिवेशन फ़ंक्शन के विकल्प,

134
00:09:24,024 --> 00:09:26,752
कैसे आप प्रारम्भिक वैल्यूज़ देते हैं लेयर्स को 
जो हम देखेंगे बाद में.

135
00:09:26,752 --> 00:09:28,310
बहुत से विकल्प उस तरह के.

136
00:09:28,310 --> 00:09:32,630
और ऐसा होता है कि कभी-कभी कठिन होता है 
मिल पाना अच्छे दिशा निर्देश

137
00:09:32,630 --> 00:09:35,280
कि क्या करेगा श्रेष्ठतम कार्य आपकी समस्या के लिए.

138
00:09:35,280 --> 00:09:38,410
तो पूरे इस कोर्स में में देता रहूँगा आपको एक समझ

139
00:09:38,410 --> 00:09:41,890
जो मैं देखता हूँ इंडस्ट्री में कि क्या कम या अधिक लोकप्रिय है.

140
00:09:41,890 --> 00:09:45,215
लेकिन आपकी ऐप्लिकेशन के लिए, 
आपकी ऐप्लिकेशन की बारीकियों के साथ,

141
00:09:45,215 --> 00:09:49,330
यह वास्तव में बहुत कठिन है जान पाना 
पहले से कि क्या श्रेष्ठतम रहेगा.

142
00:09:49,330 --> 00:09:52,480
तो एक सामान्य सलाह होगी कि यदि आप 
सुनिश्चित नहीं है कौन सा इन

143
00:09:52,480 --> 00:09:57,220
activation functions करेगा श्रेष्ठतम कार्य,
सबको ट्राइ करो, और मूल्यांकन करो

144
00:09:57,220 --> 00:10:02,220
एक होल्डआउट वैलिडेशन सेट पर, या एक 
डिवेलप्मेंट सेट पर, जिसकी हम बाद में बात करेंगे, और

145
00:10:02,220 --> 00:10:05,780
कौन सा काम करता है बेहतर और फिर चुने उसे.

146
00:10:05,780 --> 00:10:10,303
और मुझे लगता हैं परीक्षण करने से भिन्न 
विकल्प आपकी ऐप्लिकेशन के लिए

147
00:10:10,303 --> 00:10:15,189
आपको भविष्य अशुद्धि जांच में बेहतर होगा 
आपके न्यूरल नेटवर्क आर्किटेक्चर को

148
00:10:15,189 --> 00:10:20,030
आपकी प्रॉब्लम की बारीकियों में, 
तथा अल्गोरिद्म्स के विकास में.

149
00:10:20,030 --> 00:10:24,694
बजाय इसके कि मैं आपको बोलूँ कि 
हमेशा इस्तेमाल करें ReLU ऐक्टिवेशन और

150
00:10:24,694 --> 00:10:26,720
न इस्तेमाल करें कुछ भी और.

151
00:10:26,720 --> 00:10:30,877
वह शायद हो या न हो अप्लाई आपकी समस्या पर
 जिस पर आप काम करें

152
00:10:30,877 --> 00:10:33,925
या निकट भविष्य में या आने वाले भविष्य में.

153
00:10:33,925 --> 00:10:37,420
ठीक है, तो वे थे विकल्प ऐक्टिवेशन फ़ंक्शन्स के और

154
00:10:37,420 --> 00:10:39,890
आपने देखे सबसे लोकप्रिय ऐक्टिवेशन फ़ंक्शन्स.

155
00:10:39,890 --> 00:10:43,120
वहां एक अन्य सवाल है जो 
कभी-कभी आप पूछ सकते हैं,

156
00:10:43,120 --> 00:10:46,490
जो है, क्यों आपको इस्तेमाल करना ही है 
एक ऐक्टिवेशन फ़ंक्शन?

157
00:10:46,490 --> 00:10:48,260
क्यों उसे छोड़ नहीं सकते?

158
00:10:48,260 --> 00:10:51,510
तो चलो बात करते हैं उस बारे में अगले वीडियो में, जहाँ

159
00:10:51,510 --> 00:10:56,230
आप देखेंगे कि क्यों न्यूरल नेटवर्क्स को चाहिए 
किसी तरह का नॉन-लीनियर ऐक्टिवेशन फ़ंक्शन.