上一段的影片我們描述了 深度 l 層神經網路， 也 談到了有關的記號我們用來 描述這樣的網路, 在這段影片您 會看到如何執行正向傳播 在深度網路，像往常一樣我們先 看看正向傳播 在單一訓練例子 x 的樣子 然後我們會談到 向量化版本當您 同時執行正向傳播在 整個訓練集, 但 給予一個單一訓練例子 X 這是您如何計算 第一層的啟動, 對於第一 層您計算 z1 等於 w1 乘 x 加 b1, 所以 w1 跟 b1 是 影響第一層的參數 這是第一層 神經網路, 然後您計算 那一層的啟動值等於 g of z1, 而啟動函數 g 取決於您在第幾層 也許用指標來標示 這是第一層的啟動函數, 如果您這樣做 您就計算了 第一層的啟動值 第二層又是如何呢? 您會這樣計算 z2 等於 w2 a1 加 b2 所以啟動值在 第二層是權值矩陣乘上 第一層的輸出, 這個值, 加上 第二層偏差向量然後 a2 等於啟動函數應用到 z2, 所以這是第二層等等 直到您算到輸出層 就是第四層您會 有 z4 等於這層參數 乘上啟動值 從上一層來的 加上偏差向量然後 同樣的 a4 等於 g of z4 這是您如何計算預估 輸出 y-hat, 一件事情要提醒 這裡的 x 也是 a0, 因為 輸入特徵向量 x 也是 第 0 層的啟動值, 所以我們挖出 x 劃掉 x 然後放 a0 然後您看所有這些 方程式看起來都一樣 一般的規則是 zl 等於 wl 乘 l - 1 加上 bl 然後那一層的啟動值會是 啟動函數應用到 z 值, 所以這是一般化正向 傳播方程式, 所以也們做完了所有這些 對於單一訓練例子, 如何 做向量化 同時作用在整個訓練集 方程式看起來相當類似於 之前, 對於第一層您會有 大寫 Z1 等於 W1 乘上 大寫 X 加 b1, 然後  A1 等於 g of Z1, 注意到這個 X 是等於 A0, 這是 將訓練例子疊在 不同的列上, 您可以 挖出 X, 您可以放 A0 在這裡, 而下一層 像是 Z2 等於 W2 A1 加 b2, 然後 A2 等於 g of Z2 我們只是拿這些向量 z 或 a 等等將它們疊起來, 所以 這是 z 向量對於第一個訓練 例子, z 向量對於第二個訓練 例子等等直到第 m 個訓練 例子然後依列疊起來 稱之為大寫 Z 同樣的對於大寫 A 也只是像大寫 X 所有訓練例子是列 向量從左疊到右, 然後 一樣在這個程序完成時 y-hat 等於 g of z4 這也是等於 A4, 這是 對所有訓練例子的預估 水平疊起來 總結一下我們的符號我將 修改一下所們的符號 取代小寫 z 跟 a 用 大寫的同樣字母這已經 像是大寫 Z, 而這給 您向量化的正向 傳播讓您同時執行在 整個訓練集而 A0 是 X, 如果您看這個 向量化的建置它像是 會有一個迴圈 也就是 for l 等於 1 到 4, l 從 1 到大寫 L, 您需要計算 第一層的啟動值, 然後第二層 然後第三層, 第四層 似乎要用一個迴圈 我知道當建置您的 神經網路時我們通常想去掉 明顯迴圈，但這個地方 我不認為有辦法 建置這個而不用 迴圈， 所以當我們建置正向 傳播時是完全可以有 迴圈當我們計算啟動值 第一層, 第二層, 第 三層, 第四層, 沒有人知道可不可以 我想沒辦法 做這些不用迴圈 從 1 到大寫 L, 從 1 到 您有幾層在您的神經網路 在這裡是完全沒問題的 使用明顯的迴圈 這是深度神經網路的記號 跟我們如何做正向 傳播在這個網路, 如果 我們看到目前的片段有一點點 似曾相似, 那是因為 我們看到的是拿一些片段 類似於您在 神經網路使用一個隱藏層 只是重複多做幾次 實際上當我們建置深度 神經網路, 一種方式 增加您的建置無臭蟲(錯誤)的機會 是用很 系統化的方式來想, 很小心在 您使用矩陣的維度 當我試著寫我的程式時, 我 通常用一張紙 很小心地想著 我使用的矩陣的維度 讓我們看看您也如何可以做到 在下一段影片