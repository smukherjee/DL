1
00:00:00,320 --> 00:00:04,140
إذا كانت الأفكار التقنية الأساسية التي
يقوم عليها التعلم العميق

2
00:00:04,140 --> 00:00:07,110
والتي تقوم عليها الشبكات العصبية
الجديدة موجودة منذ عقود

3
00:00:07,110 --> 00:00:08,970
لماذا لم تأخذ في الانتشار بهذه
الطريقة إلا الآن؟

4
00:00:08,970 --> 00:00:11,310
في هذا الفيديو سوف نقوم بتوضيح بعض

5
00:00:11,310 --> 00:00:14,690
الأسباب الرئيسية لازدياد الاهتمام بالتعلم
العميق حيث أظن أن ذلك

6
00:00:14,690 --> 00:00:17,540
سوف يساعدك على اقتناص أفضل الفرص

7
00:00:17,540 --> 00:00:20,560
في المؤسسة التي تعمل بها
لتطبيق هذه المفاهيم عليها.

8
00:00:20,560 --> 00:00:21,690
خلال السنوات القليلة الأخيرة

9
00:00:21,690 --> 00:00:23,095
سألني العديد من الأشخاص
السؤال التالي

10
00:00:23,095 --> 00:00:25,880
"أندرو، لماذا يعمل التعليم العميق
بهذا الشكل الجيد للغاية؟"

11
00:00:25,880 --> 00:00:30,460
وعندما كان يُطرح ذلك السؤال
عليّ، غالبًا كنت قوم برسم
الصورة التالية لهم.

12
00:00:30,460 --> 00:00:33,830
لنفترض أننّا رسمنا الشكل
 البياني حيث المحور الأفقي

13
00:00:33,830 --> 00:00:37,675
يمثل مقدار البيانات
التي لدينا لمهمة معينة.

14
00:00:37,675 --> 00:00:40,040
لنفترض أننا نستخدم المحور الرأسي

15
00:00:40,040 --> 00:00:43,610
ونرسم أداء خوارزمية التعلم الخاصة بنا

16
00:00:43,610 --> 00:00:47,660
مثل دقة مُصنف البريد
الإلكتروني المزعج لدينا أو

17
00:00:47,660 --> 00:00:50,870
برنامج التنبؤ بالنقرات الخاص بنا أو دقة

18
00:00:50,870 --> 00:00:55,980
شبكتنا العصبية لاستكشاف مواقع
 السيارات الأخرى النسبة لسيارتنا
ذاتية القيادة.

19
00:00:55,980 --> 00:01:00,680
يتضح أنه عندما تقوم برسم منحنى
الأداء خوارزمية تعلم تقليدية مثل

20
00:01:00,680 --> 00:01:02,090
ماكينة متجه الدعم أو

21
00:01:02,090 --> 00:01:06,340
التراجع اللوجيستي كدالة لمقدار
البيانات لمتاحة لدينا

22
00:01:06,340 --> 00:01:09,185
فقد تحصل على منحنى قوسي مثل هذا

23
00:01:09,185 --> 00:01:14,220
حيث يتحسن الأداء لمدة معينة
 كلما قمت إضافة المزيد
من البيانات، ولكن بعد فترة معينة

24
00:01:14,220 --> 00:01:17,565
يحافظ منحني الأداء على
نفس القيمة تقريبًا.

25
00:01:17,565 --> 00:01:20,100
هذا الجزء من المفترض أن يكون
 عبارة عن خطوط أفقية
ولكني رسمته بشكل غير متقن.

26
00:01:20,100 --> 00:01:26,760
يمكن تفسير ذلك كأنهم لم يكونوا قادرين
 على التعامل مع كل هذا القدر
الضخم من البيانات.

27
00:01:26,760 --> 00:01:29,270
والذي حدث في مجتمعنا على مدار

28
00:01:29,270 --> 00:01:32,360
العشرين عامًا الماضية تقريبًا أنه
فيما يخص العديد من المشكلات

29
00:01:32,360 --> 00:01:35,900
انتقلنا من مرحلة كنا نملك فيها كمية
ضئيلة نسبيًا من البيانات

30
00:01:35,900 --> 00:01:39,985
إلى مرحلة نملك فيها
كمية هائلة من البيانات

31
00:01:39,985 --> 00:01:43,430
ومعظم ذلك ناجم عن رقمنة

32
00:01:43,430 --> 00:01:48,020
المجتمع حيث تتم إدارة الكثير من
 الأنشطة البشرية
حاليًا عبر المنصات الإلكترونية.

33
00:01:48,020 --> 00:01:50,810
فنحن نمضي الكثير من الوقت أمام
أجهزة الكمبيوتر وعلى مواقع الويب

34
00:01:50,810 --> 00:01:56,475
وعلى تطبيقات الهاتف المحمول
 والأنشطة التي نستخدم فيها
أجهزة رقمية وتقوم بخلق بيانات.

35
00:01:56,475 --> 00:01:59,720
وبفضل انتشار الكاميرات رخيصة الثمن

36
00:01:59,720 --> 00:02:01,970
والموجودة في الهواتف الجوالة وكذلك مقاييس السرعة

37
00:02:01,970 --> 00:02:05,645
و كل أنواع المستشعرات التي تستخدم في إنترنت الأشياء

38
00:02:05,645 --> 00:02:09,350
بدأنا كذلك في جمع المزيد والمزيد من البيانات.

39
00:02:09,350 --> 00:02:12,590
لذلك، خلال العشرين عامًا الماضية
بالنسبة للعديد من التطبيقات

40
00:02:12,590 --> 00:02:15,935
قمنا بحفظ وتجميع المزيد من المعلومات أكثر

41
00:02:15,935 --> 00:02:20,015
من القدر الذي كانت خوارزميات التعلم التقليدية
تستطيع الاستفادة منه بشكل فعال.

42
00:02:20,015 --> 00:02:23,465
ومع الشبكات العصبية، تبين أنه إذا قمت بتدريب

43
00:02:23,465 --> 00:02:28,865
شبكة عصبية صغيرة، فإن الأداء سوف يكون شيئًا
مشابهًا لما يحدث عند

44
00:02:28,865 --> 00:02:31,765
تدريب شبكة عصبية أكبر إلى حد ما

45
00:02:31,765 --> 00:02:34,840
والتي تُدعى شبكة عصبية متوسطة الحجم

46
00:02:34,840 --> 00:02:40,275
حيث يكون الأداء أفضل دائمًا. وإذا قمت بتدريب
شبكة عصبية كبيرة للغاية

47
00:02:40,275 --> 00:02:43,750
فسوف يستمر أداء الشبكة في التحسن أكثر وأكثر.

48
00:02:43,750 --> 00:02:45,965
ونتيجة لذلك، هناك ملاحظتان تظهران، الأولى

49
00:02:45,965 --> 00:02:49,150
أنه إذا أردت أن تصل لهذا الأداء المرتفع للغاية

50
00:02:49,150 --> 00:02:50,720
فسوف تحتاج إلى شيئين اثنين.

51
00:02:50,720 --> 00:02:53,660
الشيء الأول أنك في الغالب يجب أن تكون
قادرًا على تدريب

52
00:02:53,660 --> 00:02:56,540
شبكة عصبية كبيرة بشكل كافٍ لكي تستطيع

53
00:02:56,540 --> 00:02:59,630
الاستفادة من القدر الكبير من البيانات التي لديك
والشيء الثاني

54
00:02:59,630 --> 00:03:01,760
أنك يجب أن تكون هنا على محور x

55
00:03:01,760 --> 00:03:03,850
وتحتاج إلى الكثير من البيانات.

56
00:03:03,850 --> 00:03:09,590
لذا، دائمًا ما نقول إن النطاق كان يدفع
التعلم العميق نحو التقدم، وما أعنيه بالنطاق

57
00:03:09,590 --> 00:03:12,380
هو حجم الشبكة العصبية

58
00:03:12,380 --> 00:03:16,090
أي تلك الشبكة العصبية التي تحتوي على الكثير
من الوحدات المخفية والكثير من المعلمات

59
00:03:16,090 --> 00:03:21,055
والكثير من الصلات وكذلك نطاق البيانات.

60
00:03:21,055 --> 00:03:25,190
في الواقع، في الغالب يكون من بين الطرق ذات
أفضل درجات الموثوقية للحصول على الأداء الجيد

61
00:03:25,190 --> 00:03:29,690
في الشبكات العصبية تدريب شبكة أكبر أو توفير
المزيد من البيانات

62
00:03:29,690 --> 00:03:32,210
لها، وهذا يجدي حتى نقطة معينة فقط لأنه

63
00:03:32,210 --> 00:03:35,510
في نهاية المطاف سوف تقوم بتشغيل كل البيانات أو
في نهاية المطاف تكون الشبكة العصبية

64
00:03:35,510 --> 00:03:38,120
ضخمة للغاية بحيث تستغرق وقتًا طويلاً للغاية
لتدريبها، إلا أن

65
00:03:38,120 --> 00:03:43,325
مجرد تحسين النطاق قد دفعنا للأمام كثيرًا في الواقع
في عالم التعلم العميق.

66
00:03:43,325 --> 00:03:46,860
ولجعل هذا الرسم البياني أكثر دقة من الناحية الفنية

67
00:03:46,860 --> 00:03:48,815
دعوني أضيف بعض الأشياء الأخرى.

68
00:03:48,815 --> 00:03:51,465
لقد كتبت مقدار البيانات على المحور "x".

69
00:03:51,465 --> 00:03:56,240
من الناحية الفنية، هذا مقدار البيانات المصنفة

70
00:03:56,240 --> 00:04:02,795
حيث ما أعنيه بالبيانات المصنفة أمثلة التدريب، لدينا
المدخلات "x" والتصنيف "y".

71
00:04:02,795 --> 00:04:07,500
لإعطاء المزيد من الرموز التي سوف نستخدمها
في وقت لاحق في هذه الدورة التدريبية

72
00:04:07,500 --> 00:04:12,460
سوف نستخدم الحرف "m" اللاتيني الصغير للإشارة إلى
حجم مجموعة التدريب الخاصة بي.

73
00:04:12,460 --> 00:04:17,390
لذا، يشير الحرف اللاتيني "m" الصغير إلى عدد أمثلة التدريب.
ويكون هذا هو المحور الأفقي.

74
00:04:17,390 --> 00:04:20,300
هنا تفصيلان آخران في هذا الشكل

75
00:04:20,300 --> 00:04:24,080
في هذا النظام الخاص بمجموعات التدريب الصغيرة

76
00:04:24,080 --> 00:04:28,970
لا يكون الترتيب النسبي للخوارزميات معرفًا
بشكل جيد في الواقع.

77
00:04:28,970 --> 00:04:32,690
لذا، يعتمد ما إذا لم يكن لديك الكثير من بيانات التدريب

78
00:04:32,690 --> 00:04:37,120
على مهارتك في سمات الهندسة اليدوية والتي تحدد الأداء.

79
00:04:37,120 --> 00:04:41,810
وبالتالي، يكون من الممكن إلى حد كبير أن يكون الشخص
الذي يقوم بالتدريب في SDM

80
00:04:41,810 --> 00:04:46,640
أكثر تحفزًا فيما يخص سمات الهندسة اليدوية عن شخص
يقوم بالتدريب على شبكة عصبية أكبر

81
00:04:46,640 --> 00:04:49,260
وربما في أنظمة مجموعات التدريب الصغيرة

82
00:04:49,260 --> 00:04:50,815
يمكن أن يكون SDM أفضل.

83
00:04:50,815 --> 00:04:54,140
لذا، في هذه المنطقة إلى يسار الشكل

84
00:04:54,140 --> 00:04:56,990
لا يكون الترتيب النسبي بين الخوارزميات

85
00:04:56,990 --> 00:05:00,800
معرفًا جيدًا، ويعتمد الأداء بشكل أكبر على مهارتك

86
00:05:00,800 --> 00:05:03,380
في سمات التعليق وغير ذلك من تفاصيل الخوارزميات

87
00:05:03,380 --> 00:05:07,700
البسيطة، وفي نظام البيانات الضخم هذا فقط

88
00:05:07,700 --> 00:05:09,080
ومجموعات التدريب الضخمة للغاية

89
00:05:09,080 --> 00:05:12,320
وأنظمة "m" الضخمة للغاية في اليمين، نرى بشكل أكثر

90
00:05:12,320 --> 00:05:16,960
اتساقًا الشبكات العصبية الضخمة وهي تسود
على المنهجيات الأخرى.

91
00:05:16,960 --> 00:05:21,315
لذا، إذا سألك أحد أصدقائك عن سبب انتشار الشبكات العصبية

92
00:05:21,315 --> 00:05:24,820
فإني أشجعك على رسم هذه الصورة له كذلك.

93
00:05:24,820 --> 00:05:27,695
ويمكن أن أقول أنه في الأيام الأولى للتعلم

94
00:05:27,695 --> 00:05:29,450
العميق وفي بداية نشأته المعاصرة

95
00:05:29,450 --> 00:05:33,395
كان الأمر يتعلق بنطاق البيانات ونطاق العمليات الحسابية.

96
00:05:33,395 --> 00:05:36,425
إن قدرتنا على تدريب الشبكات العصبية الضخمة للغاية

97
00:05:36,425 --> 00:05:38,420
سواء في وحدات المعالجة المركزية أو وحدات معالجة الرسومات

98
00:05:38,420 --> 00:05:41,555
هي ما ساعدتنا على تحقيق الكثير من التقدم.

99
00:05:41,555 --> 00:05:44,320
لكن، بشكل متزايد، على وجه الخصوص في
السنوات العديدة الأخيرة

100
00:05:44,320 --> 00:05:47,980
شهدنا ابتكارًا ضخمًا في مجال الخوارزميات كذلك.

101
00:05:47,980 --> 00:05:50,830
لذا، فأنا أيضًا لا أريد أن أتفهم هذا الأمر.

102
00:05:50,830 --> 00:05:54,995
ومن المثير للاهتمام أن الكثير من الابتكارات في
مجال الخوارزميات

103
00:05:54,995 --> 00:06:00,285
نجم عن محاولة دفع الشبكات العصبية نحو التقدم بسرعة أكبر.

104
00:06:00,285 --> 00:06:02,940
ومن بين الأمثلة الراسخة على ذلك

105
00:06:02,940 --> 00:06:05,210
أن واحدة من الطفرات الضخمة للغاية في الشبكات العصبية

106
00:06:05,210 --> 00:06:08,125
كانت تتمثل في التحول من استخدام دالة Sigmoid

107
00:06:08,125 --> 00:06:09,840
التي تبدو مثل ذلك

108
00:06:09,840 --> 00:06:13,535
إلى دالة ReLU التي تحدثنا عنها

109
00:06:13,535 --> 00:06:17,855
باختصار في فيديو سابق والتي تبدو مثل ذلك.

110
00:06:17,855 --> 00:06:22,010
إذا لم تكن تفهم تفاصيل ما أتحدث عنه
فلا تقلق بشأن ذلك.

111
00:06:22,010 --> 00:06:25,820
لكن، اتضح أن إحدى المشكلات الناجمة عن
استخدام دوال Sigmoid

112
00:06:25,820 --> 00:06:29,980
في تعليم الماكينات كانت وجود
تلك المناطق هنا في ميل الدالة

113
00:06:29,980 --> 00:06:31,905
حيث يكون التدرج قريبًا من الصفر

114
00:06:31,905 --> 00:06:34,685
وبالتالي يصبح التعلم بطيئًا للغاية

115
00:06:34,685 --> 00:06:37,945
لأنك عندما تقوم بتنفيذ انحدار تدريجي ويكون التدرج صفرًا

116
00:06:37,945 --> 00:06:41,820
تتغير المعلمات ببطء شديد، وبالتالي يصبح التعليم بطيئًا للغاية.

117
00:06:41,820 --> 00:06:47,090
في حين أنه من خلال تغيير دالة التنشيط في الشبكة العصبية

118
00:06:47,090 --> 00:06:52,835
لاستخدام هذه الدالة التي يطلق عليها دالة القيمة أو
وحدة التصحيح الخطية

119
00:06:52,835 --> 00:06:58,350
REOU، يساوي التدرج "واحد" لكل
قيم المدخلات الإيجابية حسنًا؟

120
00:06:58,350 --> 00:07:03,130
وبالتالي، تكون احتمالية تقليص التدرج إلى الصفر أقل بكثير.

121
00:07:03,130 --> 00:07:05,720
والتدرج هنا، ميل هذا الخط صفر في

122
00:07:05,720 --> 00:07:08,960
اليسار، إلا أنه يتضح أنه من خلال الانتقال من

123
00:07:08,960 --> 00:07:12,320
دالة Sigmoid إلى دالة القيمة

124
00:07:12,320 --> 00:07:16,380
جعل ذلك خوارزمية اسمها الانحدار
التدريجي تعمل بشكل أسرع.

125
00:07:16,380 --> 00:07:21,350
لذا، ربما يكون ذلك مثالاً على الابتكار البسيط
نسبيًا في مجال الخوارزميات

126
00:07:21,350 --> 00:07:26,755
إلا أن تأثير هذا الابتكار في مجال الخوارزميات كان في
مساعدته في العمليات الحسابية بشدة.

127
00:07:26,755 --> 00:07:29,765
وفي الواقع، هناك الكثير من الأمثلة المشابهة لذلك

128
00:07:29,765 --> 00:07:33,230
حيث قمنا بتغيير الخوارزمية لأن ذلك يؤدي إلى تشغيل التعليمات

129
00:07:33,230 --> 00:07:37,400
البرمجية بسرعة أكبر، كما أن ذلك يساعدنا على تدريب شبكات
عصبية أكبر أو استنتاج ذلك بحيث

130
00:07:37,400 --> 00:07:42,065
يكون مقدار التعليمات البرمجية معقولاً حتى عندما يكون
لدينا شبكة ضخمة أو الكثير من البيانات.

131
00:07:42,065 --> 00:07:47,060
السبب الآخر الذي يجعل العمليات الحسابية السريعة أمرًا مهمًا

132
00:07:47,060 --> 00:07:52,525
هو أنه اتضح أن عملية تدريب الشبكة الخاصة بك أمر يتكرر للغاية.

133
00:07:52,525 --> 00:07:54,050
في الغالب، تكون لديك فكرة

134
00:07:54,050 --> 00:07:58,285
لبنية الشبكة العصبية لديك، وبالتالي تقوم بتنفيذ الفكرة
في شكل تعليمات برمجية.

135
00:07:58,285 --> 00:08:01,999
وبالتالي، يساعدك تنفيذ الفكرة في إجراء تجربة

136
00:08:01,999 --> 00:08:05,880
تخبرك بمدى جودة عمل الشبكة
العصبية، ومن خلال النظر في ذلك

137
00:08:05,880 --> 00:08:08,240
يمكنك العودة لتغيير تفاصيل

138
00:08:08,240 --> 00:08:13,190
الشبكة العصبية الخاصة بك، وبعد ذلك تمر
في هذه الدائرة مرارًا وتكرارًا.

139
00:08:13,190 --> 00:08:17,089
وعندما تحتاج الشبكة العصبية
الخاصة بك إلى وقت كبير لتدريبها

140
00:08:17,089 --> 00:08:21,140
يتطلب الأمر الكثير من الوقت لكي تمر بهذه الدورة

141
00:08:21,140 --> 00:08:27,215
ويظهر فرق ضخم في إنتاجيتك في بناء الشبكات العصبية عندما

142
00:08:27,215 --> 00:08:32,455
تكون لديك فكرة وتجربها وتراها وهي تعمل خلال 10 دقائق

143
00:08:32,455 --> 00:08:38,570
أو حتى خلال جزء كبير من اليوم في مقابل تدريب
الشبكة العصبية الخاصة بك لمدة شهر.

144
00:08:38,570 --> 00:08:41,480
حسنًا، في بعض الأحيان يحدث ذلك، لأنك عندما تحصل

145
00:08:41,480 --> 00:08:44,630
على نتيجة خلال 10 دقائق أو حتى خلال يوم

146
00:08:44,630 --> 00:08:47,750
يمكن أن تقوم بتجريب المزيد من الأفكار وتزداد احتمالية

147
00:08:47,750 --> 00:08:52,005
اكتشاف شبكة عصبية تعمل بشكل جيد للتطبيق الخاص بك.

148
00:08:52,005 --> 00:08:57,320
لذا، فقد ساعد تسريع العمليات الحسابية بشدة فيما يتعلق

149
00:08:57,320 --> 00:09:02,495
بتسريع معدل الحصول على نتائج التجريب، وقد ساعد

150
00:09:02,495 --> 00:09:06,950
ذلك بشدة ممارسي الشبكات العصبية وكذلك

151
00:09:06,950 --> 00:09:10,550
الباحثين الذين يعملون في مجال التعلم العميق على التكرار

152
00:09:10,550 --> 00:09:14,530
بشكل أسرع كثيرًا وتحسين الأفكار بشكل أسرع.

153
00:09:14,530 --> 00:09:17,570
لذا، فقد كان ذلك كله بمثابة التطور الضخم

154
00:09:17,570 --> 00:09:22,390
في مجتمع أبحاث التعلم العميق برمته
وهو تطور مذهل في رأيي

155
00:09:22,390 --> 00:09:27,470
فيما يتعلق بابتكار خوارزميات جديدة وتحقيق
تقدم مستمر على تلك الجبهة.

156
00:09:27,470 --> 00:09:33,200
فهذه بعض القوى التي تعيق تطور التعلم
العميق إلا أن الخبر السار هنا

157
00:09:33,200 --> 00:09:38,825
أن هذه القوى ما زالت تعمل بكل قوة
لجعل التعلم العميق أقوى بالنسبة للبيانات.

158
00:09:38,825 --> 00:09:42,890
إن المجتمع ما زال ينتج المزيد من البيانات الرقمية أو يعتمد على

159
00:09:42,890 --> 00:09:47,630
العمليات الحسابية مع انتشار الأجهزة المتخصصة
مثل وحدات معالجة الرسومات والشبكات الأسرع

160
00:09:47,630 --> 00:09:49,595
والعديد من أنواع البرامج.

161
00:09:49,595 --> 00:09:54,260
أنا واثق للغاية في أن قدرتنا على بناء
شبكات عصبية ضخمة للغاية من

162
00:09:54,260 --> 00:09:59,300
وجهة نظر العمليات الحسابية سوف تستمر في التحسن
وتعتمد على الخوارزميات

163
00:09:59,300 --> 00:10:02,150
في حين أن مجتمعات أبحاث التعلم العميق برمتها

164
00:10:02,150 --> 00:10:06,170
سوف تستمر في أن تكون استثنائية فيما يتعلق
بالابتكار على جبهة الخوارزميات.

165
00:10:06,170 --> 00:10:07,550
لذا، لهذا السبب

166
00:10:07,550 --> 00:10:09,530
أعتقد أننا يمكن أن نشعر بالتفاؤل.

167
00:10:09,530 --> 00:10:11,720
أنا متفائل بالقطع حيال أن التعلم العميق سوف يستمر

168
00:10:11,720 --> 00:10:14,570
في التحسن لعدة سنوات قادمة.

169
00:10:14,570 --> 00:10:17,855
وبناءً على ذلك، دعونا ننتقل
إلى آخر فيديو في هذا القسم

170
00:10:17,855 --> 00:10:21,200
حيث سنتحدث بشكل أكبر على
ما تعلمناه في هذه الدورة التدريبية.