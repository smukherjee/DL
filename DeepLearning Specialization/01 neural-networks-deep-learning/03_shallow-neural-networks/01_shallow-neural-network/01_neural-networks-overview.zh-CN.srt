1
00:00:00,030 --> 00:00:05,279
欢迎回来 在这一周你将学习

2
00:00:02,639 --> 00:00:07,440
实现一个神经网络

3
00:00:05,279 --> 00:00:09,059
在了解技术细节之前

4
00:00:07,440 --> 00:00:10,889
我将在本视频中做一个快速的介绍

5
00:00:09,059 --> 00:00:13,679
关于本周将学习的内容

6
00:00:10,889 --> 00:00:15,389
所以如果你没有掌握完全

7
00:00:13,679 --> 00:00:17,100
不要担心

8
00:00:15,389 --> 00:00:19,680
它的技术细节

9
00:00:17,100 --> 00:00:21,660
将在之后的视频中详细介绍

10
00:00:19,680 --> 00:00:24,269
但现在我们只是先大概地

11
00:00:21,660 --> 00:00:26,250
了解一下如何实现神经网络

12
00:00:24,269 --> 00:00:30,300
上周我们讲了逻辑回归

13
00:00:26,250 --> 00:00:32,430
以及这个模型是怎么对应到

14
00:00:30,300 --> 00:00:35,520
下面的计算图

15
00:00:32,430 --> 00:00:38,370
其中我们通过特征x以及

16
00:00:35,520 --> 00:00:40,620
参数w和b的运算得到z

17
00:00:38,370 --> 00:00:44,219
然后通过z计算得到a

18
00:00:40,620 --> 00:00:47,190
接着我们用a或者同等的y^

19
00:00:44,219 --> 00:00:51,059
来计算得到损失函数L

20
00:00:47,190 --> 00:00:52,920
一个神经网络就像这样形成了

21
00:00:51,059 --> 00:00:54,930
正如我之前有顺便提到

22
00:00:52,920 --> 00:00:57,239
我们可以通过堆叠

23
00:00:54,930 --> 00:01:00,420
一系列的σ单元

24
00:00:57,239 --> 00:01:02,969
来构建一个神经网络

25
00:01:00,420 --> 00:01:04,920
前面提到这个节点对应了

26
00:01:02,969 --> 00:01:07,680
两个计算步骤<br />第一个是计算Z的值

27
00:01:04,920 --> 00:01:11,640
第二个是计算a的值

28
00:01:07,680 --> 00:01:14,549
在这个神经网络中

29
00:01:11,640 --> 00:01:17,880
节点的堆叠对应一个z值

30
00:01:14,549 --> 00:01:21,720
像这样计算<br />还有一个a值

31
00:01:17,880 --> 00:01:24,090
像这样计算

32
00:01:21,720 --> 00:01:26,790
然后这个节点会对应下一个z值

33
00:01:24,090 --> 00:01:29,040
和下一个a值的计算

34
00:01:26,790 --> 00:01:30,000
之后我们会用到的表示符号

35
00:01:29,040 --> 00:01:32,759
就像这样

36
00:01:30,000 --> 00:01:35,430
首先是输入特征x

37
00:01:32,759 --> 00:01:40,320
以及一些参数w和b

38
00:01:35,430 --> 00:01:42,930
从而我们可以计算得到z1

39
00:01:40,320 --> 00:01:45,600
这里我们需要用到的新的符号就是

40
00:01:42,930 --> 00:01:48,689
我们用右上角[1]

41
00:01:45,600 --> 00:01:50,759
来在表示这一层堆叠的节点的参数

42
00:01:48,689 --> 00:01:53,579
也就是所谓的神经网络的一层

43
00:01:50,759 --> 00:01:56,280
然后用右上角的[2]

44
00:01:53,579 --> 00:01:58,920
来表示下一组节点的参数

45
00:01:56,280 --> 00:02:01,200
也就是所谓的

46
00:01:58,920 --> 00:02:04,140
下一层神经网络

47
00:02:01,200 --> 00:02:06,719
注意不要把这里的右上角的方括号

48
00:02:04,140 --> 00:02:10,319
和我们用来表示

49
00:02:06,719 --> 00:02:12,390
单个训练样本的

50
00:02:10,319 --> 00:02:14,080
右上角的圆括号

51
00:02:12,390 --> 00:02:16,300
弄混淆了

52
00:02:14,080 --> 00:02:19,030
这里的x(i)我用来表示第i个

53
00:02:16,300 --> 00:02:21,340
训练样本

54
00:02:19,030 --> 00:02:25,570
但是[1]和[2]却用来表示

55
00:02:21,340 --> 00:02:28,600
不用的神经网络层<br />这里表示神经网络层1和层2

56
00:02:25,570 --> 00:02:32,860
我们继续

57
00:02:28,600 --> 00:02:35,350
计算得到z[1]以后

58
00:02:32,860 --> 00:02:39,000
类似于逻辑回归

59
00:02:35,350 --> 00:02:44,550
这里会有一个对a[1]的计算<br />也就是z[1]的σ值

60
00:02:39,000 --> 00:02:49,270
然后用另一个线性方程来计算z[2]

61
00:02:44,550 --> 00:02:54,610
然后再计算a[2]

62
00:02:49,270 --> 00:02:57,370
这里的a[2]就是

63
00:02:54,610 --> 00:02:59,890
这个神经网络的最终结果

64
00:02:57,370 --> 00:03:01,390
这里同样的可以用输出y^表示

65
00:02:59,890 --> 00:03:03,730
我明白这里有很多的细节

66
00:03:01,390 --> 00:03:06,460
但是最关键的点在于

67
00:03:03,730 --> 00:03:09,220
逻辑回归中这个z

68
00:03:06,460 --> 00:03:11,590
以及后面的a的计算

69
00:03:09,220 --> 00:03:13,780
这个神经网络中我们

70
00:03:11,590 --> 00:03:16,390
做了多次通过z计算得到a

71
00:03:13,780 --> 00:03:19,959
然后通过新的z得到新的a

72
00:03:16,390 --> 00:03:22,600
最后我们就能计算得到

73
00:03:19,959 --> 00:03:24,670
最终的损失值

74
00:03:22,600 --> 00:03:27,959
你应该还记得对于逻辑回归

75
00:03:24,670 --> 00:03:30,970
我们通过向后传播的计算来

76
00:03:27,959 --> 00:03:34,750
计算(每一层的)参数的梯度

77
00:03:30,970 --> 00:03:36,580
如da dz等等

78
00:03:34,750 --> 00:03:38,860
在神经网络的构建中

79
00:03:36,580 --> 00:03:44,910
我们最终会像这样<br />做向后传播式的运算

80
00:03:38,860 --> 00:03:50,890
这里我们最后会计算da[2] dz[2]

81
00:03:44,910 --> 00:03:57,850
从而我们可以计算得到dw[2] db[2]

82
00:03:50,890 --> 00:04:01,090
然后继续

83
00:03:57,850 --> 00:04:05,020
这就是从右到左红色箭头表示的

84
00:04:01,090 --> 00:04:05,360
向后的运算

85
00:04:05,020 --> 00:04:07,970
你你

86
00:04:05,360 --> 00:04:09,770
今天我们简单概览了

87
00:04:07,970 --> 00:04:12,950
神经网络大概是什么样子

88
00:04:09,770 --> 00:04:14,630
我们拿一个逻辑回归<br />然后重复这个过程两次

89
00:04:12,950 --> 00:04:16,880
我明白今天课程中有许多的新的符号

90
00:04:14,630 --> 00:04:18,980
以及新的知识细节<br />请不要担心跟不上

91
00:04:16,880 --> 00:04:20,900
因为我们在接下来的几节课的视频中

92
00:04:18,980 --> 00:04:22,820
我们会慢慢地讲解细节

93
00:04:20,900 --> 00:04:24,620
接下来让我们进入下一节课

94
00:04:22,820 --> 00:04:27,850
我们将会开始介绍

95
00:04:24,620 --> 00:04:27,850
神经网络的表示方法