في الفيديوهات السابقة من هذا الأسبوع، وكذلك في الفيديوهات
من الأسابيع العديدة السابقة، لقد تعرّفتم على المكونات الأساسية
للانتشار الأمامي والانتشار الخلفي والمكونات الرئيسية التي
تحتاجونها لتطبيق شبكة عصبية عميقة. دعونا نرَ كيف يمكنكم تجميع
هذه المكونات معًا لبناء شبكة عميقة. إليكم شبكة تحتوي على طبقات قليلة. ودعونا نختر طبقة واحدة. وننظر إلى العمليات الحسابية مع
التركيز على هذه الطبقة فقط حتى الآن. بالنسبة للطبقة L،
لديكم بعض المتغيرات wl وbl وبالنسبة للانتشار الأمامي، يمكنكم إدخال معلمات التنشيط a l-1
من الطبقة السابقة وإخراج a l. حسنًا، طريقة تنفيذ هذا سابقًا
كانت من خلال حساب z l = w l ضرب al - 1 + b l. ثم al = g(z) l‎. حسنًا. إذًا، هكذا تنتقلون من الإدخال
al - 1 إلى الإخراج al. وقد تبين أنه للاستخدام اللاحق،
فسيكون من المفيد أيضًا أن يتم تخزين القيمة zl مؤقتًا. لذا، دعوني أدرج هذا التخزين المؤقت أيضًا
لأن تخزين قيمة zl سيكون مفيدًا للخلف
لخطوة الانتشار الخلفي لاحقًا. ثم للخطوة للخلف أو
لخطوة الانتشار الخلفي، مرة أخرى، من خلال التركيز على حساب هذه الطبقة l، فإنكم تنفّذون الدالة
التي تتضمن إدخال da(l)‎. وإخراج da(l-1)‎
وفقط للتوسع في التفاصيل، يكون الإدخال da(l)‎ بالفعل،
وكذلك التخزين المؤقت لذا فإنه تتوفر لكم قيمة zl
التي قمتم بحسابها وبالإضافة إلى ذلك، من خلال إخراج da(l)‎
ناقص 1، فإنكم تحضرون الإخراج أو التدرجات التي تريدونها
لتنفيذ انحدار التدرج للتعلم، حسنًا؟ فهذه هي البنية الأساسية
لكيفية تنفيذ هذه الخطوة إلى الأمام، والتي نطلق عليها الدالة الأمامية
وكذلك هذه الخطوة للوراء، والتي سنطلق عليها الدالة الخلفية. إذًا لتلخيص ذلك فقط، في الطبقة l، ستكون لديكم الخطوة للأمام أو
الانتشار الأمامي للدالة الأمامية. والإدخال al- 1 والإخراج al، ولإجراء هذه العملية الحسابية
فإنكم تحتاجون إلى استخدام wl وbl. وكذلك إخراج تخزين مؤقت
يحتوي على zl، حسنًا؟ ثم الدالة "خطوات للخلف"،
باستخدام خطوة الانتشار الخلفي، ستكون دالة أخرى تحتوي على المدخلات da(l)‎ والمخرجات da(l-1)‎. ويعني هذا، أنه في ضوء المشتقات
المتعلقة بحالات التنشيط هذه، da(l)‎ هذه، ما المشتقات؟ ما المقدار الذي أريده؟ تعلمون أن al- 1 يغيّر
المشتقات المحسوبة مع مراعاة معلمات إلغاء التنشيط من طبقة سابقة. داخل هذا المربع، حسنًا؟ يجب عليكم استخدام wl وbl،
وقد تبين أنكم في نهاية المطاف تحسبون dzl، ثم هذا المربع، قد يكون إخراج دالة "خطوات للخلف"
هذه هو dwl وdbl، ولكنني أستخدم أحيانًا الأسهم الحمراء
للإشارة إلى التكرار الخلفي. لذا إذا كنتم تفضلون،
يمكننا رسم هذه الأسهم باللون الأحمر. إذا كان بإمكانكم تنفيذ
هاتين الدالتين فسيكون الحساب الأساسي
للشبكة العصبية كما يلي. ستأخذون الإدخال الذي يبرز a0،
وتعملون على تزويد ذلك، وسيؤدي ذلك إلى حساب حالات تنشيط
الطبقة الأولى، لنطلق عليها a1 وللقيام بذلك، تحتاجون إلى w1
وb1 وبعد ذلك، ستقومون بتخزين z1 مؤقتًا، حسنًا؟ الآن وبعد القيام بذلك، فإنكم تعملون على توفير ذلك
إلى الطبقة الثانية، ثم تستخدمون w2 وb2، فإنكم تحسبون حالات
إلغاء التنشيط في الطبقة التالية a2 وما إلى ذلك. حتى تُخرجوا في النهاية a l وهو ما يساوي y hat. وخلال سير العملية،
قمنا بتخزين جميع قيم z هذه مؤقتًا، فهذه هي خطوة الانتشار الأمامي. الآن، لخطوة الانتشار الخلفي
ما سنقوم به هو تسلسل للخلف لحالات التكرار التي تتجهون فيها للخلف
وحساب التدرجات مثل ذلك. لذا فإن ما ستقومون بتزويده هنا،
هو da(l)‎ ثم سيعطينا هذا المربع da(l- 1)‎
وما إلى ذلك حتى نحصل على da(2) da(1)‎. ويمكنكم بالفعل الحصول على
نتيجة أخرى لحساب da(0)‎، ولكن هذه المشتقة بالنسبة إلى ميزات الإدخال، والتي
لا تعد مفيدة على الأقل لتدريب أوزان هذه
الشبكات العصبية الخاضعة للإشراف، وبالتالي يمكنكم التوقف هنا.
ولكن خلال سير العملية، فإن الانتشار الخلفي أيضًا
ينتهي به الحال بالناتج dwl، وdbl. فقط استخدمت المطالبة كـ wl وbl. وسيؤدي هذا إلى إخراج dw3 وdb3 وما إلى ذلك. حسنًا، لقد انتهيتم من حساب كل
المشتقات التي تحتاجون إليها. وفقط لتعبئة بنية
هذا بصورة أكثر قليلاً، ستستخدم هذه المربعات
هذه المتغيرات أيضًا. wl وbl وقد تبين أننا سنرى لاحقًا أننا داخل هذه المربعات
سينتهي بنا الحال ونحن نحسب dz أيضًا. لذا فإن تكرار واحد للتدريب من خلال
شبكة عصبية ينطوي على: البدء بـ a(0)‎ وهي x ومن خلال
المرور في الانتشار الأمامي كما يلي، وحساب ŷ ثم
استخدام هذا لحساب ذلك، ثم القيام بذلك في الانتشار الخلفي والآن، لديكم كل هذه المصطلحات المشتقة
وبالتالي فإنكم تعلمون، w سيتم تحديثها إلى w1 =
معدل التعلم مضروبًا في dw، حسنًا؟ لكل الطبقات
وبالمثل لمعدل b. الآن يحتوي الانتشار الخلفي
المحسوب على كل هذه المشتقات. لذا، فإن هذا تكرار للانحدار
التدريجي للشبكة العصبية. الآن قبل المضي قدمًا، لدينا
واحدة من التفاصيل المعلوماتية الإضافية. نظريًا، سيكون من المفيد أن يتم
التفكير في التخزين المؤقت هنا كتخزين لقيمة z لدوال
"خطوات للخلف"، ولكن عند تنفيذ ذلك،
وترون ذلك في تمرين البرمجة، عندما تنفذون ذلك،
فإنكم تجدون أن التخزين المؤقت قد يمثل طريقة مريحة للحصول على هذه
القيمة الخاصة بمتغيرات w1 وb1، في دالة "خطوات للخلف" أيضًا.
لذا بالنسبة لهذا التمرين، فإنكم تخزنون بالفعل في ذاكرة
التخزين المؤقت لديكم لـ z كلٍ من w وb أيضًا. لذا فإن هذا يخزن z2 وw2 وb2.
ولكن من منطلق التنفيذ، وجدت هذه الطريقة مريحة
لنسخ المعلمات، إلى أينما تريدون استخدامها لاحقًا
عند حسابكم للانتشار الخلفي. وهذه مجرد تفاصيل تنفيذية
ترونها عند ممارسة تمرين البرمجة. إذًا، لقد رأيتم الآن
المكونات الأساسية لتنفيذ شبكة عصبية عميقة. وفي كل طبقة، هناك
خطوة انتشار أمامي وهناك خطوة
انتشار خلفي مقابلة. وتخزين مؤقت لإيداع المعلومات
من خطوة إلى الأخرى. ففي الفيديو التالي، سنتحدث حول كيف يمكنكم
تنفيذ هذه المكونات الأساسية. لننتقل الى الفيديو التالي.