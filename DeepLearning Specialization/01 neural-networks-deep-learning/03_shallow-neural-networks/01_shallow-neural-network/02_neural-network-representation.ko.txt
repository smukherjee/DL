여러분은 제가 신경망을 그린 적을 몇번 보셨을텐데요, 이번 비디오에서는 이런 그림이 정확히 의미하는 것이 
무엇인지 살펴보겠습니다. 다시 말해, 달리 말하면, 저희가 그려왔던 이런 신경망들이 
어떤 것을 대표하는지 보겠다는 것입니다. 신경망에서 신걸 숨겨진 층이라고 하는 것의 경우를 먼저 집중적으로 보겠습니다. 이것은 신경망의 그림입니다. 여기 이 그림의 일부분을 
명칭을 부여하겠습니다. x1, x2, x3와 같이 입력 특성이
이렇게 세로로 쌓여있는데요, 이것은 그리고 신경망의 
입력값 층입니다. 그럼 놀랍지 않게, 이것은 
신경망의 입력 값을 보유하고 있겠죠. 그리고 또한, 동그란 층을 추가적으로 가지고 있을 텐데요, 이것은 숨겨진 유니으로, 
신경망의 숨겨진 층이라고 합니다. 여기서 의미하는 수겨진이 무엇을 뜻하는지는
조금 있다 설명드리겠습니다. 여기 마지막 층은, 이 경우에는 
1개의 노드로 구성되어 있는데요, 여기 이렇게 1개의 노드로 이루어진 층을
결과값층, 아웃풋 레이어라고 합니다. 그리고 ŷ의 값을 생성하는 역학을 합니다. 지도학습의 신경망에서는 트레이닝세트가 x의 값과 목표 y결과값을 
모두 가지고 있습니다. 그렇기 때문에 숨겨진 층이라는 단어는 
트레이닝 세트에서 여기 가운데 노드들의 실제값이 보이지 않는 경우를 말합니다. 즉, 트레이닝세트에서 보여야 할 것들이 보이지 않는 것을 말합니다. 입력값은 보이고, 또, 결과값도 보이나, 숨겨진 층에 있는 것들은 트레이닝 세트에서 보이지 않습니다. 그러면 hidden이라는 이름이 
무엇일 뜻하는지 어느 정도 
설명이 됐을 텐데요, 트레이닝세트에서 안보이는 경우를 뜻하니 말이죠. 새로운 표기를 조금 더 소개해보겠습니다. 이전에는 입력값을 x로 표현했는데요, 입력특성 값들에 대한 또 다른 표기법으로는, a 위첨자 괄호 0입니다. a는 여기서 activation을 상징합니다. 서로 다른 신경망들의 층이 다음 이어지는 신경망 층들로 전달하는 
값을 뜻합니다. 그러므로 입력 층이 x 값을
숨겨진 층으로 전달하고, 이것을 activation, 입력 층 A 위첨자 0으로
나타낼텐데요, 다음 층에서, 숨겨진 층은 activation을 
생성할 텐데요, 이것은 a 위첨자 대괄호 1로 표현하겠습니다. 그럼 특히, 여기 이 첫번째 유닛, 
또는 첫번째 노드 a 위첨자 대괄호 1와 같은 값을 생성하는데요, 두번째 값은, 여기 이렇게 위첨자2의 값으로 생성됩니다. 그러면, 여기 위 첨자 대괄호 1의 값은 4차원 벡터인데요, 파이썬에서 구현할 것입니다. 4 X 1 
common 벡터 매트릭스 인데요. 이렇게 생겼습니다. 이것은 4차원인데요, 그 이유는 
이 경우 4개의 노드가 있고, 또는 4개의 유닛이 있고, 
또는 숨겨진 층에 4개의 숨겨진 유닛이 있기 때문입니다. 마지막으로 오픈 layer가 A2 값을 재생성합니다. 이 값은 그냥 실수입니다. 그러면 ŷ은 a2의 값을 가질텐데요, 이것은 ŷ 이 a였던 로지스틱 회귀분석때이 
경우와 비슷한데요, 이때는 1개의 결과값 층밖에 없었습니다. 그렇기 때문에 위첨자 대괄호를 사용하지 않았습니다. 이제 새로운 신경망에서는 
어떤 층에서 왔는지를 나타내기 위해서 위첨자를 사용할 것입니다. 신경망의 표기법이 웃긴 것 중 한가지는 여기에서 본 네트워크가 two layer 신경망이라는 것입니다. 신경망에서 그렇게 불리는 이유는 입력 층을 세지 않기 때문입니다. 그러므로 숨겨진 층 이 1번 층이고, 
결과값 층이 2번 층입니다. 그러면 표기법에서
입력값 층을 0번 층이라고하고, 엄밀히 이야기하면 3개의 층이 있다고
할 수 있겠죠. 그 이유는 입력 층이 있고, 
숨겨진 층이 있고, 그리고 결과값 층이 있기 
때문입니다. 하지만 공식적인 표기로는, 
리서치 페이퍼 논문이나 다른 코스에서 보면, 사람들이 이렇게 생긴 신경망을 two layer 신경망이라고 부르는 것을 볼 수 있을 것입니다.
입력 층을 공식적인 층으로 세지 않기 때문이죠. 마지막으로, 나중에 보겠지만
숨겨진 층과 결과값 층은 각각 연관된 매개 변수가 있습니다. 숨겨진 층은 연계된 매개 변수 
w와 b가 있을 것입니다. 그리고 저는 위점차 대괄호 1로 숨겨진 층을 가진 첫번째 층과 연계되는 것을
나타낼 것입니다. 나중에 보겠지만 w는 
4 X 3 매트릭스 입니다. b는 4 X 1 벡터이구요. 4라는 첫번째 좌표는 4개의 숨겨진 층과 층이 있는 노드가 있다는 것을 나타내구요, 그리고 3이라는 좌표 숫자는
우리가 입력 특성이 3개를 가지고 있다는 것입니다. 이런 매트릭스의 다이멘션에 대한 내용은 
나중에 또 더 이야기하겠습니다. 더 이야기하는 시점에 이해가 많이 되실 수 있습니다. 그리고 비슷하게 결과값도 w 위첨가 대괄호 2 와
b 위첨자 대괄호 2와 연관되어 있을 수 있는데요, 각각의 dimension은 1 X 4, 그리고
1 X 1입니다. 그리고 여기서 1 X 4인 이유는, 
숨겨진 층이 4개 있기 때문입니다. 그리고 결과값은 1개의 유닛이 있습니다. 여기에서 나오는 다이멘션에 대한 내용은
나중에 이어지는 비디오 강의에서 배우겠습니다. 이제 여러분은 two layered 신경망이 
어떻게 생겼는지 보았는데요, 이것은 1개의 숨겨진 층으로 이루어진 것을 보았습니다. 다음 비디오에서는 이런 신경망이 정확히 어떤 것을 계산하는지 알아보겠습니다. 즉, 이 신경망이 x를 어떻게 입력하고, Ŷ 결과값을 산출하기까지 어떻게 이어지는지 알아보겠습니다.