1
00:00:00,028 --> 00:00:04,605
當建置深度神經網路時
一個我常用的除錯工具來

2
00:00:04,605 --> 00:00:08,118
檢查我的程式
是拿一張紙

3
00:00:08,118 --> 00:00:11,727
一一檢查我用的
矩陣的維度

4
00:00:11,727 --> 00:00:15,895
讓我向您展示如何做到
因為我希望這會讓您

5
00:00:15,895 --> 00:00:18,275
建置您的深度網路容易一點

6
00:00:18,275 --> 00:00:23,174
大寫 L 等於 5, 很快的算
不算輸入層

7
00:00:23,174 --> 00:00:27,390
這個有五層
四層隱藏層跟一層輸出層

8
00:00:27,390 --> 00:00:34,878
所以
當您建置正向傳播時

9
00:00:34,878 --> 00:00:41,408
第一步會是 z1 = w1x + b1

10
00:00:41,408 --> 00:00:48,144
讓我們先忽略偏差值 b
先將注意力放在參數 w

11
00:00:48,144 --> 00:00:54,501
第一個隱藏層有三個
隱藏單元, 這是第 0 層

12
00:00:54,501 --> 00:00:59,517
第一層, 第二層, 第三層
第四層，第五層

13
00:00:59,517 --> 00:01:05,741
用我們之前的記號
我們有 n1

14
00:01:05,741 --> 00:01:11,265
也就是第一層
的隱藏單元數目, 等於 3

15
00:01:11,265 --> 00:01:16,202
這裡我們有
 n2 等於 5

16
00:01:16,202 --> 00:01:23,018
n3 等於 4, n4 等於 2
n5 等於 1

17
00:01:23,018 --> 00:01:27,715
目前為止我們看過的神經網路
只有一個輸出單元, 但在以後

18
00:01:27,715 --> 00:01:32,497
課程裡, 我們會談到神經網路
也有多個輸出單元

19
00:01:32,497 --> 00:01:36,989
最後對於輸入層

20
00:01:36,989 --> 00:01:40,443
我們有 n0 = nx = 2

21
00:01:40,443 --> 00:01:45,860
現在, 讓我們想想看
z, w, 跟 x 的維度

22
00:01:45,860 --> 00:01:49,120
z 是啟動向量對於

23
00:01:49,120 --> 00:01:54,244
第一個隱層層, 所以
z 會是 3 乘 1

24
00:01:54,244 --> 00:01:58,675
這會是 維度 3 向量

25
00:01:58,675 --> 00:02:03,093
我將它寫成 n1
乘 1 維度向量

26
00:02:03,093 --> 00:02:08,546
n1 乘 1 維度矩陣
所以是 3 乘 1 在這個例子

27
00:02:08,546 --> 00:02:12,319
那有關輸入特徵 x 呢
我們有兩個輸入特徵

28
00:02:12,319 --> 00:02:18,622
所以 x 在這個例子是 2 乘 1
一般而言會是 n0 乘 1

29
00:02:18,622 --> 00:02:24,082
我們需要
矩陣 w1 是某些當我們

30
00:02:24,082 --> 00:02:30,181
乘上 n0 乘 1 向量時
我們會得到 n1 乘 1 向量, 是吧?

31
00:02:30,181 --> 00:02:34,747
您會有 3 
維向量等於

32
00:02:34,747 --> 00:02:38,600
某項乘上一個 2 維向量

33
00:02:38,600 --> 00:02:42,993
所以
用矩陣相乘規則

34
00:02:42,993 --> 00:02:46,041
這應該會是 3 乘 2 矩陣

35
00:02:46,041 --> 00:02:51,138
因為 3 乘 2 矩陣
乘上 2 乘 1 矩陣, 或者說

36
00:02:51,138 --> 00:02:56,249
乘上 2 乘 1 向量
會給您 3 乘 1 向量

37
00:02:56,249 --> 00:03:02,771
一般化以後, 這會是
一個 n1 乘 n0 維度的矩陣

38
00:03:02,771 --> 00:03:07,167
所以我們發現

39
00:03:07,167 --> 00:03:12,665
w1 的維度會是 n1 乘 n0

40
00:03:12,665 --> 00:03:20,191
而更一般化, 
wl 的維度會是 nl 乘 nl-1

41
00:03:20,191 --> 00:03:26,021
舉個例子, w2 的維度

42
00:03:26,021 --> 00:03:31,508
會是 5 乘 3

43
00:03:31,508 --> 00:03:35,119
或者說 n2 乘 n1

44
00:03:35,119 --> 00:03:40,036
因為我們將計算

45
00:03:40,036 --> 00:03:45,132
z2 為 w2 乘 a1, 再一次

46
00:03:45,132 --> 00:03:50,059
讓我們先忽略偏差值

47
00:03:50,059 --> 00:03:54,584
所以這會是 3 乘 1

48
00:03:54,584 --> 00:03:59,432
而我們需要這個為 5 乘 1, 所以

49
00:03:59,432 --> 00:04:03,169
這個應該是 5 乘 3

50
00:04:03,169 --> 00:04:10,273
同樣的, w3
是下一層的維度

51
00:04:10,273 --> 00:04:15,501
逗號, 
前一層的維度

52
00:04:15,501 --> 00:04:19,266
所以這會是 4 乘 5, w4

53
00:04:22,055 --> 00:04:27,489
會是 2 乘 4, 而

54
00:04:27,489 --> 00:04:34,405
w5 會是 1 乘 2 對吧?

55
00:04:34,405 --> 00:04:38,730
所以一般公式
來檢查當

56
00:04:38,730 --> 00:04:43,416
您建置
l 層矩陣時

57
00:04:43,416 --> 00:04:48,475
這個矩陣
維度會是 nl 乘 nl - 1

58
00:04:48,475 --> 00:04:55,362
現在讓我們看看
向量 b 的維度

59
00:04:55,362 --> 00:05:01,017
這個會是 3 乘 1 向量
所以您需要加另一個

60
00:05:01,017 --> 00:05:06,008
3 乘 1 向量，為了要
得到 3 乘 1 向量的結果

61
00:05:06,008 --> 00:05:11,287
或者在這個例子, 我們需要這個
會是 5 乘 1

62
00:05:11,287 --> 00:05:14,823
所以這會是
另一個 5 乘 1 向量

63
00:05:14,823 --> 00:05:19,122
為了要
將這兩項我用框框畫起來相加

64
00:05:19,122 --> 00:05:22,767
它也是 5 乘 1 向量

65
00:05:22,767 --> 00:05:30,090
所以一般化的規則
在左邊這個例子

66
00:05:30,090 --> 00:05:35,470
b1 是 n1 乘 1, 也就是 3 乘 1

67
00:05:35,470 --> 00:05:41,156
在第二個例子
這是 n2 乘 1

68
00:05:41,156 --> 00:05:45,891
所以更一般化後

69
00:05:45,891 --> 00:05:50,637
bl 應該是 nl 乘 1 維度

70
00:05:50,637 --> 00:05:56,402
希望這兩個方程式幫助
您再次檢查

71
00:05:56,402 --> 00:06:02,091
您個矩陣 w 維度跟您的
向量 b 是正確的維度

72
00:06:02,091 --> 00:06:06,206
當然
如果您建置反向傳播

73
00:06:06,206 --> 00:06:10,657
dw 的維度應該
跟 w 維度一樣

74
00:06:10,657 --> 00:06:16,373
所以 dw 跟 w 維度一樣

75
00:06:16,373 --> 00:06:22,276
db 應該跟 b 維度一樣

76
00:06:22,276 --> 00:06:28,399
其他重要的變數
也需要檢查維度的是 z,

77
00:06:28,399 --> 00:06:33,658
x, 跟 a of l
我們並沒多談

78
00:06:33,658 --> 00:06:39,856
因為 z of l 等於 
g of l 用逐元素運算

79
00:06:39,856 --> 00:06:46,914
所以 z 跟 a 應該
是同一維度在這樣的網路

80
00:06:46,914 --> 00:06:51,582
現在我們來看看當您
向量化建置同時在

81
00:06:51,582 --> 00:06:53,258
多重例子上

82
00:06:53,258 --> 00:06:56,092
即使是向量化建置

83
00:06:56,092 --> 00:07:00,687
當然 w b
dw 跟 db 的維度會是一樣

84
00:07:00,687 --> 00:07:04,929
但是 z, 
a, x 的維度

85
00:07:04,929 --> 00:07:09,771
會在向量化建置時改變

86
00:07:09,771 --> 00:07:13,420
前面

87
00:07:13,420 --> 00:07:18,372
我們用 z1 = w1x + b1

88
00:07:18,372 --> 00:07:23,845
而這是 n1 乘 1

89
00:07:23,845 --> 00:07:28,276
也就是 n1 乘 n0

90
00:07:28,276 --> 00:07:35,846
x 是 n0 乘 1, b 是 n1 乘 1

91
00:07:35,846 --> 00:07:40,979
在向量化

92
00:07:40,979 --> 00:07:46,398
建置您

93
00:07:46,398 --> 00:07:53,536
有 Z1 = W1X + b1

94
00:07:53,536 --> 00:07:58,023
現在 Z1 是
拿

95
00:07:58,023 --> 00:08:03,575
每一個單獨的例子
所以 z11, z12

96
00:08:03,575 --> 00:08:10,207
直到 z1m, 疊起來
這給您 Z1

97
00:08:10,207 --> 00:08:15,042
所以 Z1 的維度
不再是 n1 乘 1

98
00:08:15,042 --> 00:08:20,285
它會是 n1 乘 m
m 是您訓練集的大小

99
00:08:20,285 --> 00:08:26,140
W1 的維度不變
所以還是 n1 乘 n0

100
00:08:26,140 --> 00:08:29,201
而 X, 不再是 n0 乘 1

101
00:08:29,201 --> 00:08:33,431
所有您的訓練例子
水平疊起來

102
00:08:33,431 --> 00:08:38,565
現在是 n0 乘 m
您注意到當您

103
00:08:38,565 --> 00:08:43,833
用 n1 乘 n0 矩陣
乘上一個 n0 乘 m 矩陣

104
00:08:43,833 --> 00:08:50,160
它們一起會給您一個
預期中的 n1 乘 m 維度矩陣

105
00:08:50,160 --> 00:08:55,030
最後一個細節是
 b1 還是 n1 乘 1, 但是

106
00:08:55,030 --> 00:09:01,147
當您用這個加上 b
應用 Python 傳播

107
00:09:01,147 --> 00:09:08,218
這會複製成為 n1 乘
 m 矩陣, 然後逐元素相加

108
00:09:08,218 --> 00:09:14,977
在前面的影片中, 我們談到
 w, b, dw 跟 db維度

109
00:09:14,977 --> 00:09:21,143
這裡我們看到 zl

110
00:09:21,143 --> 00:09:26,922
跟 al 的維度是 nl 乘 1

111
00:09:26,922 --> 00:09:34,650
我們現在的
Zl 跟 Al 是 nl 乘 m

112
00:09:34,650 --> 00:09:40,410
而特別情況是
當 l 等於 0

113
00:09:40,410 --> 00:09:45,188
這種情況下 A0, 也就是等於

114
00:09:45,188 --> 00:09:49,543
您的訓練集輸入特徵 X

115
00:09:49,543 --> 00:09:54,616
是等於
期望中的 n0 乘 m 

116
00:09:54,616 --> 00:10:01,259
當然當您
建置這個在反向傳播時

117
00:10:01,259 --> 00:10:06,749
我們晚點會看到
計算 dZ 跟 dA

118
00:10:06,749 --> 00:10:11,327
這些當然是

119
00:10:11,327 --> 00:10:15,736
跟 Z 跟 A 同維度

120
00:10:15,736 --> 00:10:19,467
我希望這段小小的練習
幫助您釐清

121
00:10:19,467 --> 00:10:21,685
您使用矩陣的維度

122
00:10:21,685 --> 00:10:25,947
當您建置反向傳播對於
一個神經網路時，記得當您

123
00:10:25,947 --> 00:10:30,350
寫您的程式時請確認所有
這些矩陣的維度是一致的

124
00:10:30,350 --> 00:10:31,825
這通常會有幫助

125
00:10:31,825 --> 00:10:35,908
這會某種程度消除
一些造成錯誤的原因

126
00:10:35,908 --> 00:10:40,325
我希望這個練習來找出
不同矩陣的維度

127
00:10:40,325 --> 00:10:41,979
您會覺得有幫助

128
00:10:41,979 --> 00:10:44,788
當您建置一個深度學習網路
如果您確保了

129
00:10:44,788 --> 00:10:48,241
這些您使用的矩陣
跟向量的維度

130
00:10:48,241 --> 00:10:52,162
希望會幫助您去除
一些可能的錯誤形成

131
00:10:52,162 --> 00:10:54,467
這當然幫助我使得我的程式正確

132
00:10:54,467 --> 00:10:58,882
下一段, 我們已經看過一些
正向傳播如何作用的機制

133
00:10:58,882 --> 00:11:01,227
在一個神經網路

134
00:11:01,227 --> 00:11:04,163
但為什麼深度學習網路
如此有效率

135
00:11:04,163 --> 00:11:07,243
為什麼它們比
淺層做得更好

136
00:11:07,243 --> 00:11:09,939
讓我們花幾分鐘在
下段影片來討論它