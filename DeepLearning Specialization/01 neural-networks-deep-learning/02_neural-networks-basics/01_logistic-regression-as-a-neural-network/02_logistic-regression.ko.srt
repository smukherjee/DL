1
00:00:00,000 --> 00:00:03,475
In this video, we'll go over logistic regression.

2
00:00:03,475 --> 00:00:07,080
이 러닝 알고리즘은 지도학습 문제에서 결과값 레이블 Y의 값이  

3
00:00:07,080 --> 00:00:10,690
0 이거나 1인 경우 사용되는 학습 알고리즘입니다

4
00:00:10,690 --> 00:00:13,600
즉 이진 분류 문제의 경우이죠. 

5
00:00:13,600 --> 00:00:18,350
입력 특성 벡터 X가 

6
00:00:18,350 --> 00:00:23,150
고양이이거나 고양이가 아닌 사진을 분류하는 이미지에 해당하는 경우

7
00:00:23,150 --> 00:00:26,525
결과값을 예측하는 알고리즘이 필요할 것입니다.

8
00:00:26,525 --> 00:00:28,254
이것을 ŷ (y hat) 이라고 하겠습니다.

9
00:00:28,254 --> 00:00:31,130
이것은 Y의 평균값이죠. 

10
00:00:31,130 --> 00:00:35,896
공식적으로는, ŷ 의 값이 Y가 

11
00:00:35,896 --> 00:00:40,630
주어진 입력 특성 X가 되는 확률로 만드는게 좋습니다. 

12
00:00:40,630 --> 00:00:43,880
다시 말해서, X 가 사진이면,

13
00:00:43,880 --> 00:00:45,530
저번 비디오에서 본 것처럼 말이죠,

14
00:00:45,530 --> 00:00:47,300
ŷ 은

15
00:00:47,300 --> 00:00:49,820
이 사진이 고양이일 확률은 얼마나 되는가? 를 말해주는 것이 좋습니다.

16
00:00:49,820 --> 00:00:53,420
그러므로, 저번 비디오에서 본 것과 같이 X는

17
00:00:53,420 --> 00:00:56,960
X 다이멘션얼 벡터입니다.

18
00:00:56,960 --> 00:01:02,000
로지스틱 회귀분석법의 매개변수가 W이고,

19
00:01:02,000 --> 00:01:07,745
W가 동시에 X 다이멘션얼 벡터이며, 

20
00:01:07,745 --> 00:01:11,670
실수인 b도  x 다이멘션얼 벡터인 경우 말이죠.

21
00:01:11,670 --> 00:01:16,055
그러므로, 입력값 X와, 매개변수가 W와 b인 경우, 

22
00:01:16,055 --> 00:01:20,595
ŷ 결과값은 어떻게 생성할까요?

23
00:01:20,595 --> 00:01:22,970
시도할 수 있는 방법은, 물론 안되겠지만,

24
00:01:22,970 --> 00:01:27,590
ŷ 이 w transpose값 곱하기 X + b입니다.

25
00:01:27,590 --> 00:01:33,045
입력값 X의 일차함수인 것처럼 말이죠. 

26
00:01:33,045 --> 00:01:37,145
사실 말한 것 그대로, 이것을 선형회귀에 씁니다.

27
00:01:37,145 --> 00:01:41,345
그렇지만 이항분류문제로는 그닥 좋지 않은 알고리즘인데요,

28
00:01:41,345 --> 00:01:45,575
ŷ 이 Y가 1일 확률이 되도록 만드는 것이 좋기 때문입니다.

29
00:01:45,575 --> 00:01:50,480
결과적으로 ŷ 이0에서 1사이에 있어야 하는데,

30
00:01:50,480 --> 00:01:54,697
이렇게 만들기가 쉽지 않죠, W transpose 곱하기 X 플러스 B값이 

31
00:01:54,697 --> 00:01:58,475
1보다 훨씬 더 크고 마이너스일 수도 있기 때문입니다.

32
00:01:58,475 --> 00:02:00,905
확률적인 측면으로 비추어보았을때, 말이 안되죠.

33
00:02:00,905 --> 00:02:03,620
이것이 0에서 1사이에 있도록 하는 것이요.

34
00:02:03,620 --> 00:02:07,670
로지스틱 회귀분석법에서는 결과값이 ŷ = 이 만큼의 양이

35
00:02:07,670 --> 00:02:12,050
적용된 시그모이드 함수가 될 것입니다.

36
00:02:12,050 --> 00:02:14,850
시그모이드 함수는 이렇게 생겼습니다.

37
00:02:14,850 --> 00:02:24,000
가로축에 Z를 넣으면, Z의 시그모이드 함수는 이렇게 생깁니다.

38
00:02:24,000 --> 00:02:28,050
부드럽게 0에서 1로 가죠.

39
00:02:28,050 --> 00:02:30,120
축을 이렇게 넣겠습니다.

40
00:02:30,120 --> 00:02:34,915
이 값은 0이고 세로축을 0.5에서 교차합니다.

41
00:02:34,915 --> 00:02:41,305
Z의 시그모이드 함수는 이렇게 생겼구요, 이 W transpose 곱하기
X 더하기 B에 대한 양을 

42
00:02:41,305 --> 00:02:43,020
나타내기 위해 Z를 쓰겠습니다.

43
00:02:43,020 --> 00:02:46,230
여기 시그모이드 함수에 대한 공식입니다.

44
00:02:46,230 --> 00:02:49,380
Z의 시그모이드에서 Z는 실수, 

45
00:02:49,380 --> 00:02:52,510
1 나누기 1+E의 의 마이어스 Z승입니다.

46
00:02:52,510 --> 00:02:54,695
2가지 알만한 사항이 있는데요.

47
00:02:54,695 --> 00:03:01,255
만약 Z의 값이 매우 크면 E의 -Z승은 0에 가까울 것입니다.

48
00:03:01,255 --> 00:03:03,420
Z의 시그모이드는 

49
00:03:03,420 --> 00:03:07,255
대략 1 나누기 1 플러스 0에 가까운 값일 것입니다.

50
00:03:07,255 --> 00:03:11,280
그 이유는 E의 아주 큰 숫자의 승은 0에 가깝디 때문이죠.

51
00:03:11,280 --> 00:03:13,505
그러므로 이 값은 1에 가깝죠.

52
00:03:13,505 --> 00:03:16,255
그럼 보시다시피, 왼쪽을 보시면

53
00:03:16,255 --> 00:03:20,475
Z의 값이 매우 큰 경우, Z의 시그모이드는 1과 매우 가깝운 값이 됩니다.

54
00:03:20,475 --> 00:03:24,105
반대로 Z의 값이 작으면, 

55
00:03:24,105 --> 00:03:28,970
또는 아주 큰 음수인 경우, 

56
00:03:29,180 --> 00:03:39,640
Z의 시그모이드는 1 나누기 1+E의 -Z승이 됩니다.

57
00:03:39,640 --> 00:03:42,565
이 값은 아주 큰 값이 되겠죠. 

58
00:03:42,565 --> 00:03:47,944
그러면 이 것은, 1 나누기 1 더하기 아주 큰값이라고 

59
00:03:47,944 --> 00:03:54,473
생각해보십시요. 

60
00:03:54,473 --> 00:03:56,570
0에 가까운 값이 되겠죠. 

61
00:03:56,570 --> 00:04:00,325
그러면 Z의 값이 매우 큰 음수의 값이 되면서, 

62
00:04:00,325 --> 00:04:03,505
Z의 시그모이드는 0과 가까운 값이 됩니다.

63
00:04:03,505 --> 00:04:06,070
그러므로 로지스틱 회귀분석을 도입하는 경우에는, 

64
00:04:06,070 --> 00:04:10,350
여러분의 임무는 parameter W와 B를 배워서 

65
00:04:10,350 --> 00:04:15,220
ŷ 이 Y=1이 되는 확률을 잘 추정한 수치가 나올 수 있도록 하는 것입니다. 

66
00:04:15,220 --> 00:04:18,955
넘어가기에 앞서, 표기에 관한 내용을 이야기하겠습니다.

67
00:04:18,955 --> 00:04:20,830
신경망을 프로그래밍할때,

68
00:04:20,830 --> 00:04:26,855
W와 B 매개변수를 따로 다룰텐데요,

69
00:04:26,855 --> 00:04:30,000
여기서 B는 인터-스펙트럼의 끝을 나타냅니다.

70
00:04:30,000 --> 00:04:31,295
다른 코스에서

71
00:04:31,295 --> 00:04:35,110
이 표기방법을 다르게 하는 경우를 보셨을 수도 있는데요,

72
00:04:35,110 --> 00:04:42,205
간혹, x0이라는 특성을 정의해서 이 값을 1로 만들어주는데요,

73
00:04:42,205 --> 00:04:47,250
이 경우 x는 R의 nx+1승에 속하는데요,

74
00:04:47,250 --> 00:04:53,865
그 다음에 ŷ 은 theta transpose x 곱하기 시그마가 되도록 합니다.

75
00:04:53,865 --> 00:04:56,685
다른 방법의 표기방식에서는, 

76
00:04:56,685 --> 00:05:00,510
벡터 parameters 인 쎄타가 있는데요

77
00:05:00,510 --> 00:05:03,175
쎄타0, 쎄타1, 쎄타2,

78
00:05:03,175 --> 00:05:09,520
쎄타 NX까지 말이죠, 

79
00:05:09,520 --> 00:05:11,723
쎄타0은 쎄타 B줄이구요

80
00:05:11,723 --> 00:05:13,663
이것은 단순 줄을 나타내는 것입니다.

81
00:05:13,663 --> 00:05:18,505
쎄타1에서 쎄타 nx까지는 W줄가지 속합니다.

82
00:05:18,505 --> 00:05:20,350
여러분이 신경망 네트워크를 도입하는 경우, 

83
00:05:20,350 --> 00:05:26,145
B와 W 매개변수를 따로 취급하는 것이 훨씬 더 쉽습니다.

84
00:05:26,145 --> 00:05:27,430
이번 수업에서는 

85
00:05:27,430 --> 00:05:32,087
여기 빨간색으로 적은 표기법은 쓰지 않도록 하겠습니다.

86
00:05:32,087 --> 00:05:36,330
이전에 다른 코스에서 이런 표기법을 본 적이 없으시면, 신경 쓰실 필요 없습니다.

87
00:05:36,330 --> 00:05:39,610
이미 이 표기방식을 접한 분들을 위해서 단순히 

88
00:05:39,610 --> 00:05:43,730
직접적으로 언급해서 이러한 표기법을 해당 코스에서느 쓰지 않는 것을
알려드리고 싶었습니다. 

89
00:05:43,730 --> 00:05:45,235
본 적이 없으시면

90
00:05:45,235 --> 00:05:48,430
중요하지 않기 때문에 걱정하지 않으셔도 됩니다.

91
00:05:48,430 --> 00:05:52,465
이제 여러분은 로지스틱 회귀분석 모델이 어떻게 생겼는지 보셨는데요,

92
00:05:52,465 --> 00:05:57,140
다음은, W와 B 매개변수를 바꾸기 위해서는 비용함수를 정의해야 합니다. 

93
00:05:57,140 --> 00:05:58,830
다음 비디오에서 해보도록 하죠.