1
00:00:00,590 --> 00:00:03,210
Ти бачив/ла модель логістичної регресії (ЛР).

2
00:00:03,210 --> 00:00:06,560
Ти бачив/ла функцію Втрати, 
що оцінює наскільки добре ти справляєшся з

3
00:00:06,560 --> 00:00:08,780
одним тренувальним зразком.

4
00:00:08,780 --> 00:00:13,530
Ти також бачив/ла функцію Витрат, 
що оцінює наскільки добре наші параметри w і

5
00:00:13,530 --> 00:00:16,590
b справляються з усім тренувальним набором.

6
00:00:16,590 --> 00:00:21,600
Тепер давай поговоримо про те, 
як можна використати алгоритм Градієнтного спуску для тренування

7
00:00:21,600 --> 00:00:25,730
чи то навчання параметрів w і b на нашому тренувальному наборі.

8
00:00:25,730 --> 00:00:30,030
Згадаємо. Це - знайомий вже алгоритм ЛР.

9
00:00:31,130 --> 00:00:34,700
В другому рядку ми маємо функцію Витрат J,

10
00:00:34,700 --> 00:00:37,879
що є функцією від наших параметрів w і b,

11
00:00:37,879 --> 00:00:39,960
і яка визначається як середнє,

12
00:00:39,960 --> 00:00:44,140
себто 1/m помножене на суму функцій Втрати.

13
00:00:44,140 --> 00:00:48,470
Тож функція Втрати оцінює наскільки добре наш алгоритм

14
00:00:48,470 --> 00:00:53,170
обчислює ŷ⁽ⁱ⁾ для кожного тренувального зразка

15
00:00:53,170 --> 00:00:58,000
в порівнянні з базовою справжньою міткою y⁽ⁱ⁾ 
для кожного тренувального зразка.

16
00:00:58,000 --> 00:01:00,886
Повна формула розписана справа.

17
00:01:00,886 --> 00:01:04,130
Функція Витрат оцінює наскільки добре параметри w і

18
00:01:04,130 --> 00:01:06,760
b підходять для усього тренувального набору.

19
00:01:06,760 --> 00:01:11,510
Тож для того, щоб навчити набір параметрів w і b, 
природно, що ми захочемо

20
00:01:11,510 --> 00:01:17,930
знайти такі w і b, які зроблять функцію Витрат J(w, b) якомога меншою.

21
00:01:17,930 --> 00:01:21,320
Ось ілюстрація Градієнтного спуску.

22
00:01:21,320 --> 00:01:25,320
На цій діаграмі горизонтальні осі відображають

23
00:01:25,320 --> 00:01:28,510
значення параметрів w і b.

24
00:01:28,510 --> 00:01:32,350
На практиці, w має більші значення, але для відображення

25
00:01:32,350 --> 00:01:38,190
на діаграмі, давай покажемо w і b 
приблизно рівними реальними числами.

26
00:01:38,190 --> 00:01:40,770
Функція Витрат J(w, b)

27
00:01:40,770 --> 00:01:45,130
в цьому випадку буде площиною між цими двома 
горизонтальними осями w і b.

28
00:01:45,130 --> 00:01:50,720
А висота площини буде відображати значення J(w, b) 
в конкретній точці.

29
00:01:50,720 --> 00:01:55,070
Тож те, що ми хочемо знайти, це значення w і

30
00:01:55,070 --> 00:01:59,730
b, що відповідають мінімуму функції Витрат J.

31
00:02:00,830 --> 00:02:06,050
Як бачимо, дана функція Витрат J є опуклою.

32
00:02:06,050 --> 00:02:10,327
Вона має єдину велику "чашу", через це є опуклою функцією і

33
00:02:10,327 --> 00:02:13,717
цим відрізняється від отаких функцій,

34
00:02:13,717 --> 00:02:18,120
які є неопуклими і мають багато мінімумів.

35
00:02:18,120 --> 00:02:22,240
Тож те, що наша функція Витрат J(w, b), за означенням,

36
00:02:22,240 --> 00:02:27,020
є опуклою, є вагомою причиною для вибору саме її функцією Витрат J

37
00:02:27,020 --> 00:02:29,610
для ЛР.

38
00:02:29,610 --> 00:02:33,810
Щоб знайти потрібні значення для параметрів

39
00:02:33,810 --> 00:02:39,160
ми спочатку ініціалізуємо w і b деякими первинними значеннями,

40
00:02:39,160 --> 00:02:43,360
хай вони будуть = вказаним нам червоною точкою.

41
00:02:43,360 --> 00:02:47,562
Для ЛР підходять практично будь-які методи ініціалізації.

42
00:02:47,562 --> 00:02:50,690
зазвичай, при ініціалізації присвоюють 0.

43
00:02:50,690 --> 00:02:52,910
Випадкові числа також підходять, але

44
00:02:52,910 --> 00:02:55,630
зазвичай ніхто цього не робить для ЛР.

45
00:02:55,630 --> 00:02:59,310
Це через те, що функція опукла. Тому немає значення де ініціалізувати 
(мається на увазі точка на графіку),

46
00:02:59,310 --> 00:03:02,180
все одно ми дістанемось одної точки або близько до неї.

47
00:03:02,180 --> 00:03:06,450
Градієнтний спуск стартує з ініціалізованої точки і

48
00:03:06,450 --> 00:03:10,310
далі робить крок в напрямку найнижчої точки.

49
00:03:10,310 --> 00:03:15,290
Тож після першого кроку Градієнтний спуск може опинитись ось тут, бо

50
00:03:15,290 --> 00:03:19,320
він намагається крокувати якомога стрімкіше вниз

51
00:03:19,320 --> 00:03:21,250
і якомога швидше.

52
00:03:21,250 --> 00:03:23,600
Це одна ітерація Градієнтного спуску.

53
00:03:23,600 --> 00:03:27,084
Після двох ітерацій Градієнтного спуск може опинитись тут,

54
00:03:27,084 --> 00:03:28,830
після трьох - тут і т.д. (Схоже,

55
00:03:28,830 --> 00:03:32,640
зараз наступні кроки будуть прикриті передньою стороною графіка). 
І так поки, сподіваємось,

56
00:03:32,640 --> 00:03:38,880
збіжиться з цим глобальним оптимумом 
чи з точкою близькою до нього.

57
00:03:38,880 --> 00:03:42,300
Тож це зображення ілюструє алгоритм Градієнтного спуску.

58
00:03:42,300 --> 00:03:44,310
Давай опишемо це більш детально.

59
00:03:44,310 --> 00:03:47,750
Задля ілюстрації, давай скажемо, що деяка функція J(w),

60
00:03:47,750 --> 00:03:51,700
яку ми хочемо мінімізувати, виглядає приблизно так.

61
00:03:51,700 --> 00:03:54,650
Щоб було легше зобразити, я поки ігноруватиму b.

62
00:03:54,650 --> 00:03:59,210
Просто щоб отримати двовимірний графік замість багатовимірного.

63
00:03:59,210 --> 00:04:01,240
Тож Градієнтний спуск робить наступне.

64
00:04:01,240 --> 00:04:06,740
Ми будемо повторно виконувати наступну корекцію.

65
00:04:06,740 --> 00:04:09,467
Ми братимемо значення w і коригувати його.

66
00:04:09,467 --> 00:04:12,508
Позначати корекцію будемо ":=".

67
00:04:12,508 --> 00:04:17,426
Коригуємо w до w-α помножене на

68
00:04:17,426 --> 00:04:22,200
(це похідна) dJ(w)/dw.

69
00:04:22,200 --> 00:04:26,230
І будемо повторювати це поки алгоритм не збіжиться.

70
00:04:26,230 --> 00:04:30,666
Дещо про позначення. α - це навчальний темп, він

71
00:04:30,666 --> 00:04:36,820
контролює величину кроку в кожній ітерації Градієнтного спуску.

72
00:04:36,820 --> 00:04:41,200
Пізніше ми обговоримо як вибирати навчальний темп α.

73
00:04:41,200 --> 00:04:44,490
Друга величина, ось ця похідна,

74
00:04:44,490 --> 00:04:48,010
це фактично корекція або величина зміни параметру w, 
яку ми хочемо застосувати.

75
00:04:48,010 --> 00:04:52,700
Коли ми почнемо писати код для застосування Градієнтного спуску,

76
00:04:52,700 --> 00:04:57,380
ми будемо використовувати конвенцію, згідно якої назву змінної в коді

77
00:04:58,620 --> 00:05:02,300
будемо записувати як dw, щоб позначити цю похідну.

78
00:05:02,300 --> 00:05:06,551
Тож при написанні коду будемо писати щось типу

79
00:05:06,551 --> 00:05:10,046
w:=w-α*dw.

80
00:05:10,046 --> 00:05:14,750
Тож dw ми будемо використовувати як назву змінної 
для позначення цієї похідної.

81
00:05:14,750 --> 00:05:19,330
Тепер давай пересвідчимось, 
що дана корекція Градієнтним спуском має зміст.

82
00:05:19,330 --> 00:05:21,880
Скажімо, w знаходиться ось тут.

83
00:05:21,880 --> 00:05:26,060
Тож ми знаходимось в цій точці функції Витрат J(w).

84
00:05:26,060 --> 00:05:29,270
Запам'ятай, що згідно визначення, похідна -

85
00:05:29,270 --> 00:05:31,420
це нахил [кутовий коефіцієнт дотичної] функції в заданій точці.

86
00:05:31,420 --> 00:05:36,190
Тобто, нахил - це відношення висоти до ширини 
[протилежного катета до прилеглого],

87
00:05:36,190 --> 00:05:40,290
ось цього маленького трикутника, що і є дотичною до J(w) в цій точці.

88
00:05:40,290 --> 00:05:43,900
Тут бачимо, що похідна має позитивне значення,

89
00:05:43,900 --> 00:05:48,830
w коригується на величину 
w мінус навчальний темп помножений на похідну.

90
00:05:48,830 --> 00:05:53,310
Похідна має позитивне значення, тож ми зменшуємо w

91
00:05:53,310 --> 00:05:55,260
і просуваємось вліво, здійснюючи цей крок.

92
00:05:55,260 --> 00:05:59,380
Тож Градієнтний спуск змушує алгоритм повільно

93
00:05:59,380 --> 00:06:04,450
зменшувати параметр, якщо ми почали з оцього великого значення w.

94
00:06:04,450 --> 00:06:08,545
Інший приклад. Якби w було отут,

95
00:06:08,545 --> 00:06:15,050
то в цій точці нахил dJ(w)/dw 
матиме негативне значення, тож

96
00:06:15,050 --> 00:06:22,771
корекція Градієнтним спуском буде віднімати добуток 
α і негативного числа.

97
00:06:22,771 --> 00:06:27,122
А значить, буде повільно збільшувати w, тож ми будемо збільшувати і

98
00:06:27,122 --> 00:06:31,530
збільшувати w з кожною успішною ітерацією Градієнтного спуску.

99
00:06:31,530 --> 00:06:34,387
Тож, сподіваємось, що незалежно від ініціалізації (зліва

100
00:06:34,387 --> 00:06:39,000
чи справа) Градієнтний спуск буде рухатись 
назустріч оцьому глобальному мінімуму.

101
00:06:39,000 --> 00:06:43,100
Якщо ти незнайомий/а з похідними чи 
диференціальним численням і

102
00:06:43,100 --> 00:06:49,710
не розумієш, що означає dJ(w)/dw, не переймайся дуже через це.

103
00:06:49,710 --> 00:06:53,770
Ми поговоримо дещо детальніше про похідні в наступному відео.

104
00:06:53,770 --> 00:06:56,761
Якщо ти добре ознайомлений/а з диф.численням,

105
00:06:56,761 --> 00:07:02,321
то, можливо, будеш глибше розуміти роботу нейронних мереж (НМ).

106
00:07:02,321 --> 00:07:05,471
Проте, навіть якщо ти зовсім незнайомий/а з диф.численням,

107
00:07:05,471 --> 00:07:10,091
в кількох наступних відео ми дамо тобі розуміння похідних і

108
00:07:10,091 --> 00:07:14,980
диф.числення достатнє для ефективного використання НМ.

109
00:07:14,980 --> 00:07:16,410
Зараз достатньо розуміти,

110
00:07:16,410 --> 00:07:21,520
що цей вираз відображає нахил функції і що

111
00:07:21,520 --> 00:07:26,760
нам потрібно знати нахил функції і 
актуальні значення параметрів,

112
00:07:26,760 --> 00:07:31,140
щоб ми могли крокувати якомога стрімкіше вниз, тобто могли знати

113
00:07:31,140 --> 00:07:35,450
напрямок, який поведе нас вниз по функції Витрат J.

114
00:07:36,660 --> 00:07:42,520
Тож ми записали Градієнтний спуск для J лише для параметра w.

115
00:07:42,520 --> 00:07:47,150
А в ЛР наша функція Витрат залежить і від w і від b.

116
00:07:47,150 --> 00:07:50,894
Тож в цьому випадку, вкладений цикл Градієнтного спуску, 
ось цей вираз,

117
00:07:50,894 --> 00:07:53,302
те, що потрібно повторювати, стає наступним.

118
00:07:53,302 --> 00:07:57,970
Робимо корекцію w: w мінус навчальний темп помножений

119
00:07:57,970 --> 00:08:02,030
на похідну від J(w, b) по w.

120
00:08:02,030 --> 00:08:07,460
І робимо корекцію b: b мінус навчальний темп помножений

121
00:08:07,460 --> 00:08:12,270
на похідну функції Витрат по b.

122
00:08:12,270 --> 00:08:17,300
Тож ці 2 вирази внизу - це фактична корекція, яку ми впроваджуємо.

123
00:08:17,300 --> 00:08:22,320
Також хочу просто згадати одну конвенцію позначень 
в диф.численні,

124
00:08:22,320 --> 00:08:24,560
яка може декого заплутати.

125
00:08:24,560 --> 00:08:28,387
Я не вважаю, що розуміти диф.числення це дуже важливо, 
але якщо ти

126
00:08:28,387 --> 00:08:32,411
побачиш отаке, то я хочу запевнити, 
що не варто надто цим перейматись.

127
00:08:32,411 --> 00:08:35,519
Я маю на увазі, що в диф.численні оцей вираз

128
00:08:35,519 --> 00:08:40,730
ми записуємо ось так, отаким черв'ячком.

129
00:08:40,730 --> 00:08:46,160
Цей символ, це насправді маленьке d,

130
00:08:46,160 --> 00:08:51,070
записане вигадливим стилізованим шрифтом. 
Коли ти побачиш його,

131
00:08:51,070 --> 00:08:56,145
то знай, що це позначення похідної J(w, b), 
тобто нахил функції

132
00:08:56,145 --> 00:09:01,580
J(w, b), тобто величина нахилу функції в напрямку до w.

133
00:09:01,580 --> 00:09:06,640
І, згідно правил позначень в диф.численні, 
яке, на мою думку, не надто логічне,

134
00:09:06,640 --> 00:09:11,780
проте, згідно цих правил позначень в диф.численні, 
які, на мою думку, набагато

135
00:09:11,780 --> 00:09:16,940
складніші ніж необхідно, якщо J є функцією від двох чи більше

136
00:09:16,940 --> 00:09:21,550
змінних, то замість використання маленького d 
використовується цей вигадливий знак.

137
00:09:21,550 --> 00:09:24,380
Він називається знаком частинної похідної.

138
00:09:24,380 --> 00:09:26,120
Але не переймайся цим,

139
00:09:26,120 --> 00:09:31,090
бо якщо J є функцією від однієї змінної, 
то використовується маленька d.

140
00:09:31,090 --> 00:09:33,960
Тож, єдина причина для використанням цього вигадливого

141
00:09:33,960 --> 00:09:38,040
знаку частинної похідної замість маленької d (як вгорі) -

142
00:09:38,040 --> 00:09:41,570
це чи є J функцією від двох чи більше змінних.

143
00:09:41,570 --> 00:09:45,900
В цьому випадку, використовується знак частинної похідної. А

144
00:09:45,900 --> 00:09:51,480
якщо J є функцією від однієї змінної, то використовується маленька d.

145
00:09:51,480 --> 00:09:55,410
Це одне з вигадливих правил позначень диф.числення, які,

146
00:09:55,410 --> 00:09:58,540
на мою думку, лише ускладнюють те, що цього не вимагає.

147
00:09:58,540 --> 00:10:03,300
Тож, якщо ти бачиш знак частинної похідної, 
то це значить лише, що обчислюється

148
00:10:03,300 --> 00:10:07,290
значення нахилу функції від однієї зі змінних.

149
00:10:07,290 --> 00:10:12,530
Аналогічно, щоб дотримуватись, скажімо, 
формально правильних математичних

150
00:10:12,530 --> 00:10:18,070
позначень в диф.численні, 
так як J тут має два вхідних параметри, а не один,

151
00:10:18,070 --> 00:10:22,540
цей вираз внизу повинен бути записаний зі знаком частинної похідної.

152
00:10:22,540 --> 00:10:28,290
Але це насправді те ж саме, майже те саме, що й маленька d.

153
00:10:28,290 --> 00:10:31,360
Врешті, при реалізації цього в коді,

154
00:10:31,360 --> 00:10:36,220
ми використовуватимемо конвенцію, згідно якої 
оце значення - число, на яке

155
00:10:36,220 --> 00:10:41,980
ми коригуємо w - в коді позначається dw.

156
00:10:41,980 --> 00:10:44,220
А оце значення -

157
00:10:44,220 --> 00:10:47,230
число, на яке ми хочемо скоригувати b -

158
00:10:47,230 --> 00:10:50,740
в коді позначається як змінна db.

159
00:10:50,740 --> 00:10:55,580
Добре. Отож, так реалізується Градієнтний спуск.

160
00:10:55,580 --> 00:10:59,830
Тепер, якщо ти не стикався з диф.численням багато років, 
я розумію, що, можливо,

161
00:10:59,830 --> 00:11:03,770
до цього часу ти звик/ла використовувати 
багато інших похідних диф.числення.

162
00:11:03,770 --> 00:11:06,330
Якщо це так, не переймайся.

163
00:11:06,330 --> 00:11:10,150
В наступному відео ми дано краще розуміння похідних.

164
00:11:10,150 --> 00:11:13,560
І, навіть, без глибокого математичного розуміння диф.числення,

165
00:11:13,560 --> 00:11:16,310
з просто інтуїтивним розумінням диф.числення

166
00:11:16,310 --> 00:11:19,130
ти будеш здатен/на заставляти НМ ефективно працювати.

167
00:11:19,130 --> 00:11:22,743
Тож з цим, давай перейдемо до наступного відео, 
в якому ми детальніше обговоримо

168
00:11:22,743 --> 00:11:23,470
похідні.