1
00:00:00,000 --> 00:00:04,360
क्यों चाहिए एक न्यूरल नेटवर्क को 
एक नॉन-लीनियर ऐक्टिवेशन फ़ंक्शन?

2
00:00:04,360 --> 00:00:07,425
ऐसा होता है कि आपके न्यूरल नेटवर्क 
करने के लिए दिलचस्प फ़ंक्शन्स,

3
00:00:07,425 --> 00:00:10,335
आपको अवश्य लेना चाहिए 
एक नॉन-लीनियर ऐक्टिवेशन फ़ंक्शन.

4
00:00:10,335 --> 00:00:15,830
चलिए देखते हैं क्यों. तो यहाँ है 
फ़ॉर्वर्ड प्रॉपगेशन न्यूरल नेटवर्क का.

5
00:00:15,830 --> 00:00:17,770
हम इससे छुटकारा क्यों नहीं पा लेते?

6
00:00:17,770 --> 00:00:22,315
हटा दें फ़ंक्शन g और सेट करें a1 बराबर z1 के.

7
00:00:22,315 --> 00:00:27,690
या वैकल्पिक रूप से, आप कह सकते है 
कि g ऑफ़ z बराबर है z के, है न?

8
00:00:27,690 --> 00:00:31,813
कभी-कभी इसे कहते हैं लीनियर ऐक्टिवेशन फ़ंक्शन.

9
00:00:31,813 --> 00:00:33,205
शायद इसके लिए एक बेहतर नाम होगा

10
00:00:33,205 --> 00:00:37,800
आयडेंटिटी ऐक्टिवेशन फ़ंक्शन क्योंकि यह 
सिर्फ़ आउट्पुट करता है जो भी इनपुट था.

11
00:00:37,800 --> 00:00:39,600
इस उद्देश्य से,

12
00:00:39,600 --> 00:00:43,310
क्या हो यदि a2 सिर्फ़ बराबर हो z2 के?

13
00:00:43,310 --> 00:00:45,183
परिणाम यह निकला कि, यदि आप ऐसा करते हैं,

14
00:00:45,183 --> 00:00:53,620
तब यह मॉडल सिर्फ़ कम्प्यूट कर रहा है y या y-हैट 
इनपुट फ़ीचर्ज़ x के एक लीनियर फ़ंक्शन की तरह.

15
00:00:53,620 --> 00:00:55,940
लेने के लिए पहली दो इक्वेज़न्स,

16
00:00:55,940 --> 00:01:04,547
यदि आपके पास है वह a1 बराबर z1 है बराबर w1

17
00:01:04,547 --> 00:01:15,965
x प्लस b और सिर्फ़ तब a2 है बराबर z2 है बराबर w2,

18
00:01:15,965 --> 00:01:25,370
a1 प्लस b, तब यदि आप लेते हैं 
परिभाषा a1 और रखते हैं यहाँ,

19
00:01:25,370 --> 00:01:32,585
आपको मिलेगा a2 है बराबर w2 गुणा w1

20
00:01:32,585 --> 00:01:35,695
x प्लस b1, ठीक है?

21
00:01:35,695 --> 00:01:40,985
तो, यह है a1 प्लस

22
00:01:40,985 --> 00:01:47,460
b2 और यह सरलीकृत होता है w2,

23
00:01:47,460 --> 00:01:53,120
w1, x प्लस w2,

24
00:01:53,120 --> 00:01:57,966
b1 प्लस b2.

25
00:01:57,966 --> 00:02:01,930
तो यह है सिर्फ़,

26
00:02:01,930 --> 00:02:06,726
चलिए कहते हैं इसे w प्राइम, b प्राइम.

27
00:02:06,726 --> 00:02:10,935
तो यह है सिर्फ़ w प्राइम x प्लस b प्राइम.

28
00:02:10,935 --> 00:02:13,720
यदि आपको लेना होता लीनियर ऐक्टिवेशन फ़ंक्शन

29
00:02:13,720 --> 00:02:17,095
या जिन्हें हम आयडेंटिटी ऐक्टिवेशन फ़ंक्शन भी कहते हैं,

30
00:02:17,095 --> 00:02:23,335
तब न्यूरल नेटवर्क सिर्फ़ आउट्पुट करता 
एक लीनियर फ़ंक्शन इनपुट का.

31
00:02:23,335 --> 00:02:26,260
और हम बात करेंगे डीप नेटवर्क्स की बाद में.

32
00:02:26,260 --> 00:02:27,460
न्यूरल नेटवर्कस जिनमे हैं बहुत सी,

33
00:02:27,460 --> 00:02:29,167
बहुत सी लेयर्स, बहुत अधिक हिडन लेयर्स.

34
00:02:29,167 --> 00:02:34,460
और ऐसा होता है कि यदि आप लेते हैं एक 
लीनियर ऐक्टिवेशन फ़ंक्शन या वैकल्पिक रूप से,

35
00:02:34,460 --> 00:02:36,760
यदि आपके पास नहीं है एक ऐक्टिवेशन फ़ंक्शन तब

36
00:02:36,760 --> 00:02:39,250
कोई अंतर नहीं है कि कितनी लेयर्स हैं आपके न्यूरल नेटवर्क में,

37
00:02:39,250 --> 00:02:42,970
यह सिर्फ़ कम्प्यूट कर रहा है एक लीनियर ऐक्टिवेशन फ़ंक्शन.

38
00:02:42,970 --> 00:02:45,905
तो शायद आपको चाहिए ही नहीं कोई हिडन लेयर्स.

39
00:02:45,905 --> 00:02:49,335
जैसे कि मैंने संक्षेप में उल्लेख किया है,

40
00:02:49,335 --> 00:02:50,880
ऐसा होता है कि अगर आपके पास है

41
00:02:50,880 --> 00:02:55,170
एक लीनियर ऐक्टिवेशन फ़ंक्शन यहाँ और 
एक सिग्मोईड ऐक्टिवेशन फ़ंक्शन यहाँ,

42
00:02:55,170 --> 00:02:58,275
तब यह मॉडल कुछ ज़्यादा नहीं बता रहा तुलना में

43
00:02:58,275 --> 00:03:02,505
स्टैंडर्ड लॉजिसिटिक रेग्रेशन के बिना किसी हिडन लेयर के.

44
00:03:02,505 --> 00:03:05,910
तो मैं उसे साबित नहीं करूँगा लेकिन 
आप कर सकते हैं यदि आप चाहें.

45
00:03:05,910 --> 00:03:07,465
लेकिन शुद्ध संदेश है कि,

46
00:03:07,465 --> 00:03:11,265
एक हिडन लेयर लगभग बेकार है

47
00:03:11,265 --> 00:03:17,130
क्योंकि संरचना दो लीनियर फ़ंक्शनज़ की
 ख़ुद भी एक लिनीयर फ़ंक्शन है.

48
00:03:17,130 --> 00:03:19,950
तो, जबतक आप वहाँ नॉन-लिनियरिटी नहीं लाते हैं,

49
00:03:19,950 --> 00:03:21,235
तब आप नहीं कम्प्यूट कर रहे

50
00:03:21,235 --> 00:03:25,350
अधिक दिलचस्प फ़ंक्शन्स, चाहे आप डीप 
भी जाते हैं नेटवर्क में.

51
00:03:25,350 --> 00:03:29,820
वहाँ सिर्फ़ एक स्थान है जहाँ आप शायद ले 
सकते हैं एक लीनियर ऐक्टिवेशन फ़ंक्शन,

52
00:03:29,820 --> 00:03:36,810
g ऑफ़ z बराबर है z और वह है जब आप कर रहे हैं
 मशीन लर्निंग एक रेग्रेशन प्रॉब्लम पर.

53
00:03:36,810 --> 00:03:39,420
तो, यदि y है एक रियल नम्बर.

54
00:03:39,420 --> 00:03:42,675
तो उदाहरण के लिए, यदि आप प्रिडिक्ट करना 
चाह रहे हैं क़ीमत घरों की,

55
00:03:42,675 --> 00:03:46,935
तो y नहीं है 0,1, लेकिन यह है एक रियल नम्बर,

56
00:03:46,935 --> 00:03:54,660
कहीं $0 क़ीमत से आगे कितना महँगा घर हो सकता है.

57
00:03:54,660 --> 00:03:58,640
मुझे लगता है शायद, घर हो सकते हैं संभवत: मिल्यन डालर्ज़ तक.

58
00:03:58,640 --> 00:04:04,580
हालाँकि, कितनी भी क़ीमत है घर की आपके डेटा सेट में,

59
00:04:04,580 --> 00:04:09,705
परंतु यदि y लेता है ये रियल वैल्यूज़,

60
00:04:09,705 --> 00:04:14,700
तब हो सकता है सही लेना 
लीनियर ऐक्टिवेशन फ़ंक्शन यहाँ ताकि

61
00:04:14,700 --> 00:04:17,805
आपकी आउट्पुट y-हैट है

62
00:04:17,805 --> 00:04:24,215
एक रियल नम्बर ही, माइनस इन्फ़िनिटी से 
प्लस इन्फ़िनिटी तक कुछ भी.

63
00:04:24,215 --> 00:04:28,700
लेकिन तब, हिडन यूनिट्स को नहीं इस्तेमाल करना 
चाहिए लीनियर ऐक्टिवेशन फ़ंक्शन.

64
00:04:28,700 --> 00:04:34,380
वे ले सकते हैं ReLU या tanh या 
leaky ReLU या शायद कुछ और.

65
00:04:34,380 --> 00:04:39,995
तो, स्थान जहाँ आप शायद ले सकते हैं 
एक लीनियर ऐक्टिवेशन है अक्सर आउट्पुट लेयर.

66
00:04:39,995 --> 00:04:41,595
लेकिन उसके अलावा,

67
00:04:41,595 --> 00:04:44,730
लेना एक लीनियर ऐक्टिवेशन फ़ंक्शन

68
00:04:44,730 --> 00:04:50,135
एक हिडन लेयर में विशेष परिस्थियों के अलावा,

69
00:04:50,135 --> 00:04:52,320
कम्प्रेशन से सम्बंधित जिसकी हम बात नहीं करेंगे

70
00:04:52,320 --> 00:04:56,250
लेना एक लीनियर ऐक्टिवेशन फ़ंक्शन बहुत ही कम है.

71
00:04:56,250 --> 00:04:59,130
ओह, और हां, अगर आप वास्तव में घरों की कीमतें 
प्रिडिक्ट कर रहे है जैसे आपने देखा

72
00:04:59,130 --> 00:05:03,795
सप्ताह एक के वीडियो में क्योंकि 
घरों की क़ीमतें हैं नॉन-नेगेटिव शायद तब भी,

73
00:05:03,795 --> 00:05:07,060
आप ले सकते हैं ReLU ऐक्टिवेशन फ़ंक्शन ताकि

74
00:05:07,060 --> 00:05:11,580
आपकी आउट्पुट्स y-हैट हैं सभी बड़ी या बराबर ज़ीरो के.

75
00:05:11,580 --> 00:05:13,980
तो मुझे आशा है कि यह देता है आपको एक ज्ञान 
 कि क्यों होना

76
00:05:13,980 --> 00:05:19,290
एक नॉन-लीनियर ऐक्टिवेशन फ़ंक्शन है 
एक महत्वपूर्ण हिस्सा न्यूरल नेटवर्कस का.

77
00:05:19,290 --> 00:05:23,945
आगे, हम बात करेंगे ग्रेडीयंट डिसेंट के बारे में, 
और वह करने के लिए,

78
00:05:23,945 --> 00:05:27,440
ग्रेडीयंट डिसेंट के बारे में चर्चा करने के लिए, अगले वीडियो में,

79
00:05:27,440 --> 00:05:29,230
मैं आपको दिखाना चाहता हूँ कैसे अनुमान करना है,

80
00:05:29,230 --> 00:05:34,105
कैसे कम्प्यूट करनी है स्लोप या डेरिवेटिव्स ऐक्टिवेशन फ़ंक्शन्स के.

81
00:05:34,105 --> 00:05:35,600
तो चलिए चलते हैं उस पर.