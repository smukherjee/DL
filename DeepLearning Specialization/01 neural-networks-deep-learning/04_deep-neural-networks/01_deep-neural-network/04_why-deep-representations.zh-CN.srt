1
00:00:00,000 --> 00:00:03,339
我们都听说过 深度神经网络
对于很多问题确实很有效

2
00:00:03,339 --> 00:00:07,073
而它们不仅仅需要是 大型的 神经网络

3
00:00:07,073 --> 00:00:10,718
具体而言 它们需要是 深度的
或者 有很多隐藏层

4
00:00:10,718 --> 00:00:12,208
为什么会这样？

5
00:00:12,208 --> 00:00:15,833
让我们来看几个例子
并试图获得一些直观的感受

6
00:00:15,833 --> 00:00:17,720
为什么深度网络可能会很有效

7
00:00:17,720 --> 00:00:22,181
那么首先 什么是深度网络计算?

8
00:00:22,181 --> 00:00:25,393
如果我们搭建一个系统用于 面部识别

9
00:00:25,393 --> 00:00:29,631
或者 面部检测
那么神经网络就可以在此运用

10
00:00:29,631 --> 00:00:35,059
如果我们输入一张面部图片
那么 神经网络的第一层

11
00:00:35,059 --> 00:00:40,000
可以被我们认为是一个
特征检测器 或 边缘检测器

12
00:00:40,000 --> 00:00:45,519
在这个例子中 我正在绘制一个具有
20个隐藏神经元的 神经网络

13
00:00:45,519 --> 00:00:48,017
可能是 在这个图像上的 某种算法

14
00:00:48,017 --> 00:00:52,357
且这20个隐藏神经元通过这些小方块可视化

15
00:00:52,357 --> 00:00:57,325
所以例如 这个 这个微型可视化图
表示一个 隐藏神经元 正在试图找出

16
00:00:57,325 --> 00:01:01,978
在DMH (Depth-MHI-HOG) 中该方向的边缘位置

17
00:01:01,978 --> 00:01:05,914
也许 这个隐藏神经元可能试图

18
00:01:05,914 --> 00:01:09,955
找出这幅图像中的 水平边缘 在哪里

19
00:01:09,955 --> 00:01:13,184
并且当我们在稍后的课程中讨论 卷积网络 时

20
00:01:13,184 --> 00:01:16,129
这个特殊的可视化可能会更有意义

21
00:01:16,129 --> 00:01:19,562
但是 形式上
我们可以认为神经网络的第一层就好比

22
00:01:19,562 --> 00:01:22,690
看一张图片 并尝试找出这张图片的边缘

23
00:01:22,690 --> 00:01:27,356
现在 让我们来找出这张图片的边缘

24
00:01:27,356 --> 00:01:28,730
通过 将像素分组 来形成边缘的方法

25
00:01:28,730 --> 00:01:34,670
然后 可以取消检测边缘
并将边缘组合在一起 以形成面部的一部分

26
00:01:34,670 --> 00:01:40,289
所以例如 我们可能有一个低神经元
试图看看它是否发现了一个眼睛

27
00:01:40,289 --> 00:01:44,480
或者一个不同的神经元
试图找到鼻子的一部分

28
00:01:44,480 --> 00:01:47,463
所以通过把 大量的边缘 放置在一起

29
00:01:47,463 --> 00:01:50,970
我们可以开始检测面部的不同部位

30
00:01:50,970 --> 00:01:56,035
然后 最后 通过将面部的不同部位

31
00:01:56,035 --> 00:02:01,006
如眼睛 鼻子 耳朵或下巴组合在一起

32
00:02:01,006 --> 00:02:03,564
然后 可以尝试识别或检测不同类型的面部

33
00:02:03,564 --> 00:02:07,755
所以直观地 我们可以将神经网络的浅层

34
00:02:07,755 --> 00:02:10,190
看作是简单的检测函数 如 (检测) 边缘

35
00:02:10,190 --> 00:02:14,573
然后 在神经网络的后一层 将它们组合在一起

36
00:02:14,573 --> 00:02:17,625
以便它可以学习 更多和更复杂 的功能

37
00:02:17,625 --> 00:02:23,640
当我们谈论 卷积网络时 这些可视化将更有意义

38
00:02:23,640 --> 00:02:26,203
(值得注意的是) 这种可视化的一个技术细节

39
00:02:26,203 --> 00:02:29,802
边缘检测器检测图像中相对较小的区域

40
00:02:29,802 --> 00:02:31,703
也许是像这样的非常小的区域

41
00:02:31,703 --> 00:02:36,616
然后 面部检测器可能会看到更大的图像区域

42
00:02:36,616 --> 00:02:41,308
但是 我们从中获取的进一步的关键信息
仅仅是找到像边缘这样的简单事物

43
00:02:41,308 --> 00:02:43,675
然后构建它们

44
00:02:43,675 --> 00:02:47,216
将它们组合在一起<br />以检测更复杂的事物 例如

45
00:02:47,216 --> 00:02:50,530
然后 (再次) 将它们组合在一起<br />以找到更复杂的事物

46
00:02:50,530 --> 00:02:55,665
这种由简单到复杂的 分层表示

47
00:02:55,665 --> 00:02:58,508
或 组合表示

48
00:02:58,508 --> 00:03:04,114
不仅适用于图像和面部识别<br />也适用于其它类型的数据

49
00:03:04,114 --> 00:03:08,593
例如 如果我们尝试建立语音识别系统

50
00:03:08,593 --> 00:03:10,908
则很难难形象化演说

51
00:03:10,908 --> 00:03:15,684
但如果我们输入一段音频剪辑<br />那么 神经网络的第一层

52
00:03:15,684 --> 00:03:20,863
可能学会检测低级的音频波形特征<br />比如这个音调升高了吗?

53
00:03:20,863 --> 00:03:21,703
亦或是降低了?

54
00:03:21,703 --> 00:03:26,869
这是白噪声还是滑动声 如 [声音]?

55
00:03:26,869 --> 00:03:27,903
什么是音调?

56
00:03:27,903 --> 00:03:31,124
当涉及到这一点时
如上所述 检测低级的波形特征

57
00:03:31,124 --> 00:03:34,233
然后通过组合低级波形 (特征)

58
00:03:34,233 --> 00:03:37,937
我们就可以学到检测基本的声音单位

59
00:03:37,937 --> 00:03:40,297
在语言学中 他们称之为音素

60
00:03:40,297 --> 00:03:45,098
但是 例如 在猫这个词中 C是一个音素 A是一个音素

61
00:03:45,098 --> 00:03:46,787
T又是另一个音素

62
00:03:46,787 --> 00:03:49,987
但是学着找到可能的语音基本单元

63
00:03:49,987 --> 00:03:54,688
然后把它们组合在一起 可能用来识别语音中的单词

64
00:03:54,688 --> 00:03:58,270
然后 再把它们组合在一起

65
00:03:58,270 --> 00:04:02,912
用以识别整个短语或句子

66
00:04:02,912 --> 00:04:07,572
因此 拥有众多隐藏层的深度神经网络可能能够让早期的

67
00:04:07,572 --> 00:04:10,477
神经层学习这些较低级别的简单特征

68
00:04:10,477 --> 00:04:15,339
然后让后面更深的神经层
汇聚前面所检测到的简单信息

69
00:04:15,339 --> 00:04:19,392
以便检测更复杂的事物
例如 识别特定的单词

70
00:04:19,392 --> 00:04:21,040
甚至是短语或句子

71
00:04:21,040 --> 00:04:24,745
说出来才能进行语音识别

72
00:04:24,745 --> 00:04:30,168
我们看到的是 尽管其它神经层正在计算着

73
00:04:30,168 --> 00:04:35,673
类似边缘这类 似乎相对简单的输入函数

74
00:04:35,673 --> 00:04:41,046
而当我们深入研究 (神经) 网络的时候
我们实际上可以做一些令人惊讶复杂的事情

75
00:04:41,046 --> 00:04:44,876
例如 检测面部 或是 检测单词 短语 句子

76
00:04:44,876 --> 00:04:48,767
某些人喜欢在深度神经网络与人脑之间做类比

77
00:04:48,767 --> 00:04:52,656
我们认为 或神经科学家认为

78
00:04:52,656 --> 00:04:57,162
人类的大脑开始也检测简单的东西
如你所看到的边缘

79
00:04:57,162 --> 00:05:00,370
然后 (将这些信息) 建立起来
以检测更复杂的事物

80
00:05:00,370 --> 00:05:02,440
如你所看到的面孔

81
00:05:02,440 --> 00:05:05,038
我认为将深度神经网络

82
00:05:05,038 --> 00:05:08,276
与人脑做类比是有点危险的

83
00:05:08,276 --> 00:05:13,301
但这对我们认知人脑的工作存在着很多启迪

84
00:05:13,301 --> 00:05:18,102
那就是 人脑可能检测像边缘这类简单的信息

85
00:05:18,102 --> 00:05:22,598
然后将它们汇聚在一起 来形成愈加复杂的物体

86
00:05:22,598 --> 00:05:27,430
从而成为 人们学习 的松散灵感形式

87
00:05:27,430 --> 00:05:29,850
本周 我们将在视频领域中
看到更多有关人类大脑或生物大脑的内容

88
00:05:29,850 --> 00:05:33,065
本周 我们将在视频领域中
看到更多有关人类大脑或生物大脑的内容

89
00:05:35,534 --> 00:05:40,407
关于深度网络似乎有效的另一条直觉如下

90
00:05:40,407 --> 00:05:42,756
关于深度网络似乎有效的另一条直觉如下

91
00:05:42,756 --> 00:05:47,868
所以这个结果来自电路理论

92
00:05:47,868 --> 00:05:53,760
它涉及到什么类型的函数可以用<br />不同的 (与 或 非) 逻辑情况来计算

93
00:05:53,760 --> 00:05:58,860
所以非正式地 我们用相对较小但具有深度的神经网络来计算这些功能

94
00:05:58,860 --> 00:06:03,595
而这里的小 是说隐藏神经元的数量相对较小

95
00:06:03,595 --> 00:06:07,553
但是 如果我们尝试使用浅网络计算相同的功能

96
00:06:07,553 --> 00:06:09,178
它没有足够多的隐藏层

97
00:06:09,178 --> 00:06:13,296
然后 我们可能需要更多的<br />指数级的隐藏神经元来进行计算

98
00:06:13,296 --> 00:06:18,109
所以让我们来看一个例子<br />来非正式的说明这一点

99
00:06:18,109 --> 00:06:21,423
假设我们尝试计算所有输入特征的异或

100
00:06:21,423 --> 00:06:23,349
或者奇偶性

101
00:06:23,349 --> 00:06:28,430
所以我们尝试计算 X1异或X2异或X3异或... 直到Xn

102
00:06:28,430 --> 00:06:33,064
如果我们有n或者nX特征的话

103
00:06:33,064 --> 00:06:39,924
如果我们像这样自由建立异或树<br />则首先计算X1和X2的异或

104
00:06:39,924 --> 00:06:44,586
然后是X3和X4的异或

105
00:06:44,586 --> 00:06:49,392
而在技术上 如果我们只是使用 与 或 非  门

106
00:06:49,392 --> 00:06:54,196
我们可能需要几层来计算异或函数 而不只是一层

107
00:06:54,196 --> 00:06:58,791
但是使用相对较小的电路<br />我们可以计算异或 等等

108
00:06:58,791 --> 00:07:03,987
然后 你可以建立一个真正的异或树

109
00:07:03,987 --> 00:07:12,090
直到最终 你有一个电路输出<br />好吧 让我们称之为Y

110
00:07:12,090 --> 00:07:15,236
输出向量Y等于Y

111
00:07:15,236 --> 00:07:18,398
也就是 所有这些输入位的异或 或 奇偶性

112
00:07:18,398 --> 00:07:24,790
因此 为了计算异或<br />左侧确定的网络将有logN阶的复杂度

113
00:07:24,790 --> 00:07:27,410
我们就有了一个异或树

114
00:07:27,410 --> 00:07:30,836
所以在此网络中的节点数或电路组件数

115
00:07:30,836 --> 00:07:33,929
或门级数并不是很多

116
00:07:33,929 --> 00:07:38,452
我们并不需要很多门去计算异或

117
00:07:38,452 --> 00:07:43,458
但现在 如果我们不能使用具有多层隐藏层的神经网络

118
00:07:43,458 --> 00:07:48,203
 如本例 对数阶的隐藏层

119
00:07:48,203 --> 00:07:53,382
如果我们强制采用一层隐藏层来计算该函数

120
00:07:53,382 --> 00:07:57,912
就像这样 那么 通过隐藏层

121
00:07:57,912 --> 00:08:02,116
将输出Y

122
00:08:02,116 --> 00:08:07,120
那么 为了计算出奇偶性或异或函数

123
00:08:07,120 --> 00:08:12,124
这些隐藏层将会呈指数级的扩大 因为必然地

124
00:08:12,124 --> 00:08:18,397
我们需要穷举2到N种可能性

125
00:08:18,397 --> 00:08:23,139
也就是从2到N

126
00:08:23,139 --> 00:08:27,898
异或为1或0的输入位的所有可能性

127
00:08:27,898 --> 00:08:32,213
所以我们最终需要一个位数是指数级的隐藏层

128
00:08:32,213 --> 00:08:33,554
所以我们最终需要一个位数是指数级的隐藏层

129
00:08:33,554 --> 00:08:38,229
我认为技术上你可以用2到N-1个隐藏单元来做到这一点

130
00:08:38,229 --> 00:08:43,948
但2的指数阶显然比常数阶大得多

131
00:08:43,948 --> 00:08:49,149
所以我希望这个例子告诉我们 对一些数学函数

132
00:08:49,149 --> 00:08:55,275
使用深层网络进行计算比使用浅层网络更容易

133
00:08:55,275 --> 00:09:01,028
实际上 我个人发现电路理论很难理解

134
00:09:01,028 --> 00:09:05,985
但它是人们在解释 深度学习网络的价值时<br />经常引用的例子之一

135
00:09:05,985 --> 00:09:11,223
但它是人们在解释 深度学习网络的价值时<br />经常引用的例子之一

136
00:09:11,223 --> 00:09:13,600
除了这个原因之外 我认为深度学习这个词

137
00:09:13,600 --> 00:09:16,897
除了这个原因之外 我认为深度学习这个词

138
00:09:16,897 --> 00:09:22,204
已经被滥用 被炒作了

139
00:09:22,204 --> 00:09:26,776
神经网络原指有很多的隐藏层的网络

140
00:09:26,776 --> 00:09:31,198
但深度学习已经是一个很大的概念了<br />它包罗万象

141
00:09:31,198 --> 00:09:36,284
所以我认为 我们应该给原来的神经网络

142
00:09:36,284 --> 00:09:39,622
就是有很多隐藏层的神经网络<br />再取一个名字

143
00:09:39,622 --> 00:09:42,970
还给他本来的面目

144
00:09:42,970 --> 00:09:47,479
大家都形成了这样一个概念<br />那就是深度网络确实很有效

145
00:09:47,479 --> 00:09:51,342
有时候人们滥用神经网络 弄太多隐藏层了

146
00:09:51,342 --> 00:09:55,500
但是当我开始解决一个新的问题时<br />我经常会首先采用甚至是逻辑回归

147
00:09:55,500 --> 00:09:58,803
然后尝试一个或两个的隐藏层

148
00:09:58,803 --> 00:10:01,722
把它当作超参数

149
00:10:01,722 --> 00:10:05,731
将其用作我们调整的参数或超参数

150
00:10:05,731 --> 00:10:07,935
来为我们的神经网络找到合适的深度

151
00:10:07,935 --> 00:10:12,800
但是在过去的几年中 人们发现

152
00:10:12,800 --> 00:10:17,590
对于某些应用来说<br />超深的深度神经网络

153
00:10:17,590 --> 00:10:22,264
有时是(解决)问题的最佳模型

154
00:10:22,264 --> 00:10:27,605
这就是为什么深度学习看起来管用的原因

155
00:10:27,605 --> 00:10:31,411
现在让我们看看如何实现正向传播机制

156
00:10:31,411 --> 00:10:33,769
以及反向传播机制