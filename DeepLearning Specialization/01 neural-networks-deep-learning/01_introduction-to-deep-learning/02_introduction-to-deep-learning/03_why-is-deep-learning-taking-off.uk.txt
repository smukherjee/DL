Якщо технічні ідеї, на яких базується глибоке навчання (ГН) і нейронні мережі (НМ) витають в повітрі останні кілька десятиліть, то чому вони стали популярні лише тепер? Давай в цьому відео розглянемо кілька основних рушіїв розквіту ГН, тому що я думаю, це допоможе тобі підібрати ті, які найбільше підходять для твоєї компанії. За останні кілька років, багато людей мене питали: "Андрію, чому ГН так добре працює"? Коли мене про це питають, я зазвичай малюю таку картинку. Скажімо, ми малюємо графік, де на горизонтальній осі ми відображаємо об'єм даних, які дано для завдання. І, скажімо, на вертикальній осі відображаємо продуктивність нашого алгоритму навчання. Наприклад, точність нашого класифікатора спаму або передбачення клацання по рекламі або точність нашої НМ при обчисленні позицій інших авто для нашого безпілотного авто. Виявляється, якщо зобразити продуктивність 
традиційного алгоритму навчання такого як метод опорних векторів (МОВ) або логістичної регресії (ЛР), як функцію від об'єму вхідних даних, ти, ймовірно, ми отримаємо ось таку криву, на якій продуктивність збільшується певний час, 
якщо збільшувати кількість вхідних даних, але в якийсь момент переходить в плато. Вона має бути більш схожа на горизонтальну лінію, 
я просто погано намалював. Ці алгоритми не знають що робити з величезною кількістю даних. В нашому суспільстві за останні, можливо, 20 років для великої кількості задач ми перейшли від оцих відносно малих об'ємів даних до, часто, дійсно великих. Для більшості випадків це відбулось завдяки оцифруванню нашого суспільства, в якому вже так багато 
людської діяльності - в царстві цифр. Ми проводимо дуже багато часу 
за нашими комп'ютерами, на веб сайтах, в мобільних додатках... 
А активність на цифрових пристроях породжує дані. Завдяки появі недорогих камер, що вбудовані в наші телефони, акселерометрів, усіх тих сенсорів інтернету речей, ми також накопичуємо все більше, більше і більше даних. Тож за останні 20 років для більшості застосунків ми назбирали набагато більше даних, ніж традиційні алгоритми навчання 
могли ефективно опрацювати. А з допомогою НМ, виявляється, що тренуючи малу НМ, продуктивність буде десь отакою. Тренуючи дещо більшу НМ, назвімо її середнього розміру НМ, продуктивність часто буде дещо кращою. 
А, тренуючи велику НМ, продуктивність буде ставати все кращою і кращою. Отже, кілька зауважень. По-перше, Якщо ми хочемо досягти дуже високої продуктивності, то нам потрібні дві речі: перша - можливість тренувати досить велику НМ, щоб скористатись перевагами великих об'ємів даних, а друга - нам потрібно бути ось тут на осі x, тобто потрібно мати дійсно багато даних. Тож, ми часто кажемо, що розмір рухає прогрес ГН. І під розміром я маю на увазі як розмір НМ, тобто НМ з великою кількістю прихованих вузлів, параметрів і зв'язків, так і розмір даних. Фактично, зараз, один з найнадійніших способів 
отримати вищу продуктивність НМ, найчастіше, - або натренувати більшу НМ, або дати їй більше даних. Хоча це працює лише до певного моменту, бо, врешті, або закінчаться дані або, врешті, НМ стане настільки великою, що тренуватиметься занадто довго. Але підбір розмірів вже пройшов достатньо довгий шлях в ГН. Щоб зробити цю діаграму дещо технічно точнішою, дозволь додати ще кілька елементів. Я підписав вісь x "Кількість даних". Технічно, це - кількість позначених даних, де під позначеними даними маються на увазі 
тренувальні зразки - і вхідні x, і позначки y. Також хочу представити позначення, яке будемо 
використовувати трохи пізніше в цьому курсі. Ми будемо використовувати маленьку m, щоб позначати 
розмір нашого тренувального набору. Тож кількість тренувальних зразків - маленька m. 
Вона і відображається на горизонтальній осі. І ще декілька деталей про цей графік. В оцьому режимі - малих тренувальних наборів - точно визначити який алгоритм кращий, 
насправді, неможливо. Тож, за відсутності великої кількості тренувальних даних, часто підбір інженерних показників, що визначають продуктивність, 
залежить від навичок інженера. Тож, достатньо ймовірно, що якщо комусь 
більше подобається тренувати за допомогою МОВ і він/вона там підбирає інженерні показники, 
а хтось в цей час тренує навіть велику НМ, то може статись, що в режимі малих тренувальних наборів МОВ буде продуктивнішим. Тож, скажімо, в цій ділянці, в лівій частині зображення, визначити більш продуктивний алгоритм не завжди можна, 
а продуктивність залежить більше від наших навичок підбирати інженерні показники 
та інших низькорівневих показників алгоритмів. І лише в цьому режимі великих даних, дуже великих тренувальних наборів, в режимі дуже великого m, справа, ми більш чітко бачимо домінування великих НМ над іншими підходами. Тож коли хтось з твоїх друзів спитає чому НМ так злетіли, раджу намалювати і їм цей малюнок. Отже, я б сказав, що на початку теперішнього злету ГН було збільшення кількості даних і обчислювальних можливостей. Тож коли ми змогли тренувати дуже великі НМ (чи то на центральному процесорі (ЦП) [CPU], 
чи на графічному (ГП) [GPU]) це дозволило нам здійснити суттєвий прогрес. Проте, все частіше, особливо останні кілька років, ми бачимо також колосальні нововведення і в алгоритмах. Тож я не хотів би недооцінити значення цього. Цікаво, що багато нововведень в алгоритмах були намаганнями значимо прискорити нейронні мережі. Тож, як конкретний приклад, одним з великих досягнень НМ був перехід від сигмоїди, що виглядає ось так, до функції ВЛВ, про яку ми згадували в попередніх відео і яка виглядає ось так. Якщо зрозуміло не все, 
про що я почав говорити, не хвилюйся. Виявляється, що одна з проблем використання сигмоїди в машинному навчанні - це ось ці ділянки, 
де кути нахилу дотичних або Градієнт дуже близький до 0. Через це навчання стає дуже повільним. Тому що при реалізації Градієнтного спуску, коли Градієнт =0, параметри змінюються дуже повільно, 
а, отже, і навчання сповільнюється. В той час як, змінивши функцію активації НМ, на оцю, яка називається ВЛВ, 
тобто Випрямленим лінійним вузлом, В-Л-В, Градієнт стає =1 для всіх додатніх вхідних значень. Вірно? Тож Градієнт менш схильний збігатись до 0. А отут зліва Градієнт або дотична до цієї прямої =0, але, виявляється, що проста заміна сигмоїди на функцію ВЛВ, зробило алгоритм Градієнтного спуску набагато швидшим. Тож, це - приклад, можливо, відносно простого 
алгоритмічного нововведення, проте, в остаточному підсумку, це алгоритмічне нововведення, 
насправді, дуже допомогло обчисленням. Насправді, є дуже багато схожих прикладів, в яких ми змінили алгоритм, 
тому що це значно прискорювало виконання коду і дозволяло не тренувати більші НМ чи зменшувати обґрунтовану кількість коду навіть коли в нас є велика НМ або багато даних. Ще одне, що показує важливість швидких обчислень - це те, що, виявляється, тренування НМ - дуже циклічне. Часто в нас є ідея архітектури НМ і ми починаємо її реалізувати - писати код. Реалізація ідеї потім дозволяє запустити експеримент, який показує наскільки добре працює наша НМ, а потім на основі цього ми вертаємось назад, щоб змінити налаштування НМ і далі знову і знову по колу. І коли наша НМ потребує дуже багато часу для тренування, то і рух по колу буде займати дуже багато часу. Тому відчувається величезна різниця в продуктивності 
при побудові ефективної НМ, коли від ідеї до її випробування проходить 10 хвилин або навіть цілий день і коли доводиться тренувати НМ цілий місяць (іноді це, все-таки, трапляється), тому що коли ми отримуємо результат через 10 хвилин чи день, то можемо випробувати набагато більше ідей і з більшою імовірністю знайти НМ, що підходитиме для нашого застосунку. Також, швидші обчислення дійсно допомогли в сенсі зменшення часу отримання експериментальних результатів. А це дійсно допомогло як інженерам НМ, так і дослідникам ГН рухатись циклом набагато швидше і вдосконалювати ідеї швидше. І все це вилилось в гучний бум для всієї дослідницької спільноти ГН, просто неймовірний я б сказав, в сенсі розробки нових алгоритмів і невпинного прогресу на цьому фронті. Тож це - деякі з рушіїв, що спричинили злет ГН. І, на щастя, ці рушійні сили досі потужно діють, щоб зробити ГН ще кращим. 
Візьмімо дані. Суспільство виробляє все більше і більше цифрових даних. 
Або візьмімо обчислення. З появою спеціалізованого апаратного забезпечення, 
як ГП [GPU], і збільшення швидкості мереж та багато іншого апаратного забезпечення, я впевнений, що наші можливості будувати дуже великі НМ з точки зору обчислень продовжать збільшуватись. 
І візьмімо алгоритми. Поки вся дослідницька спільнота ГН продовжує феноменально нововводити на алгоритмічному фронті. Тож через це, думаю, ми можемо бути оптимістами. Я - оптиміст. Я думаю, що ГН продовжить покращуватись ще багато років. Тож давай перейдемо до останнього відео розділу де поговоримо трохи детальніше про те, що ти вивчиш на цьому курсі.