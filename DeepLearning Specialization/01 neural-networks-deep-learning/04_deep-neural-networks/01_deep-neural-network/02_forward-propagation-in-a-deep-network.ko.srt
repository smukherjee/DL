1
00:00:00,060 --> 00:00:04,380
이번 비디오에서는, 

2
00:00:02,250 --> 00:00:06,150
deep l-larry 신경망이 무엇인지 다루었고, 

3
00:00:04,380 --> 00:00:08,550
또한, 이런 네트워크를 표현하는데 사용하는 

4
00:00:06,150 --> 00:00:10,650
표기방식에 대해 이야기 했습니다.

5
00:00:08,550 --> 00:00:13,769
이번 비디오에서는 어떻게 깊은 네트워크에서
전 방향전파를

6
00:00:10,650 --> 00:00:16,440
진행하는지 알아보겠습니다. 이전과 같이, 

7
00:00:13,769 --> 00:00:18,660
전 방향전파가 어떻게 생겼는지 

8
00:00:16,440 --> 00:00:21,330
1개의 트레이닝 샘플 x에 대하여 한번 보겠습니다.

9
00:00:18,660 --> 00:00:22,920
그리고 나중에는, 전체의 트레이닝 세트에

10
00:00:21,330 --> 00:00:24,810
한번에 전 방향전파를 

11
00:00:22,920 --> 00:00:26,849
진행할 수 있도록

12
00:00:24,810 --> 00:00:29,660
벡터화된 버전을 이야기해보겠습니다.

13
00:00:26,849 --> 00:00:32,579
먼저, 1개의 트레이닝 샘플 x의 경우, 

14
00:00:29,660 --> 00:00:34,800
첫번째 층의 activation을 

15
00:00:32,579 --> 00:00:42,329
계산하는 방법입니다. 이 첫번째

16
00:00:34,800 --> 00:00:48,239
층에서는 z1=w1 곱하기 x 더하기 

17
00:00:42,329 --> 00:00:51,120
b1인데요, w1 과 b1의 파라미터 

18
00:00:48,239 --> 00:00:53,879
즉 첫번째 층의 activation에 영향을 주는 
파라미터인데요, 

19
00:00:51,120 --> 00:00:56,899
이것은 신경망의 

20
00:00:53,879 --> 00:00:59,280
첫번째 층인데요, 그리고 다음으로

21
00:00:56,899 --> 00:01:04,979
그 층의 activation을 구하는데요, 

22
00:00:59,280 --> 00:01:06,810
g(z1)입니다. 그리고 이 activation 함수는 

23
00:01:04,979 --> 00:01:09,090
그 층이 어디 있는지, 

24
00:01:06,810 --> 00:01:11,010
여기 지수 인덱스는 

25
00:01:09,090 --> 00:01:12,689
첫번째 층에 해당하는 1의 값을 나타내죠. 

26
00:01:11,010 --> 00:01:13,320
그렇게하면 첫번째 층의 activation

27
00:01:12,689 --> 00:01:18,360
값을 구한 것인데요, 

28
00:01:13,320 --> 00:01:24,470
여기 이 두번째 층은 어떨까요?

29
00:01:18,360 --> 00:01:32,189
그러면 z2=W 2 A 1 

30
00:01:24,470 --> 00:01:34,950
더하기 b2 그리고 2번째 층의 activation은 

31
00:01:32,189 --> 00:01:39,180
weight 매트릭스 곱하기 

32
00:01:34,950 --> 00:01:44,270
첫번째 층의 결과값입니다. 이 값이죠 

33
00:01:39,180 --> 00:01:49,579
더하기 2번째 층의 bias 벡터, 그리고 a2는 

34
00:01:44,270 --> 00:01:55,770
z2에 적용된 activation 함수입니다.

35
00:01:49,579 --> 00:01:57,990
2번째 층은 이게 전부입니다. 

36
00:01:55,770 --> 00:02:00,299
이렇게 계속 결과값 층까지 반복해서 진행합니다.

37
00:01:57,990 --> 00:02:06,240
4번째 층까지 말이죠, 

38
00:02:00,299 --> 00:02:09,959
Z4는 그 층의 파라미터 곱하기 

39
00:02:06,240 --> 00:02:11,780
이전 층의 

40
00:02:09,959 --> 00:02:14,569
activation 더하기

41
00:02:11,780 --> 00:02:23,930
벡터 와 bias 벡터

42
00:02:14,569 --> 00:02:26,720
그 다음에는 a4=g의 z4입니다.

43
00:02:23,930 --> 00:02:29,900
이런 과정을 통해 결과값 

44
00:02:26,720 --> 00:02:35,390
ŷ의 예상수치를 구랍니다.

45
00:02:29,900 --> 00:02:38,270
한가지 염두할 것은 여기 x는 a0와 같습니다.

46
00:02:35,390 --> 00:02:41,209
입력 특성 벡터 x는 또한 

47
00:02:38,270 --> 00:02:44,000
0번째의 층의 activation과도 같기 때문입니다.

48
00:02:41,209 --> 00:02:47,000
그러므로 x를 지우고, 없애버리고, 0을 놓겠습니다.

49
00:02:44,000 --> 00:02:48,709
그렇게하면 여기 모든 공식은

50
00:02:47,000 --> 00:02:53,980
똑같이 생긴 공식이 되죠. 

51
00:02:48,709 --> 00:03:02,750
일반적인 규칙은 Zㅣ은 

52
00:02:53,980 --> 00:03:05,750
W ㅣ 곱하기 a 의 l-1 더하기 b 의 ㅣ입니다.

53
00:03:02,750 --> 00:03:10,630
그리고 여기 이 층의 activation은 

54
00:03:05,750 --> 00:03:16,850
z값에 대해 적용된 

55
00:03:10,630 --> 00:03:20,120
activation 함수입니다. 그러면 이것이 

56
00:03:16,850 --> 00:03:23,540
일반적인 전 방향전파 공식이 됩니다.

57
00:03:20,120 --> 00:03:26,299
1개의 트레이닝 샘플에 대해서 공식을 만들어 봤는데요, 

58
00:03:23,540 --> 00:03:29,660
그러면 전체 트레이닝 세트에 대해서 

59
00:03:26,299 --> 00:03:32,720
한번에 벡터화시키는 것을 어떨까요?

60
00:03:29,660 --> 00:03:35,030
공식은 사실 이전과 비슷하게 보이는데요.

61
00:03:32,720 --> 00:03:40,060
첫번째 층에서는, 

62
00:03:35,030 --> 00:03:48,410
대문자 Z1=w 1 곱하기 

63
00:03:40,060 --> 00:03:54,650
대문자 X 더하기 B1 그리고, 

64
00:03:48,410 --> 00:03:57,920
A1은 g의 1 곱하기 z의 1입니다. 기억하실 것은, 

65
00:03:54,650 --> 00:03:59,959
X가 0이라는 것입니다. 이것은 

66
00:03:57,920 --> 00:04:01,850
트레이닝 샘플들이 줄별로 

67
00:03:59,959 --> 00:04:05,450
쌓아진 것인데요, 이 부분은 빼서, 

68
00:04:01,850 --> 00:04:08,269
X를 지울 수 있습니다. 그래서 값을 0으로 

69
00:04:05,450 --> 00:04:08,720
두면 되구요, 다음 층은 

70
00:04:08,269 --> 00:04:16,720
비슷하게 보일텐데요, 

71
00:04:08,720 --> 00:04:21,980
z2=W 2 A 1 더하기 B 2, 그리고 a 2는

72
00:04:16,720 --> 00:04:24,530
g의 z2입니다.

73
00:04:21,980 --> 00:04:28,370
여기 이런 z와 a와 같은 벡터를 갖고

74
00:04:24,530 --> 00:04:29,810
이런 벡터들의 값을 쌓아올 리는 것입니다.

75
00:04:28,370 --> 00:04:34,310
첫번째 트레이닝 샘플은 z 벡터이고요, 

76
00:04:29,810 --> 00:04:37,310
이것은 2번째 트레이닝에 대한 z 벡터이고요, 

77
00:04:34,310 --> 00:04:39,830
m번째 트레이닝 샘플까지 반복합니다.

78
00:04:37,310 --> 00:04:43,700
그리하여 세로줄에 쌓아 올립니다.

79
00:04:39,830 --> 00:04:47,390
대문자 Z로 표현하는데요, 

80
00:04:43,700 --> 00:04:50,000
대문자 A도, 

81
00:04:47,390 --> 00:04:52,040
대문자 X도 마찬가지로 

82
00:04:50,000 --> 00:04:53,720
이것은 세로 벡터를 왼쪽에서 오른쪽으로 

83
00:04:52,040 --> 00:04:59,450
쌓는 것입니다. 이런 절차의 마지막에는 

84
00:04:53,720 --> 00:05:03,200
ŷ값이 나오는데요 이 값이 g(z4) 이고

85
00:04:59,450 --> 00:05:04,670
이 값은 a 4와 같습니다.

86
00:05:03,200 --> 00:05:08,000
이 예측수치는 모든 트레이닝 샘플들을

87
00:05:04,670 --> 00:05:09,980
가로로 쌓아올린 것입니다.

88
00:05:08,000 --> 00:05:12,590
요약하자면, 여기 위에 변경해서

89
00:05:09,980 --> 00:05:17,720
적을텐데요, 다른 표기 방식을 통해

90
00:05:12,590 --> 00:05:19,820
여기 소문자 z와 a를 각각 

91
00:05:17,720 --> 00:05:22,070
상응하는 대문자 값으로 변경해주겠습니다. 

92
00:05:19,820 --> 00:05:23,810
이것은 이미 대문자 Z로 보이네요, 

93
00:05:22,070 --> 00:05:25,790
이렇게하면 전 방향전파의 

94
00:05:23,810 --> 00:05:29,060
벡터화된 버전이 나오는 것입니다.

95
00:05:25,790 --> 00:05:32,990
한번에 전체 트레이닝 세트에대해 진행하는
경우 말이죠. 여기서 a0는 

96
00:05:29,060 --> 00:05:35,240
x입니다. 여기서 보는 

97
00:05:32,990 --> 00:05:37,670
벡터화의 도입을 보면, 여기에서 

98
00:05:35,240 --> 00:05:40,370
for lop이 있을 것처럼 보여집니다.

99
00:05:37,670 --> 00:05:44,360
for l=1에서 4까지이죠. 

100
00:05:40,370 --> 00:05:47,000
for l 은 1에서 

101
00:05:44,360 --> 00:05:48,950
대문자 L까지 입니다. 그리고 

102
00:05:47,000 --> 00:05:51,860
첫번째 층에 대한 activation을 계산하고, 

103
00:05:48,950 --> 00:05:54,370
두번째 층, 그리고 세번째 층, 4번째 층 이런식으로

104
00:05:51,860 --> 00:05:56,660
말이죠. 이와 같이 for loop 있는 것처럼 보입니다.

105
00:05:54,370 --> 00:05:58,550
그리고 저는 네트워크를 도입하는 경우, 

106
00:05:56,660 --> 00:06:00,770
explicit for loop를 제거하고 

107
00:05:58,550 --> 00:06:03,290
싶다는 것을 아는데요, 하지만

108
00:06:00,770 --> 00:06:05,060
여기서는 아마 explicit 

109
00:06:03,290 --> 00:06:06,590
for loop 없이 도입할 수 있는 방법이 

110
00:06:05,060 --> 00:06:09,080
없는 것 같습니다. 그러므로 전 방향전파을

111
00:06:06,590 --> 00:06:10,700
도입할 때는 for loop이 있는

112
00:06:09,080 --> 00:06:12,740
것이 괜찮습니다. 첫번째 층에 대한 

113
00:06:10,700 --> 00:06:15,050
activation을 계산해주고, 그 다음 2번째, 

114
00:06:12,740 --> 00:06:17,210
3번째 층, 4번째 층, 이렇게 말이죠, 

115
00:06:15,050 --> 00:06:19,970
아무도 모르고, 제 생각에는 

116
00:06:17,210 --> 00:06:23,060
for loop없이 하는 방법은 없는 것 같습니다.

117
00:06:19,970 --> 00:06:24,620
여기 1에서 대문자 L까지 즉, 1에서 

118
00:06:23,060 --> 00:06:27,830
네트워크에 있는 모든 층까지 말이죠. 

119
00:06:24,620 --> 00:06:30,980
그렇기 때문에 이런 경우에는, 

120
00:06:27,830 --> 00:06:32,690
explcit form을 가져도 괜찮습니다.

121
00:06:30,980 --> 00:06:35,300
심층 네트워크에 대한 표기부분과

122
00:06:32,690 --> 00:06:37,760
이런 네트워크에서 전 방향전파을 하는 방법에 대한

123
00:06:35,300 --> 00:06:39,680
강의는 여기까지입니다.

124
00:06:37,760 --> 00:06:41,900
여기서 본 내용들이 익숙해 보인다면, 

125
00:06:39,680 --> 00:06:44,000
그 이유는 저희가 공부한 내용이

126
00:06:41,900 --> 00:06:45,830
1개의 숨겨진 레리어로 되어있는 

127
00:06:44,000 --> 00:06:47,750
신경망에서 본 것과 거의 유사하기

128
00:06:45,830 --> 00:06:50,750
방식이기 때문인데요, 

129
00:06:47,750 --> 00:06:53,420
이전에서 본 내용에서 단순히 추가적으로 반복하는 것입니다.

130
00:06:50,750 --> 00:06:55,420
저희가 신경망을 도입하는 

131
00:06:53,420 --> 00:06:57,860
경우에는, 버그가 없는 

132
00:06:55,420 --> 00:06:59,450
도입을 추진하기 위해서는 

133
00:06:57,860 --> 00:07:01,580
더욱 더 시스템적으로 생각하고 조심이

134
00:06:59,450 --> 00:07:03,500
신중하게 그 시점에 작업하고 

135
00:07:01,580 --> 00:07:05,300
있는 매트릭스의 다이멘션에 대해 생각하는 것입니다.

136
00:07:03,500 --> 00:07:07,280
제가 직접 개인적으로 코딩을 할 때는 

137
00:07:05,300 --> 00:07:08,960
종이를 꺼내서 

138
00:07:07,280 --> 00:07:11,480
현재 작업하고 있는 것의 매트릭스

139
00:07:08,960 --> 00:07:13,940
다이멘션에 대해 신중하게 생각합니다.

140
00:07:11,480 --> 00:07:16,570
이런 것을 어떻게 하는지 다음 비디오를 통해

141
00:07:13,940 --> 00:07:16,570
알아보겠습니다.