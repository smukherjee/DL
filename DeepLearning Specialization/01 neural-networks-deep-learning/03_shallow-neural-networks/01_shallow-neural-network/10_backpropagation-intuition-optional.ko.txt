지난 강의에서 이전 비디오에서는 후 방향전파에 대한 공식들을 봤는데요, 이번 비디오에서는, 산출 그래프를 이용해서 어떻게 그 공식이 만들어졌는지 직관적인 부분을 다뤄보겠습니다. 이 비디오는 선택적인 비디오입니다. 그렇기 때문에 여러분이 보셔도 안보셔도 됩니다. 이 비디오를 시청하지 않으셔도 작업은 하실 수 있습니다. 로지스틱 회귀 분석을 하는 경우, 기억하시겠지만, 여기 forward pass를 통해 z의 값을 구했습니다. 그리고 A를 구하고나거, loss를 구했습니다. 그리고 derivative를 계산하기 위해서 이렇게 backward pass를 이용해서 처음에 먼저 da를 계산했는데요, 그 다음에 dz를 계산하고, 다음으로 dw 와 db를 계산했습니다. 그러면 loss의 정의는 L(a, y)였습니다. 이 값은 -y log a - (1 - y) 곱하기 log (1-a) 입니다. 그러므로 여러분이 미적분에 익숙하면, 여기 a에 대해 derivative 값을 구하고, 그러면 da를 위한 공식을 줄 것입니다. 그러면 da는 이것가 동일한 값이구요, 미적분을 통해서 이 부분은, - y 나누기 a 더하기 (1-y) 나누기 (1-a)입니다. 이것의 derivative를 구함으로써 이 값을 구합니다. 그리고 이제 dz를 구하기 위해 거꾸로 거슬러 올라가면, 이전에 구한 것은 dz=a-y라는 것입니다. 그 이유는 이전에 설명했었죠. 미적분에서의 chain rule로 인해, dz= da 곱하기 g 프라임 z가 됩니다. 여기서 g(z)는 z의 시그모이드가 되죠. 이것은 여기 로지스틱 회귀분석에서의 결과값에 대한 activation 함수입니다. 맞죠? 그러므로 여기서 기억할 것은 이것이 아직 로지스틱 회귀분석이라는 것이고, x1, x2, x3의 값과 한개의 시그모이드 유닛이 여기 A 값, ŷ을 줍니다. 여기서는 activation 함수가 시그머이드 함수였는데요, 따로 또 말씀드리자면, 미적분학의 chain rule이 익숙한 분들을 위해서인데요, 그 이유는, A가 z의 시그모이드 함수와 동일하고 그리고 z에 대해서 L의 partial 값은 a에 대한 partial L 값 곱하기 da.dz입니다. 이것은 z의 시그모이드 이것은 d/dz 곱하기 g(z)입니다. 이 값은 g 프라임 z죠. 그렇기 때문에, 여기 dz 라고 표현된 코드는 여기 da 곱하기 g 프라미 z와 동일한 것입니다. 그러므로 이것은 이값이 됩니다. 마지막에 파생된 공식은 여러분이 미적분학에 익숙한 경우에만 이해를 하실 수 있을텐데요, 그렇지 않은 경우라면 걱정하지 마십시요, 여기에 대한 직관적인 부분은 나중에 필요할 때마다 설명 드리겠습니다. 이제 여기 로지스틱 회귀을 위해서 DZ 계산했는데요, 이제 DW를 계산하겠습니다. 이 값은 dz 곱하기 x 이고 db는 그냥 dz입니다. 한개의 트레이닝 예시의 경우 말이죠. 이것이 로지스틱 회귄분석에 관한 내용이였는데요, 이제는 후 방향전파으로 계산을 할때, 신경망을 위해서 말이죠, 이런 계산을 진행할텐데요, 이 절차를 두번 진행할 것입니다. 왜냐면 이제는 x가 결과값 unit으로 가는 것이 아니라 x가 hidden layer로 가고 난 다음, 결과값 unit으로 가기 때문입니다. 그러므로 여기 이런 산출과정과 같이 1단계로 되는 것이 아니라, 여기와 같이 2개의 층으로 된 신경망은 이렇게 2개의 단계가 있을 것입니다. 여기 2개의 층으로 된 신경망 같은 경우, 입력 층이 있고, 숨겨진 레이어와 다음 결과 층 이렇게 있습니다. 그러면 살출 과정을 기억해보겠습니다. 처음에는 여기 이 공식을 이용해서 z1을 구합니다. 그 다음에 a1을 계산하고 다음에 z2를 계산합니다. 알아두실 것은 z2가 파라미터 w2 와 b2에 따라 달라진다는 것입니다. 그리고 z2를 바탕으로, a2를 계산하고, 마지막으로 이 값이 loss의 값을 줍니다. 후 방향전파이 하는 것은 DA2 와 DZ2를 계산하기 위해 뒤로 가는 것입니다. 그 다음에 DW2와 DP2를 계산하러 뒤로 갑니다. 그 다음에 DA1을 계산합니다. 그 다음 DZ! 등등 이렇게 말이죠. 그리고 저희는 입력값 x에 대한 derivative값을 구할 필요가 없습니다. 지도학습에서 입력값 X는 고정값입니다. 그렇기 때문에 저희는 이 값을 최적화하려고 하는 것이 아닙니다. 
그러므로 derivative는 구하지 않습니다. 적어도 지도학습 분야에서 x에 대해서 말이죠. 그리고 저는 DA2를 구하는 것을 넘어갈 것인데요. 원하시면, 여러분이 DA2를 구하셔서 이 값으로 DZ2를 구할 수 있습니다. 하지만 실제로는 이 2가지의 단계를 한가지 단계로 압축시켜 DZ2=A2-Y 가 되게 할 수 있습니다. 그리고 또한, dw2 와 db2를 여기다가 적겠습니다. 그리고 여기는 DW2=DZ2 곱하기 A1 transpose 인데요, 그리고 db2는 dz2입니다. 이 과정은 로지스틱 회귀분석에서 저희가 적용한 DW=DZ 곱하기 X와 비슷한데요, 대신에 여기서는 A1이 X의 역할을 하고, 여기 추가로 transpose가 있는데요, 그 이유는 대문자 W 매트릭스와 개인별 파라미터 W의 관계 때문에 그렇습니다. 여기 transpose가 있죠, 맞죠? 그 이유는 w 는 row vector 와 같은데요, 로지스틱 회귀분석에서 1개의 결과값이 
있는 경우에 말이죠. dw2는 그렇습니다. 반면에 w는 세로 column vector입니다. 그렇기 때문에 추가적으로 a1 이 있는 것입니다. 여기서 X는 로지스틱 회귀분석에서는 그렇지 않았죠. 이것이 후 방향전파의 반은 완성입니다. 그리고 여러분이 원하면 DA1도 계산할 수 있습니다. 하지만 실제로는 da1의 계산과 dz1의 계산은 한단계로 압축되어 진행됩니다. 그러므로 여러분이 주로 도입하는 것은 dz1=w2 transpose 곱하기 dz2 그리고 g1 프라임 z1이라는 element wise product를 곱합니다. 그리고 다이멘션을 체크하기 위해서 말이죠 만약 이렇게 생긴 신경망이 있다고 하면, 여기 Ŷ를 결과값으로 갖겠죠 만약 N0, NX=N0 입력 특성이 있으면 그리고 1개의 숨겨진 유닛이 있고 n2가 있으면, n2 같은 경우 저희의 경우, 1개의 결과값 유닛인데요, 그러면 w2 매트릭스는 (n2, n1) 차원입니다. 그러면 z2 와 dz2는 (n2, 1) 입니다. 이것은 (1, 1)이 될 것입니다. 이진분류법에서 말이죠. 그러면 z1, dz1 같은 경우에도 역시, (n1, 1)이겠죠, 그렇기 때문에 어떤 foo 변수와 어떤 D foo 이더라고 같은 다이멘션을 가질 것입니다. 그렇기 때문에 W와 DW는 항상 똑같은 다이멘션을 갖게되며, 비슷하게 B와 DB에서도, 그리고 Z와 DZ에서도 동일하게 적용됩니다. 여기서 모든 다이멘션이 매칭되도록, DZ1은 W2 transpose 곱하기 DZ2 가 있습니다. 그리고 이것은 element wise product 곱하기 g 프라임 z1인데요, 위의 다이멘션과 매칭하기 위해 이것은 (n1, 1)이 될 것이고, w2 transpose입니다. 이것을 transpose하는 것인데요, 그러므로 이 값은 (n1, n2) 이고, dz2는 (n2, 1) 이고, 이것은 z1과 같은 다이멘션입니다. 그러므로 이 값도 (n1, 1)인데요. 여기는 element wise product입니다. 이제 다이멘션이 말이 되죠. 맞죠? (N1, 1) 벡터는 (N1, N2) 차원의 매트릭스를 (N2, N1) 차원의 매트릭스를 곱하면서 얻을 수 있게 됩니다. 그 이유는 이 2개의 매트릭스를 곱하여 (N1, 1) 매트릭스를 주기 때문입니다. 그러면 이것은 (n1, 1) 차원 2개의 element wise product의 값입니다. 그러면 다이멘션이 매칭이 되겠죠. 후 방향전파를 도입할 때의 팁은, 여러분의 매트릭스 다이멘션이 매칭이 되게 하면, 여기 다양한 매트릭스 w1, w1, z1, z2 a1, a2 등등 이 무엇이지 생각하고, 여기 다인멘션들이 매칭될 수 있도록 확실히 하면, 가끔씩 후 방향전파에서는 이런 절차가 이미 상당 수의 버그를 제거할 것입니다. 그러면 이것은 DZ1을 주고요, 마지막으로, DW1과 DB1, 은 여기 적어야 하는데 공간이 부족하므로 여기 오른쪽에 적겠습니다. DW1 과 DB1은 여기 공식처럼 주어집니다. 이것은 dz1 곱하기 x transpose일 것이구요, 그리고 이것은 dz와 동일할 것입니다. 그러면 여러분은 비슷한 점을 발견하실텐데요, 여기 이 공식과 이 공식들에서 말이죠. 이것이 우연의 일치는 아닙니다. 왜냐면 X가 a0의 역할을 하기 때문입니다. 그러므로 X transpose 는 a0 transpose입니다. 여기 이 공식들은 사실 굉장히 유사한데요, 이렇게 해서 어떻게 후 방향전파 공식이 만들어지는지 봤습니다. 여기 6개의 핵심 공식이 있는데요, dz2, dw2 db2, dz1, dw1 그리고 d1에 대한 공식입니다. 여기 6개의 식을 가지고 복사해서 다음 슬라이드로 넘어가겠습니다. 현재까지 후 방향전파의 derive했는데요, 1개의 트레이닝 예시의 경우에 말이죠, 하지만 놀랍지 않게도, 한개의 예시로 작업하는 것보다 다양한 트레이닝 예시에 대해서 vectorize하고 싶을 것입니다. 기억하시겠지만, 전 방향전파에서는 1개의 예시씩 운영했는데요, 이러한 공식이 있었습니다. 그리고 a1=g1 (z1) 이런 공식도 있었죠. vectorize를 위해서, z 값을 쌓아 올렸습니다. 이렇게 줄의 형식으로 z1m까지 말이죠, 그리고 이것을
대문자 Z로 불렀습니다. 그리고 저희는 이렇게 줄에다가 쌓는 방식으로 대문자 버전을 정의했는데요, 그렇게 하여. Z1=W1 X + B가 생겼습니다. 그리고 A1은 z1의 g1이죠 맞죠? 이 코스 과정에서는 표기법을 조심히 정의합니다. 왜냐면 다른 줄로 쌓아 올리게 되면, 어느 방법으로도 풀리게 되기 때문입니다. 수학적인 부분을 조심히 보면, 똑같은 비법이 후 방향전파에서도 쓰일 수 있는 것 알게됩니다. 
vectorize공식은 이렇게 되는데요, 첫번째로, 여기 다른 트레이닝 예시에 대한 DZ값들을 쌓아올리면, 매트릭스에서 다른줄로 말이죠, 여기도 동일하게 합니다. 그러면 여기가 vectorized 도입 버전입니다. 그리고 여기가 정의인데여, 또는 dw2를 계싼하는 방법입니다. 여기 추가적인 1/m 부분이 있습니다. 왜냐면 j비용함수가 1 나누기 m 의 loss합 공식이기 때문이죠 그러므로 derivative를 계산하는 경우에, 여기 1/m 추가 항이 있는 것입니다. 저희가 로지스틱 회귀분석으로 weight 업데이트를 계산할때와 같이 말이죠. 이것이 db2의 업데이트입니다. 마찬가지로 dz의 합이고, 1/m이 있습니다. 그리고 dz1은 이렇게 산출됩니다. 다시 한번, 이것은 element wise product입니다. 이전 슬라이드에서는 (n1, 1) 차원 벡터였었는데요, 이제는 (n1, M) 차원이 매트릭스입니다. 여기도 마찬가지로 (n1, m) 차원입니다. 그렇기 때문에 여기 asterisk 별표는 element wise product인 것입니다. 나머지 2개의 업데이트인데요, 놀랍지 않을텐데요, 후 방향전파 이 어떻게 파생되었는지 직관적으로 알 수 있는 계기가 되었길 바랍니다. 모든 머신 러닝 분야에서 말이죠. 후 방향전파 알고리즘의 파생은 수학적으로 가장 복잡한 부분이기도 한데요, 선형대수도 알아야하고, 매트릭스의 derivative도 안 상태에서 아무것도 없는 첫번째 상황에서 re-derive 해야합니다. 여러분이 미적분학에 능숙하면, 이런 절차를 이용해서 derivative알고리즘을 직접 증명할 수 있겠죠. 하지만 제가 생각하기에 상당부분의 딥러닝 종사자들은 지금 여러분이 보신 정도의 레벨까지 다뤘을 것입니다. 그러면서도 직관적인 부분을 이용해서 알고리즘을 매우 효율적으로 도입할 수 있는 것이죠. 여러분이 만약 미적분 고수이면, 아무것도 없는 상황에서 이런 공식을 나오게 할 수 있는지 한번 해보세요. 수학적으로 거의 가장 어려운 것 같습니다. 머신 러닝에서 본 가장 어려운 식의 파생 과정입니다. 어떤 방식이던, 이것을 도입하면, 잘 작동할 것이고, 지금 상태에서 여러분은 직관적으로 충분히 튜닝하고
작동하게끔 만들 수 있으실 것입니다. 마지막 부분이 있는데요, 여러분이 신경망을 도입하기 전에, 신경망에서 weight 비중을 초기화하는 방법입니다. 여러분의 파라미터를 초기화하는 것이 0으로 하는 것이 아니라 임의로 하는 것입니다. 이 신경망을 트레이닝 시키는데 있어 굉장히 중요합니다. 왜 그런지 그 이유는 다음 비디오에서 보실 것입니다.