1
00:00:00,000 --> 00:00:03,810
Dans une vidéo précédente, vous avez vu 
comment calculer des dérivées et implémenter

2
00:00:03,810 --> 00:00:08,325
la descente de gradient par rapport à un seul 
exemple pour la régression logistique.

3
00:00:08,325 --> 00:00:11,370
Maintenant, nous voulons le faire 
pour m exemples d'apprentissage.

4
00:00:11,370 --> 00:00:15,420
Pour commencer, souvenez vous de
 la définition de la fonction de coût J.

5
00:00:15,420 --> 00:00:19,815
La fonction de coût en W, b est la moyenne,

6
00:00:19,815 --> 00:00:23,505
1 sur m fois la somme de 1 à m de

7
00:00:23,505 --> 00:00:28,545
la perte lorsque votre algorithme
 retourne a(i) sur l’exemple y,

8
00:00:28,545 --> 00:00:36,450
où a(i) est la prédiction pour le i-ième 
exemple d'apprentissage, c'est à dire

9
00:00:36,450 --> 00:00:45,270
sigma(z(i)) qui est égal à 
sigma de W transpose fois x(i) + b

10
00:00:45,270 --> 00:00:49,835
Nous avons montré sur la diapositive précédente, 
 pour chaque exemple d'apprentissage,

11
00:00:49,835 --> 00:00:57,005
comment calculer la dérivée quand 
on n'a qu'un seul exemple d'apprentissage,

12
00:00:57,005 --> 00:01:02,460
c'est à dire dw1, dw2 et db,

13
00:01:02,460 --> 00:01:04,670
avec maintenant l’exposant i pour désigner

14
00:01:04,670 --> 00:01:09,140
ce qu'on obtient avec les calculs
 de la diapositive précédente,

15
00:01:09,140 --> 00:01:12,665
mais en utilisant 
un seul exemple d'apprentissage

16
00:01:12,665 --> 00:01:15,065
x(i), y(i), ah pardon

17
00:01:15,065 --> 00:01:16,840
il manque un (i) ici aussi.

18
00:01:16,840 --> 00:01:22,110
Vous remarquez que maintenant, la fonction
 de coût générale est une somme, en fait

19
00:01:22,110 --> 00:01:25,805
une moyenne, à cause du terme 1/m, 
des pertes individuelles.

20
00:01:25,805 --> 00:01:28,865
Il s'avère que la dérivée par rapport à,

21
00:01:28,865 --> 00:01:35,435
disons w1, de la fonction de coût 
générale, va aussi être

22
00:01:35,435 --> 00:01:45,100
la moyenne des dérivées par rapport
 à w1 des pertes individuelles.

23
00:01:45,100 --> 00:01:51,420
Nous avons déjà montré 
comment calculer ce terme dw1(i),

24
00:01:52,100 --> 00:01:55,530
dans la diapositive précédente,

25
00:01:55,530 --> 00:01:58,275
nous avons montré comment calculer ça 
sur un seul exemple d'apprentissage.

26
00:01:58,275 --> 00:02:00,635
Donc ce que vous avez à faire, c'est calculer

27
00:02:00,635 --> 00:02:06,020
ces dérivées comme nous l'avons montré, 
sur chaque exemple d'apprentissage,

28
00:02:06,020 --> 00:02:07,280
et de les moyenner, ce qui vous donne

29
00:02:07,280 --> 00:02:12,005
le gradient général que vous pouvez 
utiliser pour la descente de gradient.

30
00:02:12,005 --> 00:02:14,330
Je sais qu'il y a beaucoup de détails, mais

31
00:02:14,330 --> 00:02:17,180
prenons tout ça et récapitulons-le en

32
00:02:17,180 --> 00:02:19,760
un algorithme concret, celui que
 vous devez implémenter

33
00:02:19,760 --> 00:02:23,480
pour que votre régression logistique 
avec descente de gradient fonctionne.

34
00:02:23,480 --> 00:02:29,105
Alors, voici ce que vous pouvez faire.
 Initialisons J = 0

35
00:02:29,105 --> 00:02:38,650
dw1 = 0, dw2 = 0, db = 0.

36
00:02:38,650 --> 00:02:43,580
Et nous allons utiliser une boucle for
 sur l’ensemble d'apprentissage,

37
00:02:43,580 --> 00:02:47,995
calculer les dérivées pour chaque exemple 
d'apprentissage et les additionner.

38
00:02:47,995 --> 00:02:50,340
Voici ce que nous faisons : pour i de 1 à m,

39
00:02:50,340 --> 00:02:52,320
où m correspond au nombre<br />d'exemples d'apprentissage,

40
00:02:52,320 --> 00:02:56,705
nous calculons z(i) égal à 
 W transpose fois x(i) + b

41
00:02:56,705 --> 00:03:00,650
et la prédiction a(i) = sigma(z(i))

42
00:03:00,650 --> 00:03:03,590
et ensuite, ajoutons ça à J.

43
00:03:03,590 --> 00:03:11,580
J += y(i) log( a(i) ) + (1- y(i)) log( 1- a(i)),

44
00:03:11,580 --> 00:03:14,405
et mettons un signe moins devant tout ça,

45
00:03:14,405 --> 00:03:15,710
et puis, comme nous 
l’avons vu précédemment,

46
00:03:15,710 --> 00:03:20,615
Nous avons dz(i), 
qui est égal à a(i) moins y(i),

47
00:03:20,615 --> 00:03:25,910
et à dw1, on ajoute x1(i)*dz(i)

48
00:03:25,910 --> 00:03:32,065
dw2 += x2(i)*dz(i)

49
00:03:32,065 --> 00:03:36,640
Je fais ce calcul en supposant qu'on 
n'a que deux caractéristiques,

50
00:03:36,640 --> 00:03:38,530
donc n = 2, sinon

51
00:03:38,530 --> 00:03:39,849
vous faites cela pour dw1,

52
00:03:39,849 --> 00:03:41,755
dw2, dw3 et ainsi de suite,

53
00:03:41,755 --> 00:03:44,750
et ensuite db += dz(i)

54
00:03:44,750 --> 00:03:47,445
et c'est la fin de la boucle for.

55
00:03:47,445 --> 00:03:50,815
Et après avoir fait ça pour
 les m exemples d'apprentissage,

56
00:03:50,815 --> 00:03:55,720
il faut encore diviser par m, puisque 
nous calculons des moyennes.

57
00:03:55,720 --> 00:03:58,870
Donc dw1 est divisé par m,

58
00:03:58,870 --> 00:04:01,465
dw2 /= m

59
00:04:01,465 --> 00:04:03,520
db /= m

60
00:04:03,520 --> 00:04:05,200
afin de calculer les moyennes.

61
00:04:05,200 --> 00:04:08,080
Avec tous ces calculs,

62
00:04:08,080 --> 00:04:11,710
vous venez de calculer les dérivées de
 la fonction de coût J par rapport à

63
00:04:11,710 --> 00:04:15,595
chacun des paramètres w1, w2 et b.

64
00:04:15,595 --> 00:04:17,685
Quelques détails sur ce que nous faisons,

65
00:04:17,685 --> 00:04:24,250
Nous utilisons dw1, dw2 et db
 en tant qu'accumulateurs,

66
00:04:24,250 --> 00:04:26,450
de façon à ce qu’après ce calcul,

67
00:04:26,450 --> 00:04:30,700
dw1 soit égal à la dérivée de votre fonction

68
00:04:30,700 --> 00:04:36,160
de coût générale par rapport à w1, 
et de la même manière pour dw2 et db.

69
00:04:36,160 --> 00:04:39,880
Remarquez que dw1 et dw2
 n'ont pas d'exposant i

70
00:04:39,880 --> 00:04:41,620
parce que nous les utilisons dans ce code

71
00:04:41,620 --> 00:04:44,300
comme des accumulateurs pour faire la somme 
sur l'ensemble d'apprentissage entier.

72
00:04:44,300 --> 00:04:46,595
En revanche, dz(i) ici,

73
00:04:46,595 --> 00:04:51,190
c'est dz par rapport à 
un seul exemple d'apprentissage,

74
00:04:51,190 --> 00:04:55,030
c'est pourquoi il a un exposant i 
pour se référer au i-ième exemple

75
00:04:55,030 --> 00:04:56,710
d'apprentissage à partir duquel il est calculé.

76
00:04:56,710 --> 00:04:59,745
Avec tous ces calculs,

77
00:04:59,745 --> 00:05:02,080
pour implémenter une étape
 de descente de gradient,

78
00:05:02,080 --> 00:05:03,730
vous implémentez w1,

79
00:05:03,730 --> 00:05:08,300
qui est mis à jour en w1 moins
 le taux d'apprentissage fois dw1,

80
00:05:08,300 --> 00:05:12,515
w2 est mis à jour en w2 moins
 le taux d'apprentissage fois dw2,

81
00:05:12,515 --> 00:05:17,390
et b est mis à jour en b moins
 le taux d'apprentissage fois db,

82
00:05:17,390 --> 00:05:22,250
où dw_1, dw_2 et db ont été calculés là.

83
00:05:22,250 --> 00:05:27,530
Et finalement, J ici sera la valeur 
correcte de votre fonction de coût.

84
00:05:27,530 --> 00:05:32,150
Ce qu'il y a sur ce slide implémente 
une seule étape de descente de gradient,

85
00:05:32,150 --> 00:05:35,270
et vous devez donc répéter tout 
ce qu'il y a sur cette diapositive

86
00:05:35,270 --> 00:05:38,815
plusieurs fois pour faire plusieurs étapes 
de descente de gradient.

87
00:05:38,815 --> 00:05:42,700
Encore une fois, si ces détails 
vous semblent trop compliqués,

88
00:05:42,700 --> 00:05:44,485
ne vous inquiétez pas,

89
00:05:44,485 --> 00:05:47,215
tout devrait devenir plus clair

90
00:05:47,215 --> 00:05:49,850
quand vous implémenterez ça 
dans l'exercice de programmation.

91
00:05:49,850 --> 00:05:53,425
Mais il s’avère qu'il y a deux faiblesses

92
00:05:53,425 --> 00:05:57,975
avec la manière dont nous 
avons implémenté ces calculs.

93
00:05:57,975 --> 00:06:01,180
Lorsque vous implémentez
 la régression logistique de cette manière,

94
00:06:01,180 --> 00:06:03,250
vous devez écrire deux boucles for.

95
00:06:03,250 --> 00:06:06,340
La première est cette boucle for sur
 les m exemples d'apprentissage,

96
00:06:06,340 --> 00:06:11,360
et la seconde est cette boucle 
sur toutes les caractéristiques ici.

97
00:06:11,360 --> 00:06:12,600
Ainsi, dans cet exemple,

98
00:06:12,600 --> 00:06:14,040
nous n'avions que deux caractéristiques,

99
00:06:14,040 --> 00:06:16,695
n, ou nx, est égal à deux,

100
00:06:16,695 --> 00:06:18,240
mais si vous avez plus de caractéristiques

101
00:06:18,240 --> 00:06:20,940
vous vous retrouvez à écrire ici dw1, dw2,

102
00:06:20,940 --> 00:06:23,295
et vous avez des calculs
, similaires pour dw3

103
00:06:23,295 --> 00:06:25,275
et ainsi de suite jusqu'à dwn.

104
00:06:25,275 --> 00:06:31,310
Donc on dirait qu'il faut une 
boucle for sur les n caractéristiques.

105
00:06:31,310 --> 00:06:34,415
Lorsque vous implémentez 
des algorithmes d’apprentissage profond,

106
00:06:34,415 --> 00:06:37,070
l'utilisation de boucles for explicites

107
00:06:37,070 --> 00:06:41,255
dans le code rend votre algorithme
 moins efficace.

108
00:06:41,255 --> 00:06:43,130
Et donc à l'ère du deep learning,

109
00:06:43,130 --> 00:06:46,130
nous brassons des ensembles de 
données de plus en plus gros,

110
00:06:46,130 --> 00:06:50,180
et pouvoir implémenter un algorithme 
sans utiliser de boucle for explicite est

111
00:06:50,180 --> 00:06:54,800
vraiment important pour pouvoir l'utiliser
 sur des gros ensembles de données.

112
00:06:54,800 --> 00:06:58,550
Et il existe une série de techniques
 appelées la vectorisation

113
00:06:58,550 --> 00:07:03,610
qui vous permettent de vous débarrasser 
des boucles for explicites dans votre code.

114
00:07:03,610 --> 00:07:06,570
Je pense qu'avant l'ère 
de l'apprentissage profond,

115
00:07:06,570 --> 00:07:08,595
c'est à dire avant l'avènement
 de l'apprentissage profond,

116
00:07:08,595 --> 00:07:10,810
la vectorisation était plutôt optionnelle

117
00:07:10,810 --> 00:07:14,780
ça pouvait parfois accélérer
 votre code, mais parfois pas.

118
00:07:14,780 --> 00:07:17,450
Mais à l'ère du deep learning, 
la vectorisation,

119
00:07:17,450 --> 00:07:19,250
c'est à dire se débarrasser des boucles for,

120
00:07:19,250 --> 00:07:20,920
comme celle ci et celle ci,

121
00:07:20,920 --> 00:07:22,930
est devenu très important,

122
00:07:22,930 --> 00:07:26,350
parce que, de plus en plus, on fait de 
l'apprentissage sur de très gros

123
00:07:26,350 --> 00:07:28,975
ensembles de données et on a
 donc besoin d'un code très efficace.

124
00:07:28,975 --> 00:07:30,680
Dans les prochaines vidéos, 

125
00:07:30,680 --> 00:07:33,590
nous parlerons donc de la vectorisation et

126
00:07:33,590 --> 00:07:38,485
de la manière d'implémenter 
tout ça sans aucune boucle for.

127
00:07:38,485 --> 00:07:41,660
J'espère que vous avez 
maintenant une bonne idée de la façon

128
00:07:41,660 --> 00:07:45,305
d'implémenter la descente de gradient 
pour la régression logistique.

129
00:07:45,305 --> 00:07:48,515
Les choses seront plus claires lorsque vous 
implémenterez l’exercice de programmation.

130
00:07:48,515 --> 00:07:51,170
Mais avant de faire
 l'exercice de programmation,

131
00:07:51,170 --> 00:07:55,580
nous allons d’abord parler de vectorisation, 
afin que vous puissiez implémenter tout cela,

132
00:07:55,580 --> 00:08:00,210
une itération de descente de gradient 
sans avoir besoin d’utiliser de boucle for.