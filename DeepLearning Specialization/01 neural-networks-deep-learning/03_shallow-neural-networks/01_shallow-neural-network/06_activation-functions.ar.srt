1
00:00:00,870 --> 00:00:04,550
عندما تقوم ببناء الشبكات العصبية الخاصة بك، من بين
الخيارات التي يجب أن تقوم باختيارها

2
00:00:04,550 --> 00:00:07,570
دالة التنشيط التي ترغب في استخدامها
في الطبقات المخفية

3
00:00:07,570 --> 00:00:10,950
بالإضافة إلى ماهية وحدات المخرجات
للشبكة العصبية الخاصة بك.

4
00:00:10,950 --> 00:00:14,360
حتى الآن، كنا نستخدم
دالة تنشيط sigmoid.

5
00:00:14,360 --> 00:00:17,980
إلا أنه في بعض الأحيان، هناك خيارات
قد تعمل بشكل أفضل بكثير.

6
00:00:17,980 --> 00:00:19,970
دعونا نلقي نظرة على بعض الخيارات.

7
00:00:19,970 --> 00:00:23,840
في خطوة الانتشار الرابعة
للشبكة العصبية

8
00:00:23,840 --> 00:00:28,279
لدينا هذه الخطوات الثلاثة حيث نستخدم
دالة sigmoid هنا.

9
00:00:28,279 --> 00:00:32,460
ودالة sigmoid يطلق عليها
دالة تنشيط.

10
00:00:32,460 --> 00:00:35,871
وهذه هي دالة sigmoid الشهيرة

11
00:00:35,871 --> 00:00:39,980
"a" يساوي واحد على واحد زائد
"e" لقيمة "z" السلبية.

12
00:00:39,980 --> 00:00:43,910
وبالتالي، في الحالة الأكثر عمومية
يمكن أن يكون لدينا دالة مختلفة

13
00:00:45,050 --> 00:00:50,100
"g" لـ "z"
والتي سوف أكتبها هنا

14
00:00:51,300 --> 00:00:57,300
حيث يمكن أن تكون "g" دالة غير خطية
لا تكون دالة sigmoid.

15
00:00:57,300 --> 00:01:02,530
وبالتالي، على سبيل المثال، تكون دالة
sigmoid بين صفر وواحد

16
00:01:02,530 --> 00:01:07,040
وهناك دالة تنشيط دائمًا ما تعمل
بشكل أفضل من دالة sigmoid

17
00:01:07,040 --> 00:01:12,220
وهي دالة ظل الزاوية أو دالة ظل
الزاوية الزائدية.

18
00:01:12,220 --> 00:01:18,909
فيكون هنا "z" وهنا "a"
وهناك "a" تساوي tanh(z)

19
00:01:18,909 --> 00:01:24,390
وهي تكون بين زائد واحد وناقص واحد.

20
00:01:24,390 --> 00:01:29,745
تكون صيغة دالة tanh هي "e" إلى

21
00:01:29,745 --> 00:01:35,920
"z" ناقص "e" لقيمة "z"
السلبية على مجموعها.

22
00:01:35,920 --> 00:01:42,829
ومن الناحية الحسابية في الواقع، هي عبارة عن
إصدار منقول من دالة sigmoid.

23
00:01:42,829 --> 00:01:47,399
تمامًا مثل دالة sigmoid مثل
هذه، ولكنها منقولة

24
00:01:47,399 --> 00:01:54,180
بحيث تعبر الآن نقطة صفر ومقياس "v"، وبالتالي
تكون 15 ناقص 1 وزائد 1.

25
00:01:54,180 --> 00:01:59,406
ويتضح أنه بالنسبة للوحدات المخفية
إذا جعلت الدالة "g"

26
00:01:59,406 --> 00:02:06,775
لـ "z" تساوي tanh(z)

27
00:02:06,775 --> 00:02:11,490
غالبًا ما يعمل ذلك بشكل أفضل
من دالة sigmoid

28
00:02:11,490 --> 00:02:16,540
لأن القيم بين زائد واحد وناقص 1
متوسط عمليات التنشيط

29
00:02:16,540 --> 00:02:20,360
التي تظهر في رأسك، ومتوسطها
أقرب إلى الصفر.

30
00:02:20,360 --> 00:02:24,468
وتمامًا كما تقوم في بعض الأحيان
بتدريب خوارزمية تعليم

31
00:02:24,468 --> 00:02:26,364
يمكن أن تقوم بتوسيط البيانات

32
00:02:26,364 --> 00:02:31,029
وجعل متوسط البيانات 0 باستخدام دالة
tanh بدلاً من دالة sigmoid.

33
00:02:31,029 --> 00:02:34,500
ويكون لذلك تأثير توسيط
للبيانات إلى حد ما

34
00:02:34,500 --> 00:02:39,420
بحيث يكون وسيط البيانات أقرب من الصفر
بدلاً من، ربما، 0.5.

35
00:02:39,420 --> 00:02:43,200
وذلك يجعل التعلم للطبقة التالية
أسهل قليلاً.

36
00:02:43,200 --> 00:02:46,900
وسوف نتحدث أكثر عن هذا الأمر في الدورة
التدريبية الثانية عندما نتحدث عن خوارزميات

37
00:02:46,900 --> 00:02:47,990
التحسين كذلك.

38
00:02:47,990 --> 00:02:50,480
إلا أنه من بين الحقائق الرئيسية أنني

39
00:02:50,480 --> 00:02:54,500
لم أعد أستخدم دالة التنشيط
sigmoid على الإطلاق.

40
00:02:54,500 --> 00:02:58,410
فدالة tanh غالبًا ودائمًا ما تكون
أفضل بشكل قاطع.

41
00:02:58,410 --> 00:03:04,880
والاستثناء الوحيد يتمثل في طبقة المخرجات
لأنه إذا كانت "y" تساوي 0 أو

42
00:03:04,880 --> 00:03:10,070
1، فمن المنطقي أن تكون
قيمة y hat رقمًا

43
00:03:10,070 --> 00:03:14,830
وهو القيمة التي يتم إخراجها بين 0 و1
وليس بين ناقص 1 و1.

44
00:03:14,830 --> 00:03:19,380
إذًا، فإن الاستثناء الوحيد الذي يمكن أن
نستخدم فيه دالة تنشيط sigmoid

45
00:03:19,380 --> 00:03:23,610
هو عندما نستخدم التصنيف الثنائي

46
00:03:23,610 --> 00:03:28,760
وفي تلك الحالة، يمكنك استخدام دالة تنشيط
sigmoid لطبقة المخرجات.

47
00:03:28,760 --> 00:03:34,828
وبالتالي تكون قيمة "g" لـ "z" هنا تساوي سيجما "z" 2.

48
00:03:34,828 --> 00:03:40,236
وبالتالي ما تراه في هذا المثال هو الحالة التي
يمكن أن يكون لديك فيها دالة تنشيط tanh

49
00:03:40,236 --> 00:03:47,102
للطبقة المخفية، ودالة تنشيط sigmoid
لطبقة المخرجات.

50
00:03:47,102 --> 00:03:50,982
وبالتالي، يمكن أن تكون دوال إلغاء التنشيط
مختلفة للطبقات المختلفة.

51
00:03:50,982 --> 00:03:55,112
ويمكن أن نذكر أنه في بعض الأحيان تكون
دوال التنشيط مختلفة

52
00:03:55,112 --> 00:03:59,907
للطبقات المختلفة، ويمكن أن نستخدم الأقواس
المربعة العلوية هذه كذلك

53
00:03:59,907 --> 00:04:05,270
للإشارة إلى أن قيمة "g" في القوس المربع الأول
قد تكون مختلفة عن قيمة "g" للقوس المربع الثاني.

54
00:04:05,270 --> 00:04:09,360
ومرة أخرى، يشير القوس المربع العلوي
الأول إلى هذه الطبقة

55
00:04:09,360 --> 00:04:12,280
فيما يشير القوس المربع العلوي الثاني
إلى طبقة المخرجات.

56
00:04:13,830 --> 00:04:17,050
والآن، من بين سلبيات دالة sigmoid

57
00:04:17,050 --> 00:04:21,920
ودالة tanh أنه إذا كانت "z"
كبير للغاية أو صغيرة للغاية

58
00:04:21,920 --> 00:04:26,380
يصبح الانحدار أو المشتقة أو الميل
لهذه الدالة صغيرًا للغاية.

59
00:04:26,380 --> 00:04:29,550
وبالتالي، إذا كانت قيمة "z" صغيرة للغاية

60
00:04:29,550 --> 00:04:33,740
ينتهي الحال بميل الدالة
بحيث يكون قريبًا من الصفر.

61
00:04:33,740 --> 00:04:35,600
وبالتالي، يمكن أن يؤدي
ذلك إلى إبطاء الانحدار التدريجي.

62
00:04:36,630 --> 00:04:39,850
وبالتالي، هناك خير آخر وهو
شهير للغاية في

63
00:04:39,850 --> 00:04:44,650
تعلم الماكينات، ويطلق عليه اسم
وحدة التصحيح الخطية.

64
00:04:44,650 --> 00:04:47,925
وبالتالي، تبدو دالة القيمة مثل ذلك.

65
00:04:50,463 --> 00:04:57,020
وتكون الصيغة هي a = max(0,z).

66
00:04:57,020 --> 00:05:01,755
وبالتالي، يتكون المشتقة 1، طالما
كانت قيمة "z" موجبة.

67
00:05:01,755 --> 00:05:05,785
وتكون المشتقة أو الميل 0، عندما
تكون قيمة "z" سلبية.

68
00:05:05,785 --> 00:05:06,855
إذا كنت تقوم بتنفيذ ذلك

69
00:05:06,855 --> 00:05:11,425
فمن الناحية الفنية، لا تكون المشتقة محددة بشكل
جيد عندما تكون قيمة "z" صفرًا بالضبط.

70
00:05:11,425 --> 00:05:13,175
لكن، عندما تقوم بتنفيذ ذلك
في الكمبيوتر

71
00:05:13,175 --> 00:05:17,947
فإن الإجابة التي تحصل عليها بالضبط
هي أن "z" تساوي 0000000000000.

72
00:05:17,947 --> 00:05:22,697
وهي قيمة صغيرة للغاية، وبالتالي لا تحتاج
إلى القلق حيالها في واقع الممارسة العملية.

73
00:05:22,697 --> 00:05:26,729
ويمكن أن نتخيل قيمة المشتقة
عندما تكون "z" تساوي الصفر

74
00:05:26,729 --> 00:05:31,775
ويمكن أن نتخيل بأن قيمته
1 أو 0، وحينها تعمل بشكل جيد.

75
00:05:31,775 --> 00:05:35,640
وبالتالي تكون الحقيقة أنه لا يمكن
تمييزها وتلك هي الحقيقة، لذا

76
00:05:35,640 --> 00:05:39,670
إليكم بعض القواعد الرئيسية
لاختيار دوال التنشيط.

77
00:05:39,670 --> 00:05:44,940
إذا كانت قيمة المخرجات 0 و1، إذا كنت
تستخدم التصنيف الثنائي

78
00:05:44,940 --> 00:05:49,390
تكون دالة تنشيط sigmoid خيارًا طبيعيًا
للغاية لطبقة المخرجات.

79
00:05:49,390 --> 00:05:54,272
وبالتالي، بالنسبة لدوال ReLU
لكل الوحدات الأخرى

80
00:05:54,272 --> 00:05:58,374
أو وحدة التصحيح الخطية

81
00:06:02,348 --> 00:06:06,689
تكون بمثابة الخيار
الافتراضي لدالة التنشيط.

82
00:06:06,689 --> 00:06:11,000
وبالتالي، إذا لم تكن متأكدًا حيال ما
تستخدمه للطبقة المخفية لديك

83
00:06:11,000 --> 00:06:14,681
يمكن أن تستخدم دالة
تنشيط ReLU.

84
00:06:14,681 --> 00:06:16,820
فهي ما نرى أن أغلب الأشخاص
يستخدمونها في أغلب الأحوال.

85
00:06:16,820 --> 00:06:21,460
رغم أنه في بعض الأحيان يستخدم الأشخاص
كذلك دالة تنشيط tanh.

86
00:06:21,460 --> 00:06:25,852
من بين عيوب دالة ReLU أن
المشتقة تساوي الصفر

87
00:06:25,852 --> 00:06:27,152
عندما تكون قيمة "z" سلبية.

88
00:06:27,152 --> 00:06:29,136
في واقع الممارسة العملية، فإن ذلك يؤتي
ثماره بشكل جيد تمامًا.

89
00:06:29,136 --> 00:06:33,323
إلا أن هناك نسخة أخرى من دالة ReLU
يطلق عليها اسم leaky ReLU.

90
00:06:33,323 --> 00:06:35,174
وسوف أعطيكم الصيغة في
الشريحة التالية.

91
00:06:35,174 --> 00:06:38,581
ولكن، بدلاً من أن تكون صفرًا عندما تكون
قيمة "z" سلبية

92
00:06:38,581 --> 00:06:43,890
فإنها فقط تأخذ ميلاً بسيطًا مثل ذلك، وبالتالي
يطلق عليها اسم دالة leaky ReLU.

93
00:06:45,870 --> 00:06:49,850
وغالبًا ما تعمل بشكل أفضل من
دالة تنشيط ReLU

94
00:06:49,850 --> 00:06:53,790
رغم أنها لا تستخدم بنفس القدر في
واقع الممارسة العملية.

95
00:06:53,790 --> 00:06:55,130
رغم ذلك، أي دالة منهما يمكن أن تكون جيدة

96
00:06:55,130 --> 00:06:59,140
وإذا كان يلزم عليك الاختيار بينهما
فإنني دائمًا ما أستخدم دالة ReLU.

97
00:06:59,140 --> 00:07:03,255
وميزة كلا دالتي ReLU و
leaky ReLU أنه بالنسبة

98
00:07:03,255 --> 00:07:07,765
للكثير من المساحة لقيمة "Z"
، مشتقة دالة التنشيط

99
00:07:07,765 --> 00:07:12,061
يكون ميل دالة التنشيط مختلفًا
عن الصفر تمامًا.

100
00:07:12,061 --> 00:07:15,472
وبالتالي، في واقع الممارسة العملية
فإن استخدام دالة تنشيط ReLU

101
00:07:15,472 --> 00:07:19,855
تتعلم الشبكة العصبية الخاصة بك بشكل أسرع في
الغالب من استخدام دالة tanh أو

102
00:07:19,855 --> 00:07:21,959
استخدام دالة تنشيط sigmoid.

103
00:07:21,959 --> 00:07:26,148
والسبب الرئيسي أن تلك التأثيرات تكون أقل لميل

104
00:07:26,148 --> 00:07:30,015
الدالة نحو الصفر، مما يؤدي
إلى إبطاء التعليم.

105
00:07:30,015 --> 00:07:34,425
وأنا أدرك أنه بالنسبة لنصف نطاق "z"
يكون ميل دالة ReLU صفرًا

106
00:07:34,425 --> 00:07:39,585
لكن، في واقع الممارسة العملية، تكون قيمة "z"
أكبر من الصفر لعدد كافٍ من الوحدات المخفية لديك.

107
00:07:39,585 --> 00:07:43,040
وبالتالي، يمكن أن يكون التعليم سريعًا تمامًا
لأغلب أمثلة التدريب.

108
00:07:43,040 --> 00:07:47,490
لذا، دعونا نقوم بتلخيص الإيجابيات والسلبيات
لدوال التنشيط المختلفة بشكل سريع.

109
00:07:47,490 --> 00:07:49,160
هذه دالة تنشيط sigmoid.

110
00:07:49,160 --> 00:07:53,360
ما أقوله أنه يجب عدم استخدام هذه الدالة
على الإطلاق باستثناء طبقة المخرجات

111
00:07:53,360 --> 00:07:56,740
إذا كنت تقوم بإجراء التصنيف الثنائي، أو
ربما لا تقم باستخدام هذه الدالة إطلاقًا.

112
00:07:57,950 --> 00:08:01,860
والسبب وراء عدم استخدامي لهذه الدالة إطلاقًا
هو أن دالة tanh

113
00:08:01,860 --> 00:08:04,330
تعتبر أفضل كثيرًا بشكل قاطع.

114
00:08:04,330 --> 00:08:06,532
وتكون دالة تنشيط tanh هكذا.

115
00:08:11,012 --> 00:08:12,476
وبالتالي تكون دالة التنشيط

116
00:08:12,476 --> 00:08:17,190
الافتراضية والأكثر شيوعًا هي
دالة ReLU وهي هكذا.

117
00:08:18,400 --> 00:08:22,790
وبالتالي، إذا لم تكن متأكدًا من الدالة التي
يجب أن تستخدمها فاستخدم هذه الدالة

118
00:08:22,790 --> 00:08:27,668
ويمكنك كذلك تجربة دالة
leaky ReLU.

119
00:08:27,668 --> 00:08:34,540
حيث يمكن أن تكون القيمة (0.01 z, z).

120
00:08:34,540 --> 00:08:39,140
حسنًا؟ بالتالي، تكون قيمة "a"
الحد الأقصى لقيمة "z" عدد 0.01 مرة

121
00:08:39,140 --> 00:08:43,200
و"z"، وبالتالي يعطينا ذلك بعض
الانحناءات في الدالة.

122
00:08:43,200 --> 00:08:48,210
ويمكن أن تتساءل
لماذا هذا الثابت 0.01؟

123
00:08:48,210 --> 00:08:53,366
حسنًا، يمكنك استخدام هذا المعامل الآخر
لخوارزمية التعلم.

124
00:08:53,366 --> 00:08:55,320
وبعض الأشخاص يقولون أن هذا يؤتي
ثماره بشكل أفضل.

125
00:08:55,320 --> 00:08:56,790
لكني بالكاد أرى أشخاصًا يقومون بذلك.

126
00:08:58,380 --> 00:09:01,910
لكن، إذا كنت ترغب في تجربة ذلك في
التطبيق الخاص بك يمكنك فعل ذلك بكل حرية.

127
00:09:01,910 --> 00:09:05,460
ويمكنك أن ترى كيف يعمل ذلك
ومدى كفاءته

128
00:09:05,460 --> 00:09:07,980
والالتزام به إذا كان
يعطيك نتائج جيدة.

129
00:09:07,980 --> 00:09:11,828
وبالتالي، أتمنى أن يمنحك ذلك بعض
المعلومات حول بعض خيارات دوال التنشيط

130
00:09:11,828 --> 00:09:13,389
التي يمكنك استخدامها في الشبكة
العصبية الخاصة بك.

131
00:09:13,389 --> 00:09:17,276
ومن بين الموضوعات التي سوف تراها في
التعلم العميق أنك في الغالب يكون لديك الكثير

132
00:09:17,276 --> 00:09:20,299
من الخيارات المختلفة فيما يتعلق بكيفية عمل
التعليمات البرمجية للشبكة العصبية الخاصة بك.

133
00:09:20,299 --> 00:09:24,024
والتي تتراوح بين عدد الوحدات المخفية
وتمر باختيار دالة التنشيط

134
00:09:24,024 --> 00:09:26,752
وصولاً إلى كيفية تهيئة الطرق والذي
سنراه بعد ذلك.

135
00:09:26,752 --> 00:09:28,310
هناك الكثير من الخيارات مثل ذلك.

136
00:09:28,310 --> 00:09:32,630
ويتضح أنه من الصعب في بعض الأحيان الحصول
على توجيهات جيدة

137
00:09:32,630 --> 00:09:35,280
لما يحل المشكلة لديك بأفضل
شكل ممكن.

138
00:09:35,280 --> 00:09:38,410
وبالتالي، طوال هذه الدورات التدريبية
سوف أستمر في إعلامكم

139
00:09:38,410 --> 00:09:41,890
بما أراه في المجال من ناحية ما
يكون أكثر شهرة أو أقل شهرة.

140
00:09:41,890 --> 00:09:45,215
ولكن، بالنسبة للتطبيق الخاص بك
والخصائص المميزة له

141
00:09:45,215 --> 00:09:49,330
من الصعب للغاية بشكل فعلي معرفة ما يؤتي
ثماره بأفضل شكل ممكن بالضبط مسبقًا.

142
00:09:49,330 --> 00:09:52,480
وبالتالي، من النصائح التي يمكن توجيهها أنك
إذا لم تكن متأكدًا حيال

143
00:09:52,480 --> 00:09:57,220
دالة التنشيط التي تعمل بأفضل شكل ممكن
فجربها جميعها، وقم بتقييمها

144
00:09:57,220 --> 00:10:02,220
اعتمادًا على مجموعة تحقق للمقاومة، أو مجموعة
تطوير، والتي سوف نتحدث عنها في وقت لاحق

145
00:10:02,220 --> 00:10:05,780
ونرى أيها يعمل بشكل أفضل
ثم استخدمها.

146
00:10:05,780 --> 00:10:10,303
وأعتقد أنه من خلال اختبار هذه الخيارات
المختلفة للتطبيق الخاص بك

147
00:10:10,303 --> 00:10:15,189
يمكن أن تكون أفضل في التدقيق المستقبلي لبنية
الشبكة العصبية الخاصة بك مقابل

148
00:10:15,189 --> 00:10:20,030
الخصائص المميزة للمشكلة الخاصة بك
بالإضافة إلى تطورات الخوارزميات.

149
00:10:20,030 --> 00:10:24,694
بدلاً من أن أقول لك أن تستخدم
دالة تنشيط ReLU دائمًا

150
00:10:24,694 --> 00:10:26,720
وعدم استخدام أي دالة أخرى.

151
00:10:26,720 --> 00:10:30,877
وقد يسري ذلك أو لا يسري
على أي مشكلة تعمل عليها

152
00:10:30,877 --> 00:10:33,925
سواء في المستقبل القريب أو في
المستقبل البعيد.

153
00:10:33,925 --> 00:10:37,420
حسنًا، لقد كان ذلك فيما
يتعلق باختيار دوال التنشيط

154
00:10:37,420 --> 00:10:39,890
ولقد رأيت دوال التنشيط
الأكثر شهرة.

155
00:10:39,890 --> 00:10:43,120
وهناك سؤال آخر يمكن أن
يراودك في بعض الأحيان

156
00:10:43,120 --> 00:10:46,490
وهو لماذا قد تحتاج إلى استخدام
دالة تنشيط في الأساس؟

157
00:10:46,490 --> 00:10:48,260
لماذا لا نتوقف عن استخدامها؟

158
00:10:48,260 --> 00:10:51,510
لذا، دعونا نتحدث عن ذلك في
الفيديو التالي، حيث

159
00:10:51,510 --> 00:10:56,230
سترى لماذا تحتاج الشبكات العصبية إلى بعض
أشكال دوال التنشيط غير الخطية.