1
00:00:00,006 --> 00:00:04,743
前回の動画では 隠れ層が一つの
ニューラルネットワークがどういうものか見ました

2
00:00:04,743 --> 00:00:08,175
今回は 実際にその
ニューラルネットワークが

3
00:00:08,175 --> 00:00:10,361
どのように出力を計算しているのか
細かく見てみましょう

4
00:00:10,361 --> 00:00:15,533
ロジスティック回帰と似たようなものが
何度も繰り返されているという感じです

5
00:00:15,533 --> 00:00:16,423
次の例をみてみましょう。

6
00:00:16,423 --> 00:00:19,364
これが二層のニューラルネットワークです

7
00:00:19,364 --> 00:00:23,973
これが正確には何を計算するのか
詳細に学びましょう

8
00:00:23,973 --> 00:00:26,653
以前 ロジスティック回帰での丸は

9
00:00:26,653 --> 00:00:31,035
二つの計算のステップを表していると
お話ししました

10
00:00:31,035 --> 00:00:34,498
まずこのようにzを計算し

11
00:00:34,498 --> 00:00:37,754
次にzのシグモイド関数である
活性化を計算します

12
00:00:37,754 --> 00:00:40,536
ニューラルネットワークは
これを何度も繰り返すだけです

13
00:00:40,536 --> 00:00:43,953
隠れ層の中のノード一つのみに
焦点を当ててみましょう

14
00:00:43,953 --> 00:00:46,320
隠れ層の一つ目のノードを見てください

15
00:00:46,320 --> 00:00:48,079
とりあえず他のノードは
薄く表示しておきます

16
00:00:48,079 --> 00:00:50,820
左のロジスティック回帰と同じように

17
00:00:50,820 --> 00:00:54,391
この隠れ層のノードも
二つの計算のステップを行います

18
00:00:54,391 --> 00:00:58,418
一つ目のステップが
この左半分にあたると考えてもらうと

19
00:00:58,418 --> 00:01:02,754
この部分はz=wT・x+bを計算します

20
00:01:02,754 --> 00:01:08,253
表記法についてですが これらの値は
全て一つ目の隠れ層についてのものなので

21
00:01:08,253 --> 00:01:13,458
このようにいっぱい角括弧を書きます

22
00:01:13,458 --> 00:01:16,597
そして隠れ層の一つ目のノードなので
下付き文字で１と書きます

23
00:01:16,597 --> 00:01:18,424
一つ目はこの計算を行って

24
00:01:18,424 --> 00:01:24,419
二つ目のステップでは
a[1]1=σ(z[1]1)を計算します

25
00:01:24,419 --> 00:01:29,013
zとaのどちらについても
表記法の慣例としては

26
00:01:29,013 --> 00:01:33,770
aに[l]とiをつけると 上に角括弧で書かれた
lは層の番号を表し

27
00:01:33,770 --> 00:01:37,720
下に書かれたiは その層の中での
ノードの番号を表します

28
00:01:37,720 --> 00:01:42,344
今見ているノードは一つ目の層
つまり隠れ層にあって

29
00:01:42,344 --> 00:01:45,878
その中の一つ目のノードなので
二つの数字がどちらも１なのです

30
00:01:45,878 --> 00:01:49,965
このニューラルネットワークでの
一つ目のノードである小さな丸が

31
00:01:49,965 --> 00:01:52,579
これらの二つのステップの計算を
行うことを表しています

32
00:01:52,579 --> 00:01:58,399
次に隠れ層の二つ目のノードを
見てみましょう

33
00:01:58,399 --> 00:02:01,482
左のロジスティック回帰のユニットと
似たような感じで

34
00:02:01,482 --> 00:02:04,781
この小さな丸は
二つの計算のステップを表します

35
00:02:04,781 --> 00:02:08,733
まずzを計算します
まだ層は１つ目ですが

36
00:02:08,733 --> 00:02:12,996
今回は二つ目のノードですね
これがw[1]2T・x+b[1]2です

37
00:02:12,996 --> 00:02:17,880
次にa[1]2=σ(z[1]2)です

38
00:02:17,880 --> 00:02:23,071
必要なら動画を止めてもらって

39
00:02:23,071 --> 00:02:28,453
ここに紫で書いた表記法と
ちゃんと合っているか確認してみてください

40
00:02:28,453 --> 00:02:32,831
ここまででニューラルネットワークの
最初の二つの隠れユニットについて見てきました

41
00:02:32,831 --> 00:02:36,940
３、４つ目の隠れユニットも
似たような計算を表します

42
00:02:36,940 --> 00:02:39,778
では この二つの等式を取って

43
00:02:39,778 --> 00:02:44,169
次のスライドにコピーします

44
00:02:44,169 --> 00:02:48,921
これがニューラルネットワークで
これが一つ目 そしてこれが二つ目の式です

45
00:02:48,921 --> 00:02:54,050
一つ目と二つ目の隠れ層について
さっき書いたものですね

46
00:02:54,050 --> 00:02:59,022
更に進んで 三つ目と四つ目の隠れ層に
対応する式を書くと

47
00:02:59,022 --> 00:03:02,093
このようになります

48
00:03:02,093 --> 00:03:06,550
表記法について再度確認すると
これはベクトルw[1]1で

49
00:03:06,550 --> 00:03:09,430
それを転置したものにxを掛けてあります
いいですか？

50
00:03:09,430 --> 00:03:13,460
ここの上付き文字のTは
ベクトルの転置を表しているのです

51
00:03:13,460 --> 00:03:17,585
想像できるかもしれませんが
実際ニューラルネットワークを実装するとき

52
00:03:17,585 --> 00:03:20,209
これらにforループを使うのは
非常に非効率です

53
00:03:20,209 --> 00:03:25,174
なので これら四つの式を取って
ベクトル化します

54
00:03:25,174 --> 00:03:29,348
まずzをベクトルとして計算したら
どうなるかを考えましょう

55
00:03:29,348 --> 00:03:30,859
これは次のようにできます

56
00:03:30,859 --> 00:03:34,843
これらのwたちを取って
行列に詰め込みます

57
00:03:34,843 --> 00:03:38,767
するとw[1]1の転置
つまり行ベクトルができます

58
00:03:38,767 --> 00:03:42,231
もともと列ベクトルだったものを
転置したので 行ベクトルになるのです

59
00:03:42,231 --> 00:03:48,494
そしてw[1]2の転置 w[1]3の転置
w[1]4の転置と続きます

60
00:03:48,494 --> 00:03:54,499
これら四つのベクトルを積み上げることで
行列が得られるのです

61
00:03:54,499 --> 00:03:59,204
もう一つの考え方としては
ロジスティック回帰のユニットが四つあって

62
00:03:59,204 --> 00:04:03,913
それぞれのユニットが
対応するパラメータベクトルwを持っていて

63
00:04:03,913 --> 00:04:06,535
それら四つのベクトルをまとめることで

64
00:04:06,535 --> 00:04:08,842
4×3の行列ができあがる
と考えることもできます

65
00:04:08,842 --> 00:04:14,281
この行列に入力の特徴量
x1、x2、x3をかけると

66
00:04:14,281 --> 00:04:19,806
行列の掛け算のやり方で計算すると

67
00:04:19,806 --> 00:04:24,546
w[1]1T・x、w[1]2T・x、
w[1]3T・x、w[1]4T・x ができます

68
00:04:24,546 --> 00:04:30,995
bも忘れてはいけませんね

69
00:04:30,995 --> 00:04:35,997
なのでここにベクトルb[1]1、b[1]2、

70
00:04:35,997 --> 00:04:40,811
b[1]3、b[1]4 を足します
この部分にあたりますね

71
00:04:40,811 --> 00:04:45,654
そしてこちらもb[1]1、b[1]2、
b[1]3、b[1]4です

72
00:04:45,654 --> 00:04:50,579
ここで得られる四行のそれぞれが

73
00:04:50,579 --> 00:04:55,772
上で計算した四つの値に
ぴったり一致しているのがわかります

74
00:04:55,772 --> 00:05:00,899
つまり これがz[1]1となって

75
00:05:00,899 --> 00:05:05,303
z[1]2、z[1]3、z[1]4となるのです
ここで定義されている通りですね

76
00:05:05,303 --> 00:05:10,289
そして 当たり前に思えるかもしれませんが
これら全体をz[1]と呼ぶことにします

77
00:05:10,289 --> 00:05:15,097
個々のzを積み上げて
列ベクトルとしたものですね

78
00:05:15,097 --> 00:05:19,524
ベクトル化をするときの
大まかなルールは

79
00:05:19,524 --> 00:05:23,966
一つの層に異なるノードがあるとき
縦に並べるということです

80
00:05:23,966 --> 00:05:27,656
そのため z[1]1からz[1]4までがあって

81
00:05:27,656 --> 00:05:31,852
それらが隠れ層の四つのノードに
対応しているとき

82
00:05:31,852 --> 00:05:36,481
それらを縦に並べてベクトルz[1]を作ったのです

83
00:05:36,481 --> 00:05:40,457
もう一つ表記法を使ってみます

84
00:05:40,457 --> 00:05:45,233
さっき小文字のw[1]1、w[1]2などを
並べて作った この4×3の行列ですが

85
00:05:45,233 --> 00:05:49,860
これを大文字でW[1]と呼ぶことにします

86
00:05:49,860 --> 00:05:54,623
同様に これはb[1]と呼びます
4×1のベクトルですね

87
00:05:54,623 --> 00:05:59,584
これで このベクトルや行列の表記法を使って
zを計算することができました

88
00:05:59,584 --> 00:06:03,535
最後にやりたいことは
このaの値も計算することです

89
00:06:03,535 --> 00:06:08,195
当然 a[1]を これらの活性化
a[1]1からa[1]4を

90
00:06:08,195 --> 00:06:13,019
縦に並べたものとして定義します

91
00:06:13,019 --> 00:06:18,202
この四つの値を取ってきて
a[1]と呼ばれるベクトルに詰め込んだだけです

92
00:06:18,202 --> 00:06:21,122
そしてこれはσ(z[1])になります

93
00:06:21,122 --> 00:06:25,794
このシグモイド関数は
四つのzの要素を取り込んで

94
00:06:25,794 --> 00:06:30,761
要素ごとにシグモイド関数を
適用するというものです

95
00:06:30,761 --> 00:06:36,750
復習ですが z[1]=W[1]x+b[1]で

96
00:06:36,750 --> 00:06:41,883
a[1]=σ(z[1])であることがわかりました

97
00:06:41,883 --> 00:06:47,321
次のスライドにこれらをコピーします

98
00:06:47,321 --> 00:06:52,156
入力xがあるとき
ニューラルネットワークの一層目では

99
00:06:52,156 --> 00:06:56,286
z[1]=W[1]x+b[1]で

100
00:06:56,286 --> 00:07:01,526
a[1]=σ(z[1])となっています

101
00:07:01,526 --> 00:07:06,563
その次元は(4,1)=(4,3)×(3,1)+(4,1)
となっています

102
00:07:06,563 --> 00:07:11,297
aはzと同じ次元数なので(4,1)ですね

103
00:07:11,297 --> 00:07:16,793
そしてx=a[0]でyハットはa[2]
だったと思い出してください

104
00:07:16,793 --> 00:07:21,560
なので 実際にxを取ってa[0]と
置き換えることもできるのです

105
00:07:21,560 --> 00:07:25,417
a[0]は入力xのただの別名だからです

106
00:07:25,417 --> 00:07:30,968
似たような方法で
次の層の計算も導くことができます

107
00:07:30,968 --> 00:07:35,972
出力層はパラメータW[2]とb[2]に

108
00:07:35,972 --> 00:07:40,770
紐づけられています

109
00:07:40,770 --> 00:07:44,549
この場合 W[2]は1×4の行列で

110
00:07:44,549 --> 00:07:47,529
b[2]は1×1のただの実数です

111
00:07:47,529 --> 00:07:51,982
z[2]は実数になるので
1×1の行列と書けます

112
00:07:51,982 --> 00:07:57,267
これは(1,4) aは(4,1)で
b[2]は(1,1)です

113
00:07:57,267 --> 00:08:02,397
なので答えとして実数が得られます

114
00:08:02,397 --> 00:08:07,787
最後の出力ユニットを
ロジスティック回帰のように捉えると

115
00:08:07,787 --> 00:08:12,517
ロジスティック回帰でのwは
W[2]Tの役割をしていて

116
00:08:12,517 --> 00:08:16,675
つまりW[2]はwの転置ですね
そしてbはb[2]と同じものです

117
00:08:16,675 --> 00:08:21,665
とりあえずネットワークの
左側を無視すると

118
00:08:21,665 --> 00:08:26,434
この最後の出力ユニットは
ロジスティック回帰によく似ています

119
00:08:26,434 --> 00:08:30,010
違うところは
パラメータをwとbと書く代わりに

120
00:08:30,010 --> 00:08:35,784
今回は(1,4)次元のW[2]と
(1,1)次元のb[2]と書くところです

121
00:08:35,784 --> 00:08:39,765
おさらいすると
ロジスティック回帰では

122
00:08:39,765 --> 00:08:44,620
出力 つまり予測を実装するには

123
00:08:44,620 --> 00:08:51,143
z=wT・x+bと
yハット=a=σ(z)を計算します

124
00:08:51,143 --> 00:08:55,499
隠れ層が一つのニューラルネットワークを
実装するためには

125
00:08:55,499 --> 00:09:00,131
この四つの式を実装すればいいだけです

126
00:09:00,131 --> 00:09:04,902
隠れ層の四つのロジスティック回帰ユニット
からの出力を

127
00:09:04,902 --> 00:09:09,329
ベクトル化して実装したのが
この部分がやっていることで

128
00:09:09,329 --> 00:09:13,867
出力層のロジスティック回帰の計算は
この部分が担っています

129
00:09:13,867 --> 00:09:18,401
この説明でわかってもらえればいいのですが
ここで学んでほしいのは

130
00:09:18,401 --> 00:09:22,001
このニューラルネットワークの出力の計算には
この四行のコードが必要ということです

131
00:09:22,001 --> 00:09:25,706
ここまでで
入力の特徴量ベクトルxが一つあるとき

132
00:09:25,706 --> 00:09:30,278
このニューラルネットワークの出力を
四行のコードで計算できるとわかりました

133
00:09:30,278 --> 00:09:34,575
ロジスティック回帰と似たような感じで

134
00:09:34,575 --> 00:09:39,002
複数の教師サンプルにまたがって
ベクトル化したいです

135
00:09:39,002 --> 00:09:43,653
教師サンプルを行列の列として並べると
少し修正するだけで実現できます

136
00:09:43,653 --> 00:09:47,396
また ロジスティック回帰と同じように

137
00:09:47,396 --> 00:09:50,514
ニューラルネットワークの出力を
一回に一つの教師サンプルではなく

138
00:09:50,514 --> 00:09:55,114
全てのサンプルに対して
一気に計算できるようになります

139
00:09:55,114 --> 00:09:57,939
次の動画で
その詳細を見てみましょう