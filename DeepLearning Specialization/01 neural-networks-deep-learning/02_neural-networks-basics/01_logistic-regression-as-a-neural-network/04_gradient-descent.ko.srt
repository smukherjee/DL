1
00:00:00,590 --> 00:00:03,210
로지스틱 회귀분석 모델을 보았는데요, 

2
00:00:03,210 --> 00:00:06,560
싱글 트레이닝 example에서 얼마나 잘하는지를 

3
00:00:06,560 --> 00:00:08,780
측정하는 비용함수를 보았었는데요, 

4
00:00:08,780 --> 00:00:13,530
또한, 파라티터 w와 b가 전체 트레이닝 세트에서

5
00:00:13,530 --> 00:00:16,590
얼마나 잘하는지를 측정하는 비용함수도 같이 봤습니다.

6
00:00:16,590 --> 00:00:21,600
이제는 기울기 강하 알고리즘을 어떻게 이용하여

7
00:00:21,600 --> 00:00:25,730
w와 b를 트레이닝세트에서 트레이닝 또는 
학습할 수 있는지 배워보도록 하겠습니다. 

8
00:00:25,730 --> 00:00:30,030
복습하자면, 이것이 어디서 많이 눈이 익숙한 로지스틱 
회귀분석 알고리즘이죠. 

9
00:00:31,130 --> 00:00:34,700
2번째 줄에는 J라는 비용함수가 있습니다. 

10
00:00:34,700 --> 00:00:37,879
이 함수는 파라티터 w와 b에 대한 함수입니다.

11
00:00:37,879 --> 00:00:39,960
그리고 이것은 평균값으로 정의되죠.

12
00:00:39,960 --> 00:00:44,140
그러므로 1 나누기 m 곱하기 이 loss함수의 합입니다.

13
00:00:44,140 --> 00:00:48,470
그렇게해서 loss함수는 

14
00:00:48,470 --> 00:00:53,170
각각의 트레이닝 example에 대해서
알고리즘에 결과값 ŷ(i)이 

15
00:00:53,170 --> 00:00:58,000
얼마나 잘 쌓거나 ground true label y(i)과 
얼마나 잘 비교되는지 측정합니다.

16
00:00:58,000 --> 00:01:00,886
전체 공식은 오른쪽에 펼쳐져있습니다.

17
00:01:00,886 --> 00:01:04,130
자 그래서, 비용함수는 트레이닝세트에서

18
00:01:04,130 --> 00:01:06,760
파라티터 w와 b가 얼마나 잘 하는지를
측정하는데요

19
00:01:06,760 --> 00:01:11,510
w와 b라는 파라티터를 배우기 위해서는, 

20
00:01:11,510 --> 00:01:17,930
w와b가 비용함수 J(w, b)를 최소화시키는 값을
찾는게 자연스러운 단계이겠죠?

21
00:01:17,930 --> 00:01:21,320
이것이 gradien descent의 모습입니다.

22
00:01:21,320 --> 00:01:25,320
이 표에서는 가로축이 

23
00:01:25,320 --> 00:01:28,510
공간 매개 변수 w와 b를 나타냅니다.

24
00:01:28,510 --> 00:01:32,350
실제로 w는 다 큰 다이멘션일 수 있지만 

25
00:01:32,350 --> 00:01:38,190
표에 나타내기 위해 w와 b가 실수라고하겠습니다. 

26
00:01:38,190 --> 00:01:40,770
J(w, b, ) 비용함수는

27
00:01:40,770 --> 00:01:45,130
그러면 이 가로축 w와 b 표면 어딘가에 있을 것입니다. 

28
00:01:45,130 --> 00:01:50,720
표면의 높이는 특정지점에서 J(w, b)의 값을 가르키는데요

29
00:01:50,720 --> 00:01:55,070
저희는 J라는 비용함수가 최소값이 되는 

30
00:01:55,070 --> 00:01:59,730
w와 b의 값을 구해야 합니다.

31
00:02:00,830 --> 00:02:06,050
알고보면, 이 J 비용함수는 볼록함수입니다.

32
00:02:06,050 --> 00:02:10,327
하나의 그릇 모양이라고 할 수 있는데요, 
이것이 볼록함수이고, 

33
00:02:10,327 --> 00:02:13,717
이런 여러개의 local이 있는 

34
00:02:13,717 --> 00:02:18,120
비볼록 함수와는 다르죠. 

35
00:02:18,120 --> 00:02:22,240
이렇게 정의된 비용함수 J(w, b)는 

36
00:02:22,240 --> 00:02:27,020
볼록하고, 그렇기 때문에 이와 같은 비용함수 J를 이용하는 것입니다.

37
00:02:27,020 --> 00:02:29,610
로지스틱 회귀분석에서 말이죠. 

38
00:02:29,610 --> 00:02:33,810
파라티터의 적합한 값을 찾으려면, 

39
00:02:33,810 --> 00:02:39,160
w와 b를 최초값으로 초기화해야 합니다.

40
00:02:39,160 --> 00:02:43,360
이 빨간 점이 상징하듯이 말이죠. 

41
00:02:43,360 --> 00:02:47,562
로지스틱 회귀분석에서는 거의 모든 
초기화 방법이 잘 구현됩니다.

42
00:02:47,562 --> 00:02:50,690
일반적으로 0으로 초기화시키는데요, 

43
00:02:50,690 --> 00:02:52,910
무작위로 초기화 시키는 방법도 효율적입니다.

44
00:02:52,910 --> 00:02:55,630
그렇지만 대부분의 사람은 보통 이러한 방법은
잘 사용하지 않죠. 

45
00:02:55,630 --> 00:02:59,310
이 함수가 볼록함수이기 때문에
어느 지점에서 초기화하더라도

46
00:02:59,310 --> 00:03:02,180
똑같은 지점에 도달하거나 거의 비슷한 점에
도달할 것입니다.

47
00:03:02,180 --> 00:03:06,450
기울기 강하 가 하는 것은, 
처음 시작점에서 시작해서 

48
00:03:06,450 --> 00:03:10,310
가장 기울기가 높은 내리막길 방향으로 이동합니다. 

49
00:03:10,310 --> 00:03:15,290
그렇기 때문에 기울기 강하 이후로는
이 점에 도착할 수도 있습니다.

50
00:03:15,290 --> 00:03:19,320
그 이유는, 기울기 강하 가 가장 경사가 높은 내리막길
방향으로 향하기 때문입니다.

51
00:03:19,320 --> 00:03:21,250
최대한 빠른 속도로 말이죠. 

52
00:03:21,250 --> 00:03:23,600
이것이 기울기 강하 의 반복업무 중 하나입니다.

53
00:03:23,600 --> 00:03:27,084
기울기 강하를 2번 진행한 이후, 
이곳으로 넘어갈 수도 있습니다.

54
00:03:27,084 --> 00:03:28,830
3번째에는 여기로 말이죠, 

55
00:03:28,830 --> 00:03:32,640
이것이 이제는 표에 가려질텐데요,

56
00:03:32,640 --> 00:03:38,880
결과적으로 이상적으로는 전역최적점으로 모이거나
근접한 지점에 도달할 것입니다.

57
00:03:38,880 --> 00:03:42,300
이 그림은 기울기 강하 알고리즘을 나타내고 있습니다.

58
00:03:42,300 --> 00:03:44,310
조금 더 상세히 적어보겠습니다.

59
00:03:44,310 --> 00:03:47,750
조금 더 쉽게 나타내기위해
J(w)라는 함수가 있다고 해보겠습니다.

60
00:03:47,750 --> 00:03:51,700
이 값을 최소화시키고 싶은데요, 
이렇게 생겼다고 해보겠습니다.

61
00:03:51,700 --> 00:03:54,650
더 쉽게 그리리 위해서, 
일단은 b를 무시하고, 

62
00:03:54,650 --> 00:03:59,210
다차원이 아닌 1차원적인 표로 만들겠습니다. 

63
00:03:59,210 --> 00:04:01,240
기울기 강하 가 이렇게해서, 

64
00:04:01,240 --> 00:04:06,740
해당 update를 계속 반복적으로 진행할 것입니다.

65
00:04:06,740 --> 00:04:09,467
w값을 갖고 업데이트할 것입니다. 

66
00:04:09,467 --> 00:04:12,508
콜론표시로 w를 업데이트함을 표시할텐데요, 

67
00:04:12,508 --> 00:04:17,426
w를 w 빼기 알파로하고, 

68
00:04:17,426 --> 00:04:22,200
이것은 derivative dJ(w)/dw입니다.

69
00:04:22,200 --> 00:04:26,230
이것을 알고리즘이 합쳐질때까지 반복하겠습니다. 

70
00:04:26,230 --> 00:04:30,666
알아둘 점은, 여기 알파는 학습 속도를 뜻하고, 

71
00:04:30,666 --> 00:04:36,820
한번에 얼마만큼 기울기 강하를 진행할 수 있는지 
조절해줍니다.

72
00:04:36,820 --> 00:04:41,200
학습 속도인 알파를 어떻게 선택하는지는 나중에 자세히
이야기하겠습니다.

73
00:04:41,200 --> 00:04:44,490
2번째로, 여기 이 값은 derivative입니다.

74
00:04:44,490 --> 00:04:48,010
이것은 업데이트 또는 w에 얼마나 변화를 줄지 여부를 알려주는 값입니다. 

75
00:04:48,010 --> 00:04:52,700
기울기 강하를 도입하기 위한 코드를 만들기 시작하면 

76
00:04:52,700 --> 00:04:57,380
dw라는 변수가 derivative를 나타내도록 

77
00:04:58,620 --> 00:05:02,300
규칙을 만들 것입니다.

78
00:05:02,300 --> 00:05:06,551
그렇게해서 코드를 만드는 경우, 

79
00:05:06,551 --> 00:05:10,046
w : 는 w 빼기 알파 곱하기 dw 라고 적어볼 텐데요

80
00:05:10,046 --> 00:05:14,750
dw를 변수 이름으로 만들어서 이 derivative 항을 뜻하게 만듭니다. 

81
00:05:14,750 --> 00:05:19,330
자 이제 해당 기울기 강하 업데이트가 말이 되도록
만들겠습니다.

82
00:05:19,330 --> 00:05:21,880
w가 여기 있었다고 하죠.

83
00:05:21,880 --> 00:05:26,060
그러면 비용함수 J(w)에서 이 지점에 있습니다.

84
00:05:26,060 --> 00:05:29,270
기억할 것은, derivative의 정의가 

85
00:05:29,270 --> 00:05:31,420
특정 지점에서 함수의 기울기라는 것입니다.

86
00:05:31,420 --> 00:05:36,190
함수의 기울기는 높이 나누기 너비, 맞죠?

87
00:05:36,190 --> 00:05:40,290
이 작은 삼각형에서 말이죠, 
기울기는 J(w) 에서 이 탄젠트 지점이 됩니다.

88
00:05:40,290 --> 00:05:43,900
그렇게해서, derivative는 양수입니다.

89
00:05:43,900 --> 00:05:48,830
W는 w 빼기 학습 속도 곱하기 derivative 값만큼 업데이트 
되는 것입니다.

90
00:05:48,830 --> 00:05:53,310
derivative값은 양수이고, 
그렇기 때문에 w에서 빼게되는 것인데요

91
00:05:53,310 --> 00:05:55,260
그러므로 왼쪽으로 진행합니다.

92
00:05:55,260 --> 00:05:59,380
만약 이 큰 w값으로 시작했다고하면

93
00:05:59,380 --> 00:06:04,450
기울기 강하는 알고리즘이 천천히
파라티터를 줄이도록 할 것입니다. 

94
00:06:04,450 --> 00:06:08,545
다른 예제로, 만약 w가 여기 있었다고 하면

95
00:06:08,545 --> 00:06:15,050
이 지점에서는 dJ/dw의 기울기는 
음수일 것입니다. 

96
00:06:15,050 --> 00:06:22,771
이 경우, 기울기 강하 업데이트는 알파 곱하기
음수의 값을 뺄 것입니다.

97
00:06:22,771 --> 00:06:27,122
결과적으로 w를 천천히 증가시킬텐데요, 
w를, 기울기 강하 의 반복을 통해

98
00:06:27,122 --> 00:06:31,530
점점 더 크게 만드는 효과가 있습니다.

99
00:06:31,530 --> 00:06:34,387
그러므로 왼쪽에서 초기화를 진행하던

100
00:06:34,387 --> 00:06:39,000
오른쪽에서 진행하던, 기울기 강하는
이 전역 최소값을 향해 움직일 것입니다.

101
00:06:39,000 --> 00:06:43,100
여러분이 derivative이나 미적분학에 익숙해 있지 
않으신 경우이거나, 

102
00:06:43,100 --> 00:06:49,710
dJ(w)/dw가 뜻하는 바가 무엇인지 잘 모르시더라도
너무 걱정하지 마십시요. 

103
00:06:49,710 --> 00:06:53,770
derivative에 대해서는 다음 비디오에서 더욱 자세히 다루겠습니다.

104
00:06:53,770 --> 00:06:56,761
미적분학을 잘 아시면, 

105
00:06:56,761 --> 00:07:02,321
신경망이 어떻게 작동하는지 잘 이해하실 수 있을 것입니다.

106
00:07:02,321 --> 00:07:05,471
하지만, 미적분학에 익숙하지 않더라도

107
00:07:05,471 --> 00:07:10,091
다음 비디오를 통해, derivative와 미적분학에 대한 직관적인 부분을 다뤄서

108
00:07:10,091 --> 00:07:14,980
신경망을 잘 다룰 수 있도록 하겠습니다.

109
00:07:14,980 --> 00:07:16,410
일단은 이런 항이 기울기를 

110
00:07:16,410 --> 00:07:21,520
뜻한다는 것을 이해하시고, 

111
00:07:21,520 --> 00:07:26,760
현재 매개 변수 값에서의 함수 기울기를 찾고

112
00:07:26,760 --> 00:07:31,140
가장 높은 기울기에서, 

113
00:07:31,140 --> 00:07:35,450
비용함수 J인 경우, 어느 방향으로 이동할지 여부를 알아내는 것입니다.

114
00:07:36,660 --> 00:07:42,520
오로지 w가 파라티터인 경우, 
J(s)에 대한 기울기 강하를 적었습니다.

115
00:07:42,520 --> 00:07:47,150
로지스틱 회귀분석에서는 
비용함수가 w와 b라는 파라티터에 대한 함수입니다.

116
00:07:47,150 --> 00:07:50,894
이 경우, 기울기 강하의 inner loop는, 
이와 같이 생긴 것인데요, 

117
00:07:50,894 --> 00:07:53,302
반복해야하는 이 것은
아래와 같습니다.

118
00:07:53,302 --> 00:07:57,970
w를 w 빼기 학습 속도 곱하기 

119
00:07:57,970 --> 00:08:02,030
w에 대한 J(w, b) derivative로 업데이트 합니다.

120
00:08:02,030 --> 00:08:07,460
b는 b빼기 학습속도 곱하기 

121
00:08:07,460 --> 00:08:12,270
b에 대한 비용함수의 derivative로 b를
업데이트 합니다.

122
00:08:12,270 --> 00:08:17,300
그래서 이 밑에 있는 2개의 공식이 
실제로 도입하는 업데이트입니다.

123
00:08:17,300 --> 00:08:22,320
따로 말씀 드릴 부분이 있는데요, 
미적분학에서 쓰는 표기가

124
00:08:22,320 --> 00:08:24,560
어떤 분들에게 조금 헷갈릴 수 있는데요. 

125
00:08:24,560 --> 00:08:28,387
미적분학을 이해하는 것이 굉장히 중요하다고
할 수 없지만

126
00:08:28,387 --> 00:08:32,411
이런 것을 보면, 너무 많이 생각하지 않으시길 바랍니다.

127
00:08:32,411 --> 00:08:35,519
여기서 보이는 미적분학에서 쓰는 항은

128
00:08:35,519 --> 00:08:40,730
이렇게 쓰는데요, 이 구부려진 모양입니다.

129
00:08:40,730 --> 00:08:46,160
이 심볼은, 이렇게 멋지게 쓰인 것은 소문자 d이구요

130
00:08:46,160 --> 00:08:51,070
조금 특화된 글꼴로 쓰는데 
이러한 표기를 보면, 

131
00:08:51,070 --> 00:08:56,145
J(w, b)의 기울기라는 것입니다.

132
00:08:56,145 --> 00:09:01,580
w방향으로 J(w, b)함수가 얼마나 기울고 있는지를 나타냅니다.

133
00:09:01,580 --> 00:09:06,640
미적분학에서의 규칙이
아주 타당하다고는 생각되지 않지만, 

134
00:09:06,640 --> 00:09:11,780
이 규칙이 오히려 더 헷갈리게 만드는 것 같은데요

135
00:09:11,780 --> 00:09:16,940
만약 J가 2개 또는 그 이상의 변수로 이루어져 있을 시, 

136
00:09:16,940 --> 00:09:21,550
소문자 d를 쓰는 대신에 
이 우습게 생긴 심볼을 사용합니다.

137
00:09:21,550 --> 00:09:24,380
이것을 partial derivative 심볼이라고 하는데요. 

138
00:09:24,380 --> 00:09:26,120
일단 신경쓰지 마십시요. 

139
00:09:26,120 --> 00:09:31,090
그리고 J가 만약 한개의 변수를 가진 함수라면
소문자 d를 사용합니다.

140
00:09:31,090 --> 00:09:33,960
그렇기 때문에 이 우습게 생긴 partial derivative

141
00:09:33,960 --> 00:09:38,040
심볼을 사용하는 경우와 소문자 d를 위와 같이 사용하는 경우의 차이는

142
00:09:38,040 --> 00:09:41,570
J 함수가 2개 이상의 변수를 가지고 있는지의 차이입니다.

143
00:09:41,570 --> 00:09:45,900
그런 경우, 이 partial derivative 심볼을 쓰고, 

144
00:09:45,900 --> 00:09:51,480
1개의 변수가 있는 경우, 소문자 d를 사용합니다.

145
00:09:51,480 --> 00:09:55,410
이런 것이 바로 미적분학에서 쓰이는 조금은 웃긴 표기법인데요.

146
00:09:55,410 --> 00:09:58,540
오히려 더 헷갈리게 만드는 것 같습니다.

147
00:09:58,540 --> 00:10:03,300
partial derivative 심볼를 보시면

148
00:10:03,300 --> 00:10:07,290
단순히 함수의 기울기가 이 변수들 중 하나에 대하여 계산된다고 생각하면 됩니다. 

149
00:10:07,290 --> 00:10:12,530
비슷하게, 앞서 다룬 미적분학 표기법을 동일하게 

150
00:10:12,530 --> 00:10:18,070
적용합니다, 그 이유는 여기서는 J가 2개의 입력값이 있기 때문이죠. 

151
00:10:18,070 --> 00:10:22,540
이 밑에 있는 부분은 이 partial derivative 심볼과 같이
쓰여야 합니다.

152
00:10:22,540 --> 00:10:28,290
이 것은 거의 똑같은데요, 소문자 d가 의미하는 것과
거의 비슷합니다.

153
00:10:28,290 --> 00:10:31,360
마지막으로 이것을 코드에 도입시킬 때는, 

154
00:10:31,360 --> 00:10:36,220
이 양이, 즉 w를 업데이트하는 정도의 양을, 

155
00:10:36,220 --> 00:10:41,980
dw 변수로 코드에서는 표기될텐데요

156
00:10:41,980 --> 00:10:44,220
이 값 맞죠?

157
00:10:44,220 --> 00:10:47,230
b를 업데이트하고 싶은 정도는 

158
00:10:47,230 --> 00:10:50,740
코드에서의 db 변수로 표기될 것입니다.

159
00:10:50,740 --> 00:10:55,580
자 이렇게 gradiend descent를 도입할 수 있는 것입니다.

160
00:10:55,580 --> 00:10:59,830
만약 여러분이 미적분학을 수년동안
접하지 않으셨다면, 

161
00:10:59,830 --> 00:11:03,770
지금 본인의 현재 편안한 정도의 레벨보다
미적분학에서 더 많은 derivative를 다룬다고 생각하실텐데요, 

162
00:11:03,770 --> 00:11:06,330
만약 여러분이 이렇게 느끼시면 걱정하지 마십시요.

163
00:11:06,330 --> 00:11:10,150
다음 비디오에서는 derivative에 대한 조금 더 직관적인 이해를 
돕도록 하겠습니다.

164
00:11:10,150 --> 00:11:13,560
미적분학에 대한 깊은 수학적인 지식없이도

165
00:11:13,560 --> 00:11:16,310
직관적인 미적분학의 이해만으로도

166
00:11:16,310 --> 00:11:19,130
신경망을 조금 더 효율적으로 만들 수 있을 것입니다.

167
00:11:19,130 --> 00:11:22,743
다음 비디오로 넘어가서

168
00:11:22,743 --> 00:11:23,470
derivatice에 대해 조금 더 이야기해보도록 하겠습니다.