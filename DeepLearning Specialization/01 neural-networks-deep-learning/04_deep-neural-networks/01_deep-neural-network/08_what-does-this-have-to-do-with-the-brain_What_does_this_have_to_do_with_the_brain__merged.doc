<html xmlns:o="urn:schemas-microsoft-com:office:office" xmlns:w10="urn:schemas-microsoft-com:office:word" xmlns:m="http://schemas.microsoft.com/office/omml/2004/12/core" xmlns:ve="http://schemas.openxmlformats.org/markup-compatibility/2006" xmlns:o12="http://schemas.microsoft.com/office/2004/7/core" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:v="urn:schemas-microsoft-com:vml" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/3/wordprocessingDrawing" xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/3/main" xmlns="http://www.w3.org/TR/REC-html40">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <head>
    <style>style</style>
  </head>
  <body>
    <div class="Section1">
      <p>
        <b>
          <u>
            <span style="font-size:20.0pt">What does this have to do with the brain?</span>
          </u>
        </b>
      </p>
      <p>
(SPEECH)        <br/>
So,        <br/>
      </p>
      <p>
(DESCRIPTION)        <br/>
Text, deep learning dot AI. Deep Neural Networks. What does this have to do with the brain?        <br/>
      </p>
      <p>
(SPEECH)        <br/>
what does deep learning have to do with the brain?        <br/>
      </p>
      <p>
At the risk of giving away the punchline, I would say not a whole lot.        <br/>
      </p>
      <p>
But let's take a quick look at why people keep making the analogy between deep learning and the human brain.        <br/>
      </p>
      <p>
(DESCRIPTION)        <br/>
Text, forward and backward propagation. Formulas for forward propagation, starting with i equals 1, Z i equals W i x plus b i, A i equals g i times Z i, Repeat cycle L times to yield A L, which equals output label Y        <br/>
      </p>
      <p>
(SPEECH)        <br/>
When you implement a neural network, this is what you do, forward prop and back prop.        <br/>
      </p>
      <p>
I        <br/>
      </p>
      <p>
(DESCRIPTION)        <br/>
Formulas for backward propagation, d Z L equals A L minus Y, d W L equals 1 over m, times, d Z L, A L transpose, d b L equals 1 over m times the Num Py summation of d Z L, Repeat cycle L times        <br/>
      </p>
      <p>
(SPEECH)        <br/>
think because it's been difficult to convey intuitions about what these equations are doing really gradient descent on a very complex function, the analogy that is like the brain has become really an oversimplified explanation for what this is doing, but the simplicity of this makes it seductive for people to just say it publicly, as well as, for media to report it, and certainly caught the popular imagination.        <br/>
      </p>
      <p>
There is a very loose analogy between, let's say a logistic regression unit with a sigmoid activation function, and here's a cartoon of a single neuron in the brain.        <br/>
      </p>
      <p>
In        <br/>
      </p>
      <p>
(DESCRIPTION)        <br/>
Diagrams of a regression unit, with three input features entering a node that produces an output label. Comparison diagram of a real neuron, where signals pass into an axon terminal and another signal leaves through the axon        <br/>
      </p>
      <p>
(SPEECH)        <br/>
this picture of a biological neuron, this neuron, which is a cell in your brain, receives electric signals from your other neurons, X_1, X_2, X_3, or maybe from other neurons A_1, A_2, A_3, does a simple thresholding computation, and then if this neuron fires, it sends a pulse of electricity down the axon, down this long wire perhaps to other neurons.        <br/>
      </p>
      <p>
So, there is a very simplistic analogy between a single neuron in a neural network and a biological neuron-like that shown on the right, but I think that today even neuroscientists have almost no idea what even a single neuron is doing.        <br/>
      </p>
      <p>
A single neuron appears to be much more complex than we are able to characterize with neuroscience, and while some of what is doing is a little bit like logistic regression, there's still a lot about what even a single neuron does that no human today understands.        <br/>
      </p>
      <p>
For example, exactly how neurons in the human brain learns, is still a very mysterious process.        <br/>
      </p>
      <p>
It's completely unclear today whether the human brain uses an algorithm, does anything like back propagation or gradient descent or if there's some fundamentally different learning principle that the human brain uses?        <br/>
      </p>
      <p>
So, when I think of deep learning, I think of it as being very good at learning very flexible functions, very complex functions to learn X to Y mappings, to learn input-output mappings in supervised learning.        <br/>
      </p>
      <p>
Whereas this is like the brain analogy, maybe that was useful ones.        <br/>
      </p>
      <p>
I think the field has moved to the point where that analogy is breaking down and I tend not to use that analogy much anymore.        <br/>
      </p>
      <p>
So, that's it for neural networks and the brain.        <br/>
      </p>
      <p>
I do think that maybe the few that computer vision has taken a bit more inspiration from the human brain than other disciplines that also apply deep learning, but I personally use the analogy to the human brain less than I used to.        <br/>
      </p>
      <p>
So, that's it for this video.        <br/>
      </p>
      <p>
You now know how to implement forward prop and back prop and gradient descent even for deep neural networks.        <br/>
      </p>
      <p>
Best of luck with the problem exercise, and I look forward to sharing more of these ideas with you in the second course.        <br/>
      </p>
    </div>
  </body>
</html>
