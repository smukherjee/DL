1
00:00:00,000 --> 00:00:04,360
Чому нейронна мережа (НМ) потребує нелінійної функції активації?

2
00:00:04,360 --> 00:00:07,425
Виявляється, що, для того щоб твоя НМ обчислила цікаву функцію,

3
00:00:07,425 --> 00:00:10,335
тобі потрібно використати нелінійну функцію активації.

4
00:00:10,335 --> 00:00:15,830
Давай подивимось чому. 
Ось рівняння прямого поширення для НМ.

5
00:00:15,830 --> 00:00:17,770
Чому б просто не позбутись цього,

6
00:00:17,770 --> 00:00:22,315
просто позбутись функції g і прирівняти a до z?

7
00:00:22,315 --> 00:00:27,690
Або чому б не сказати, що g(z)=z?

8
00:00:27,690 --> 00:00:31,813
Інколи це називають лінійною функцією активації.

9
00:00:31,813 --> 00:00:33,205
Можливо, її краще називати

10
00:00:33,205 --> 00:00:37,800
тотожною функцією активації, 
тому що вона видає те, що приймає на вхід.

11
00:00:37,800 --> 00:00:39,600
І, так само,

12
00:00:39,600 --> 00:00:43,310
що якщо a[2] просто =z[2]?

13
00:00:43,310 --> 00:00:45,183
Виявляється, якщо ми це зробимо,

14
00:00:45,183 --> 00:00:53,620
то ця модель буде обчислювати y або ŷ 
як лінійну функцію від вхідних ознак x.

15
00:00:53,620 --> 00:00:55,940
Візьмемо перших 2 рівняння.

16
00:00:55,940 --> 00:01:04,547
Тут матимемо a[1]=z[1]=w[1]+b[1].

17
00:01:04,547 --> 00:01:15,965
Далі матимемо a[2]=z[2]=w[2]+b[2].

18
00:01:15,965 --> 00:01:25,370
Тепер, якщо візьмемо нове визначення a[1] і підставимо сюди,

19
00:01:25,370 --> 00:01:32,585
то отримаємо, що a[2]=w[2](w[1]x+b[1])+b[2].

20
00:01:32,585 --> 00:01:35,695
Правильно?

21
00:01:35,695 --> 00:01:40,985
Тобто оце - a[1].

22
00:01:40,985 --> 00:01:47,460
І це спрощується до

23
00:01:47,460 --> 00:01:53,120
(w[2]w[1])x+

24
00:01:53,120 --> 00:01:57,966
+(w[2]b[1]+b[2])

25
00:01:57,966 --> 00:02:01,930
А це буде просто...

26
00:02:01,930 --> 00:02:06,726
Давай назвемо оце [головним] w' і [головним] b'.

27
00:02:06,726 --> 00:02:10,935
То це буде просто w'x+b'.

28
00:02:10,935 --> 00:02:13,720
Якби ми використовували лінійні функції активації

29
00:02:13,720 --> 00:02:17,095
або (ми їх також називаємо) тотожні функції активації,

30
00:02:17,095 --> 00:02:23,335
то НМ обчислювала б просто лінійну функцію від вхідних даних.

31
00:02:23,335 --> 00:02:26,260
Ми поговоримо пізніше про глибокі НМ,

32
00:02:26,260 --> 00:02:27,460
НМ з багатьма-

33
00:02:27,460 --> 00:02:29,167
багатьма шарами, багатьма-багатьма прихованими шарами.

34
00:02:29,167 --> 00:02:34,460
То, виявиться, що, при використанні лінійних функцій активації або

35
00:02:34,460 --> 00:02:36,760
взагалі без функцій активації, немає

36
00:02:36,760 --> 00:02:39,250
значення скільки шарів має НМ,

37
00:02:39,250 --> 00:02:42,970
все що вона робитиме - це обчислювати лінійну функцію активації.

38
00:02:42,970 --> 00:02:45,905
Тож можна взагалі обійтись без прихованих шарів,

39
00:02:45,905 --> 00:02:49,335
Як деякі мимохіть згадані випадки.

40
00:02:49,335 --> 00:02:50,880
Виявляється, якщо ми маємо

41
00:02:50,880 --> 00:02:55,170
лінійну функцію активації отут і сигмоїду отут

42
00:02:55,170 --> 00:02:58,275
то така модель не більш виразна ніж

43
00:02:58,275 --> 00:03:02,505
стандартна логістична регресія (ЛР) без прихованих шарів.

44
00:03:02,505 --> 00:03:05,910
Я не набридатиму доведенням, 
але якщо хочеш, то можеш перевірити сам/а.

45
00:03:05,910 --> 00:03:07,465
Варто запам'ятати, що

46
00:03:07,465 --> 00:03:11,265
лінійний прихований шар практично зайвий,

47
00:03:11,265 --> 00:03:17,130
тому що сукупність 2-ох лінійних функцій 
залишається лінійною функцією.

48
00:03:17,130 --> 00:03:19,950
Тож, якщо не додати нелінійності,

49
00:03:19,950 --> 00:03:21,235
то неможливо буде обчислити

50
00:03:21,235 --> 00:03:25,350
більш цікаві функції, 
навіть якщо обчислення проводяться глибоко в НМ.

51
00:03:25,350 --> 00:03:29,820
Є лише один випадок, де підходять лінійні функції активації

52
00:03:29,820 --> 00:03:36,810
(g(z)=z), і це - машинне навчання (МН) для задачі регресії.

53
00:03:36,810 --> 00:03:39,420
Тобто, якщо y - дійсне число.

54
00:03:39,420 --> 00:03:42,675
Наприклад, для передбачення цін нерухомості,

55
00:03:42,675 --> 00:03:46,935
де y - не 0 або 1, а дійсне число

56
00:03:46,935 --> 00:03:54,660
в межах від $0 і до ціни найдорожчого будинку,

57
00:03:54,660 --> 00:03:58,640
скажімо, потенційно мільйони доларів

58
00:03:58,640 --> 00:04:04,580
(в будь-якому випадку, найдорожчий в твоєму наборі даних).

59
00:04:04,580 --> 00:04:09,705
Тож якщо y лежить у цих межах,

60
00:04:09,705 --> 00:04:14,700
то, напевно, підійде і лінійна функція активації. А

61
00:04:14,700 --> 00:04:17,805
твій результат ŷ

62
00:04:17,805 --> 00:04:24,215
теж буде дійсним числом в межах від -нескінченності до +нескінченності.

63
00:04:24,215 --> 00:04:28,700
Проте, в цьому випадку, приховані вузли 
не повинні використовувати лінійні функції активації.

64
00:04:28,700 --> 00:04:34,380
Вони можуть використовувати випрямлений лінійний вузол (ВЛВ), 
гіперболічний тангенс (th), нещільний ВЛВ чи щось інше.

65
00:04:34,380 --> 00:04:39,995
Тож єдине місце, де може бути використана лінійна 
функція активації, - це, зазвичай, вихідний шар.

66
00:04:39,995 --> 00:04:41,595
В іншому місці

67
00:04:41,595 --> 00:04:44,730
використання лінійної функції активації -

68
00:04:44,730 --> 00:04:50,135
в прихованому шарі, окрім деяких дуже специфічних умов,

69
00:04:50,135 --> 00:04:52,320
пов'язаних зі стисненням, яке ми не будемо обговорювати -

70
00:04:52,320 --> 00:04:56,250
зустрічається надзвичайно рідко.

71
00:04:56,250 --> 00:04:59,130
І, звичайно, якщо ми й справді передбачаємо ціну нерухомості, яку

72
00:04:59,130 --> 00:05:03,795
розглядали минулого тижня, то через те, 
що вартість невід'ємна, і в цьому випадку

73
00:05:03,795 --> 00:05:07,060
можна використовувати ВЛВ функцію активації, щоб

74
00:05:07,060 --> 00:05:11,580
результат ŷ був більшим або рівним 0.

75
00:05:11,580 --> 00:05:13,980
Отже, надіюсь, я дав тобі розуміння того, що

76
00:05:13,980 --> 00:05:19,290
нелінійна функція активації - критичний пункт в НМ.

77
00:05:19,290 --> 00:05:23,945
Далі ми почнемо говорити про Градієнтний спуск. А перед цим,

78
00:05:23,945 --> 00:05:27,440
щоб підготуватись до обговорення Градієнтного спуску, 
в наступному відео

79
00:05:27,440 --> 00:05:29,230
я хочу показати як оцінювати,

80
00:05:29,230 --> 00:05:34,105
як обчислювати нахил або похідну окремої функції активації.

81
00:05:34,105 --> 00:05:35,600
Тож давай перейдемо до наступного відео.