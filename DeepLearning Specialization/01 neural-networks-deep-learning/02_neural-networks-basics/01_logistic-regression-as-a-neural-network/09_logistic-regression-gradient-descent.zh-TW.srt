1
00:00:00,000 --> 00:00:02,250
歡迎回來.歡迎回來. 這個影片中

2
00:00:02,250 --> 00:00:04,980
我們將談到有關如何計算導數讓您

3
00:00:04,980 --> 00:00:08,330
建置羅吉斯迴歸分析梯度下降

4
00:00:08,330 --> 00:00:11,040
重點是您需要建立什麼?

5
00:00:11,040 --> 00:00:13,230
也就是您需要的關鍵方程式來

6
00:00:13,230 --> 00:00:17,725
建立羅吉斯迴歸分析梯度下降法

7
00:00:17,725 --> 00:00:22,185
在這段影片中, 我希望使用計算圖形來做計算

8
00:00:22,185 --> 00:00:25,320
我必須承認, 使用計算圖形是有點

9
00:00:25,320 --> 00:00:29,342
殺雞焉用牛刀對於推論羅吉斯迴歸分析的梯度下降法

10
00:00:29,342 --> 00:00:31,183
但我想開始如此解釋

11
00:00:31,183 --> 00:00:33,975
讓您熟悉這種觀念

12
00:00:33,975 --> 00:00:38,370
希望, 當我們談到正式的神經網路時會比較有感覺

13
00:00:38,370 --> 00:00:44,235
讓我們開始進入羅吉斯迴歸分析的梯度下降法

14
00:00:44,235 --> 00:00:49,070
回顧一下，我們已經建立羅吉斯迴歸分析如下

15
00:00:49,070 --> 00:00:53,220
您的預估 y-hat 定義為

16
00:00:53,220 --> 00:00:56,490
而 z 是這項

17
00:00:56,490 --> 00:01:01,800
如果我們只注意在一個例子上, 那這個損失

18
00:01:01,800 --> 00:01:03,630
相對於那個例子是

19
00:01:03,630 --> 00:01:05,190
定義如下

20
00:01:05,190 --> 00:01:07,855
a 是羅吉斯迴歸分析的輸出

21
00:01:07,855 --> 00:01:10,535
y 是真正的標籤值

22
00:01:10,535 --> 00:01:15,735
讓我們為這個例子寫下計算圖形

23
00:01:15,735 --> 00:01:20,520
假設我們只有兩個特徵 x1 跟 x2

24
00:01:20,520 --> 00:01:22,860
為了計算 Z

25
00:01:22,860 --> 00:01:27,030
我們需要輸入 w1

26
00:01:27,030 --> 00:01:31,130
w2 跟 b, 除了特徵 x1, x2 外

27
00:01:31,130 --> 00:01:33,705
這些東西, 在計算圖形中

28
00:01:33,705 --> 00:01:36,910
用來計算 z , 也就是 w1

29
00:01:36,910 --> 00:01:41,588
x1+w2x2+b

30
00:01:41,588 --> 00:01:45,380
用ㄧ個長方形框住

31
00:01:45,380 --> 00:01:48,555
然後我們計算 y-hat

32
00:01:48,555 --> 00:01:52,244
或者說 a = Sigma of z

33
00:01:52,244 --> 00:01:55,740
這是在這計算圖形地下一步, 最後

34
00:01:55,740 --> 00:01:58,725
我們計算 L(a,y)

35
00:01:58,725 --> 00:02:01,840
我不再複製這個公式

36
00:02:01,840 --> 00:02:06,900
在羅吉斯迴歸分析, 我們要做的是修改這些參數

37
00:02:06,900 --> 00:02:12,830
w 跟 b 為了減少損失

38
00:02:12,830 --> 00:02:15,870
我們已經描述了正向傳播的步驟如何來實際

39
00:02:15,870 --> 00:02:19,280
計算單一訓練例子的損失

40
00:02:19,280 --> 00:02:23,940
現在讓我們談談如何使用反向傳播來計算導數

41
00:02:23,940 --> 00:02:26,025
這個是乾淨的版本

42
00:02:26,025 --> 00:02:30,690
因為我們要計算的是相對於這個損失的導數

43
00:02:30,690 --> 00:02:33,570
第一件事我們要做的是反向來

44
00:02:33,570 --> 00:02:38,010
計算這個損失的導數相對於

45
00:02:38,010 --> 00:02:41,940
花體的 L, 對於變數 a

46
00:02:41,940 --> 00:02:43,570
在程式裡

47
00:02:43,570 --> 00:02:49,000
您只用 da 來記這個變數

48
00:02:49,000 --> 00:02:52,725
如果您熟悉微積分

49
00:02:52,725 --> 00:03:02,004
您可以知道這是 - y/a  + 1-y/1-a

50
00:03:02,004 --> 00:03:06,185
而您要做的是您拿損失函數的公式

51
00:03:06,185 --> 00:03:07,535
如果您熟悉微積分

52
00:03:07,535 --> 00:03:10,515
您可以計算相對於這個變數的導數

53
00:03:10,515 --> 00:03:12,792
小寫 a, 您就會得到這個公式

54
00:03:12,792 --> 00:03:15,280
但如果你不熟悉微積分，別擔心

55
00:03:15,280 --> 00:03:17,960
我們會提供導數公式

56
00:03:17,960 --> 00:03:20,100
您所需要的, 在整個課程裡

57
00:03:20,100 --> 00:03:21,185
如果您是微積分的專家

58
00:03:21,185 --> 00:03:24,590
我鼓勵您看這前面投影片的損失函數的公式

59
00:03:24,590 --> 00:03:29,504
使用微積分來算相對於 a 的導數

60
00:03:29,504 --> 00:03:32,635
但如果您不熟悉微積分, 也不用擔心

61
00:03:32,635 --> 00:03:35,491
現在已經計算了這個項目 da 

62
00:03:35,491 --> 00:03:38,825
您最後輸出變數相對於 a 的導數

63
00:03:38,825 --> 00:03:40,715
您可以反向

64
00:03:40,715 --> 00:03:45,525
事實上您可以證明 dz

65
00:03:45,525 --> 00:03:47,648
這是 python 用的變數名稱

66
00:03:47,648 --> 00:03:51,200
這將是損失函數的導數

67
00:03:51,200 --> 00:03:53,618
相對於 z, 對於 L 函數

68
00:03:53,618 --> 00:03:59,850
您也可以這樣寫包含 a 跟 y 明確的作為參數, 是吧?

69
00:03:59,850 --> 00:04:04,230
兩種方式的寫法都可接受

70
00:04:04,230 --> 00:04:09,605
我們可以證明這個是 a - y

71
00:04:09,605 --> 00:04:14,685
對於微積分的專家們這裡有一些註解

72
00:04:14,685 --> 00:04:16,795
如果您不熟悉微積分, 不用擔心

73
00:04:16,795 --> 00:04:20,320
但實際上這個 dL/dz

74
00:04:20,320 --> 00:04:27,850
這可以表示成 dL/da 乘 da/dz

75
00:04:27,850 --> 00:04:29,940
而這個 da/dz 

76
00:04:29,940 --> 00:04:33,755
實際上是 a 乘 (1-a)

77
00:04:33,755 --> 00:04:37,800
而 dL/da 前面我們計算過

78
00:04:37,800 --> 00:04:41,530
如果您拿這兩個項目 dL/da

79
00:04:41,530 --> 00:04:43,846
也就是這個項目, 跟 da/dz 一起

80
00:04:43,846 --> 00:04:47,165
也就是這個項目, 拿這兩項相乘

81
00:04:47,165 --> 00:04:51,915
您可以證明這個方程式變成 a - y

82
00:04:51,915 --> 00:04:53,220
這就是您如何推導出這個項目

83
00:04:53,220 --> 00:04:57,390
而這實際上是應用連鎖律來導出

84
00:04:57,390 --> 00:05:02,770
如果您有足夠的微積分知識, 歡迎用微積分自己導出

85
00:05:02,770 --> 00:05:05,345
但如果您不是, 您只需知道您可以計算

86
00:05:05,345 --> 00:05:09,365
dz 是 a - y, 我們為您用微積分算出

87
00:05:09,365 --> 00:05:13,010
最後計算的步驟是返回

88
00:05:13,010 --> 00:05:17,480
計算您需要改變多少 w 跟 b

89
00:05:17,480 --> 00:05:24,610
尤其是, 您可以證明對於 w1 的微分, 在程式中

90
00:05:24,610 --> 00:05:31,810
稱為 dw1, 這是等於 x1 乘 dz

91
00:05:31,810 --> 00:05:36,485
同樣地，dw2, 也就是您想改變多少 w2

92
00:05:36,485 --> 00:05:39,455
是 x2 乘 dz, 而  b

93
00:05:39,455 --> 00:05:42,585
是 db  等於 dz

94
00:05:42,585 --> 00:05:47,375
如果您要做梯度下降相對於這個例子

95
00:05:47,375 --> 00:05:49,280
您要做的是如下

96
00:05:49,280 --> 00:05:52,640
您會用這個公式計算 dz

97
00:05:52,640 --> 00:05:56,707
然後用這個公式計算 dw1, dw2

98
00:05:56,707 --> 00:06:01,170
跟 db, 然後進行更新

99
00:06:01,170 --> 00:06:04,538
w1 更新為 w1 減

100
00:06:04,538 --> 00:06:06,575
學習率 alpha 乘 dw1

101
00:06:06,575 --> 00:06:09,245
w2 也用同樣方式更新

102
00:06:09,245 --> 00:06:14,170
而 b 更新為 b 減學習率乘 db

103
00:06:14,170 --> 00:06:18,860
所以這是梯度下降法的一個遞迴相對於單一例子

104
00:06:18,860 --> 00:06:22,130
您看到了如何計算導數及建立

105
00:06:22,130 --> 00:06:27,200
羅吉斯迴歸分析梯度下降法對於單一訓練例子

106
00:06:27,200 --> 00:06:28,987
但訓練羅吉斯迴歸分析模型

107
00:06:28,987 --> 00:06:34,700
您不只用一的訓練例子, 您的訓練集有 m 個例子

108
00:06:34,700 --> 00:06:36,120
在下一個影片中

109
00:06:36,120 --> 00:06:39,350
讓我們看您如何用這個觀念應用到

110
00:06:39,350 --> 00:06:40,760
不只一個例子

111
00:06:40,760 --> 00:06:42,400
但是在整個訓練集