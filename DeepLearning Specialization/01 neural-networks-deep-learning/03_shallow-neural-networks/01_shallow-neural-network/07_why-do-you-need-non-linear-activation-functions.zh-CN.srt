1
00:00:00,000 --> 00:00:04,360
为什么神经网络需要使用非线性激活函数

2
00:00:04,360 --> 00:00:07,425
事实上 想要让神经网络 实现一些有趣的功能

3
00:00:07,425 --> 00:00:10,335
非线性激活函数是不可或缺的

4
00:00:10,335 --> 00:00:15,830
我们来看下原因 这是神经网络前向传播的方程

5
00:00:15,830 --> 00:00:17,770
为什么我们不能把这个去掉？

6
00:00:17,770 --> 00:00:22,315
去掉函数g 并让a[1]等于z[1]

7
00:00:22,315 --> 00:00:27,690
或者说g(z)等于z

8
00:00:27,690 --> 00:00:31,813
通常 我们把这称为线性激活函数

9
00:00:31,813 --> 00:00:33,205
有时候我们也会用专业性更强的名称

10
00:00:33,205 --> 00:00:37,800
恒等激活函数 因为它直接将输入的值输出

11
00:00:37,800 --> 00:00:39,600
为了说明问题

12
00:00:39,600 --> 00:00:43,310
我们一起来看看如果a2等于z2会怎样

13
00:00:43,310 --> 00:00:45,183
假如这么做

14
00:00:45,183 --> 00:00:53,620
那么这个模型计算的仅仅是输入特征x的线性函数y或y帽

15
00:00:53,620 --> 00:00:55,940
根据前两个等式

16
00:00:55,940 --> 00:01:04,547
如果令a[1]等于z[1]等于W[1]*x+b[1]

17
00:01:04,547 --> 00:01:15,965
并令a[2]等于z[2]等于W[2]*a[1]+b[2]

18
00:01:15,965 --> 00:01:25,370
如果把a[1]的定义代入

19
00:01:25,370 --> 00:01:32,585
你就会发现a[2]等于

20
00:01:32,585 --> 00:01:35,695
W[2]*(W[1]*x+b[1])+b[2]

21
00:01:35,695 --> 00:01:40,985
这一部分是a[1]+b[2]

22
00:01:40,985 --> 00:01:47,460
简化一下就是

23
00:01:47,460 --> 00:01:53,120
W[2]*W[1]*x 加上

24
00:01:53,120 --> 00:01:57,966
W[2]*b[1]+b[2]

25
00:01:57,966 --> 00:02:01,930
然后

26
00:02:01,930 --> 00:02:06,726
让我们把这两项标记为W' b'

27
00:02:06,726 --> 00:02:10,935
表达式就等于W'*x+b'

28
00:02:10,935 --> 00:02:13,720
如果你使用线性激活函数

29
00:02:13,720 --> 00:02:17,095
或者叫恒等激活函数

30
00:02:17,095 --> 00:02:23,335
那么神经网络的输出 仅仅是输入函数的线性变化

31
00:02:23,335 --> 00:02:26,260
我们之后会讲到深度网络

32
00:02:26,260 --> 00:02:27,460
也就是有很多层

33
00:02:27,460 --> 00:02:29,167
很多隐藏层的神经网络

34
00:02:29,167 --> 00:02:34,460
以上推导证明 如果你使用线性激活函数

35
00:02:34,460 --> 00:02:36,760
或者说 没有使用激活函数

36
00:02:36,760 --> 00:02:39,250
那么无论你的神经网络有多少层

37
00:02:39,250 --> 00:02:42,970
它所做的仅仅是计算线性激活函数

38
00:02:42,970 --> 00:02:45,905
这还不如去除所有隐藏层

39
00:02:45,905 --> 00:02:49,335
在上述例子中

40
00:02:49,335 --> 00:02:50,880
如果

41
00:02:50,880 --> 00:02:55,170
在这里使用线性激活函数 在这里使用sigmoid激活函数

42
00:02:55,170 --> 00:02:58,275
那么这个模型其实和没有任何隐藏层的

43
00:02:58,275 --> 00:03:02,505
逻辑回归模型相同

44
00:03:02,505 --> 00:03:05,910
我不会在这里证明 如果你有时间 可以自己尝试

45
00:03:05,910 --> 00:03:07,465
但请记得

46
00:03:07,465 --> 00:03:11,265
线性的隐藏层没有任何用处

47
00:03:11,265 --> 00:03:17,130
因为两个线性函数的组合 仍然是线性函数

48
00:03:17,130 --> 00:03:19,950
除非你在这里引入非线性函数

49
00:03:19,950 --> 00:03:21,235
否则无论神经网络模型包含多少隐藏层

50
00:03:21,235 --> 00:03:25,350
都无法实现更有趣的功能

51
00:03:25,350 --> 00:03:29,820
只有一个地方会使用线性激活函数

52
00:03:29,820 --> 00:03:36,810
当g(z)等于z 就是使用机器学习解决回归问题的时候

53
00:03:36,810 --> 00:03:39,420
y是一个实数

54
00:03:39,420 --> 00:03:42,675
比如你想预测房价

55
00:03:42,675 --> 00:03:46,935
y不是0或1 而是一个实数

56
00:03:46,935 --> 00:03:54,660
从0美元到可能存在的最高价格 任何值都有可能

57
00:03:54,660 --> 00:03:58,640
也许要几百万美元

58
00:03:58,640 --> 00:04:04,580
但无论数据集中的房价是多少

59
00:04:04,580 --> 00:04:09,705
只要y是实数

60
00:04:09,705 --> 00:04:14,700
那么在这里使用线性激活函数就没问题

61
00:04:14,700 --> 00:04:17,805
这样的预测输出y帽

62
00:04:17,805 --> 00:04:24,215
也是从负无穷到正无穷的任意实数

63
00:04:24,215 --> 00:04:28,700
但此时 隐藏单元不应该使用线性激活函数

64
00:04:28,700 --> 00:04:34,380
可以使用ReLU或者Tanh 或者leaky ReLU等

65
00:04:34,380 --> 00:04:39,995
所以唯一可以使用线性激活函数的地方 通常就是输出层

66
00:04:39,995 --> 00:04:41,595
除此之外

67
00:04:41,595 --> 00:04:44,730
只有在一些极为特殊的情况下

68
00:04:44,730 --> 00:04:50,135
才会在隐藏层使用线性激活函数

69
00:04:50,135 --> 00:04:52,320
比如与压缩有关的处理 我们就不进行深入讨论了

70
00:04:52,320 --> 00:04:56,250
此外 使用线性激活函数的情况极为少见

71
00:04:56,250 --> 00:04:59,130
当然 如果你要预测房价

72
00:04:59,130 --> 00:05:03,795
例如在第一周视频中看到的 因为房价是非负数

73
00:05:03,795 --> 00:05:07,060
这种情况下也可以使用ReLU激活函数

74
00:05:07,060 --> 00:05:11,580
使你输出的y帽都大于等于0

75
00:05:11,580 --> 00:05:13,980
希望以上这些已经让你了解

76
00:05:13,980 --> 00:05:19,290
为什么使用非线性激活函数 对神经网络至关重要

77
00:05:19,290 --> 00:05:23,945
下期预告 我们将会讨论梯度下降

78
00:05:23,945 --> 00:05:27,440
为了先让大家熟悉梯度下降的基础 在下个视频中

79
00:05:27,440 --> 00:05:29,230
我会展示如何估计以及

80
00:05:29,230 --> 00:05:34,105
如何计算单个激活函数的斜率或者导数

81
00:05:34,105 --> 00:05:35,600
下个视频见<br />翻译 | 审阅：Cousera Global Translator Community