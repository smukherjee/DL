1
00:00:00,000 --> 00:00:04,360
여러분의 신경망이 비선형 activation 
함수가 필요한 것일까요?

2
00:00:04,360 --> 00:00:07,425
알고 보니, 신경망이 흥미로운 함수들을 산출하기 위해서는

3
00:00:07,425 --> 00:00:10,335
비선형 activation 함수를 골라야 합니다.

4
00:00:10,335 --> 00:00:15,830
왜 그런지 한번 보겠습니다. 여기서 
보면 신경망의 전 방향전파 공식인데요,

5
00:00:15,830 --> 00:00:17,770
여기 이 부분을 빼도록 하죠

6
00:00:17,770 --> 00:00:22,315
g의 함수를 빼겠습니다. 
그리고 a1을 z1으로 지정하겠습니다.

7
00:00:22,315 --> 00:00:27,690
또는 g(z)가 z와 동일하다고 표현해도 됩니다.

8
00:00:27,690 --> 00:00:31,813
가끔씩은 이것이 선형 activation 
함수라고 표현하는데요,

9
00:00:31,813 --> 00:00:33,205
더 나은 이름은 identity activation 함수일 것입니다. 
이유는 이것들은 단지 입력값이 들어간 그대로 결과값이 나오기 때문입니다.

10
00:00:33,205 --> 00:00:37,800
더 나은 이름은 identity activation 함수일 것입니다. 
이유는 이것들은 단지 입력값이 들어간 그대로 결과값이 나오기 때문입니다.

11
00:00:37,800 --> 00:00:39,600
여기 이것의 목적을 위해서,

12
00:00:39,600 --> 00:00:43,310
만약 a2가 여기서 z2가되면 어떻게 될까요?

13
00:00:43,310 --> 00:00:45,183
이렇게 하면 이 모델은

14
00:00:45,183 --> 00:00:53,620
단지 y 와 ŷ을 선형함수로 계산합니다. 
입력값 x에 대해서 말이죠.

15
00:00:53,620 --> 00:00:55,940
첫번째 2개의 공식을 가지고 오면,

16
00:00:55,940 --> 00:01:04,547
a1=z1=w1 곱하기

17
00:01:04,547 --> 00:01:15,965
x 더하기 b1인데요, 그리고 a2=z2=w2

18
00:01:15,965 --> 00:01:25,370
a1 더하기 b2인데요 여기서 a1의 
정의를 가지고 여기에 대입시키면,

19
00:01:25,370 --> 00:01:32,585
a2=w2 (w1 x + b1)

20
00:01:32,585 --> 00:01:35,695
a2=w2 (w1 x + b1)

21
00:01:35,695 --> 00:01:40,985
이 부분은 a1이 구요

22
00:01:40,985 --> 00:01:47,460
그리고 더하기 b2 인데요

23
00:01:47,460 --> 00:01:53,120
그러므로 이것은 w2 w1 X 더하기 W2

24
00:01:53,120 --> 00:01:57,966
b1 더하기 b2 인데요,

25
00:01:57,966 --> 00:02:01,930
이것은 그러면 w 프라임 그리고

26
00:02:01,930 --> 00:02:06,726
b 프라임이라 하겠습니다.

27
00:02:06,726 --> 00:02:10,935
그러면 이 공식은 w 프라임 x 더하기 b 프라임이 됩니다.

28
00:02:10,935 --> 00:02:13,720
여러분이 선형 activation 함수를 사용한다고 하거나,

29
00:02:13,720 --> 00:02:17,095
identity activation 함수를 쓴다고 하면,

30
00:02:17,095 --> 00:02:23,335
그러면 신경망은 선형함수에서 
입력값에 대한 결과값을 주게 되는 것입니다.

31
00:02:23,335 --> 00:02:26,260
나중에 많은 층으로 이루어진 
심츠신경망에 대해서 이야기 하겠습니다.

32
00:02:26,260 --> 00:02:27,460
나중에 많은 층으로 이루어진 
심츠신경망에 대해서 이야기 하겠습니다.

33
00:02:27,460 --> 00:02:29,167
나중에 많은 층으로 이루어진 
심츠신경망에 대해서 이야기 하겠습니다.

34
00:02:29,167 --> 00:02:34,460
만약 여러분이 선현 activation 함수를 쓰는 경우,

35
00:02:34,460 --> 00:02:36,760
또는 activation 함수가 없는 경우에는,

36
00:02:36,760 --> 00:02:39,250
신경망이 몇개의 층으로 이루어 졌다고 하더라도,

37
00:02:39,250 --> 00:02:42,970
이것이 하는 것은 단순히 
선형 activation 함수를 산출하는 것입니다.

38
00:02:42,970 --> 00:02:45,905
그렇기 때문에 이왕이면 
숨겨진 레이어가 없는 것이 좋겠죠.

39
00:02:45,905 --> 00:02:49,335
몇개의 케이스를 제가 말한 것중에,

40
00:02:49,335 --> 00:02:50,880
여기 선형 activation 함수가 있는데,

41
00:02:50,880 --> 00:02:55,170
그리고 여시 시그모이드 함수가 있는경우

42
00:02:55,170 --> 00:02:58,275
이 모델은 더이상 표현적이지 않습니다.

43
00:02:58,275 --> 00:03:02,505
숨겨진 레이어 가 없는 일반 
로지스틱 회귀 분석과 비교했을 때 말이죠,

44
00:03:02,505 --> 00:03:05,910
그것을 구지 증명하지는 않겠습니다.
여러분께서 나중에 증명은 혼자 진행하셔도 됩니다.

45
00:03:05,910 --> 00:03:07,465
중요한 부분은

46
00:03:07,465 --> 00:03:11,265
선형에서의 숨겨진 레이어는 
거의 쓸모가 없다는 것입니다.

47
00:03:11,265 --> 00:03:17,130
왜냐면 2개의 선형 함수의 구성요소는 
그 자체가 선형함수이기 때문입니다.

48
00:03:17,130 --> 00:03:19,950
그러므로 비선형 특성을 기입하지 않는 이상은

49
00:03:19,950 --> 00:03:21,235
더 흥미로운 함수를 산출하는 것이 아닙니다.

50
00:03:21,235 --> 00:03:25,350
신경망이 더 깊어질 때 말이죠.

51
00:03:25,350 --> 00:03:29,820
이런 경우, 선형 함수를 쓰는 
한가지의 경우가 있기는 한데요

52
00:03:29,820 --> 00:03:36,810
g(z)=z 의 경우입니다. 이 같은 경우는 머신러닝에서

53
00:03:36,810 --> 00:03:39,420
그럼 만약 Y가 실수인 경우

54
00:03:39,420 --> 00:03:42,675
예를 들어, 여러분이 집값을 예측하려고 한다면

55
00:03:42,675 --> 00:03:46,935
Y는 0 1 이 아니라 실수가 될 것입니다.

56
00:03:46,935 --> 00:03:54,660
0달러에서 집 값이 얼마이던지 
그 값만큼의 범위를 가질 것입니다.

57
00:03:54,660 --> 00:03:58,640
집은 사실 잠재적으로 
몇십억까지 가격이 올라갈 수 있겠죠.

58
00:03:58,640 --> 00:04:04,580
그 값이 얼마던 Y값에 그 값만큼 
데이터세트로 반영될 것입니다.

59
00:04:04,580 --> 00:04:09,705
그러나 만약 Y가 이러한 실수를 갖고, 이런 경우는

60
00:04:09,705 --> 00:04:14,700
사실 선형함수를 가져도 괜찮을 수 있습니다.

61
00:04:14,700 --> 00:04:17,805
그러면 결과값 ŷ은 또한

62
00:04:17,805 --> 00:04:24,215
마이너스 무한대에서 플러스 무한대의 값을 범위로 할텐데요

63
00:04:24,215 --> 00:04:28,700
이 경우, 숨겨진 유닛은 새로운 
activation 함수를 쓰면 안됩니다.

64
00:04:28,700 --> 00:04:34,380
숨겨진 유닛은 ReLu나 tan h Leaky ReLu를 
쓸수도 있거나 다른 것들을 쓸수도 있겠죠.

65
00:04:34,380 --> 00:04:39,995
그래서 선형함수를 쓰는 
경우는 보통 결과값 층에서 입니다.

66
00:04:39,995 --> 00:04:41,595
하지만, 그 이외의

67
00:04:41,595 --> 00:04:44,730
선형 activation 함수를 숨겨진 레이어에서 쓰는 것은

68
00:04:44,730 --> 00:04:50,135
아주 compression 과 같은 아주 특별한 경우를 제외하고는

69
00:04:50,135 --> 00:04:52,320
이 경우에 대해서는 직접 다루진 않겠지만

70
00:04:52,320 --> 00:04:56,250
그 외에는 선형 activation 함수를 
쓰는 것은 매우 드문 일입니다.

71
00:04:56,250 --> 00:04:59,130
그리고 당연히 오늘날 집값을 예측하는 것은

72
00:04:59,130 --> 00:05:03,795
첫째주에서 봤듯이, 집값이 모두 양수의 값이기 때문에,

73
00:05:03,795 --> 00:05:07,060
이 경우에도 ReLu 함수를 사용할 수 있을 것입니다.

74
00:05:07,060 --> 00:05:11,580
ŷ 결과값이 0 또는 더 큰 값이 되도록 말이죠.

75
00:05:11,580 --> 00:05:13,980
이 강의가 여러분이 왜 비선형 함수를

76
00:05:13,980 --> 00:05:19,290
신경망에서 반드시 가져야 하는지 설명해줬기 바랍니다.

77
00:05:19,290 --> 00:05:23,945
다음으로는 기울기 강하에 대해 이야기할 것인데요,

78
00:05:23,945 --> 00:05:27,440
또 기울기 강하와 관련하여 
토론을하기 위해서는 다음 비디오에서

79
00:05:27,440 --> 00:05:29,230
여러분에게 함수의 slope를 구하는 방법이나

80
00:05:29,230 --> 00:05:34,105
또는 개인 activation 함수에 대한 
derivative를 구하는 방법을 알려드리겠습니다.

81
00:05:34,105 --> 00:05:35,600
그러면 다음 비디오로 넘어가겠습니다.