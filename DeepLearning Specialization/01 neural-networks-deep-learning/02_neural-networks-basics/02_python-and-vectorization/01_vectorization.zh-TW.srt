1
00:00:00,000 --> 00:00:03,195
歡迎回來, 向量化基本上是

2
00:00:03,195 --> 00:00:07,315
一種去掉明顯迴圈的技術在您的程式中

3
00:00:07,315 --> 00:00:11,835
在深度學習時代, 安全的深度學習實踐時

4
00:00:11,835 --> 00:00:15,210
您通常發現您自己訓練到一個相對大的資料集

5
00:00:15,210 --> 00:00:18,475
因為這是使用深度學習演算法發光發亮的時候

6
00:00:18,475 --> 00:00:22,790
所以很重要的是您的程式很快速, 要不然

7
00:00:22,790 --> 00:00:24,525
如果您跑在一個大的資料集

8
00:00:24,525 --> 00:00:27,000
您的程式會花很久的時間來跑而您

9
00:00:27,000 --> 00:00:30,255
會等很長的時間來獲得結果

10
00:00:30,255 --> 00:00:32,035
所以在深度學習時代

11
00:00:32,035 --> 00:00:37,490
我想能夠進行向量化變成一個重要的技術

12
00:00:37,490 --> 00:00:40,010
讓我們先從一個例子開始

13
00:00:40,010 --> 00:00:42,225
所以, 什麼是向量化

14
00:00:42,225 --> 00:00:48,780
在羅吉斯迴歸分析您需要計算 z 等於 w 轉置 x 加 b

15
00:00:48,780 --> 00:00:55,405
而 w 是這個欄向量而 x 是這個向量

16
00:00:55,405 --> 00:00:58,000
也許這是一個很大的向量如果您有很多特徵

17
00:00:58,000 --> 00:01:07,080
所以, w 跟 x 都是 R n 或者 R nx 維度向量

18
00:01:07,080 --> 00:01:10,170
所以要計算 w轉置x

19
00:01:10,170 --> 00:01:15,660
如果您使用非向量化建置

20
00:01:15,660 --> 00:01:18,725
您會用像類似這樣, z 等於 0

21
00:01:18,725 --> 00:01:24,860
然後 對於 i in range of nx

22
00:01:24,860 --> 00:01:27,330
所以, 對於 i 等於 1, 2 到 nx

23
00:01:27,330 --> 00:01:34,040
z 加等於 w i 乘 x i

24
00:01:34,040 --> 00:01:37,100
然後也許您做 z 加等於 b 在最後

25
00:01:37,100 --> 00:01:39,855
所以這是非向量化的做法

26
00:01:39,855 --> 00:01:43,090
然後您會發現這樣做會很慢

27
00:01:43,090 --> 00:01:48,560
相對的, 向量化建置會只直接計算 w轉置x

28
00:01:48,560 --> 00:01:52,085
在 Python 或者說 numpy

29
00:01:52,085 --> 00:02:01,428
您用的指令是 z 等於 np.dot(w,

30
00:02:01,428 --> 00:02:06,270
x), 所以這是計算 w轉置x

31
00:02:06,270 --> 00:02:09,075
而您可以直接加 b

32
00:02:09,075 --> 00:02:12,400
您會發現這快多了

33
00:02:12,400 --> 00:02:17,075
讓我們實際上演練一下

34
00:02:17,075 --> 00:02:21,960
這個是我的 jupyter notebook, 我將使用他來寫 python 程式

35
00:02:21,960 --> 00:02:28,041
首先我們先導入 numpy 函式庫

36
00:02:28,041 --> 00:02:30,000
np 舉個例子

37
00:02:30,000 --> 00:02:36,570
我可以建立一個矩陣 A 像這樣

38
00:02:36,570 --> 00:02:39,560
讓我們印 a

39
00:02:39,560 --> 00:02:41,160
現在有了這一段程式

40
00:02:41,160 --> 00:02:43,170
如果我按 shift enter

41
00:02:43,170 --> 00:02:44,847
它會執行這對程式

42
00:02:44,847 --> 00:02:47,970
所以它會建立一個矩陣 a 然後印出

43
00:02:47,970 --> 00:02:50,580
現在讓我們做向量化的展示

44
00:02:50,580 --> 00:02:51,990
我將導入 time 函式庫

45
00:02:51,990 --> 00:02:53,580
因為我們即將使用到

46
00:02:53,580 --> 00:02:56,565
為了要算不同的運算的時間

47
00:02:56,565 --> 00:02:59,139
我們先建立一個矩陣 a

48
00:02:59,139 --> 00:03:02,905
等於 random.rand

49
00:03:02,905 --> 00:03:10,065
這會用亂數建立一個百萬維度的矩陣

50
00:03:10,065 --> 00:03:13,300
b = np.random.rand

51
00:03:13,300 --> 00:03:16,120
另一個百萬維度的矩陣

52
00:03:16,120 --> 00:03:20,810
現在 tic = time.time(), 所以這是用來測量現在的時刻

53
00:03:20,810 --> 00:03:26,395
c = np.dot(a, b)

54
00:03:26,395 --> 00:03:28,649
toc = time.time()

55
00:03:28,649 --> 00:03:31,950
然後印出

56
00:03:31,950 --> 00:03:34,857
這個是向量化版本

57
00:03:34,857 --> 00:03:37,685
是向量化版本

58
00:03:37,685 --> 00:03:41,985
我們將它印出來

59
00:03:41,985 --> 00:03:45,060
讓我看這是後面的時間

60
00:03:45,060 --> 00:03:48,330
所以是 toc - tic 乘 1000

61
00:03:48,330 --> 00:03:52,075
所以我們可以表示成 毫秒

62
00:03:52,075 --> 00:03:54,075
所以 ms 是毫秒

63
00:03:54,075 --> 00:03:56,435
我將按 shift Enter

64
00:03:56,435 --> 00:04:01,890
所以這個程式花了 3 毫秒, 也許這次 1.5 毫秒

65
00:04:01,890 --> 00:04:06,170
也許 1.5 或者 3.5 毫秒

66
00:04:06,170 --> 00:04:08,370
它每次跑都差一點點

67
00:04:08,370 --> 00:04:12,085
但似乎也許平均大約是 1.5 毫秒

68
00:04:12,085 --> 00:04:15,665
也許是 2 毫秒當我跑這次

69
00:04:15,665 --> 00:04:16,967
好吧。好吧。

70
00:04:16,967 --> 00:04:19,005
讓我們繼續加入一些程式

71
00:04:19,005 --> 00:04:22,270
讓我們建置一個非向量化的版本

72
00:04:22,270 --> 00:04:24,151
讓我用, c = 0

73
00:04:24,151 --> 00:04:27,750
然後 tic = time.time()

74
00:04:27,750 --> 00:04:29,335
現在讓我們建立一個迴圈

75
00:04:29,335 --> 00:04:35,348
for i in range of 一百萬

76
00:04:35,348 --> 00:04:38,676
我要把零的數目弄對

77
00:04:38,676 --> 00:04:43,936
c += a i 乘 b

78
00:04:43,936 --> 00:04:50,775
i, 然後 toc = time.time()

79
00:04:50,775 --> 00:04:57,725
最後印出 for loop

80
00:04:57,725 --> 00:05:15,225
總共花費時間是 1000 乘 toc - tic + "ms"

81
00:05:15,225 --> 00:05:17,505
讓自己知道我們用毫秒

82
00:05:17,505 --> 00:05:19,735
讓我們再做一件事

83
00:05:19,735 --> 00:05:22,802
讓我們印出 計算出來 c 的值

84
00:05:22,802 --> 00:05:27,960
確定它們都是一致的在兩種情況下

85
00:05:27,960 --> 00:05:35,770
我將按 shift enter 跑這段程式跟檢查結果

86
00:05:35,770 --> 00:05:38,305
在兩種情況下, 向量化版本

87
00:05:38,305 --> 00:05:41,125
跟非向量化版本計算出同一個值

88
00:05:41,125 --> 00:05:45,355
您看到 250286.99 等等

89
00:05:45,355 --> 00:05:48,670
向量化版本花了 1.5 毫秒

90
00:05:48,670 --> 00:05:57,555
而明顯的迴圈跟非向量化版本花了約 400 將近 500 毫秒

91
00:05:57,555 --> 00:06:01,285
而非向量化版本花了約 300 倍

92
00:06:01,285 --> 00:06:05,660
長比起向量化版本

93
00:06:05,660 --> 00:06:11,230
在這個例子您看到如果您只記得向量化您的程式

94
00:06:11,230 --> 00:06:15,120
您的程式跑大約 300 倍快速

95
00:06:15,120 --> 00:06:16,540
讓我們再跑一次

96
00:06:16,540 --> 00:06:18,930
再跑一次

97
00:06:18,930 --> 00:06:22,235
向量化版本 1.5 毫秒而迴圈

98
00:06:22,235 --> 00:06:25,960
花了 481 毫秒, 再次

99
00:06:25,960 --> 00:06:29,535
約 300 倍慢使用明顯的迴圈

100
00:06:29,535 --> 00:06:30,980
如果整個慢了下來

101
00:06:30,980 --> 00:06:33,880
兩個程式的差別一個會是跑一分鐘

102
00:06:33,880 --> 00:06:37,615
一個則會跑五個小時

103
00:06:37,615 --> 00:06:41,410
而當您要建置深度學習算法時

104
00:06:41,410 --> 00:06:43,300
您真的可以比較快得到結果

105
00:06:43,300 --> 00:06:46,590
如果您向量化您的程式會快很多

106
00:06:46,590 --> 00:06:49,300
您們一些人也許聽過很多

107
00:06:49,300 --> 00:06:54,260
可擴展的深度學習建置在 GPU 或圖形處理單元

108
00:06:54,260 --> 00:06:59,515
但所有我做的這些展示在 jupyter notebook 都實際上是在 CPU 上

109
00:06:59,515 --> 00:07:04,530
實際上 GPU 跟 CPU 都有平行指令

110
00:07:04,530 --> 00:07:07,530
有時候稱為 SIMD 指令

111
00:07:07,530 --> 00:07:11,190
是 單一指令多資料 (Single Instruction Multiple Data) 的簡寫

112
00:07:11,190 --> 00:07:13,045
但基本上這個意思是

113
00:07:13,045 --> 00:07:16,835
如果您使用內建函數像這樣

114
00:07:16,835 --> 00:07:23,495
np.function 或者其他函數不需要您明顯的建立一個迴圈

115
00:07:23,495 --> 00:07:28,150
它使得 Python numpy 

116
00:07:28,150 --> 00:07:33,640
利用平行處理使得計算快很多

117
00:07:33,640 --> 00:07:38,610
而這在 CPU 的計算跟 GPU 的計算都是相同的

118
00:07:38,610 --> 00:07:41,070
就只是 GPU 是顯著擅長在

119
00:07:41,070 --> 00:07:44,980
SIMD 計算但 CPU 實際上做得也不差

120
00:07:44,980 --> 00:07:47,510
也許不像 GPU 那麼好

121
00:07:47,510 --> 00:07:51,660
您見過了向量化如何顯著的加快您的程式

122
00:07:51,660 --> 00:07:54,685
重要的法則是記得當可能時

123
00:07:54,685 --> 00:07:57,425
盡量避免明顯的迴圈

124
00:07:57,425 --> 00:07:59,980
讓我們繼續到下一段影片看更多的

125
00:07:59,980 --> 00:08:04,000
向量化例子同時開始做向量化羅吉斯迴歸分析