यदि मूल तकनीकी विचार डीप लर्निंग के पीछे, न्यूरल नेटवर्क्स के पीछे, हैं कई दशकों से, तो वे अब क्यों प्रचलित हो रहे हैं? इस वीडियो में, चलो देखते हैं कुछ मुख्य घटक डीप लर्निंग की प्रसिद्धि के पीछे 
क्योंकि मैं सोचता हूँ यह मदद करेगा आपकी जानने के लिए बढ़िया अवसर आपके अपने संगठन में इन्हें अप्लाई करने के लिए. पिछले कुछ सालों में, बहुत से लोगों ने मुझसे पूछा है, "एंड्रयू, क्यों डीप लर्निंग निश्चित रूप से इतनी अच्छी तरह 
से काम कर रहा है?" जब मुझसे पूछा जाता है वह सवाल,
 मैं आम तौर पर यह चित्र उनके लिए बनाता हूँ. मान लो हम प्लॉट करते हैं एक चित्र 
जहाँ हॉरिज़ॉंटल ऐक्सिस पर प्लॉट करते हैं डेटा की मात्रा जो हमारे पास है 
एक टास्क के लिए. मान लो वर्टिकल ऐक्सिस पर, प्लॉट करते हैं हमारे लर्निंग अल्गोरिद्म की पर्फ़ॉर्मन्स. जैसे एक्यूरेसी हमारे स्पैम क्लैसिफ़ायअर की या हमारे ऐड क्लिक प्रिडिक्टर की या एक्यूरेसी हमारे न्यूरल नेट की समझने के लिए स्थिति 
अन्य कारों की हमारी सेल्फ़-ड्राइविंग कार के लिए. ऐसा होता है यदि आप प्लॉट करते हैं पर्फ़ॉर्मन्स 
एक पारम्परिक लर्निंग अल्गोरिद्म की जैसे सपोर्ट वेक्टर मशीन या लॉजिस्टिक रेग्रेशन की डेटा की मात्रा के 
एक फ़ंक्शन की तरह जो आपके पास है, आपको शायद मिले एक कर्व जो शायद दिखे ऐसा, जहां पर्फ़ॉर्मन्स में सुधार होता है कुछ समय के लिए 
जैसे आप जोड़ते हैं अधिक डेटा लेकिन कुछ समय के बाद, पर्फ़ॉर्मन्स समतल हो जाती है. यह हो जाती है एक हॉरिज़ॉंटल लाइन, 
बहुत अच्छा बनाया नहीं. यह ऐसा था कि उन्हें नहीं पता कि क्या करें डेटा
 की विशाल मात्रा से. हमारे समाज में क्या हुआ कुछ पिछले 20 सालों में शायद कि बहुत से प्राब्लम्ज़ के लिए, हम पहुँचे अपेक्षाकृत बहुत कम मात्रा से डेटा की डेटा की एक काफी बड़ी मात्रा पर जहाँ इसमें से बहुत था- डिजिटलीकरण की वजह से एक समाज के जहाँ इतनी मानवीय गतिविधि है 
अब डिजिटल क्षेत्र में. हम अपने कंप्यूटर पर इतना समय खर्च करते हैं,
 वेबसाइटों पर, ऐप्स पर और गतिविधियाँ डिजिटल डिवाइसेज़ 
पर बनाती हैं डेटा. वृद्धि सस्ते कैमरों की, जो हमारे सेल फ़ोन में हैं, एक्सेलेरोमीटर्स, सभी तरह के सेन्सर्ज़ इंटर्नेट ऑफ़ थिंग्ज़ में, हम इसके अलावा भी इकट्ठा कर रहे हैं और अधिक डेटा. तो, पिछले 20 सालों में बहुत सी ऐप्लिकेशन्स के लिए, हम सिर्फ संचित कर रहे हैं बहुत अधिक डेटा जो ज़्यादा है उससे जिसका पारम्परिक लर्निंग अल्गोरिद्म्स
 लाभ उठा पा रहे थे. न्यूरल नेटवर्क्स से, ऐसा हुआ कि यदि आप ट्रेन करते हैं एक छोटा न्यूरल नेट, तब इसकी पर्फ़ॉर्मन्स होगी 
शायद दिखेगी ऐसी, यदि आप ट्रेन करते हैं एक थोड़ा बड़ा न्यूरल नेटवर्क, जिसे कहते हैं एक मध्यम साइज़ का न्यूरल नेट, पर्फ़ॉर्मन्स होगी थोड़ी बेहतर, 
और यदि आप ट्रेन करते हैं एक बहुत बड़ा न्यूरल नेट, तब पर्फ़ॉर्मन्स होती जाती है बेहतर और बेहतर. तो, कुछ पर्यवेक्षण, एक है, यदि आप चाहते हैं इस बहुत ऊँचे स्तर पर पर्फ़ॉर्मन्स तब, दो चीज़ों की जरूरत है. पहला, अक्सर आपको ट्रेन कर पाना चाहिए एक पर्याप्त बड़ा न्यूरल नेटवर्क ले पाने के लिए लाभ एक बड़ी विशाल मात्रा में डेटा का और दूसरा, आपको होना चाहिए यहाँ x ऐक्सिस पर, आपको चाहिए बहुत सा डेटा. तो, हम अक्सर कहते है कि स्केल चला रहा है 
डीप लर्निंग की प्रगति और स्केल से, मेरा मतलब है दोनो साइज़ न्यूरल नेटवर्क का, मतलब सिर्फ़ एक न्यूरल नेटवर्क जिसमें हैं 
बहुत सी हिडन यूनिट्स, बहुत से पेरमिटरज़, बहुत से कनेक्शनज़ तथा डेटा का स्केल भी. वास्तव में आज सबसे भरोसेमंद तरीकों में से 
एक तरीका बेहतर पर्फ़ॉर्मन्स का एक न्यूरल नेटवर्क में है अक्सर या तो ट्रेन करना 
एक बड़ा नेटवर्क या डालना अधिक डेटा उसमें और वह काम करता है केवल एक पोईँट तक क्योंकि अंततः आपका सभी डेटा समाप्त हो जाएगा
 या अंततः न्यूरल नेटवर्क हो जाएगा इतना बड़ा कि इसे बहुत समय लगेगा ट्रेन करने में. लेकिन सिर्फ़ बढ़ाने से स्केल हम पहुँच पाए हैं 
इतना आगे डीप लर्निंग की दुनिया में. बनाने के लिए इस चित्र को थोड़ा और तकनीकी रूप से सही, मैं जोड़ता हूँ इसमें कुछ और चीज़ें. मैंने लिखा डेटा की मात्रा x ऐक्सिस पर. तकनीकी रूप से, यह है मात्रा लेबल्ड डेटा की जहाँ लेबल्ड डेटा से मेरा मतलब है ट्रेनिंग इग्ज़ाम्पल्ज़ 
जिनमें है दोनो इनपुट x और लेबल y. देने के लिए परिचय थोड़ी और नोटेशन का जो 
हम बाद में इस्तेमाल करेंगे इस कोर्स में, हम अपने प्रशिक्षण सेट के आकार को निरूपित करने के 
लिए लोअरकेस वर्णमाला के m का उपयोग करेंगे. तो, ट्रेनिंग इग्ज़ाम्पल्ज़ की संख्या है लोअरकेस m. 
तो वह है हॉरिज़ॉंटल ऐक्सिस. कुछ और विवरण इस चित्र में, छोटे ट्रेनिंग सेट की इस प्रणाली में, आपेक्षिक क्रम अल्गोरिद्म्स का वास्तव में 
अच्छे ढंग से परिभाषित नहीं है. आपके पास नहीं है बहुत अधिक ट्रेनिंग डेटा, यह है अक्सर आपके ऊपर ख़ुद से बनाना फ़ीचर्स 
जो निर्धारित करता है पर्फ़ॉर्मन्स. तो, यह काफ़ी सम्भव है कि यदि कोई ट्रेन
 कर रहा है एक SVM वह है अधिक प्रेरित हाथ से फ़ीचर बनाने में अपेक्षा में किसी से 
जो ट्रेन कर रहा है एक उससे बड़ा न्यूरल नेट, शायद छोटे ट्रेनिंग सेट क्षेत्र में, SVM शायद बेहतर काम करे. तो इस क्षेत्र में बाईं तरफ़ चित्र में, आपेक्षिक क्रम अल्गोरिद्म्स का वास्तव में 
नहीं है उतने अच्छे ढंग से परिभाषित और पर्फ़ॉर्मन्स निर्भर करता है 
अधिक आपके कौशल पर फ़ीचर्ज़ को हैंडल करने की तथा अन्य विवरण पर अल्गोरिद्म्स के और केवल इस बिग डेटा के क्षेत्र में, बहुत बड़े ट्रेनिंग सेट्स, बहुत बड़े m के क्षेत्र में दाईं तरफ़ जहाँ हम अधिक लगातार रूप से देखते हैं न्यूरल नेट श्रेष्ठ है 
अन्य तरीक़ों की अपेक्षा. यदि आपके कोई मित्र पूछते हैं कि 
क्यों न्यूरल नेट प्रचलित हो रहे हैं? आपको प्रोत्साहित करूँगा बनाने के लिए
 यह चित्र उनके लिए भी. तो, मैं कहूंगा कि शुरुआती दिनों में, आधुनिक डीप लर्निंग की प्रगति के पीछे, यह था स्केल किया गया डेटा और स्केल कॉम्प्यूटेशन का. और हमारी क्षमता ट्रेन करने की बहुत बड़े न्यूरल नेटवर्क्स, एक CPU पर या एक GPU पर, जिसने हमें सक्षम किया बहुत सी प्रगति करने में. लेकिन तेज़ी से, विशेष रूप से पिछले कुछ सालों में हम देख रहे हैं ज़बरदस्त नवीनता अल्गोरिद्म्स में भी. तो, मैं नहीं चाहता उसे कम श्रेय देना. दिलचस्प रूप से, बहुत से अल्गोरिद्म्स में नवीनता है करने के लिए कि न्यूरल नेट्वर्क अधिक तेज़ी से काम करें. एक ठोस उदाहरण के जैसे, एक बहुत बड़ी सफलता न्यूरल नेटवर्क्स में हुई है परिवर्तन एक सिग्मोईड फ़ंक्शन से जो, जो ऐसा दिखता है, से एक ReLu फ़ंक्शन पर जिसकी हमने बात की संक्षेप में एक पहले के वीडियो में जो दिखता है ऐसा. यदि आप नहीं समझ पा रहे हैं जो मैं कहना चाहता हूँ, 
चिंता न करें इसकी. लेकिन ऐसा होता है कि बहुत में से एक प्रॉब्लम इस्तेमाल 
करने में सिग्मोईड फ़ंक्शनज़ मशीन लर्निंग में है कि ऐसे क्षेत्र हैं यहाँ 
जहाँ स्लोप फ़ंक्शन की, जहां ग्रैडिएंट लगभग शून्य है, और इसलिए लर्निंग बहुत धीरे होती है क्योंकि जब आप इम्प्लमेंट करते हैं 
ग्रेडीयंट डिसेंट और ग्रेडीयंट है ज़ीरो, पेरमिटर्स बदलते हैं बहुत धीरे और 
इसलिए लर्निंग बहुत धीरे होती है. जबकि बदलने से ऐक्टिवेशन फ़ंक्शन न्यूरल नेटवर्क का इस फ़ंक्शन में जिसे कहते हैं ReLu फ़ंक्शन 
या रेक्टिफ़ायड लिनीअर यूनिट, ReLu, ग्रेडीयंट है बराबर एक सभी पॉज़िटिव
 वैल्यूज़ के लिए इनपुट की, ठीक है. तो ग्रेडीयंट की काफ़ी कम सम्भावना है 
धीरे-धीरे शून्य होने की. ग्रेडीयंट यहाँ, स्लोप इस लाइन की है ज़ीरो बाईं तरफ़, लेकिन ऐसा होता है सिर्फ़ बदलने से सिग्मोईड फ़ंक्शन को ReLu फ़ंक्शन में बना दिया है एक अल्गोरिद्म को जिसे कहते हैं 
ग्रेडीयंट डिसेंट, काम अधिक तेज़ गति से. तो यह है उदाहरण शायद अपेक्षाकृत 
सरल नवीनता का अल्गोरिद्म में लेकिन अंततः प्रभाव जो यह एल्गोरिद्म की नवीनता का था 
इसने वास्तव में गणना में मदद की. तो, वास्तव में ऐसे बहुत से उदाहरण हैं इस तरह के जहाँ हमने अल्गोरिद्म को बदला क्योंकि 
इसमें हम चला पाए हमारा कोड तेज़ी से और इससे हम ट्रेन कर पाए एक बड़ा न्यूरल 
नेटवर्क या अनुमान कर पाए उचित मात्रा कोड की जब हमारे पास है
 एक बड़ा नेटवर्क या बहुत सारा डेटा. अन्य कारण कि त्वरित कॉम्प्यूटेशन है महत्वपूर्ण है कि ऐसा होता है कि प्रक्रिया न्यूरल नेटवर्क 
को ट्रेन करने की बहुत इटरेटिव है. अक्सर, आपके पास होता हैं एक आइडिया एक न्यूरल नेटवर्क के आर्किटेक्चर का और इसलिए 
आप इम्प्लमेंट करते हैं आपका आइडिया एक कोड में. जो इम्प्लमेंट करता है आपका आइडिया 
फिर करने देता है आपको रन एक प्रयोग जो बताता है आपको कितना बेहतर काम कर रहा है 
आपका न्यूरल नेटवर्क और फिर उसको देखने उसे, आप वापिस जाते हैं बदलने के लिए विवरण आपके न्यूरल नेटवर्क का और फिर आप जाते हैं 
इस चक्र में बार-बार. जब आपका न्यूरल नेट्वर्क लेता है बहुत 
अधिक समय ट्रेन होने में, सिर्फ़ अधिक समय लगता है इस साइकल में और वहाँ है एक बहुत अंतर आपकी उत्पादकता में बनाने में प्रभावी 
न्यूरल नेटवर्कस जब आपके पास हो सकता है एक आइडिया और आप परीक्षण कर सकते हैं और 
देख सकते हैं कि यह करेगा या नहीं 10 मिनट में. या शायद अधिक से अधिक एक दिन में तुलना में
 यदि आप ट्रेन करते हैं एक न्यूरल नेटवर्क एक महीने में. बेशक, कभी-कभी यह होता भी है 
क्योंकि यदि आपको मिलता है एक परिणाम वापिस 10 मिनट में या शायद एक दिन में, आप सिर्फ़ परीक्षण कर सकते है और 
अधिक आइडिया और पा सकते हैं संभवत: एक न्यूरल नेटवर्क जो बेहतर 
काम करता हैं आपकी ऐप्लिकेशन के लिए. तो, त्वरित कॉम्प्यूटेशन ने वास्तव में सहायता की है गति बढ़ाने में दर की जिस में आपको मिल सकता है 
प्रयोग का परिणाम वापिस और इससे वास्तव में मदद हुई है दोनो न्यूरल 
नेटवर्क बनाने वालों की तथा शोधकर्ताओं की जो डीप लर्निंग में 
काम कर रहे हैं इटरेट करने में अधिक तेज़ी से और सुधारने में आपके 
आइडिया अधिक तेज़ी से. तो, इस सब से आयी है अधिक तेज़ी पूरे डीप लर्निंग के शोध समुदाय में जो 
मुझे लगता है अविश्वसनीय है, नए अल्गोरिदम्स की खोज में और
 लगातार प्रगति कर रही है उस मोर्चे पर. तो हैं कुछ शक्तियाँ जो मदद कर रही हैं वृद्धि में 
डीप लर्निंग की लेकिन अच्छी ख़बर है कि ये ताक़तें अभी भी काम कर रही हैं शक्तिशाली
 रूप से बनाने के डीप लर्निंग और भी बेहतर डेटा को. समाज अभी भी बना रहा है अधिक और 
अधिक डिजिटल डेटा या ले जा रहा है कॉम्प्यूटेशन को, उदय से विशेष हार्डवेयर के 
जैसे GPU और तेज़ नेटवर्किंग से, कई प्रकार के हार्डवेयर. मुझे वास्तव में काफी विश्वास है कि हमारी क्षमता बहुत 
बड़े न्यूरल नेटवर्क का निर्माण करने की में हिस्सा होगा कॉम्प्यूटेशन दृष्टिकोण होगा आगे बेहतर और बनाएगा अल्गोरिद्म. जबकि पूरे डीप लर्निंग शोध समुदाय हालाँकि बने हुए है अभूतपूर्ण खोज करने में नए अल्गोरिद्म्स की. तो, इस वजह से, मुझे लगता है कि हम आशावादी हो सकते हैं. मैं निश्चित रूप से आशावादी हूं कि डीप लर्निंग होता रहेगा बेहतर और बेहतर आने वाले बहुत से वर्षों में. तो, उसके साथ, चलो जाते हैं इस अनुभाग के अंतिम वीडियो पर हम थोड़ा और अधिक बात करेंगे कि 
आप क्या सीखेंगे इस कोर्स में.