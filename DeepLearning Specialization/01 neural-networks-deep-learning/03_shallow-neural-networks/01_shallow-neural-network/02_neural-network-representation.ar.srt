1
00:00:00,000 --> 00:00:03,116
يمكنكم رؤيتي وأنا أرسم
بعض الصور للشبكات العصبية.

2
00:00:03,116 --> 00:00:05,712
في هذا الفيديو، سوف نتحدث
عن ماهية هذه الصور بدقة.

3
00:00:05,712 --> 00:00:06,728
بمعنى آخر،

4
00:00:06,728 --> 00:00:11,235
ما تمثله هذه الشبكات العصبية
التي كنا نرسمها بدقة.

5
00:00:11,235 --> 00:00:15,014
وسنبدأ من خلال التركيز على
حالة الشبكات العصبية مع

6
00:00:15,014 --> 00:00:17,290
ما كان يعرف باسم طبقة مخفية واحدة.

7
00:00:17,290 --> 00:00:19,667
إليكم صورة للشبكة العصبية.

8
00:00:19,667 --> 00:00:22,986
دعونا نطلق بعض الأسماء على
الأجزاء المختلفة من هذه الصور.

9
00:00:22,986 --> 00:00:27,447
لدينا ميزات الإدخال، x1 و
x2 وx3 مصفوفة عموديًا.

10
00:00:27,447 --> 00:00:30,694
ويعرف هذا باسم طبقة الإدخال
للشبكة العصبية.

11
00:00:30,694 --> 00:00:35,764
لذا فربما بصورة غير مفاجئة، يحتوي هذا على
الإدخالات إلى الشبكة العصبية.

12
00:00:35,764 --> 00:00:37,990
وهناك طبقة أخرى من الدوائر.

13
00:00:37,990 --> 00:00:41,663
وتُعرف باسم طبقة مخفية
للشبكة العصبية.

14
00:00:41,663 --> 00:00:45,414
سأعود بعد لحظات لتوضيح
ما تعنيه كلمة "مخفية".

15
00:00:45,414 --> 00:00:49,509
ولكن الطبقة النهائية هنا مشكّلة من خلال،
عقدة واحدة فقط في هذه الحالة.

16
00:00:49,509 --> 00:00:53,894
وهذه الطبقة المكوّنة من عقدة واحدة تُعرف باسم
طبقة الإخراج وهي مسؤولة عن

17
00:00:53,894 --> 00:00:57,000
إنشاء القيمة المتوقعة لـ y hat.

18
00:00:57,000 --> 00:00:59,932
في شبكة عصبية تقوم بتدريبها من خلال
التعلم الخاضع للإشراف،

19
00:00:59,932 --> 00:01:05,237
تحتوي مجموعة التدريب على قيم
الإدخال x وكذلك المخرجات المستهدفة y.

20
00:01:05,237 --> 00:01:09,239
لذا فإن المصطلح طبقة مخفية يشير إلى
حقيقة أنه في مجموعة التدريب،

21
00:01:09,239 --> 00:01:12,702
لا تتم مراعاة القيم الحقيقية
لهذه العُقد في المنتصف.

22
00:01:12,702 --> 00:01:15,280
بمعنى أنكم لم تروا ما الذي ينبغي
أن تكون عليه في مجموعة التدريب.

23
00:01:15,280 --> 00:01:16,640
فأنتم ترون قيمة المدخلات.

24
00:01:16,640 --> 00:01:18,094
وترون ما الذي يجب أن تكون عليه المخرجات.

25
00:01:18,094 --> 00:01:20,992
ولكن الأشياء الموجودة في الطبقة المخفية
لا تتم رؤيتها في مجموعة التدريب.

26
00:01:20,992 --> 00:01:25,542
ولذلك فإن هذا يشرح نوعًا ما الاسم
طبقة مخفية، لأنكم

27
00:01:25,542 --> 00:01:27,050
لم تروها في مجموعة التدريب.

28
00:01:27,050 --> 00:01:28,400
دعونا نقدم المزيد من الرموز.

29
00:01:28,400 --> 00:01:34,660
حيث كنا نستخدم في السابق
المتجه X للإشارة إلى ميزات الإدخال

30
00:01:34,660 --> 00:01:36,400
وسيكون الرمز البديل

31
00:01:36,400 --> 00:01:41,680
لقيم ميزات الإدخال A
القوس المربع المرتفع 0.

32
00:01:41,680 --> 00:01:44,934
كما يشير المصطلح A
إلى حالات التنشيط

33
00:01:44,934 --> 00:01:47,733
ويشير إلى القيم
التي تقوم الطبقات المختلفة

34
00:01:47,733 --> 00:01:51,651
للشبكة العصبية بتمريرها
إلى الطبقات اللاحقة.

35
00:01:51,651 --> 00:01:55,998
لذا فإن طبقة الإدخال تقوم بتمرير
القيمة x إلى الطبقة المخفية،

36
00:01:55,998 --> 00:02:01,110
وبالتالي سنطلق على حالات التنشيط
لطبقة الإدخال A رقم 0 مرتفع.

37
00:02:01,110 --> 00:02:05,990
الطبقة التالية، وهي الطبقة المخفية،
ستقوم بدورها بإنشاء مجموعة من حالات التنشيط،

38
00:02:05,990 --> 00:02:09,601
والتي سأكتبها بصيغة
A القوس المربع المرتفع 1.

39
00:02:09,601 --> 00:02:13,306
لذا بصورة خاصة، فإن هذه
الوحدة الأولى أو هذه العقدة الأولى،

40
00:02:13,306 --> 00:02:17,824
سيتم إنشاء القيمة A القوس
المربع المرتفع 1 ورقم 1 منخفض.

41
00:02:17,824 --> 00:02:20,735
في هذه العقدة الثانية، سنقوم بإنشاء قيمة.

42
00:02:20,735 --> 00:02:23,311
لدينا الآن رقم 2 منخفض وما إلى ذلك.

43
00:02:23,311 --> 00:02:26,488
لذا فإن، A الأقواس المربعة المرتفعة 1،

44
00:02:26,488 --> 00:02:30,350
هو متجه رباعي الأبعاد
تريدونه في لغة بايثون

45
00:02:30,350 --> 00:02:34,707
لأن المصفوفة 4x1 أو
المتجه المكوّن من 4 أعمدة، يبدو هكذا.

46
00:02:34,707 --> 00:02:39,205
وهو رباعي الأبعاد، لأنه
في هذه الحالة لدينا أربع عقد، أو

47
00:02:39,205 --> 00:02:43,330
أربع وحدات أو أربع
وحدات مخفية في هذه الطبقة المخفية.

48
00:02:43,330 --> 00:02:46,800
وأخيرًا، فإن الطبقة المفتوحة
تعمل على إنشاء بعض قيم A2،

49
00:02:46,800 --> 00:02:48,440
وهو مجرد رقم حقيقي.

50
00:02:48,440 --> 00:02:53,100
وبالتالي y hat
ستأخذ قيمة A2.

51
00:02:53,100 --> 00:02:57,950
وهذا مشابه للكيفية التي يكون لدينا خلالها
في الانحدار اللوجستي y hat تساوي a

52
00:02:57,950 --> 00:03:02,560
وفي الانحدار اللوجستي حيث يكون
لدينا فقط طبقة مخرجات واحدة،

53
00:03:02,560 --> 00:03:04,500
لذا فإننا لا نستخدم
الأقواس المربعة المرتفعة،

54
00:03:04,500 --> 00:03:07,916
ولكن مع الشبكة العصبية لدينا،
سنستخدم الآن الأقواس المربعة المرتفعة

55
00:03:07,916 --> 00:03:11,653
للإشارة صراحة إلى
الطبقة التي أتت منها.

56
00:03:11,653 --> 00:03:15,468
من الأشياء المرحة بخصوص
الاصطلاحات الترميزية في الشبكات العصبية

57
00:03:15,468 --> 00:03:20,194
هو أن هذه الشبكة التي رأيتموها هنا
تُعرف باسم شبكة عصبية من طبقتين.

58
00:03:20,194 --> 00:03:24,360
والسبب في ذلك أنه عندما
نحسب الطبقات في الشبكات العصبية،

59
00:03:24,360 --> 00:03:25,990
فإننا لا نحسب طبقة الإدخال.

60
00:03:25,990 --> 00:03:30,680
لذا فإن الطبقة المخفية هي الطبقة الأولى،
وطبقة الإخراج هي الطبقة الثانية.

61
00:03:30,680 --> 00:03:34,700
في الاصطلاح الترميزي، نطلق على
طبقة الإدخال، الطبقة صفر

62
00:03:34,700 --> 00:03:37,700
لذا، من الناحية التقنية، ربما تكون هناك
ثلاث طبقات في هذه الشبكة العصبية.

63
00:03:37,700 --> 00:03:40,260
لأنه هناك طبقة الإدخال والطبقة
المخفية وطبقة الإخراج.

64
00:03:40,260 --> 00:03:44,600
ولكن في الاستخدام التقليدي،
إذا قرأتم الأوراق البحثية وفي مكان آخر في الدورة التدريبية

65
00:03:44,600 --> 00:03:48,700
فإنكم سترون الأشخاص يشيرون إلى
هذه الشبكة العصبية الخاصة

66
00:03:48,700 --> 00:03:52,630
كشبكة عصبية من طبقتين، لأننا
لا نحسب طبقة الإدخال كطبقة رسمية.

67
00:03:52,630 --> 00:03:55,912
أخيرًا، من الأشياء التي سنصل إليها
لاحقًا هو أن الطبقة المخفية

68
00:03:55,912 --> 00:03:59,670
وطبقات الإخراج ستحتوي على
معلمات مرتبطة بها.

69
00:03:59,670 --> 00:04:04,560
لذا، فإن الطبقة المخفية ستكون
مرتبطة بالمعلمات w وb.

70
00:04:04,560 --> 00:04:08,218
وسأكتب القوس المربع المرتفع
1 للإشارة إلى أن

71
00:04:08,218 --> 00:04:12,395
هذه معلمات مرتبطة بالطبقة
1 بالطبقة المخفية.

72
00:04:12,395 --> 00:04:15,416
سنرى لاحقًا أن w ستكون
مصفوفة 4 في 3

73
00:04:15,416 --> 00:04:19,830
وb ستكون عبارة عن متجه 4 في 1 في هذا المثال.

74
00:04:19,830 --> 00:04:22,750
حيث يأتي الإحداثي الأول أربعة
من حقيقة أن لدينا

75
00:04:22,750 --> 00:04:25,300
أربع عقد أو أربع
وحدات مخفية وطبقة

76
00:04:25,300 --> 00:04:28,120
وثلاثة تأتي من حقيقة أن
لدينا ثلاث ميزات إدخال.

77
00:04:28,120 --> 00:04:31,600
سنتحدث لاحقًا حول
أبعاد هذه المصفوفات.

78
00:04:31,600 --> 00:04:33,540
وربما تكون أكثر ملاءمة في هذا الوقت.

79
00:04:33,540 --> 00:04:37,813
ولكن بصورة مشابهة، تكون
طبقة الإخراج مرتبطة بالمعلمات

80
00:04:37,813 --> 00:04:42,400
w قوس مربع مرتفع 2،
وb قوس مربع مرتفع 2.

81
00:04:42,400 --> 00:04:45,747
وتبيّن أن أبعاد هذه هي
(1،4) و(1،1).

82
00:04:45,747 --> 00:04:49,297
وصيغة (1،4) هذه لأن الطبقة
المخفية تحتوي على أربع وحدات مخفية،

83
00:04:49,297 --> 00:04:51,177
وطبقة الإخراج تحتوي على وحدة واحدة فقط.

84
00:04:51,177 --> 00:04:56,378
ولكننا سنستعرض أبعاد هذه
المصفوفات والمتجهات في فيديو لاحق.

85
00:04:56,378 --> 00:04:59,839
حسنًا، رأيتم للتو كيف تبدو
الشبكة العصبية المكوّنة من طبقتين.

86
00:04:59,839 --> 00:05:03,108
أي، شبكة عصبية
بطبقة مخفية واحدة.

87
00:05:03,108 --> 00:05:04,260
ففي الفيديو التالي،

88
00:05:04,260 --> 00:05:08,513
دعونا نتعمق أكثر وأكثر فيما تقوم هذه
الشبكة العصبية بحسابه بدقة.

89
00:05:08,513 --> 00:05:11,223
أي كيف تقوم هذه الشبكة
العصبية بإدخال x

90
00:05:11,223 --> 00:05:14,169
وتمتد لكي تحسب
الإخراج y hat.