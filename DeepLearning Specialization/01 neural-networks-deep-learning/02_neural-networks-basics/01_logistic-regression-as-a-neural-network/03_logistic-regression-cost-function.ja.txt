前回の動画では、 ロジスティック回帰モデルについて理解しました。 ロジスティック回帰モデルのパラメータWとbを訓練するためには、 コスト関数を定義する必要があります。 では、ロジスティック回帰の学習に使えるコスト関数について確認していきましょう。 まとめると、これが前回のスライドから抜粋した式です。 出力結果yハットは wT(転置)x+bにシグモイド関数を適用したもので、zのシグモイド関数の定義はこちらです。 ここで、モデルのパラメータを学習するために m個のサンプルを含む教師データのセットが与えられています。そして、 その教師データに含まれる出力値をほぼ得られるような パラメータWとbを求めるものとします。 教師データから得られる予測値は、 ここではyハット(i)と表記し、同様に 教師データの中に含まれる正解値を表すラベルをy_iと記載します。 式の初めの方をもう少し詳しく確認してみると、 初めのyハットは、ある教師サンプルxに対して先ほど定義したものですが、 もちろん各々の教師サンプルに対して定義するならば、 各サンプルを区別するために、 インデックスの丸かっこを上付き添字にして使用します。 i番目の教師サンプルに対する予測値、つまりｙハット(i)は、 シグモイド関数をWの転置行列と教師サンプルの入力値x(i) の積にbを加えた値に適用すれば得られます。ここで、z(i)は次のように定義することもできます。 z(i)=wT・x(i)+b。 この講座全体を通して、 私たちは、これまでに述べた表記法を使うつもりです。つまり、 丸かっこiの上付き添字は、 教師データのi番目、i番目のサンプルに関連付けられた x,y,zあるいはその他の変数を示します。 上付き添字のかっこ(i)の意味の説明は以上です。 ではこれから、どのような損失関数または誤差関数を使えば 私たちのアルゴリズムがうまく評価できるのか一緒に見てみましょう。 一つできることと言えば、損失を次のように定義することです。アルゴリズムが出力する値を yハット、真の値を示すラベルをyとすると、損失が二乗誤差かあるいはその半分となるようにすればよいでしょう。 このように定義できそうなことが分かりましたが、 ロジスティック回帰において、一般的にはその定義は使われていません。 何故なら、いざパラメータを学習させようとすると、 後で述べるような非凸な最適化問題を解くことになるからです。 すると、結果的には複数の局所解を持つ最適化問題を扱うこととなり、 勾配降下法では一つの大域解を見つけることができないでしょう。 もしたった今話したいくつかのコメントが理解できなくても 心配しないでください。後の動画でその話を扱う予定です。 ただ、持ち合わせるべきは以下のような洞察だと思います。 損失関数と呼ばれる関数Lは、 正解のラベルがyのときに実際の出力値yハットがどれほど正しいかを測るために定義する必要があります。 二乗誤差を採用することは合理的な気がしますが、 勾配降下法がうまく動かない場合はそうではありません。 従って、ロジスティック回帰において実際には 二乗誤差と似たような役割を持つ別の損失関数を使います。 その損失関数を使えば、私たちは凸な最適化問題を得られ、 後の動画で見られるように、より簡単な方法で最適化することができるのです。 ロジスティック回帰では実際には、 以下のような損失関数を使います。今ここで書き下すと、 -ylog(yハット)+(1-y)(1-log(yハット)) となります。 なぜこの損失関数が理に適うのか、直観的に説明してみましょう。 留意するべき点は、 もし二乗誤差を使うならば、私たちはその誤差をなるべく小さくしようとすることです。 すなわち、このロジスティック回帰の損失関数についても 同様に私たちはその値をなるべく小さくしようとするでしょう。 何故このことが理に適うのか理解するため、 次に示す2つのケースを考えてみてください。 一つめのケースは、 yが1の場合で、そのとき損失関数 (yハット,y)は丁度マイナス符号で書けますから、 -log(yハット)となります。 もしyが1と等しいなら、yが1なので 2番目の項(1-y)は0となるからです。 つまり、もしyが1ならば、なるべくlog(yハット)を大きくしようとします。 log(yハット)を大きくしたいということは、 yハットをなるべく大きくしたいということを意味しますね。 しかし、yハットはご存知の通り、 シグモイド関数であり、1よりも大きくなることはあり得ません。 すると、もしyが1と等しいならば、 yハットをなるべく大きくしたい一方、 その値が1を超えることは決してなく、要するになるべく1に近づけようとしていることになります。 もう一方のケースは、yが0の場合です。 もしyが0と等しいなら、yは0であるから損失関数の第1項目は0になります。 そうすると損失関数は2つ目の項で定まります。 つまり損失関数は-log(1-yハット)となり、 学習させる段階で、損失関数を小さくしようとするならば、 log(1-yハット)を大きくすることを意味します。 なぜなら、そこにマイナス符号があるからです。 そして同じような理由から、結論として、 この損失関数はyハットをなるべく小さくしようとしていることになります。 そして再び、yハットは0と1の間でなければならないことから、 もしyが0と等しいならば、 yハットがなるべく0に近づくように損失関数のパラメータが変化することになります。 ここで、yが1と等しい時は yハットを大きくしようとし、yが0ならばyハットを小さくしようとするような関数は数多く存在します。 ちょうど緑色で記載した部分により、 少し厳密ではないですが先ほどの損失関数を与えれば良い根拠が分かりました。 この後のオプション講座では、 ロジスティック回帰でこのような特殊な式の損失関数を好んで使う理由を、より厳密に示すことになります。 最後に、損失関数は、ひとつの教師サンプルについて定義されていました。 サンプル一つ分に対してどれほど良い具合に学習できているのかを測っています。 ここで、コスト関数と呼ばれるものをここで定義してみましょう。 コスト関数は、教師データのデータセット全体に対してうまく学習できているのかを測ります。 なので、コスト関数Jは、 パラメータWとbに対して適用されますが、 各々の教師サンプルに対する損失関数の総和を1/m倍した平均値となるはずです。 一方、ここでyハットはもちろん ロジスティック回帰アルゴリズムによる予測結果であり、ご存知の通り、 パラメータWとbの特別なセットを利用してます。 そしてこの式をそのまま展開すると、 マイナスに1/mをかけて シグマi=1からm、シグマの中はJの定義式となります。 なので、y(i)log(yハット)(i) プラス(1-y(i))log(1-yハット(i)) おそらく、私は角括弧をここで書くでしょう。 そしてマイナス符号は丸ごと外に配置します。 私が用語定義として使うつもりなのが、 損失関数は、ちょうどひとつの教師サンプルに対して適用するということ。 そしてコスト関数は全てのパラメータに関するコストであるということ。 なので、ロジスティック回帰モデルの学習では、 私たちは一番下に記載したように、全ての損失Jに渡るコストを最小化する パラメータWとbを見つけることになります。 ここまで、ロジスティック回帰アルゴリズム、 教師データに対する損失関数、そして アルゴリズムの全パラメータに渡るコスト関数の組み立て方を見てきました。 実はロジスティック回帰は、とても、とても小さなニューラルネットワークとして捉えることができるのです。 次の動画では、私たちは ニューラルネットワークで何を行っているか直観的に理解するための考察に入りたいと思います。 なので是非とも次の動画へ進んでください。 ロジスティック回帰をどのようにしてとても小さなニューラルネットワークとみなすのかお話します。