1
00:00:00,000 --> 00:00:01,440
在先前的影片中,

2
00:00:01,440 --> 00:00:05,700
您看到您如何使用向量化來計算他們的預估值

3
00:00:05,700 --> 00:00:11,485
小寫的 a's, 在同一個時間內計算整個訓練集

4
00:00:11,485 --> 00:00:15,030
在這段影片您看到如何使用也用向量化

5
00:00:15,030 --> 00:00:19,205
執行在梯度計算所有的 m 訓練例子

6
00:00:19,205 --> 00:00:21,380
同樣的都在同ㄧ個時間

7
00:00:21,380 --> 00:00:22,890
在這段影片最後內

8
00:00:22,890 --> 00:00:26,175
我們會將所有教過的放在一起, 告訴您如何可以推導

9
00:00:26,175 --> 00:00:29,730
非常有效的羅吉斯迴歸分析建置

10
00:00:29,730 --> 00:00:32,505
您也許記得在梯度計算

11
00:00:32,505 --> 00:00:36,910
我們計算 dz1 在第一個例子

12
00:00:36,910 --> 00:00:43,870
也就是 a1 減 y1 , 然後 dz2 等於

13
00:00:43,870 --> 00:00:52,134
a2 減 y2 等等

14
00:00:52,134 --> 00:00:56,425
等等對於所有 m 訓練例子

15
00:00:56,425 --> 00:01:01,218
我們要做的是定義一個新的變數

16
00:01:01,218 --> 00:01:08,595
dZ 就是 dz1, dz2, .. dzm

17
00:01:08,595 --> 00:01:13,910
再次, 所有 d 小寫z 變數水平疊一起

18
00:01:13,910 --> 00:01:21,200
這個是 1 乘 m 矩陣或者說一個 m 維度行向量

19
00:01:21,200 --> 00:01:23,520
現在回想前面的投影片中

20
00:01:23,520 --> 00:01:28,405
我們已經弄清楚了如何去計算大寫 A 就是 a1 到

21
00:01:28,405 --> 00:01:36,735
am, 我們已經定義大寫 Y 就是 y1 到 ym

22
00:01:36,735 --> 00:01:39,200
您知道, 水平疊起來

23
00:01:39,200 --> 00:01:42,780
基於這些定義

24
00:01:42,780 --> 00:01:46,770
也許您可以自己看 dz 可以計算為

25
00:01:46,770 --> 00:01:52,750
就是 A 減 Y 因為這將是等於 a1 - y1

26
00:01:52,750 --> 00:01:55,670
第一個元素, a2 - y2

27
00:01:55,670 --> 00:01:59,980
在第二個元素等等

28
00:01:59,980 --> 00:02:06,115
第一個元素 a1-y1 正是 dz1 的定義

29
00:02:06,115 --> 00:02:11,670
第二個元素正是 dz2 等等

30
00:02:11,670 --> 00:02:13,965
所以只用一行程式

31
00:02:13,965 --> 00:02:20,095
您可以在一個時間內計算所有的元素

32
00:02:20,095 --> 00:02:24,010
現在, 在前面的建置中

33
00:02:24,010 --> 00:02:27,695
我們已經去掉一個for循環(for loop)但我們還

34
00:02:27,695 --> 00:02:31,600
有這第二個for循環經過這些訓練例子

35
00:02:31,600 --> 00:02:35,440
我們初始 dw 為 0 即 0 向量

36
00:02:35,440 --> 00:02:38,905
但我們還必須迴過 20 個例子

37
00:02:38,905 --> 00:02:43,015
dw 加等於 x1 乘 dz1

38
00:02:43,015 --> 00:02:50,440
第一個訓練例子 dw 加等於 x2 dz2 等等

39
00:02:50,440 --> 00:02:56,980
這樣總共做 m 次, 然後 dw 除以 m, 而 b 同樣作法

40
00:02:56,980 --> 00:03:03,370
db 初始為 0, db 加等於 dz1

41
00:03:03,370 --> 00:03:09,120
db 加等於 dz2 直到

42
00:03:09,120 --> 00:03:16,835
dzm 然後 db 除以 m, 所以這是我們在前面做過的

43
00:03:16,835 --> 00:03:18,700
我們已經去掉了一個循環。

44
00:03:18,700 --> 00:03:25,045
至少現在 dw 是一個向量, 我們分開更新 dw1

45
00:03:25,045 --> 00:03:26,850
dw2 等等

46
00:03:26,850 --> 00:03:29,860
我們已經去掉那個迴圈, 但我們還是需要

47
00:03:29,860 --> 00:03:33,630
有 for循環 在訓練集中的 m 個例子上。

48
00:03:33,630 --> 00:03:36,290
讓我們用這個操作跟向量化它們

49
00:03:36,290 --> 00:03:38,380
這是我們可以做的

50
00:03:38,380 --> 00:03:42,745
db的向量化的建置,它基本上就是總和

51
00:03:42,745 --> 00:03:47,940
所有這些 dz 然後除以 m

52
00:03:47,940 --> 00:03:51,580
db 基本上是 1 除以 m

53
00:03:51,580 --> 00:03:56,530
總和dzi從 i 等於 1 到 m

54
00:03:56,530 --> 00:04:03,055
所有 dz 是行向量, 在 python

55
00:04:03,055 --> 00:04:04,765
您要建置的是

56
00:04:04,765 --> 00:04:08,155
1 / m 乘 np.

57
00:04:08,155 --> 00:04:12,210
dz 的總和。

58
00:04:12,210 --> 00:04:15,115
您可以拿這個變數然後呼叫 np.

59
00:04:15,115 --> 00:04:19,195
sum 函數在它就會給您 db

60
00:04:19,195 --> 00:04:22,330
dw 如何做呢? 我寫下

61
00:04:22,330 --> 00:04:26,375
正確的公式就可以驗證是否在做正確的事

62
00:04:26,375 --> 00:04:28,164
dw 原本就是 1 / m

63
00:04:28,164 --> 00:04:34,485
乘上矩陣 X 乘上 dz 轉置

64
00:04:34,485 --> 00:04:37,990
來看看為什麼是這樣

65
00:04:37,990 --> 00:04:41,806
這個等於 1 / m, 然後是矩陣 X

66
00:04:41,806 --> 00:04:48,325
x1 到 xm 疊在欄位像這樣, dz 

67
00:04:48,325 --> 00:04:56,040
轉置就是 dz1 直到 dz(m) 像這樣

68
00:04:56,040 --> 00:05:00,900
所以, 如果您瞭解這個矩陣乘上這個向量

69
00:05:00,900 --> 00:05:05,585
它實際上是 1 / m 乘 x1

70
00:05:05,585 --> 00:05:12,523
dz1 加 ... 加 xm dzm

71
00:05:12,523 --> 00:05:21,405
所以這是一個 m 乘 1 向量, 而這是您最終得到的

72
00:05:21,405 --> 00:05:24,725
dw 因為 dw 是從

73
00:05:24,725 --> 00:05:27,710
xi dzi 總和起來所以這正是

74
00:05:27,710 --> 00:05:32,300
這個矩陣向量乘積, 再一次

75
00:05:32,300 --> 00:05:35,655
只用一行程式您可以計算 dw

76
00:05:35,655 --> 00:05:40,010
所以向量化建置的導數計算就是

77
00:05:40,010 --> 00:05:44,540
您用這行程式來建置 db 用

78
00:05:44,540 --> 00:05:50,540
這行程式來建置 dw, 並注意,在訓練集上沒有 for 循環,

79
00:05:50,540 --> 00:05:55,265
您可以計算您想更新的參數

80
00:05:55,265 --> 00:06:01,185
現在, 讓我們把所有放在一起您實際上用來建置羅吉斯迴歸分析

81
00:06:01,185 --> 00:06:02,550
這是原本的版本

82
00:06:02,550 --> 00:06:07,866
非常沒有效率的非向量化建置

83
00:06:07,866 --> 00:06:11,775
在前面頭影片中我們做的第一件事是去除這個迴圈

84
00:06:11,775 --> 00:06:14,400
與其迴圈過 dw1,

85
00:06:14,400 --> 00:06:15,755
dw2 等等

86
00:06:15,755 --> 00:06:23,595
我們取代這個為向量值 dw 為 dw += xi

87
00:06:23,595 --> 00:06:28,775
是一個向量乘上 dz(i)

88
00:06:28,775 --> 00:06:32,000
現在, 我們看到我們不只可以去掉

89
00:06:32,000 --> 00:06:36,670
只是下面的一個for循環,
但也可以去掉這個for循環。

90
00:06:36,670 --> 00:06:38,654
您可以這麼做

91
00:06:38,654 --> 00:06:42,925
用前面的投影片

92
00:06:42,925 --> 00:06:46,085
您會說, 大寫 Z

93
00:06:46,085 --> 00:06:57,625
Z 等於 w 轉置 X + B 您的程式寫成大寫 Z 等於 np.dot

94
00:06:57,625 --> 00:07:07,315
W轉置X + B 等於 S型函數 of 大寫 Z

95
00:07:07,315 --> 00:07:12,710
您已經計算了所有這些 i 的值

96
00:07:12,710 --> 00:07:14,715
接下來前一個投影片

97
00:07:14,715 --> 00:07:21,070
我們說過你計算 dz 等於 A-Y.

98
00:07:21,070 --> 00:07:24,460
現在您計算所有這些 i 值

99
00:07:24,460 --> 00:07:31,495
最後 dw 等於 1 / m X

100
00:07:31,495 --> 00:07:39,700
dz 轉置而 db 等於 1/m 您知道 np.

101
00:07:39,700 --> 00:07:43,328
和 dz

102
00:07:43,328 --> 00:07:49,120
所以,你剛剛做了正向傳播和反向傳播,

103
00:07:49,120 --> 00:07:53,030
真正計算了預估值跟導數在

104
00:07:53,030 --> 00:07:57,340
所有 m 個訓練例子,而不使用 for循環。

105
00:07:57,340 --> 00:08:00,835
所以梯度下降更新會是 w

106
00:08:00,835 --> 00:08:04,462
更新為 w 減學習率乘上

107
00:08:04,462 --> 00:08:12,020
dw 就是上面計算的跟 B 更新為 B 減學習率乘 db

108
00:08:12,020 --> 00:08:17,341
有時用冒號來表示作為一個指定值,

109
00:08:17,341 --> 00:08:21,675
但我想我對於符號並不一致

110
00:08:21,675 --> 00:08:25,450
但有了這些, 您已經建置了

111
00:08:25,450 --> 00:08:29,635
單次反覆運算的梯度下降,使用羅吉斯迴歸分析

112
00:08:29,635 --> 00:08:32,308
我知道我說過我們應該去掉

113
00:08:32,308 --> 00:08:35,260
明顯的迴圈盡我們可能, 但如果您

114
00:08:35,260 --> 00:08:38,230
建置多重反覆運算

115
00:08:38,230 --> 00:08:42,880
梯度下降, 那您仍然需要完整迴圈經過這遞迴

116
00:08:42,880 --> 00:08:47,860
所以,如果你想做上千次的梯度下降反覆運算,

117
00:08:47,860 --> 00:08:53,675
您還是需要迴圈來一個一個做

118
00:08:53,675 --> 00:08:55,870
這是最外層的迴圈

119
00:08:55,870 --> 00:08:59,210
我想沒辦法來去掉它

120
00:08:59,210 --> 00:09:02,390
但我想已經是很酷的您可以建置

121
00:09:02,390 --> 00:09:07,117
一次遞迴的梯度下降不用到迴圈

122
00:09:07,117 --> 00:09:09,880
就這樣, 您現在有高度向量化跟

123
00:09:09,880 --> 00:09:14,745
高效率的低度下降建置對於羅吉斯迴歸分析

124
00:09:14,745 --> 00:09:18,850
在下一段影片我想談剩下一個細節

125
00:09:18,850 --> 00:09:24,155
就是我們這裡簡單描述過的技術稱為廣播

126
00:09:24,155 --> 00:09:28,240
廣播實際上是 python 跟 numpy 的一個技術

127
00:09:28,240 --> 00:09:32,915
讓您用來使得一部份的程式變得更有效率

128
00:09:32,915 --> 00:09:37,000
讓我們在下一段影片看更多廣播的細節