1
00:00:00,000 --> 00:00:03,116
您看過我畫一些
神經網路的圖

2
00:00:03,116 --> 00:00:05,712
在這段影片中, 我們將談談
這些圖的意義

3
00:00:05,712 --> 00:00:06,728
換句話說,

4
00:00:06,728 --> 00:00:11,235
我們畫過的這些神經網路圖
真正代表什麼意思

5
00:00:11,235 --> 00:00:15,014
我們先把焦點放在
一種神經網路

6
00:00:15,014 --> 00:00:17,290
稱為單一隱藏層

7
00:00:17,290 --> 00:00:19,667
這是神經網路的圖

8
00:00:19,667 --> 00:00:22,986
讓我們賦予這些圖形
不同部分的名稱

9
00:00:22,986 --> 00:00:27,447
我們有輸入特徵, x1,
x2,x3 垂直疊起來

10
00:00:27,447 --> 00:00:30,694
這是所謂神經網路的
輸入層

11
00:00:30,694 --> 00:00:35,764
或許並不意外,這包含了神經網路的輸入

12
00:00:35,764 --> 00:00:37,990
然後有另一層小圓圈

13
00:00:37,990 --> 00:00:41,663
而這是稱為神經網路的
隱藏層

14
00:00:41,663 --> 00:00:45,414
我回頭再來解釋為什麼
稱為隱藏

15
00:00:45,414 --> 00:00:49,509
最後這一層, 
在這個例子, 只有一個節點

16
00:00:49,509 --> 00:00:53,894
而這個單一節點層稱為
輸出層, 負責

17
00:00:53,894 --> 00:00:57,000
產生估計值 y-hat

18
00:00:57,000 --> 00:00:59,932
在神經網路
使用監督式學習訓練

19
00:00:59,932 --> 00:01:05,237
訓練集包含了
輸入 x 跟目標輸出 y

20
00:01:05,237 --> 00:01:09,239
所以這個名詞'隱藏層'指的是
在訓練集裡

21
00:01:09,239 --> 00:01:12,702
這些在中間節點的值
並不被觀察到.

22
00:01:12,702 --> 00:01:15,280
也就是說您在訓練集裡
看不到他們應該是什麼值

23
00:01:15,280 --> 00:01:16,640
您看到了輸入

24
00:01:16,640 --> 00:01:18,094
您看到輸出應該是多少

25
00:01:18,094 --> 00:01:20,992
但在隱藏層的值
並沒有在訓練集裡

26
00:01:20,992 --> 00:01:25,542
這樣解釋了 隱藏層 的名字
就因為您

27
00:01:25,542 --> 00:01:27,050
並沒看到它們在訓練集

28
00:01:27,050 --> 00:01:28,400
讓我們介紹一下更多的符號。

29
00:01:28,400 --> 00:01:34,660
以前,我們使用
向量  X  表示輸入特徵和

30
00:01:34,660 --> 00:01:36,400
另一種記號

31
00:01:36,400 --> 00:01:41,680
輸入特徵的值
是 A 上標方括號 0

32
00:01:41,680 --> 00:01:44,934
這個專有名詞 A 代表
啟動而

33
00:01:44,934 --> 00:01:47,733
它指的是不同層的值

34
00:01:47,733 --> 00:01:51,651
在神經網路傳遞到
下一層

35
00:01:51,651 --> 00:01:55,998
所以輸入層傳遞
x的值到隱藏層

36
00:01:55,998 --> 00:02:01,110
我們將之稱為
輸入層A上標0的啟動

37
00:02:01,110 --> 00:02:05,990
下一層,隱藏層,將
持續來產生一些啟動值

38
00:02:05,990 --> 00:02:09,601
我將它寫成
A 上標方括號 1

39
00:02:09,601 --> 00:02:13,306
特別來說,
第一單元或者說第一個節點

40
00:02:13,306 --> 00:02:17,824
我們產生A上標
方括號1下標1

41
00:02:17,824 --> 00:02:20,735
第二個節點,我們產生一個值。

42
00:02:20,735 --> 00:02:23,311
我們用下標 2, 等等..

43
00:02:23,311 --> 00:02:26,488
所以,一個上標 方括弧1,

44
00:02:26,488 --> 00:02:30,350
這是一個 4 維向量
要用在Python中

45
00:02:30,350 --> 00:02:34,707
因為這是 4x1 矩陣,或
4列 向量,像這樣。

46
00:02:34,707 --> 00:02:39,205
它是 4 維, 因為
這個例子裡以們有四個節點

47
00:02:39,205 --> 00:02:43,330
四個單元, 或者說
四個隱藏單元在隱藏層

48
00:02:43,330 --> 00:02:46,800
最後
輸出層產生 A2值

49
00:02:46,800 --> 00:02:48,440
就只是一個實數

50
00:02:48,440 --> 00:02:53,100
所以
y-hat 就用A2的值

51
00:02:53,100 --> 00:02:57,950
因此,這類似于
羅吉斯迴歸分析那樣,我們有 y-hat 等於 a

52
00:02:57,950 --> 00:03:02,560
在羅吉斯迴歸分析,我們
只有一個輸出層,

53
00:03:02,560 --> 00:03:04,500
所以我們並不使用上標
方括號

54
00:03:04,500 --> 00:03:07,916
但是有了我們的神經網路
我們現在要使用上標方括號

55
00:03:07,916 --> 00:03:11,653
方括號來明顯表示
它是從哪一層來的

56
00:03:11,653 --> 00:03:15,468
一件關於符號有趣的事
在神經網路

57
00:03:15,468 --> 00:03:20,194
是您看到的這個網路
稱為兩層神經網路

58
00:03:20,194 --> 00:03:24,360
而原因是當我們
數神經網路幾層時

59
00:03:24,360 --> 00:03:25,990
我們不算輸入層

60
00:03:25,990 --> 00:03:30,680
所以隱藏層是第一層
輸出層是第二層

61
00:03:30,680 --> 00:03:34,700
用我們的符號約定我們
稱輸入層是第零層

62
00:03:34,700 --> 00:03:37,700
技術上而言在這個神經網路
應該是三層

63
00:03:37,700 --> 00:03:40,260
因為有輸入層, 隱藏層, 和 輸出層。

64
00:03:40,260 --> 00:03:44,600
但常規的說法是, 如果您
讀研究論文跟其他時候在

65
00:03:44,600 --> 00:03:48,700
這個課程裡, 您看到人們提及
這特定的神經網路是兩層

66
00:03:48,700 --> 00:03:52,630
神經網路, 因為我們不當
輸入層是正式的一層

67
00:03:52,630 --> 00:03:55,912
最後, 一些會進入
隱藏層跟

68
00:03:55,912 --> 00:03:59,670
輸出層的會是
伴其相關的參數

69
00:03:59,670 --> 00:04:04,560
隱藏層會有
相關的參數 w 跟 b

70
00:04:04,560 --> 00:04:08,218
我將寫成上標
方括號 1 來表明這些

71
00:04:08,218 --> 00:04:12,395
參數相關於
第一層也就是隱藏層

72
00:04:12,395 --> 00:04:15,416
我們以後會
看到 w 會是一個 4 乘 3 矩陣

73
00:04:15,416 --> 00:04:19,830
b 會是 4 乘 1 向量在這個例子

74
00:04:19,830 --> 00:04:22,750
而第一個維度 4 
來自於我們有

75
00:04:22,750 --> 00:04:25,300
四個節點在我們的
隱藏層的事實，而

76
00:04:25,300 --> 00:04:28,120
3 來自於
我們有三個輸入特徵的事實

77
00:04:28,120 --> 00:04:31,600
我們會在以後談論
這些矩陣的維度

78
00:04:31,600 --> 00:04:33,540
也許屆時會比較有感覺

79
00:04:33,540 --> 00:04:37,813
但在一些輸出層有
也有與之關聯的參數 w

80
00:04:37,813 --> 00:04:42,400
上標方括號 2 跟
b 上標方括號 2

81
00:04:42,400 --> 00:04:45,747
而這些的維度是
1 乘 4 跟 1 乘 1

82
00:04:45,747 --> 00:04:49,297
而這個 1 乘 4 因為隱藏
層有四個單元

83
00:04:49,297 --> 00:04:51,177
輸出層只有一個單元

84
00:04:51,177 --> 00:04:56,378
但我們會在以後影片中談到這些
矩陣跟向量的維度

85
00:04:56,378 --> 00:04:59,839
您剛剛看到一個兩
層神經網路的樣子

86
00:04:59,839 --> 00:05:03,108
也就是
使用一個隱藏層的神經網路

87
00:05:03,108 --> 00:05:04,260
在下一個影片中

88
00:05:04,260 --> 00:05:08,513
讓我們更進一步看
這個神經網路怎麼做計算

89
00:05:08,513 --> 00:05:11,223
也就是這個神經
網路如何從輸入 x 

90
00:05:11,223 --> 00:05:14,169
一直到
計算它的輸出 y-hat