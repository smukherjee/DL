В останньому відео ти побачив/ла як виглядає 
нейронна мережа (НМ) з одним прихованим шаром. В цьому відео давай детальніше розглянемо як ця НМ обчислює вихідні дані. Ти побачиш щось дуже схоже на логістичну регресію (ЛР), 
просто повторене багато разів. Давайте поглянемо. Ось як виглядає двошарова НМ. Давай детальніше розглянемо що саме обчислює ця НМ. Тож раніше ми казали, що ЛР (цикл ЛР) складається з 2-ох кроків обчислень. 1-ий обчислює z ось таким чином, а 2-ий обчислює активатори сигмоїди від z. А НМ просто робить це багато разів. Давай для початку сфокусуємось на одному з вузлів прихованого шару. Давай глянемо на перший вузол прихованого шару. Тож я поки приховаю інші вузли. Аналогічно як в ЛР зліва ці вузли прихованого шару мають 2 кроки обчислень. 1-ий крок, ліва половина цього вузла, обчислює z=wᵀx+b. І ми повинні позначити, що всі ці величини пов'язані з першим прихованим шаром. 
Ось чому ми маємо тут багато квадратних дужок. І це - перший вузол прихованого шару. Ось чому ми маємо там ще й верхній індекс 1. Тож це 1-ий крок, а далі 2-ий крок обчислює a[1]₁, що = сигмоїді від z[1]₁. Тож для обох z та a, згідно конвенції позначень, - a[l]ᵢ - верхній індекс l в квадратних дужках - це номер шару (1), а нижній індекс i - це вузол вказаного шару. Тож вузол, який ми розглядаємо, 
знаходиться в першому (прихованому) шарі і є першим в ньому. Ось чому верхній і нижній індекси =1. Тож цей маленький круг (перший вузол) НМ відображає виконання цих 2-ох кроків обчислень. Тепер давай глянемо на 2-ий вузол прихованого шару НМ. Аналогічно як в ЛР зліва, маленький круг відображає 2 кроки обчислень. 1-ий крок обчислює z (це все ще шар 1, але вже 2-ий вузол) =wᵀx+b[1]₂, а потім a[1]₂= сигмоїді від z[1]₂. Постав відео на паузу якщо хочеш, щоб перепровірити, що верхні і нижні індекси відповідають написаному фіолетовим. Тож ми поговорили про перші 2 приховані вузли НМ. Приховані вузли 3 і 4 відображають аналогічні обчислення. Тепер давай візьмемо цю пару рівнянь і цю пару рівнянь і скопіюємо їх на наступний слайд. Тож ось НМ і перша та друга пари рівнянь, які ми нещодавно вивели для 1-ого і 2-ого прихованих вузлів. Якщо продовжити і записати відповідні рівняння для 3-ого і 4-ого прихованих вузлів, то отримаємо таке. Просто пересвідчимось, що позначення правильні. Оцей вектор w[1]₁ транспонується і множиться на x. Так? Ось що позначає верхній індекс T - що вектор транспонований. Тепер, як ти, можливо, вже здогадався/лась, для реалізації НМ робити це циклом for видається неефективним. Тож ми візьмемо ці 4 пари рівнянь і векторизуємо їх. Почнемо з того, що подивимось як обчислити z як вектор. Виявляється, це можна зробити так. Давай візьмемо ці w і складемо їх в матрицю. Ми матимемо w[1]₁ᵀ, тож це буде вектор-рядок. (Транспонований вектор-стовпець дає вектор-рядок). Далі w[1]₂ᵀ, w[1]₃ᵀ, w[1]₄ᵀ. Тож, склавши ці 4 вектори w, отримуємо матрицю. Можна також розглядати це як 4 вузли ЛР і що кожен вузол ЛР має відповідний вектор параметрів w, а, склавши разом ці вектори, ми отримаємо цю матрицю 4*3. Якщо далі взяти цю матрицю і помножити її на вхідні ознаки x₁, x₂, x₃, то, згідно правил множення матриць, отримаємо w[1]₁ᵀx, w[1]₂ᵀx, w[1]₃ᵀx, w[1]₄ᵀx. І не забуваємо про b. Тож додаємо до цього оцей вектор b[1]₁, b[1]₂, b[1]₃, b[1]₄. Тобто ось це. Тепер тут b[1]₁, b[1]₂, b[1]₃, b[1]₄. І бачимо, що кожний рядок результату чітко відповідає кожному з цих рядків, кожному з цих рівнянь вгорі. Іншими словами, ми щойно показали, що ось це = z[1]₁, z[1]₂, z[1]₃, z[1]₄ за визначенням. І, можливо очевидно, ми це будемо називати вектором z[1]. Він утворюється складанням всіх z в вектор-стовпець. Векторизація має одне практичне правило, 
що допомагає орієнтуватись [в позначеннях]: коли шар має кілька вузлів, то їх складають по вертикалі. Ось чому наші z[1]₁ по z[1]₄ відповідають 4-ом вузлам прихованого шару і ось чому ми складаємо ці 4 величини по вертикалі - 
щоб сформувати вектор z[1]. І ще дещо про позначення. Ось ця матриця 4*3, яка отримана складанням маленьких w[1]₁, w[1]₂ і т.д., яку ми позначатимемо великою W[1]. І, аналогічно, цей вектор позначатимемо b[1]. Тож це вектор 4*1. Тож тепер ми обчислили z, 
використовуючи векторно-матричні позначення. Останнє, що потрібно зробити, це обчислити значення a. І навряд чи це несподівано, ми побудуємо a[1] складанням значень активаторів a[1]₁ по a[1]₄. Тобто беремо оці 4 значення і складаємо по вертикалі в вектор a[1]. І це буде сигмоїдою від z[1]. А це буде реалізацією сигмоїдальної функції, яка бере 4 елементи z і застосовує до них поелементно сигмоїдальну функцію. Тож резюмуємо. Ми визначили, що z[1]=W[1] помножити на вектор x + вектор b[1], а a[1]= сигмоїді від z[1]. Тож давай скопіюємо це на наступний слайд і побачимо, що для 1-ого шару НМ з заданими вхідними x ми маємо z[1]=W[1]x+b[1] і a[1]=σ(z[1]). І розміри відповідно 4*1=4*3 * 3*1 + 4*1. А це - 4*1 - такий же розмір, який ми отримуємо вкінці. Пам'ятаєш ми казали, що x це a[0]? Тож ŷ=a[2]. Якщо захочеш, то можеш замінити оцей z на a[0]. Тому що a[0] - це всього лише псевдонім для вектора вхідних ознак x. А тепер, аналогічно, можемо вивести, що розрахунки в наступному шарі можуть бути записані подібним чином. 
Вихідний шар має пов'язані параметри w[2] і b[2]. w[2] в цьому випадку буде матрицею 1*4, b[2] - це просто дійсне число, тобто 1*1. А z[2] буде дійсним числом і записується як матриця 1*1. Оце буде матрицею 1*4 помноженою на 4*1 плюс b 1*1. І в результаті отримаємо дійсне число. 
Якщо розглядати результат як останній вихідний вузол, аналогічно як в ЛР, де є параметри w і b, то w відіграватиме роль w[2] транспонованого (w[2]=wᵀ), а b=b[2]. Тож якщо закрити ліву частину НМ, проігнорувати її, то залишиться лише останній вихідний вузол, дуже схожий на ЛР, який відрізняється лише тим, що замість параметрів w і b ми пишемо w[2] і b[2] з розмірами 4*1 і 1*1. Резюмуємо. Для ЛР щоб отримати результат або реалізувати передбачення потрібно обчислити z=wᵀx+b та ŷ=a=σ(z). А для НМ з одним прихованим шаром потрібно реалізувати обчислення всього лише оцих 4-ох рівнянь. І, по суті, це являється векторизованою реалізацією обчислень спочатку оцих 4-ох ЛР вузлів прихованого шару (це роблять оці рівняння), а потім оцей ЛР шар, вихідний шар (це роблять оці рівняння). Надіюсь я зрозуміло описав. 
Проте, головне запам'ятати, що щоб обчислити цю НМ, все, що потрібно, - це оці 4 рядки коду. Тож ти побачив/ла як, маючи один вектор вхідних ознак x, з допомогою 4-ох рядків коду можна обчислити результати цієї НМ. Аналогічно як для ЛР ми теж хочемо провести векторизацію для багатьох тренувальних зразків. І ми це побачимо коли складемо 3 тренувальні зразки в стовпці, в матрицю і дещо модифікуємо її. Також, аналогічно як в ЛР, ти зможеш обчислити результати НМ не лише для 1 тренувального зразка, а для всього тренувального набору. Тож давай розглянемо це детальніше в наступному відео.