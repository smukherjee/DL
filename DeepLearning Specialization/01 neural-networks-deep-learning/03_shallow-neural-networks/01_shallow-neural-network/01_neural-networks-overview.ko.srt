1
00:00:00,030 --> 00:00:05,279
이 비디오에서는 신경망을

2
00:00:02,639 --> 00:00:07,440
도입하는 방법을 배울 것입니다.

3
00:00:05,279 --> 00:00:09,059
기술적인 부분을 다루기에 앞서, 

4
00:00:07,440 --> 00:00:10,889
이 비디오에서는 이번주에 보게될 내용의 

5
00:00:09,059 --> 00:00:13,679
개용를 간단히 설명하고자 합니다.

6
00:00:10,889 --> 00:00:15,389
여기 비디오에서 다루는 내용을 

7
00:00:13,679 --> 00:00:17,100
전부 이해하시지 못하더라도 걱정하지 마십시요.

8
00:00:15,389 --> 00:00:19,680
다음 몇개의 비디오에서 조금 더 

9
00:00:17,100 --> 00:00:21,660
상세적인 내용에대해 알아보도록 하겠습니다.

10
00:00:19,680 --> 00:00:24,269
신경망 네트워크를 어떻게 도입하는지에 대해

11
00:00:21,660 --> 00:00:26,250
간략히 이야기해보겠습니다. 저번 주에는

12
00:00:24,269 --> 00:00:30,300
로지스틱 회귀분석법에 대해 이야기 했었는데요, 

13
00:00:26,250 --> 00:00:32,430
이 모델이 어떻게 여기 보이는 

14
00:00:30,300 --> 00:00:35,520
산출 그래프로 이어지는지 알아보았습니다.

15
00:00:32,430 --> 00:00:38,370
여기서는 x 특성을 z를 산출해주는 

16
00:00:35,520 --> 00:00:40,620
w 와 B 파라미터에 넣지 않았는데요.

17
00:00:38,370 --> 00:00:44,219
여기 z는 그 다음에 a를 산출하는데 쓰입니다.

18
00:00:40,620 --> 00:00:47,190
여기 ŷ값가 교차로 쓰입니다.

19
00:00:44,219 --> 00:00:51,059
그 다음에 여기 loss 함수를 계산을 

20
00:00:47,190 --> 00:00:52,920
할 수 있게 됩니다. 신경망은 여기

21
00:00:51,059 --> 00:00:54,930
이렇게 생겼습니다. 이전에 말했듯이, 

22
00:00:52,920 --> 00:00:57,239
신경망은 여거 시그모이드 

23
00:00:54,930 --> 00:01:00,420
유닛을 쌓아 합쳐서 

24
00:00:57,239 --> 00:01:02,969
구성할 수 있습니다. 이전에는 

25
00:01:00,420 --> 00:01:04,920
이 노드 부분이 2가지의 

26
00:01:02,969 --> 00:01:07,680
계산 단계를 따랐었는데요. 처음에는 

27
00:01:04,920 --> 00:01:11,640
여기 z 의 값을 구하고요, 

28
00:01:07,680 --> 00:01:14,549
두번째로는 여기 a의 값을 구합니다. 
신경망에서는 여기 노드 뭉치는 

29
00:01:11,640 --> 00:01:17,880
여기 z의 값과 같을 것이고요, 

30
00:01:14,549 --> 00:01:21,720
그리고 여기 a와 같을 것입니다.

31
00:01:17,880 --> 00:01:24,090
그리고 여기 노드는 여기 

32
00:01:21,720 --> 00:01:26,790
또 하나의 z와 

33
00:01:24,090 --> 00:01:29,040
또 하나의 a의 값을 나타낼 것입니다.

34
00:01:26,790 --> 00:01:30,000
표기법은 나중에 소개하겠지만 

35
00:01:29,040 --> 00:01:32,759
여기 이렇게 생겼습니다.

36
00:01:30,000 --> 00:01:35,430
처음에는 입력 특성 x을 넣을 것인데요.

37
00:01:32,759 --> 00:01:40,320
그 다음에 w와 b의 파라미터와 함께 말이죠. 

38
00:01:35,430 --> 00:01:42,930
그러면 이것이 z1을 계산하게 해줄 것인데요, 

39
00:01:40,320 --> 00:01:45,600
그러면 새로 소개할 표기는 

40
00:01:42,930 --> 00:01:48,689
위 첨자 괄호 1

41
00:01:45,600 --> 00:01:50,759
여기 레어라고 부르는 노드 뭉치의

42
00:01:48,689 --> 00:01:53,579
양을 나타낼 것인데요, 

43
00:01:50,759 --> 00:01:56,280
그리고 다음에는 위첨자 괄호 2로 

44
00:01:53,579 --> 00:01:58,920
Daniel이라고 불리는 것과

45
00:01:56,280 --> 00:02:01,200
연관된 양입니다.

46
00:01:58,920 --> 00:02:04,140
이것은 또 하나의 신경망 층입니다.

47
00:02:01,200 --> 00:02:06,719
여기 있는 위첨자는 괄호는

48
00:02:04,140 --> 00:02:10,319
개인 트레이닝 예시를 

49
00:02:06,719 --> 00:02:12,390
나타낼 때 쓰는 소괄호와 

50
00:02:10,319 --> 00:02:14,080
헷갈리시면 안됩니다.

51
00:02:12,390 --> 00:02:16,300
반면에 X

52
00:02:14,080 --> 00:02:19,030
위첨자 소괄호 I는 

53
00:02:16,300 --> 00:02:21,340
트레이닝 예시를 나타내고, 

54
00:02:19,030 --> 00:02:25,570
위첨자 대괄호 1과 2는 여기 이렇게 

55
00:02:21,340 --> 00:02:28,600
다른 층을 나타냅니다. 1번 층, 2번 층

56
00:02:25,570 --> 00:02:32,860
이렇게 여기 네트워크에서 말이죠, 

57
00:02:28,600 --> 00:02:35,350
z1을 산출한 다음에는, 

58
00:02:32,860 --> 00:02:39,000
로지스틱 회귀분석과 유사하게, a1을

59
00:02:35,350 --> 00:02:44,550
계산하는 방식이 있고, 이것은 그냥

60
00:02:39,000 --> 00:02:49,270
z1으 ㅣ시그모이드인데요, 그 다음에는

61
00:02:44,550 --> 00:02:54,610
z2를 계산하는데요, 또 하나의 선형 식을

62
00:02:49,270 --> 00:02:57,370
이용해서 말이죠, 그 다음에는 a2를 계산합니다.

63
00:02:54,610 --> 00:02:59,890
a2는 신경망 네트워크의 최종 결과값입니다.

64
00:02:57,370 --> 00:03:01,390
이것은 ŷ과 교차적으로 쓰일텐데요, 

65
00:02:59,890 --> 00:03:03,730
지금 이야기한 내용이 많은 내용인 것을 저도 

66
00:03:01,390 --> 00:03:06,460
알고 있는데요, 여러분이 기억하실 핵심 부분은

67
00:03:03,730 --> 00:03:09,220
로지스틱 회위분석에서는 여기 Z값과 

68
00:03:06,460 --> 00:03:11,590
그 다음에 a 계산이 따랐고, 

69
00:03:09,220 --> 00:03:13,780
그 다음에 신경망 네트워크는

70
00:03:11,590 --> 00:03:16,390
여러번 진행합니다. z 다음에 

71
00:03:13,780 --> 00:03:19,959
a 계산 다음에 또, Z 계산 진행 후, 

72
00:03:16,390 --> 00:03:22,600
다음에 a 계산이 따릅니다. 그리고 최종적으로

73
00:03:19,959 --> 00:03:24,670
loss의 값을 구합니다. 기억하시겠지만 

74
00:03:22,600 --> 00:03:27,959
로지스틱 회귀분석에서는 여기 이런 

75
00:03:24,670 --> 00:03:30,970
backward calculation 이 있었습니다.

76
00:03:27,959 --> 00:03:34,750
derivative를 계산하기 위해서 말이죠.

77
00:03:30,970 --> 00:03:36,580
여기서는 da와 dz를 계산하는 것이죠. 같은 방법으로, 

78
00:03:34,750 --> 00:03:38,860
신경망에서는 backward calculation을 

79
00:03:36,580 --> 00:03:44,910
할텐데요, 이렇게 생겼습니다.

80
00:03:38,860 --> 00:03:50,890
이 경우에서는 dz1과 dz2 를 계산해서 

81
00:03:44,910 --> 00:03:57,850
dw2 와 db2의 값을 얻을 수 있게 합니다.

82
00:03:50,890 --> 00:04:01,090
이것은 어떻게보면 오른쪽에서 왼쪽으로

83
00:03:57,850 --> 00:04:05,020
진행되는 backward calculation인데요, 

84
00:04:01,090 --> 00:04:05,360
여기 이렇게 빨간색 화살표 방향으로 

85
00:04:05,020 --> 00:04:07,970
말이죠. 

86
00:04:05,360 --> 00:04:09,770
이 내용은 신경망이 어떻게 작동하는지 

87
00:04:07,970 --> 00:04:12,950
간단한 오버뷰를 제공해주는데요, 

88
00:04:09,770 --> 00:04:14,630
로지스틱 회귀분석법을 이용해서 2번 반복하는 것입니다.

89
00:04:12,950 --> 00:04:16,880
이 강의에서는 표기를 많이 사용했는데요, 

90
00:04:14,630 --> 00:04:18,980
새로운 정보와 표기방식이 많았는데요, 

91
00:04:16,880 --> 00:04:20,900
모든 내용을 이해하지 못하셨더라도 걱정하지 마십시요. 

92
00:04:18,980 --> 00:04:22,820
조금 더 천천히 관련 내용을 다음 몇개의 

93
00:04:20,900 --> 00:04:24,620
비디오에서 다루겠습니다. 그럼 이제 다음 비디오로 넘어가서

94
00:04:22,820 --> 00:04:27,850
신경망 representation에 대해 

95
00:04:24,620 --> 00:04:27,850
이야기하겠습니다.