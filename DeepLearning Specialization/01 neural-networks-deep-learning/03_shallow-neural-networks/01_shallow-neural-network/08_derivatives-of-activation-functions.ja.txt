誤差逆伝播法を実装するためには、 活性化関数の導関数の傾きを
計算できなくてはなりません。 活性化関数の選択肢を見て、 それらの傾きを計算する方法を学びましょう。 これが、見慣れたシグモイド活性化関数です。 与えられたzにおいて、 この関数はそこに対応した傾き、
つまり導関数を持っています。 この小さい三角形の横幅に対する高さですね。 g(z)がシグモイド関数のとき、
その傾きはd/dz g(z)です。 微積分学から、
これがg(x)のzにおける傾きだとわかります。 もしあなたが微積分学を知っていて、
微分の方法がわかるなら、 シグモイド関数を微分したとき、 それがこの式と等しくなると
わかります。 繰り返しますが、ここでは微分の方法を
一つ一つやることはありません。 しかし、もし微積分学に詳しいなら ぜひ動画を止めて自分で証明してみてください。 これはg(z)×(1-g(z))と等しくなります。 では、この式が本当に意味を成すかどうか
確認してみましょう。 まず、zがとても大きい数字、
例えば10だとしましょう。 するとg(z)は1に近くなり、 この左の式から、d/dz g(z)は 1×(1-1)で、 とても0に近い数字となることがわかります。 実際これは正しいですね。
zが大きいとき、傾きは0に近くなりますから。 逆に、もしzが-10のとき、この辺りで、
g(z)は0に近いです。 左の式によると、 d/dz g(z)は0×(1-0)に近い値となります。 なので、これもとても0に近くなり、
正しい答えとなります。 最後に、zが0のとき、g(z)は1/2です。 シグモイド関数のこの部分ですね。 このとき、導関数は 1/2×(1-1/2)で、1/4となります。 それは実際、z=0のときの導関数、
つまりこの関数の傾きとして 正しい値なのです。 最後に、もう少し表記法を
紹介します。 たまに、これを書く代わりに、 導関数を簡略化したg'(z)という書き方を
使います。 この上にあるダッシュはプライムと呼ばれます。 微積分学におけるg'(z)とは、 入力変数zについての関数gの導関数を
簡略化した書き方です。 ニューラルネットワークでは、a=g(z)で、 それはこれと等しくなります。 すると、これはa×(1-a)と書くことができます。 たまに実装するとき、 g'(z)=a(1-a)というような書き方を
目にすることがあるでしょう。 それは、導関数を意味するg'が これと等しいということを
示しているだけです。 この公式の利点は、aの値を計算してあれば、 この式を使って、傾き、つまりg'の値も すぐに計算できるということです。 ここまでが、シグモイド活性化関数でした。 次はtanh活性化関数を見てみましょう。 前と同じような感じで、d/dz g(z)の定義は あるzにおけるg(z)の傾きです。 そして、双曲線のタンジェント関数を見れば、 そしてもし微積分を知っていれば、 微分がこのシンプルな式で表せると
わかるでしょう。 前と同じように簡略化した書き方を使えば、 これはg'(z)と表せます。 やりたければ、この式が意味を成すかどうか
確認できます。 例えばz=10のとき、tanh(z)は1に近くなります。 これは+1から-1になります。 そしてg'(z)は、この式によれば、 1-1^1、すなわち0になります。 つまり、zがとても大きければ傾きは0に近くなる
ということです。 逆に、zが小さい数字、例えば-10だとしたら、 tanh(z)は-1に近くなります。 するとg'(z)は1-(-1)^2となるので、 1-1と近くなり、こちらも0に近くなります。 最後に、z=0なら、tanh(z)=0となり、 傾きは1となります。 実際z=0のときの傾きは1となっています。 要約すると、
a=g(z)つまりaがtanh(z)と等しいとき、 微分したg'(z)は1-a^2となります。 繰り返しますが、
もしaの値を前もって計算してあれば、 この公式を使って
微分も素早く計算できるのです。 最後に、ReLUとLeaky ReLU活性化関数の 微分を計算する方法です。 ReLUでは、g(z)=max(0,z)です。 なので、zが0より小さい場合、
微分は0となります。 zが0より大きい場合は1となります。 実は、zが0ちょうどの場合は
微分は定義できません。 しかし、ソフトウェアでこれを実装するとき、 数学的に完璧ではないかもしれません。 しかし、zが0ちょうどのとき 微分を1か0に設定しても問題はないでしょう。 気にする必要はありません。 最適化するときに、 g'(z)は実際は
g(z)の劣勾配と呼ばれるものになって、 そのため最急降下法がちゃんと働きます。 しかし、zがぴったり0.00000...になる可能性は
とても小さいので、 z=0のときの微分を何に定義しても あまり問題はありません。 これが、実際に微分を実装する方法です。 最後に、Leaky ReLU活性化関数で ニューラルネットワークを訓練するとき g(z)は0.01zとzの大きい方を取ります。 なので、zが0より小さいとき
g'(z)は0.01となり、 zが0より大きいとき、1になります。 もう一度言いますが、 zがちょうど0のとき、
厳密に言うと勾配は定義されません。 しかし実装する際には、z=0のとき g'(z)を0.01か1のどちらに設定しても
問題ありません。 どちらでもコードはちゃんと動きます。 これらの公式を使って、
活性化関数の勾配、つまり微分を 計算できるようになりました。 ピースが揃ったところで、 ニューラルネットワークの最急降下法を
実装する方法を学びましょう。 では、次の動画に進みましょう。