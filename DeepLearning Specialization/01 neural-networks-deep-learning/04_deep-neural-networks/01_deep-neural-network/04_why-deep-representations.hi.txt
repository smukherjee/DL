हम सब सुन रहे हैं कि डीप न्यूरल नेटवर्क 
बहुत अच्छा काम करते हैं बहुत सी समस्याओं के लिए, और ऐसा नहीं है कि
 वे होने चाहिए एक बड़े नेटवर्क्स, कि विशेष रूप से, वे होने चाहिए डीप या 
होनी चाहिए बहुत सी हिडन लेयर्स. तो ऐसा क्यों है? चलो देखते हैं कुछ उदाहरण और कोशिश 
करते हैं समझने की कि क्यों डीप नेटवर्क्स शायद अच्छा चलते हैं. तो पहले, क्या है एक डीप नेटवर्क कम्प्यूटिंग? यदि आप बना रहे हैं एक सिस्टम
 फ़ेस रेकग्निशन के लिए या फ़ेस डिटेक्शन के लिए, यहाँ है कि 
क्या एक डीप न्यूरल नेटवर्क करेगा. शायद आप इनपुट करें एक छवि एक 
फ़ेस की तब पहली लेयर न्यूरल नेटवर्क की को आप सोच सकते हैं शायद एक फ़ीचर 
डिटेक्टर जैसे या एक एज डिटेक्टर जैसे. इस उदाहरण में, मैं प्लॉट कर रहा हूँ 
क्या है एक न्यूरल नेटवर्क 
शायद 20 हिडन यूनिट्स के साथ, कोशिश करेगा कम्प्यूट करने की इस छवि पर. तो 20 हिडन यूनिट्स विज़ूअलाइज़ करती हैं 
इन छोटे वर्ग बक्सों से. तो उदाहरण के लिए, यह छोटा 
विज़ूअलाइज़ेशन दर्शाता है एक हिडन यूनिट कोशिश कर रहा हैं समझने कि कहाँ है एजेज़ 
उस ऑरीएंटेशन में उस छवि में. और शायद यह हिडन यूनिट कोशिश 
कर रहा है समझने की कहाँ है हॉरिज़ॉंटल एजेज़ उस छवि में. और फिर जब हम बात करेंगे 
कॉन्वलूशनल नेटवर्क्स की बाद के एक कोर्स में, यह विशेष विज़ूअलाइज़ेशन 
थोड़ा ज़्यादा समझ आएगी. लेकिन फ़ॉर्म, आप समझ सकते हैं पहली 
लेयर न्यूरल नेटवर्क की जैसे कि देखता है छवि को और समझने की कोशिश करता है 
कहाँ है एजेज़ इस छवि में. अब, चलो सोचते हैं कहाँ है एजेज़ 
इस छवि में इकट्ठा करके पिक्सल्स को बनाने के लिए एजेज़. फिर यह लेता है डिटेक्ट किए हुए एजेज़ को और इकट्ठा 
करता है एजेज़ को बनाने के लिए हिस्से फ़ेस के. तो उदाहरण के लिए, आपके पास हो सकता है 
एक न्यूरल न्यूरॉन देखने के लिए कि क्या यह ढूँढ रहा है एक आँख, या एक भिन्न न्यूरॉन ढूँढने की कोशिश कर रहा है नाक. और इसलिए एक साथ रखने से बहुत से एजेज़, यह शुरू करता है डिटेक्ट करने विभिन्न हिस्से फ़ेस के. और फिर, अंत में, एक साथ रखते हुए
 भिन्न हिस्से फ़ेस के, जैसे आँख या एक नाक या एक कान या एक ठोड़ी, 
यह तब कोशिश करता है पहचानने की या डिटेक्ट करने की विभिन्न प्रकार के फ़ेस. तो स्वाभाविक रूप से, आप समझ सकते हैं शुरू की 
लेयर न्यूरल नेटवर्क की को डिटेक्ट केरते हुए सरल फ़ंक्शंज़ जैसे एजेज़. और फिर इकट्ठा करते हुए इन्हें बाद की लेयर्स में 
एक न्यूरल नेटवर्क की ताकि यह लर्न कर सके और अधिक जटिल फ़ंक्शंज़. ये विज़ूअलाइज़ेशन अधिक समझ आएँगी 
जब हम बात करेंगे कॉन्वलूशनल नेट्स की. और एक तकनीकी जानकारी इस विज़ूअलाइज़ेशन की, एज डिटेक्टर्स देख रहे हैं अपेक्षाकृत 
छोटे क्षेत्रों में एक छवि के, शायद बहुत छोटे क्षेत्रों में उस जैसे. और फिर फ़ेशल डिटेक्टर्स देख सकते हैं 
शायद बड़े क्षेत्रों में छवि के लेकिन मुख्य अनुभव जो आप लेते हैं इससे कि सिर्फ़ 
ढूँढने के लिए सरल चीज़ें जैसे एजेज़ और फिर बनाना उन्हें. बनाना उन्हें एक साथ डिटेक्ट करने के लिए
 अधिक जटिल चीज़ें जैसे एक 
आँख और नाक और फिर रखना उन्हें एक साथ ढूँढने के लिए 
और भी अधिक जटिल चीज़ें. और इस तरह की सरल से जटिल पदानुक्रमित निरूपण, या कॉम्पज़िशनल निरूपण, अप्लाई होता है अन्य तरह के डेटा को भी, 
छवियों और फ़ेस रेकग्निशन के अलावा. उदाहरण के लिए, यदि आप बना रहे हैं 
एक स्पीच रेकग्निशन सिस्टम, यह कठिन होता है दोबारा विज़ूअलाइज़ 
करना स्पीच लेकिन यदि आप इनपुट करते हैं एक ऑडियो क्लिप तब 
शायद पहला स्तर एक न्यूरल नेटवर्क का शायद सीखे डिटेक्ट करना निम्न स्तर का 
ऑडियो वेव फ़ॉर्म फ़ीचर, जैसे कि 
क्या यह टोन ऊपर जा रही है? क्या यह नीचे जा रही है? क्या यह वाइट नोईस है या आवाज़ जैसे स्सस्स. और पिच क्या है? और जब वह उस पर जाता है, डिटेक्ट करना 
निम्न स्तर का वेव फ़ॉर्म फ़ीचर उस जैसा. और फिर बनाकर निम्न स्तर वेव फ़ॉर्म्स, शायद आप सीखेंगे डिटेक्ट करना 
मूल यूनिट आवाज़ के. भाषाविज्ञान में उन्हें स्वनिम / फ़ोनीम्स कहते हैं. लेकिन, उदाहरण के लिए, शब्द cat में, 
C है एक स्वनिम, A है एक स्वनिम, T एक और स्वनिम है. लेकिन सीखता हैं ढूँढना शायद मूल यूनिटस 
आवाज़ की और फिर बनाकर उन्हें एक साथ शायद 
सीखता है पहचानना शब्द ऑडियो में. और फिर शायद बनाकर उन्हें एक साथ, पहचानने के लिए पूरे वाक्यांश या वाक्य. तो बहुत सी हिडन लेयर वाला डीप न्यूरल 
नेटवर्क में हो सकती है शुरू की लेयर्स सीखें ये निम्न स्तर के सरल फ़ीचर्ज़ और फिर बाद की लेयर्स फिर रखती है एक साथ 
सरलतर चीज़ें जो उसने डिटेक्ट की हैं डिटेक्ट करने के लिए अधिक जटिल चीज़ें 
जैसे पहचानना ख़ास शब्द या यहां तक कि वाक्यांश या वाक्य भी. जो बोला गया है करने के लिए स्पीच रेकग्निशन. और हम क्या देखते हैं कि जहाँ शुरू की लेयर्स 
कम्प्यूट कर रही है जो लगते हैं अपेक्षाकृत सरल फ़ंक्शन्स इनपुट के 
जैसे कहाँ हैं एजेज़, जब तक आप पहुँचते हैं आगे नेटवर्क में आप कर सकते हैं 
आश्चर्यजनक रूप से जटिल चीज़ें. जैसे फ़ेस डिटेक्ट करना या डिटेक्ट करना 
शब्द या वाक्यांश या वाक्य. कुछ समानता दिखाते हैं डीप न्यूरल नेटवर्कस और मानव मस्तिष्क में, जहां हम मानते हैं, 
या न्यूरोसाययंटिस्ट्स विश्वास करते हैं, कि मानव मस्तिष्क भी शुरू करता है 
सरल चीजों का पता लगाने से 
जैसे एजेज़ उनमें जो आपकी आँखें देखती हैं फिर बनाना उन्हें 
डिटेक्ट करने के लिए अधिक जटिल चीज़ें जैसे कि चेहरे जो आप देखते हैं. मुझे लगता है समानता डीप लर्निंग और मानव मस्तिष्क में कभी-कभार
 थोड़ा खतरनाक भी होता है. लेकिन वहां बहुत कुछ सच है, वह यह कि 
हम कैसे सोचते है कि मानव मस्तिष्क 
कैसे काम करता है और और कि मानव मस्तिष्क शायद डिटेक्ट करता है 
सरल चीज़ें जैसे एजेज़ और रखता है उन्हें एक साथ बनाने के अधिक 
जटिल चीज़ें और उसने दी है एक कुछ प्रेरणा 
कुछ लोगों को सीखने में भी. हम देखेंगे थोड़ा और मानव मस्तिष्क के बारे में या जैविक मस्तिष्क के बारे में बाद के 
वीडियो में इस सप्ताह के. अन्य समझ कि क्यों डीप न्यूरल 
नेटवर्क प्रतीत होते हैं सही काम करते हुए है निम्नलिखत. तो यह परिणाम आता है सर्किट थ्योरी से 
जो सम्बंधित है सोच से कि किस तरह के फ़ंक्शन आप कम्प्यूट 
कर सकते हैं विभिन्न AND गेट्स, OR गेट्स से 
मूल रूप से लॉजिक गेट्स से. तो अनौपचारिक रूप से, उनके फ़ंक्शन कम्प्यूट 
करते है एक अपेक्षाकृत छोटे लेकिन डीप न्यूरल नेटवर्क से और छोटे न्यूरल नेटवर्क से मेरा 
मतलब है संख्या हिडन यूनिट्स की अपेक्षाकृत कम. लेकिन यदि आप कम्प्यूट करने की कोशिश करें 
वही फ़ंक्शन एक उथले नेटवर्क में, जिसमें बहुत सी हिडन लेयर्स नहीं है, तब आपको शायद चाहिए होंगी 
एक्स्पोनेनशीयल रूप से अधिक हिडन यूनिट्स 
कम्प्यूट करने के लिए. तो मैं देता हूँ आपको केवल एक और उदाहरण 
और समझाता हूँ इसे थोड़ा और आसान ढंग से. लेकिन मान लो आप कोशिश कर रहे हैं 
कम्प्यूट करने की इक्स्क्लूसिव OR या पैरिटी पैरिटी सारे इनपुट फ़ीचर्ज़ की. तो आप कम्प्यूट करना चाह रहे हैं X1,
XOR, X2, XOR, X3, XOR, आगे Xn यदि आपके
 पास हैं n या n X फ़ीचर्ज़. तो यदि आप बनाते हैं XOR ट्री इस तरह का, 
तो हमारे लिए यह कम्प्यूट करता है 
XOR X1 और X2, फिर लेता है X3 और X4 और 
कम्प्यूट करता है उनका XOR. और तकनीकी तौर पर, यदि आप उपयोग कर 
रहे हैं AND गेट या NOT गेट, आप को 
आवश्यकता हो सकती है कुछ लेयर्स की कम्प्यूट करने की XOR
 फ़ंक्शन बजाय सिर्फ़ एक लेयर के, लेकिन एक अपेक्षाकृत छोटे सर्किट से, आप कम्प्यूट 
कर सकते हैं XOR, और इसी प्रकार आगे. और फिर आप बना सकते हैं बूलीयन, 
एक XOR ट्री इस तरह, जब तक अंत में, आपके पास है एक सर्किट यहाँ 
जो आउट्पुट करता है, चलो, उसे कहते हैं Y. Y हैट की आउट्पुट है बराबर Y के. इक्स्क्लूसिव OR, पैरिटी इन सब इनपुट बिट्स की. तो कम्प्यूट करने के लिए गहराई बाएँ नेटवर्क 
की होगी लॉग N की रेंज में. तो हमारे पास होगा इस तरह का XOR ट्री. तो नोड्स की संख्या या सर्किट घटकों की 
संख्या या गेट्स की संख्या इस नेटवर्क में उतनी 
बड़ी नहीं है. आपको नहीं चाहिए उतने गेट्स कम्प्यूट 
करने के लिए इक्स्क्लूसिव OR. लेकिन अब, यदि आपको अनुमति नहीं है 
न्यूरल नेटवर्क इस्तेमाल करने की जिसमें 
बहुत सी हिडन लेयर्स हो, इस केस में, 
लॉग N की रेंज में, यदि आपको मजबूर किया जाता है
 कम्प्यूट करने के लिए 
यह फ़ंक्शन सिर्फ़ एक हिडन लेयर से, तो आपके पास हैं ये सब चीज़ें जाते हुए अंदर, 
तो हिडन यूनिट्स. और फिर ये चीज़ें फिर आउट्पुट Y. फिर कम्प्यूट करने के लिए यह XOR फ़ंक्शन, 
इस हिडन लेयर को होना चाहिए एक्स्पोनेनशीयल रूप से बड़ा,
 क्योंकि मूलत:, आपको आवश्यकता है एक एक कर के सारे 
सम्भव कन्फ़िग्यरेशन बताने की. तो 2 की पावर N की रेंज के सभी 
सम्भव कन्फ़िग्यरेशन इनपुट बिट्स के जिनसे मिलता है इक्स्क्लूसिव 
OR 1 या 0. तो आपको लेनी पड़ती है एक हिडन लेयर जो 
एक्स्पोनेनशीयल रूप से बड़ी है बिट्स की संख्या में. मुझे लगता है 
तकनीकी रूप से आप कर सकते हैं इसे 
2 की पॉवर N घटा 1 हिडन यूनिट्स से. लेकिन वह है 2 की पॉवर N की रेंज जो 
एक्स्पोनेनशीयल रूप से बड़ी है 
बिट्स की संख्या से. तो मुझे आशा है कि यह आपको एक ज्ञान 
देता है कि कुछ गणितीय फ़ंक्शन्स है, जो आसान है कम्प्यूट करना डीप नेटवर्क्स से 
तुलना में उथले नेटवर्क्स से. वास्तव में, मैंने व्यक्तिगत रूप से सर्किट थ्योरी से 
परिणाम कम उपयोगी पाए समझने के लिए, लेकिन एक परिणाम 
जिसका लोग अक्सर उद्धरण करते हैं जब समझाते हैं 
मूल्य बहुत डीप रेप्रेज़ेंटेशन का. अब, इस कारण के अलावा पसंद करना डीप न्यूरल नेटवर्क है, मैं सोचता हूँ अन्य कारण हैं कि टर्म डीप लर्निंग 
प्रसिद्ध हो गया है केवल ब्राण्ड की तरह. इन चीज़ों को कहते थे बहुत सी 
हिडन लेयर वाला न्यूरल नेटवर्क. वाक्यांश डीप लर्निंग है केवल एक बड़ा ब्राण्ड, 
यह सिर्फ़ उतना गहरा है. तो मैं सोचता हूँ जब वह टर्म लोकप्रिय हो गयी 
तब वास्तव में न्यूरल नेटवर्क को 
दोबारा ब्राण्ड करने से या बहुत सी हिडन लेयर वाले न्यूरल नेटवर्क को 
दोबारा ब्राण्ड करने से सहायता हुई कैप्चर करने में लोकप्रिय 
कल्पना भी. वे इसे शुद्ध ब्रैंडिंग समझते हैं कि डीप नेटवर्क्स 
बेहतर काम करते हैं. कभी-कभी लोग हद से बाहर चले जाते हैं 
और ज़ोर देते हैं बहुत सी हिडन लेयर्स 
इस्तेमाल करने पर. लेकिन जब मैं शुरू करता हूँ एक नई समस्या,
 मैं अक्सर शुरू करता हूँ लॉजिसिटिक रेग्रेशन से, फिर प्रयास करता हूँ 
कुछ एक या दो हिडन लेयर्स से और इस्तेमाल करता हूँ
 वह एक हाइपर पेरमिटर जैसे. इस्तेमाल करता हूँ वह एक पेरमिटर या 
हाइपर पेरमिटर जैसे जो आप ट्यून करते हैं 
जानने के लिए सही गहराई आपके न्यूरल नेटवर्क की. लेकिन पिछले कई वर्षों से एक प्रवृत्ति है 
लोगों को खोजने की ओर कि कुछ ऐप्लिकेशन्स के लिए बहुत ज़्यादा डीप 
न्यूरल नेटवर्क्स यह जिसमें शायद बहुत दर्जनों लेयर्स हैं कभी-कभी, कर सकती है 
सबसे अच्छा मॉडल एक समस्या को. तो बस इतना ही समझने के लिए कि क्यों 
डीप लर्निंग प्रतीत होता है बेहतर 
काम करते हुए. चलो अब देखते हैं तकनीक को कि कैसे 
इम्प्लमेंट करना है न केवल फ़ॉर्वर्ड प्रॉपगेशन बल्कि बैक प्रॉपगेशन भी.