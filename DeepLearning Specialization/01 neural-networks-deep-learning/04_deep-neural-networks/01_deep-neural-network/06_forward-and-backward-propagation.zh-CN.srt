1
00:00:00,000 --> 00:00:01,350
在上节课里

2
00:00:01,350 --> 00:00:05,065
我们学习了搭建深度神经网络时所需的基本模块

3
00:00:05,065 --> 00:00:07,696
比如每层都有前向传播的步骤

4
00:00:07,696 --> 00:00:09,975
以及对应的反向传播的步骤

5
00:00:09,975 --> 00:00:12,370
这节课我们会讲一下如何实现这些步骤

6
00:00:12,370 --> 00:00:14,230
首先 我们来说前向传播

7
00:00:14,230 --> 00:00:20,045
回忆一下 这一步做的是输入a[l-1] 输出a[l]

8
00:00:20,045 --> 00:00:21,795
以及将z[l]放入缓存

9
00:00:21,795 --> 00:00:24,615
而在实践中

10
00:00:24,615 --> 00:00:28,145
我们也同样会把w[l]和b[l]放入缓存

11
00:00:28,145 --> 00:00:31,585
这样调用起来会比较容易

12
00:00:31,585 --> 00:00:35,438
下面这个的公式大家应该很熟悉了

13
00:00:35,438 --> 00:00:46,813
前向函数就是z[l]=w[l]*a[l-1]+b[l]

14
00:00:46,813 --> 00:00:53,620
其中a[l]是作用于z[l]的激活函数

15
00:00:53,620 --> 00:00:57,250
如果你想用向量化来实现

16
00:00:57,250 --> 00:01:06,218
那就是Z[l]=W[l]*A[l-1]+b[l]

17
00:01:06,218 --> 00:01:09,930
这里加上的b能通过python的广播来实现<br />(详见第二周Broadcasting in Python课程)

18
00:01:09,930 --> 00:01:15,296
A[l]是将激活函数作用于矩阵Z[l]每一个元素后得到的结果

19
00:01:15,296 --> 00:01:20,128
你应该还记得 在前向传播的计算导图中

20
00:01:20,128 --> 00:01:22,565
我们曾经画了这样一个前向流程图

21
00:01:22,565 --> 00:01:26,775
通过输入a[0]来进行初始化

22
00:01:26,775 --> 00:01:29,305
这里的a[0]就是X

23
00:01:29,305 --> 00:01:31,440
所以初始化的这个

24
00:01:31,440 --> 00:01:33,380
就是最上面这个输入值

25
00:01:33,380 --> 00:01:40,207
如果你只有一个训练样本

26
00:01:40,207 --> 00:01:42,255
a[0]就是这个训练样本的输入特征

27
00:01:42,255 --> 00:01:45,800
如果你在处理整个训练集

28
00:01:45,800 --> 00:01:48,215
代表整个训练集的输入特征

29
00:01:48,215 --> 00:01:51,990
这就是前向传播计算导图的初始输入值

30
00:01:51,990 --> 00:01:53,970
然后以此类推

31
00:01:53,970 --> 00:01:57,085
就能从左到右计算前向传播

32
00:01:57,085 --> 00:02:00,110
接着 我们来讨论反向传播

33
00:02:00,110 --> 00:02:03,585
这个代表输入da[l]

34
00:02:03,585 --> 00:02:08,040
输出da[l-1]和dW[l]以及db[l]

35
00:02:08,040 --> 00:02:16,240
我来写一下需要用到计算步骤 dz[l]=da[l]*g[l]'(z[l])

36
00:02:16,240 --> 00:02:23,866
da[l]与g[l](z[l])矩阵元素依次相乘

37
00:02:23,866 --> 00:02:27,405
然后 计算导数

38
00:02:27,405 --> 00:02:34,420
dW[l]=dz[l]*a[l-1]

39
00:02:34,420 --> 00:02:37,445
这里我没有写把它们放入缓存

40
00:02:37,445 --> 00:02:39,114
但是你要记得这是必须的

41
00:02:39,114 --> 00:02:47,560
接着db[l]=dz[l]

42
00:02:47,560 --> 00:02:59,089
最后da[l-1]=W[l]转置乘以dz[l]

43
00:02:59,089 --> 00:03:02,310
在这里我不会把求导过程写出来

44
00:03:02,310 --> 00:03:06,605
但结果是 若将da的定义代入这里

45
00:03:06,605 --> 00:03:10,260
你会得到一个我们之前上课见过的公式

46
00:03:10,260 --> 00:03:16,617
即用之前的dz[l]来计算新的dz[l]

47
00:03:16,617 --> 00:03:18,135
如果我在这里代入

48
00:03:18,135 --> 00:03:33,817
你会得到dz[l]=w[l+1]转置乘以dz[l+1]乘以g[l]‘(z[l])

49
00:03:33,817 --> 00:03:36,165
我知道这里看起来有很多公式

50
00:03:36,165 --> 00:03:38,360
不用担心 这些公式以前都出现过

51
00:03:38,360 --> 00:03:40,820
就是上周我们

52
00:03:40,820 --> 00:03:43,055
在学习单隐藏层神经网络时

53
00:03:43,055 --> 00:03:45,930
见过的反向传播等式

54
00:03:45,930 --> 00:03:48,602
注意 这里是矩阵元素依次相乘

55
00:03:48,602 --> 00:03:54,950
所以其实只需要这四个等式来进行反向函数计算

56
00:03:54,950 --> 00:03:58,735
最后 我们来写一下向量化的版本

57
00:03:58,735 --> 00:04:04,135
第一行就是dZ[l]=dA[l]与

58
00:04:04,135 --> 00:04:11,045
g[l]'(Z[l])的元素乘积

59
00:04:11,045 --> 00:04:13,060
这比较容易理解

60
00:04:13,060 --> 00:04:23,715
dW[l]等于1/m乘以dZ[l]*A[l-1]转置

61
00:04:23,715 --> 00:04:30,844
然后db[l]等于1/m乘以np.sum(dZ[l])

62
00:04:30,844 --> 00:04:37,970
np.sum(dZ[l], axis=1, keepdims=True)

63
00:04:37,970 --> 00:04:44,095
上周我们讲过使用np.sum来计算db的方法

64
00:04:44,095 --> 00:04:56,155
最后dA[l-1]等于W[l]转置乘以dZ[l]

65
00:04:56,155 --> 00:05:02,435
这个公式模型能通过输入da[l] 这里 和这里

66
00:05:02,435 --> 00:05:07,950
来输出dW[l]和db[l]

67
00:05:07,950 --> 00:05:10,100
以及da[l-1]

68
00:05:10,100 --> 00:05:16,022
这些你需要的导数

69
00:05:16,022 --> 00:05:18,905
这就是构建反向函数的方法

70
00:05:18,905 --> 00:05:20,560
总结一下

71
00:05:20,560 --> 00:05:23,585
对于输入x

72
00:05:23,585 --> 00:05:25,060
第一层

73
00:05:25,060 --> 00:05:28,445
也许有一个ReLU激活函数

74
00:05:28,445 --> 00:05:30,570
第二层

75
00:05:30,570 --> 00:05:33,340
可能会使用另一个ReLU激活函数

76
00:05:33,340 --> 00:05:35,175
到第三层

77
00:05:35,175 --> 00:05:39,595
如果你要做二元分类 可能会用Sigmoid激活函数

78
00:05:39,595 --> 00:05:41,875
以及输出结果y帽

79
00:05:41,875 --> 00:05:43,890
通过y帽

80
00:05:43,890 --> 00:05:46,265
可以计算出损失

81
00:05:46,265 --> 00:05:49,685
然后就可以开始向后迭代了

82
00:05:49,685 --> 00:05:51,775
我们先把箭头都画好

83
00:05:51,775 --> 00:05:54,320
这样之后就不用一直换笔了

84
00:05:54,320 --> 00:06:03,430
这里我们会用反向传播计算导数

85
00:06:03,430 --> 00:06:16,063
来得到dw[3] db[3] dw[2] db[2] dw[1] db[1]

86
00:06:16,063 --> 00:06:18,865
在此过程中

87
00:06:18,865 --> 00:06:24,820
缓存输出z[1] z[2] z[3]

88
00:06:24,820 --> 00:06:32,250
这里会反向传播da[2]和da[1]

89
00:06:32,250 --> 00:06:34,730
然后就可以计算da[0]

90
00:06:34,730 --> 00:06:35,880
但这并没有意义

91
00:06:35,880 --> 00:06:37,935
所以我们直接将它去掉就好了

92
00:06:37,935 --> 00:06:40,785
以上就是实现一个三层神经网络

93
00:06:40,785 --> 00:06:44,040
前向传播和反向传播的流程

94
00:06:44,040 --> 00:06:46,140
现在 还有最后一个细节没讲

95
00:06:46,140 --> 00:06:48,735
当前向传播时

96
00:06:48,735 --> 00:06:52,420
我们会用输入数据X来进行初始化

97
00:06:52,420 --> 00:06:54,090
那反向传播又是如何初始化的呢?

98
00:06:54,090 --> 00:06:59,235
当你使用逻辑回归

99
00:06:59,235 --> 00:07:01,065
做二元分类时

100
00:07:01,065 --> 00:07:02,977
da[l]

101
00:07:02,977 --> 00:07:09,685
等于 -y/a+(1-y)/(1-a)

102
00:07:09,685 --> 00:07:12,575
可以得出最终输出的

103
00:07:12,575 --> 00:07:14,180
损失函数对y帽求导

104
00:07:14,180 --> 00:07:17,475
结果就是这样的形式

105
00:07:17,475 --> 00:07:19,105
如果你熟悉微积分

106
00:07:19,105 --> 00:07:21,328
可以尝试推导损失函数L

107
00:07:21,328 --> 00:07:24,150
对y帽或者a求导

108
00:07:24,150 --> 00:07:26,505
你就会得到这个公式

109
00:07:26,505 --> 00:07:31,350
这个输出da的公式会用在最终层L上

110
00:07:31,350 --> 00:07:35,715
当然 如果要使用向量化实现

111
00:07:35,715 --> 00:07:38,351
先要初始化反向传播

112
00:07:38,351 --> 00:07:43,706
在L层用dA[l]代表这个公式

113
00:07:43,706 --> 00:07:48,465
即使对不同的例子来说 这也是一样的

114
00:07:48,465 --> 00:07:54,011
对第一个训练样本dA[l]=(-y[1]/a[1])

115
00:07:54,011 --> 00:07:55,285
加上(1-y[1])/(1-a[1])

116
00:07:55,285 --> 00:07:58,153
从第一个训练样本

117
00:07:58,153 --> 00:08:05,185
一直到第m个训练样本 最后除以(1-a[m])

118
00:08:05,185 --> 00:08:09,423
这是实现向量化的方法

119
00:08:09,423 --> 00:08:13,055
也就是如何初始化一个向量化版本反向传播的方法

120
00:08:13,055 --> 00:08:16,100
现在 我们已经学习了前向传播

121
00:08:16,100 --> 00:08:20,030
以及反向传播的基础知识

122
00:08:20,030 --> 00:08:22,340
如果你尝试使用这些公式

123
00:08:22,340 --> 00:08:24,530
可以通过前向传播以及反向传播

124
00:08:24,530 --> 00:08:27,640
来得到你需要的导数

125
00:08:27,640 --> 00:08:29,660
你也许会觉得公式过多不便于理解

126
00:08:29,660 --> 00:08:32,080
开始迷茫 不知道这些公式能干什么

127
00:08:32,080 --> 00:08:34,645
如果你有这样的困扰 我建议

128
00:08:34,645 --> 00:08:37,205
当你在本周的编程实践时

129
00:08:37,205 --> 00:08:40,175
亲手实现这些步骤

130
00:08:40,175 --> 00:08:42,020
这会让你的理解更加深刻

131
00:08:42,020 --> 00:08:43,805
这节课有很多公式

132
00:08:43,805 --> 00:08:46,265
也有些公式不容易弄懂

133
00:08:46,265 --> 00:08:49,055
如果有条件的话 你最好自己用微积分

134
00:08:49,055 --> 00:08:50,905
和线性代数知识来推导一遍

135
00:08:50,905 --> 00:08:52,430
我知道这有难度 所以这并不强求

136
00:08:52,430 --> 00:08:56,390
事实上 这已经是机器学习中比较困难的推导了

137
00:08:56,390 --> 00:08:57,950
这些课程中列出的公式

138
00:08:57,950 --> 00:09:02,685
或者说微积分公式 仅仅是反向传播中的导数计算

139
00:09:02,685 --> 00:09:04,750
再强调一下 如果你觉得这些公式看起来有点抽象

140
00:09:04,750 --> 00:09:06,400
不容易理解

141
00:09:06,400 --> 00:09:09,108
我的建议是 认真完成作业

142
00:09:09,108 --> 00:09:11,465
然后就会豁然开朗

143
00:09:11,465 --> 00:09:14,120
但我不得不说 即便是现在

144
00:09:14,120 --> 00:09:16,805
当我实现一个机器学习算法的时候

145
00:09:16,805 --> 00:09:18,000
有时我也会惊讶

146
00:09:18,000 --> 00:09:21,170
我的机器学习算法被证明有效

147
00:09:21,170 --> 00:09:25,670
是因为机器学习的复杂度来源于数据而非一行行的代码

148
00:09:25,670 --> 00:09:27,095
所以 有时候你会觉得

149
00:09:27,095 --> 00:09:28,685
你写了几行代码

150
00:09:28,685 --> 00:09:30,110
但不确定代码在干什么

151
00:09:30,110 --> 00:09:31,625
最后它们竟然产生了神奇的结果

152
00:09:31,625 --> 00:09:35,296
因为实际上大部分神奇的地方并不在你写的

153
00:09:35,296 --> 00:09:37,090
几行短短的代码中

154
00:09:37,090 --> 00:09:38,705
可能并不是真的那么短

155
00:09:38,705 --> 00:09:40,730
但不会是成千上万行

156
00:09:40,730 --> 00:09:42,115
的代码

157
00:09:42,115 --> 00:09:44,750
而是碰巧输入了大量数据

158
00:09:44,750 --> 00:09:46,850
即使我已经从事机器学习很多年了

159
00:09:46,850 --> 00:09:49,400
有时候我依然感到意外

160
00:09:49,400 --> 00:09:53,274
我的机器学习算法起效是因为算法的复杂度来源于数据

161
00:09:53,274 --> 00:09:55,970
而不一定是

162
00:09:55,970 --> 00:10:01,020
你写的成千上万行的代码

163
00:10:01,020 --> 00:10:05,935
好了 这就是实现深度神经网络的方法

164
00:10:05,935 --> 00:10:10,325
再提醒大家一下 你完成课后作业就会更了解一些

165
00:10:10,325 --> 00:10:14,220
在进入下一个话题前 下个视频中

166
00:10:14,220 --> 00:10:17,480
我们会讨论超参数和参数

167
00:10:17,480 --> 00:10:19,681
当你在训练深度网络时

168
00:10:19,681 --> 00:10:22,225
若能正确调整超参数

169
00:10:22,225 --> 00:10:25,400
会让深度神经网络开发工作更为高效

170
00:10:25,400 --> 00:10:29,000
我们下期见<br />翻译 | 审阅：Cousera Global Translator Community