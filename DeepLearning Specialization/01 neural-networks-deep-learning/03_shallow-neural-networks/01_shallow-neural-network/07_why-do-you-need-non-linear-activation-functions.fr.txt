Pourquoi un réseau de neurones a-t-il besoin
 d'une fonction d’activation non-linéaire ? Il s’avère que pour que votre réseau de neurones
 calcule des fonctions intéressantes, vous devez choisir une fonction d’activation
 non linéaire. Nous allons voir pourquoi. Voici les équations de propagation directe
 (vers l'avant) pour le réseau de neurones. Pourquoi ne pas simplement
 se débarrasser de cela ? Se débarrasser de la fonction g, et dire que a[1] est égal à z[1] ? Ou sinon, on pourrait dire que
 g de z est égal à z, non ? Parfois, on appelle ça
 la fonction d’activation linéaire. Un meilleur nom serait peut-être
la fonction d'activation identité, parce qu'elle met simplement
 en sortie ce qu'il y a en entrée. Dans la même idée, a[2] 
pourrait être égal à z[2]. Il s’avère que si vous faites cela,
 alors ce modèle calcule juste y, ou plutôt ŷ comme une combinaison
 linéaire des caractéristiques d'entrée X. Si vous prenez les deux premières équations, si vous avez a[1] = z[1] = W[1] x + b[1] et également a[2] = z[2] = W[2] a[1] + b[2], alors si vous prenez la définition de a[1] et que vous la remplacez là-dedans,
 vous trouvez que a[2] = W[2] * (W[1]*X + b[1]), ceci est a[1], + b[2] Cela se simplifie en : (W[2]*W[1])* x + (W[2]b[1] + b[2]). (W[2]*W[1])* x + (W[2]b[1] + b[2]). Donc, c’est juste, appelons ça W' et b'. C’est juste égal à W' * x + b' Si vous utilisez des fonctions 
d’activation linéaires, ou nous les appelons également 
des fonctions d’activation identité, alors le réseau de neurones sort
 juste une fonction linéaire de l’entrée. Nous parlerons plus tard des réseaux 
profonds, les réseaux avec vraiment beaucoup de couches cachées. Si vous utilisez une fonction d’activation linéaire ou si vous n’avez pas de fonction d’activation,
alors peu importe le nombre de couches de votre réseau de neurone, il ne fait que 
calculer une fonction d'activation linéaire. Donc vous pouvez aussi bien 
n'avoir aucune couche cachée. Pour ce cas que j'ai déjà mentionné, 
si vous avez une fonction d’activation linéaire ici 
et une fonction sigmoïde ici, ce model ne fait rien de plus que la régression logistique 
standard, sans aucune couche cachée. Je ne vais pas essayer de le prouver, mais
 vous pouvez essayer de le faire si vous voulez. Mais retenez qu'une couche cachée linéaire 
est plus ou moins inutile, parce que la composition de deux fonctions
 linéaires est elle-même une fonction linéaire. Donc, si vous n'ajoutez pas de non linéarité, 
vous ne calculez pas de fonction plus intéressante même
 en allant plus profondément dans le réseau. Il y a un seul endroit où vous pouvez utiliser 
une fonction d’activation linéaire, g (x) = z. et c’est si vous faites de l’apprentissage
 automatique sur un problème de régression. Donc, si y est un nombre réel. Ainsi, par exemple, si vous essayez 
de prédire les prix des logements, y n'est pas 0 ou 1, mais un nombre réel, 
qui peux prendre des valeurs de 0$, du prix d’une maison jusqu'à des sommes 
très importantes selon les maisons. Je suppose que certaines maisons peuvent valoir
 potentiellement des millions de dollars. Peu importe combien coûtent les maisons 
dans votre jeu de données, mais si y peut prendre ces valeurs réelles, alors ce sera peut-être OK d'avoir
 une fonction d’activation linéaire ici pour que votre sortie ŷ soit aussi un nombre réel qui puisse prendre
 toutes les valeurs de moins l’infini
 à plus l’infini. Mais les unités cachées ne doivent pas
 utiliser des fonctions d'activation linéaires Elles peuvent utiliser la ReLU ou la tanh ou
 la leaky ReLU ou peut-être autre chose. Il y a un seul endroit où vous pouvez utiliser 
une fonction d’activation linéaire, et c'est généralement dans
 la couche de sortie. Mais à part ça, l'utilisation d'une fonction
 d'activation linéaire dans une couche cachée, à par pour certains cas 
très particuliers liés à la compression, l’utilisation de fonctions d'activation linéaire 
est extrêmement rare. Oh et bien sûr, si vous voulez vraiment prédire 
des prix de l’immobilier comme vous l’avez vu dans la vidéo de la semaine 1, vu que les
 prix des logements ne sont jamais négatifs, peut être que même là, vous pourriez utiliser 
la fonction d'activation ReLU, pour que votre sortie ŷ soit toujours
 supérieure ou égale à zéro. Alors j’espère que cela vous donne
 une idée de pourquoi avoir une fonction d’activation non linéaire est 
une partie essentielle des réseaux neuronaux. Ensuite, nous allons commencer à parler 
de descente de gradient et pour cela, pour préparer la discussion sur la 
descente de gradient, dans la prochaine vidéo, je veux vous montrer
 comment estimer, ou calculer, la pente, c'est à dire les dérivées, 
de vos fonctions d’activation. Continuons sur la prochaine vidéo.