1
00:00:00,060 --> 00:00:04,380
想要让你的深度神经网络

2
00:00:02,669 --> 00:00:06,870
运作得更为高效

3
00:00:04,380 --> 00:00:09,269
你不仅要好好设置参数

4
00:00:06,870 --> 00:00:11,759
更需要用心配置超参数

5
00:00:09,269 --> 00:00:15,170
让我们来看一下什么是超参数

6
00:00:11,759 --> 00:00:17,820
在神经网络模型中 我们常见的参数是W和b

7
00:00:15,170 --> 00:00:21,720
而在整个学习算法模型中

8
00:00:17,820 --> 00:00:26,220
还有另一些参数

9
00:00:21,720 --> 00:00:28,920
比如学习率α

10
00:00:26,220 --> 00:00:32,329
和梯度下降

11
00:00:28,920 --> 00:00:34,890
算法中的迭代次数

12
00:00:32,329 --> 00:00:38,190
决定了你神经网络模型中的参数

13
00:00:34,890 --> 00:00:40,170
会如何变化

14
00:00:38,190 --> 00:00:42,629
你还需要为学习算法设定其他超参数

15
00:00:40,170 --> 00:00:47,340
你还需要为学习算法设定其他超参数

16
00:00:42,629 --> 00:00:50,629
比如隐藏层的层数 即L

17
00:00:47,340 --> 00:00:56,039
还有隐藏神经元的个数

18
00:00:50,629 --> 00:00:59,670
比如n[1] n[2]等等

19
00:00:56,039 --> 00:01:03,329
还有激活函数的选择

20
00:00:59,670 --> 00:01:05,610
还有激活函数的选择

21
00:01:03,329 --> 00:01:06,869
在隐藏层中可以使用ReLu或tanh或sigmoid

22
00:01:05,610 --> 00:01:11,760
或其他激活函数

23
00:01:06,869 --> 00:01:13,590
这些参数都是你需要在学习算法中设定的

24
00:01:11,760 --> 00:01:15,990
这些参数都是你需要在学习算法中设定的

25
00:01:13,590 --> 00:01:19,640
这些参数都是你需要在学习算法中设定的

26
00:01:15,990 --> 00:01:22,200
它们会影响到参数W和b的最终结果

27
00:01:19,640 --> 00:01:25,640
它们会影响到参数W和b的最终结果

28
00:01:22,200 --> 00:01:29,340
我们称之为超参数

29
00:01:25,640 --> 00:01:30,750
因为像学习率α 迭代次数

30
00:01:29,340 --> 00:01:32,369
因为像学习率α 迭代次数

31
00:01:30,750 --> 00:01:36,000
还有隐藏层的层数等等

32
00:01:32,369 --> 00:01:39,290
都会影响W和b

33
00:01:36,000 --> 00:01:41,970
所以我们称之为超参数

34
00:01:39,290 --> 00:01:44,250
因为超参数在某种程度上

35
00:01:41,970 --> 00:01:46,950
决定了W和b的最终结果

36
00:01:44,250 --> 00:01:50,100
决定了W和b的最终结果

37
00:01:46,950 --> 00:01:53,520
实际上深度学习中有许多不同的超参数

38
00:01:50,100 --> 00:01:55,470
实际上深度学习中有许多不同的超参数

39
00:01:53,520 --> 00:01:57,899
在后面的课程中

40
00:01:55,470 --> 00:02:02,990
我们会学习其他超参数

41
00:01:57,899 --> 00:02:02,990
比如动量 最小批大小

42
00:02:05,150 --> 00:02:13,020
以及各种形式的正则化参数等等

43
00:02:07,220 --> 00:02:14,700
如果这些术语你都不理解

44
00:02:13,020 --> 00:02:16,020
如果这些术语你都不理解

45
00:02:14,700 --> 00:02:18,810
别担心

46
00:02:16,020 --> 00:02:21,870
我们会在第二课中学习这些知识

47
00:02:18,810 --> 00:02:24,120
跟之前几代的机器学习算法相比

48
00:02:21,870 --> 00:02:26,370
深度学习算法多了不少超参数

49
00:02:24,120 --> 00:02:28,890
以后我都会把学习率α称为超参数

50
00:02:26,370 --> 00:02:31,050
以后我都会把学习率α称为超参数

51
00:02:28,890 --> 00:02:33,480
以后我都会把学习率α称为超参数

52
00:02:31,050 --> 00:02:35,250
而不是参数

53
00:02:33,480 --> 00:02:37,920
在之前的机器学习时代中

54
00:02:35,250 --> 00:02:39,600
我们没有这么多的超参数

55
00:02:37,920 --> 00:02:42,120
很多人就常常偷懒把α称为参数

56
00:02:39,600 --> 00:02:44,610
很多人就常常偷懒把α称为参数

57
00:02:42,120 --> 00:02:47,580
其实α的确是个参数

58
00:02:44,610 --> 00:02:50,280
只是它是一个可以决定真正参数的参数

59
00:02:47,580 --> 00:02:51,570
所以为了在学习的过程中将概念梳理得更为清晰

60
00:02:50,280 --> 00:02:54,180
我们会将类似α和迭代次数的这类参数

61
00:02:51,570 --> 00:02:55,769
统称为超参数

62
00:02:54,180 --> 00:02:57,810
当你在训练深度神经网络的时候

63
00:02:55,769 --> 00:02:59,940
你会发现在超参数取值的问题上

64
00:02:57,810 --> 00:03:01,560
会有很多不同的选择

65
00:02:59,940 --> 00:03:04,440
而你需要尝试许多可能的值

66
00:03:01,560 --> 00:03:07,230
所以在深度学习算法中的超参数如何取值

67
00:03:04,440 --> 00:03:09,840
是一个以实验为依据的过程

68
00:03:07,230 --> 00:03:12,150
你可能经常会有些直觉

69
00:03:09,840 --> 00:03:13,549
比如 当你在设定学习率的时候

70
00:03:12,150 --> 00:03:16,739
你可能会说

71
00:03:13,549 --> 00:03:20,670
我认为α应该设为0.01

72
00:03:16,739 --> 00:03:22,530
然后你实际操作了一下

73
00:03:20,670 --> 00:03:23,910
并且得到了最终结果

74
00:03:22,530 --> 00:03:25,890
但是基于结果 你可能会说

75
00:03:23,910 --> 00:03:28,620
我觉得把学习率增加到0.05

76
00:03:25,890 --> 00:03:30,930
会比较好

77
00:03:28,620 --> 00:03:32,970
所以如果你不确定

78
00:03:30,930 --> 00:03:35,010
学习率的最优值应该是多少

79
00:03:32,970 --> 00:03:37,680
你可以先随意尝试一个学习率α

80
00:03:35,010 --> 00:03:39,690
如果发现代价函数J像这样下降

81
00:03:37,680 --> 00:03:41,820
你可能会尝试一个更大的值

82
00:03:39,690 --> 00:03:43,650
你可能会尝试一个更大的值

83
00:03:41,820 --> 00:03:45,060
却发现代价函数增长并且发散了

84
00:03:43,650 --> 00:03:47,250
你可能会再换一个值

85
00:03:45,060 --> 00:03:49,709
发现它迅速下降

86
00:03:47,250 --> 00:03:51,780
但却收敛于一个较高的值

87
00:03:49,709 --> 00:03:53,670
你就会再试一个值

88
00:03:51,780 --> 00:03:55,530
发现代价函数J如图所示

89
00:03:53,670 --> 00:03:57,840
基于你所尝试的取值集合

90
00:03:55,530 --> 00:04:00,870
你可能会说 最后这个α的取值

91
00:03:57,840 --> 00:04:02,790
让学习效率变得很高

92
00:04:00,870 --> 00:04:04,290
并且使代价函数J收敛于一个较低的值

93
00:04:02,790 --> 00:04:06,720
所以我决定就用这个值来作为学习率

94
00:04:04,290 --> 00:04:08,040
所以我决定就用这个值来作为学习率

95
00:04:06,720 --> 00:04:10,170
在之前的幻灯片中可以看到

96
00:04:08,040 --> 00:04:11,489
有许多不同的超参数

97
00:04:10,170 --> 00:04:13,830
并且当你创建新模型的时候

98
00:04:11,489 --> 00:04:15,450
并且当你创建新模型的时候

99
00:04:13,830 --> 00:04:17,940
很难提前知道超参数的最优取值是多少

100
00:04:15,450 --> 00:04:20,580
很难提前知道超参数的最优取值是多少

101
00:04:17,940 --> 00:04:22,169
所以通常我们会这么做

102
00:04:20,580 --> 00:04:24,570
首先尝试许多不同的取值

103
00:04:22,169 --> 00:04:26,970
就像图中的模型这样

104
00:04:24,570 --> 00:04:28,440
尝试不同的参数配置

105
00:04:26,970 --> 00:04:31,140
比如5个隐藏层 数个隐藏单元

106
00:04:28,440 --> 00:04:34,140
构建模型并运行 看看效果如何

107
00:04:31,140 --> 00:04:36,180
然后循环往复直到找到最优值

108
00:04:34,140 --> 00:04:38,340
这个幻灯片的标题是

109
00:04:36,180 --> 00:04:40,740
应用深度学习是一个基于实验的过程

110
00:04:38,340 --> 00:04:42,419
基于实验的过程就是

111
00:04:40,740 --> 00:04:45,330
不断尝试然后找到最优值的高端说法

112
00:04:42,419 --> 00:04:47,190
另一个我观察到的现象是

113
00:04:45,330 --> 00:04:48,810
如今深度学习被应用于众多领域

114
00:04:47,190 --> 00:04:51,990
如今深度学习被应用于众多领域

115
00:04:48,810 --> 00:04:53,789
从计算机视觉到语音识别

116
00:04:51,990 --> 00:04:55,500
到自然语言处理

117
00:04:53,789 --> 00:04:59,250
到许多结构化数据应用

118
00:04:55,500 --> 00:05:02,430
比如在线广告或者网页搜索

119
00:04:59,250 --> 00:05:05,640
或者产品推荐等等

120
00:05:02,430 --> 00:05:08,190
我观察到的情形有以下几种

121
00:05:05,640 --> 00:05:10,080
第一种 某一个领域的研究者

122
00:05:08,190 --> 00:05:12,060
尝试去拓展另一个领域

123
00:05:10,080 --> 00:05:14,400
有时候可以完美继承对超参数配置的直觉

124
00:05:12,060 --> 00:05:16,590
而有时候却得到大相径庭的结果

125
00:05:14,400 --> 00:05:17,849
所以我建议大家

126
00:05:16,590 --> 00:05:20,970
多尝试几次 取不同的值

127
00:05:17,849 --> 00:05:23,550
看看结果是不是尽如人意

128
00:05:20,970 --> 00:05:25,500
尤其是在开始新项目的时候

129
00:05:23,550 --> 00:05:27,930
下节课 我们会学习一些

130
00:05:25,500 --> 00:05:30,780
系统性方法

131
00:05:27,930 --> 00:05:32,070
来看看怎样才能取到最优值

132
00:05:30,780 --> 00:05:33,570
第二种情形是 就算你已经在某个领域上

133
00:05:32,070 --> 00:05:35,220
已经工作了很久很久

134
00:05:33,570 --> 00:05:37,979
比如 你在研究在线广告

135
00:05:35,220 --> 00:05:39,930
随着你的研究不断深入

136
00:05:37,979 --> 00:05:41,580
很有可能学习率

137
00:05:39,930 --> 00:05:43,830
和隐藏单元的个数等超参数的最优值

138
00:05:41,580 --> 00:05:46,440
会发生改变

139
00:05:43,830 --> 00:05:49,229
尽管已经把模型的参数都

140
00:05:46,440 --> 00:05:51,750
设置成当前的最优值

141
00:05:49,229 --> 00:05:53,430
有可能一年后你会发现

142
00:05:51,750 --> 00:05:55,650
这些竟然不是最优值了

143
00:05:53,430 --> 00:05:57,840
可能是因为计算机基础结构

144
00:05:55,650 --> 00:05:59,789
比如CPU GPU的类型

145
00:05:57,840 --> 00:06:01,560
或者其他一些构架发生了很大改变

146
00:05:59,789 --> 00:06:03,659
所以这里我分享一个经验之谈

147
00:06:01,560 --> 00:06:05,070
如果你研究的问题会持续比较久

148
00:06:03,659 --> 00:06:06,659
例如很多年

149
00:06:05,070 --> 00:06:09,030
你可以不时地 比如每隔几个月

150
00:06:06,659 --> 00:06:10,800
尝试一些不同的超参数取值

151
00:06:09,030 --> 00:06:12,570
来确认这些超参数

152
00:06:10,800 --> 00:06:15,150
是否存在更为合适的取值

153
00:06:12,570 --> 00:06:17,280
相信经过不断尝试并且

154
00:06:15,150 --> 00:06:18,779
累积一些经验之后

155
00:06:17,280 --> 00:06:19,870
你也会慢慢养成关于

156
00:06:18,779 --> 00:06:21,820
最优超参数取值的直觉

157
00:06:19,870 --> 00:06:24,010
说到这里 我也明白

158
00:06:21,820 --> 00:06:25,510
去逐个尝试超参数的取值

159
00:06:24,010 --> 00:06:27,940
听起来可能是深度学习中

160
00:06:25,510 --> 00:06:30,160
不那么令人愉快的部分

161
00:06:27,940 --> 00:06:32,200
但这是因为深度学习的研究

162
00:06:30,160 --> 00:06:33,850
依然处于初级阶段

163
00:06:32,200 --> 00:06:36,190
或许不久后对于

164
00:06:33,850 --> 00:06:38,350
超参数的最优值选取

165
00:06:36,190 --> 00:06:41,260
会出现更好的方法

166
00:06:38,350 --> 00:06:43,630
但也有可能因为CPU GPU 网络和数据库

167
00:06:41,260 --> 00:06:45,910
一直在不断变化

168
00:06:43,630 --> 00:06:47,680
这些方法在一段时间内不会趋于一致

169
00:06:45,910 --> 00:06:49,360
你还是需要不断地尝试

170
00:06:47,680 --> 00:06:50,860
对超参数不同的取值

171
00:06:49,360 --> 00:06:52,479
在预留的交叉验证集

172
00:06:50,860 --> 00:06:54,100
或其他集合上进行评估

173
00:06:52,479 --> 00:06:56,350
然后选取最优解

174
00:06:54,100 --> 00:06:58,870
这就是本期视频教程中

175
00:06:56,350 --> 00:07:01,030
关于超参数的简短讨论

176
00:06:58,870 --> 00:07:03,280
在第二课中 我们会就

177
00:07:01,030 --> 00:07:06,040
如何系统地探索超参数空间提供一些建议

178
00:07:03,280 --> 00:07:07,570
但是学习完本视频后

179
00:07:06,040 --> 00:07:09,430
其实你已经具备了所有

180
00:07:07,570 --> 00:07:11,470
完成编程作业需要的工具和方法

181
00:07:09,430 --> 00:07:14,050
在下个视频中 就一个提问频率比较高的问题

182
00:07:11,470 --> 00:07:16,150
深度学习和人类大脑有什么关系

183
00:07:14,050 --> 00:07:18,660
我还会分享一些观点

184
00:07:16,150 --> 00:07:18,660
翻译 | 审阅：Cousera Global Translator Community