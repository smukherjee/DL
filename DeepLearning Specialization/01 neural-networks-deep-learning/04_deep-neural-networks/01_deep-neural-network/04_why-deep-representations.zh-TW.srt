1
00:00:00,000 --> 00:00:03,339
我們已經聽過深度
神經網路作用得很好對於

2
00:00:03,339 --> 00:00:07,073
很多的問題, 這不僅僅是
需要大型的神經網路

3
00:00:07,073 --> 00:00:10,718
具體來說，需要的是
深層或者說很多隱藏層的

4
00:00:10,718 --> 00:00:12,208
為什麼會這樣?

5
00:00:12,208 --> 00:00:15,833
我們來看一些例子
試著得到一些直觀

6
00:00:15,833 --> 00:00:17,720
為什麼深度網路作用得很好

7
00:00:17,720 --> 00:00:22,181
首先
什麼是深度網路計算?

8
00:00:22,181 --> 00:00:25,393
如果您建立一個系統來
人臉辨識或

9
00:00:25,393 --> 00:00:29,631
人臉偵測, 這是深度
神經網路可以做的

10
00:00:29,631 --> 00:00:35,059
或許您可以輸入一個人臉的照片
那第一層的神經網路

11
00:00:35,059 --> 00:00:40,000
您可以想成也許是
特徵偵測或者邊緣偵測

12
00:00:40,000 --> 00:00:45,519
在這個例子我畫一個
神經網路也許有 20 個隱藏單元

13
00:00:45,519 --> 00:00:48,017
也許是一種這個圖形的計算

14
00:00:48,017 --> 00:00:52,357
所以這 20 個隱藏單元視覺化
成為這小小的正方盒

15
00:00:52,357 --> 00:00:57,325
舉個例子, 這小小的視覺化
代表一個隱藏單元是

16
00:00:57,325 --> 00:01:01,978
試著找出是否
這個影像有邊緣是在這個方向

17
00:01:01,978 --> 00:01:05,914
也許這個隱藏單元
或許試著找出

18
00:01:05,914 --> 00:01:09,955
是否有水平
邊緣在這個影像

19
00:01:09,955 --> 00:01:13,184
當我們在以後課程談到
卷積網路

20
00:01:13,184 --> 00:01:16,129
這個特別的視覺化
會比較有感覺

21
00:01:16,129 --> 00:01:19,562
這種樣式, 您可以想像第一層
的神經網路是看

22
00:01:19,562 --> 00:01:22,690
這個圖形試著找出
這個圖形的邊緣在那裡

23
00:01:22,690 --> 00:01:27,356
想像這個圖形的邊緣
是集合

24
00:01:27,356 --> 00:01:28,730
一些像素成為邊緣

25
00:01:28,730 --> 00:01:34,670
它可以偵測這些邊緣然後匯集
這些邊緣成為部分人臉

26
00:01:34,670 --> 00:01:40,289
舉個例子, 您也許有一些神經元
試著看是否可以找到眼睛

27
00:01:40,289 --> 00:01:44,480
或者不同的神經元試著
找出部分的鼻子

28
00:01:44,480 --> 00:01:47,463
將很多的邊緣放在一起

29
00:01:47,463 --> 00:01:50,970
它可以開始來偵測
人臉不同的部位

30
00:01:50,970 --> 00:01:56,035
最後將
人臉不同的部分放在一起

31
00:01:56,035 --> 00:02:01,006
像是眼睛或者鼻子或者
下巴, 它可以試著辨識

32
00:02:01,006 --> 00:02:03,564
或偵測不同的人臉

33
00:02:03,564 --> 00:02:07,755
直觀上，您可以想像這些
神經網路前面幾層是

34
00:02:07,755 --> 00:02:10,190
偵測簡單函數, 像是邊緣

35
00:02:10,190 --> 00:02:14,573
然後組合在一起
在這些神經網路的後面幾層

36
00:02:14,573 --> 00:02:17,625
它就可以學越來越複雜的函數

37
00:02:17,625 --> 00:02:23,640
這些視覺化會比較合理
當我們談到卷積網路

38
00:02:23,640 --> 00:02:26,203
而視覺化的一個技術細節

39
00:02:26,203 --> 00:02:29,802
邊緣偵測是看著
影像中很小的區域

40
00:02:29,802 --> 00:02:31,703
以許很小的區域像這樣

41
00:02:31,703 --> 00:02:36,616
而人臉辨識您看到
也許是影像中大很多的區域但

42
00:02:36,616 --> 00:02:41,308
但是主要的重點是
從這裡只是很簡單的事情

43
00:02:41,308 --> 00:02:43,675
像是邊緣然後將他們建立起來

44
00:02:43,675 --> 00:02:47,216
將它們組合來偵測
更複雜的東西像是眼睛, 鼻子

45
00:02:47,216 --> 00:02:50,530
然後再將它們組合
來找尋更複雜的東西

46
00:02:50,530 --> 00:02:55,665
而這種簡單到複雜
階層表現

47
00:02:55,665 --> 00:02:58,508
或者組合表現

48
00:02:58,508 --> 00:03:04,114
也應用到其他類型
而不僅是影像跟人臉辨識

49
00:03:04,114 --> 00:03:08,593
舉個例子, 如果您試著
建立一個語音辨識系統

50
00:03:08,593 --> 00:03:10,908
很難將語音視覺化, 但

51
00:03:10,908 --> 00:03:15,684
如果您輸入一段語音或許
第一層神經網路也許

52
00:03:15,684 --> 00:03:20,863
學著偵測低階語音波型
特徵, 像是這音調上升

53
00:03:20,863 --> 00:03:21,703
或是下降

54
00:03:21,703 --> 00:03:26,869
它是白噪音或者
嘶聲像是[聲音]

55
00:03:26,869 --> 00:03:27,903
音階是如何

56
00:03:27,903 --> 00:03:31,124
當在這個階層
偵測低階音波特徵像這些

57
00:03:31,124 --> 00:03:34,233
然後結合
低階音波樣式

58
00:03:34,233 --> 00:03:37,937
您會學習偵測
聲音基本單元

59
00:03:37,937 --> 00:03:40,297
在語言學他們叫音位

60
00:03:40,297 --> 00:03:45,098
舉個例子, cat 這個字
 c 是音位, a 是音位

61
00:03:45,098 --> 00:03:46,787
t 是另一個音位

62
00:03:46,787 --> 00:03:49,987
學著尋找或許是
聲音的基本單元

63
00:03:49,987 --> 00:03:54,688
組合在一起或許
學習辨識音訊中的單字

64
00:03:54,688 --> 00:03:58,270
也許將這些組合在一起

65
00:03:58,270 --> 00:04:02,912
為了辨識整個短句或者句子

66
00:04:02,912 --> 00:04:07,572
所以深度學習網路有很多隱藏
層也許能夠在早先

67
00:04:07,572 --> 00:04:10,477
的層學著
低階簡單的特徵

68
00:04:10,477 --> 00:04:15,339
然後在比較深層時
將這些簡單的東西放在一起偵測

69
00:04:15,339 --> 00:04:19,392
為了偵測更複雜的東西
像是辨識特別字或者

70
00:04:19,392 --> 00:04:21,040
甚至是短句或句子

71
00:04:21,040 --> 00:04:24,745
最終是為了
語音辨識

72
00:04:24,745 --> 00:04:30,168
我們看的是在早期的
階層計算的似乎是

73
00:04:30,168 --> 00:04:35,673
相對簡單的函數對於輸入
像是邊緣在哪裡, 等到了

74
00:04:35,673 --> 00:04:41,046
您進到深一點網路您
真的會驚訝它們做的複雜事情

75
00:04:41,046 --> 00:04:44,876
像是偵測人臉或者
偵測單字或短句或句子

76
00:04:44,876 --> 00:04:48,767
一些人喜歡做一種比喻
介於深度神經網路跟

77
00:04:48,767 --> 00:04:52,656
人類的大腦, 我們相信
或者說神經學家相信

78
00:04:52,656 --> 00:04:57,162
人腦也是開始
偵測簡單的東西像是您

79
00:04:57,162 --> 00:05:00,370
眼睛看到的邊緣然後建立
成偵測更複雜

80
00:05:00,370 --> 00:05:02,440
的東西像是您看到的人臉

81
00:05:02,440 --> 00:05:05,038
我想這種比喻介於
深度學習跟

82
00:05:05,038 --> 00:05:08,276
人腦有時候
有一點點危險

83
00:05:08,276 --> 00:05:13,301
但有很多的理由相信
這是我們認為人腦如此工作

84
00:05:13,301 --> 00:05:18,102
人腦也許
先偵測簡單的東西像是邊緣

85
00:05:18,102 --> 00:05:22,598
然後將他們放在一起變成
更複雜的東西所以

86
00:05:22,598 --> 00:05:27,430
這也是一種鬆散方式的啟發
對於一些深度學習

87
00:05:27,430 --> 00:05:29,850
我們會看到多一些
有關人腦或者

88
00:05:29,850 --> 00:05:33,065
有關生物大腦在
這個禮拜晚一點的影片中

89
00:05:35,534 --> 00:05:40,407
其他直觀
有關於深度網路似乎

90
00:05:40,407 --> 00:05:42,756
作用得很好如下

91
00:05:42,756 --> 00:05:47,868
這個結果來自於電路
理論, 屬於一種思考

92
00:05:47,868 --> 00:05:53,760
有關於您可以計算何種函數
使用不同的 AND閘, OR閘, NOT 閘不同的閘

93
00:05:53,760 --> 00:05:58,860
非正式而言, 它們這些函數
相對的"小"深度神經

94
00:05:58,860 --> 00:06:03,595
網路, 小的意思是
隱藏單元的數目很小

95
00:06:03,595 --> 00:06:07,553
但如果您計算同樣
函數用一個淺的網路

96
00:06:07,553 --> 00:06:09,178
沒有足夠的隱藏層

97
00:06:09,178 --> 00:06:13,296
那您需要指數般
多的隱藏單元來做計算

98
00:06:13,296 --> 00:06:18,109
讓我用一個例子
來一點點非正式的解釋

99
00:06:18,109 --> 00:06:21,423
假設您試著
計算 互斥或

100
00:06:21,423 --> 00:06:23,349
您輸入特徵的同位

101
00:06:23,349 --> 00:06:28,430
您試著計算 x1 
XOR x2 XOR

102
00:06:28,430 --> 00:06:33,064
x3 XOR 一直到 xn 如果您有 n 或者
 nx 特徵

103
00:06:33,064 --> 00:06:39,924
如果您建一個 XOR 樹像這樣
我們用它來計算 XOR of x1 跟

104
00:06:39,924 --> 00:06:44,586
x2, 然後 x3 跟
x4 計算它們的 XOR

105
00:06:44,586 --> 00:06:49,392
技術上來說如果您只用
AND, OR, NOT 閘您或許需要

106
00:06:49,392 --> 00:06:54,196
幾層來計算 XOR 
函數而不只一層

107
00:06:54,196 --> 00:06:58,791
但是相對小的電路
您可以計算 XOR 等等

108
00:06:58,791 --> 00:07:03,987
然後您可以建
真的, XOR 樹像這樣

109
00:07:03,987 --> 00:07:12,090
直到最後您有一的電路
輸出，我們稱為 y

110
00:07:12,090 --> 00:07:15,236
輸出為 y-hat 等於 y

111
00:07:15,236 --> 00:07:18,398
這個 互斥或
對於所有輸入位元的同位

112
00:07:18,398 --> 00:07:24,790
為了計算 XOR, 
左邊網路的深度是 log N 等級

113
00:07:24,790 --> 00:07:27,410
我們用一個  XOR 樹

114
00:07:27,410 --> 00:07:30,836
節點的數目或者
電路元件的數目或者

115
00:07:30,836 --> 00:07:33,929
閘的數目在這個
網路, 並不是很大

116
00:07:33,929 --> 00:07:38,452
您並不需要很多閘
為了要計算 互斥或

117
00:07:38,452 --> 00:07:43,458
現在, 如果您不允許
神經網路使用多重

118
00:07:43,458 --> 00:07:48,203
隱藏層, 在這個例子
log n 層的隱藏層

119
00:07:48,203 --> 00:07:53,382
如果您強迫計算這個
函數只用一層隱藏層

120
00:07:53,382 --> 00:07:57,912
您讓所有東西都進入
隱藏層

121
00:07:57,912 --> 00:08:02,116
然後這個東西輸出 y

122
00:08:02,116 --> 00:08:07,120
然後為了計算這個
XOR 函數, 這個隱藏層

123
00:08:07,120 --> 00:08:12,124
會需要指數般大
因為基本上

124
00:08:12,124 --> 00:08:18,397
您需要詳盡列舉我們
2 的 n 次方可能組合

125
00:08:18,397 --> 00:08:23,139
所以 2 的 n次方等級
可能的組合的輸入位元

126
00:08:23,139 --> 00:08:27,898
結果是窮舉
所有的 1 或者 0

127
00:08:27,898 --> 00:08:32,213
您最終需要隱藏層
是指數般大的

128
00:08:32,213 --> 00:08:33,554
總共的位元

129
00:08:33,554 --> 00:08:38,229
我想技術上您可以做到
 2 的 n - 1 次方

130
00:08:38,229 --> 00:08:43,948
但這也是 2 的 n 次方等級
相當大的位元數目

131
00:08:43,948 --> 00:08:49,149
我希望這給您一點感覺
用數學函數

132
00:08:49,149 --> 00:08:55,275
比較容易用深度網路計算
比起淺層網路

133
00:08:55,275 --> 00:09:01,028
實際上, 我個人認為
用電路理論的結果比較不能

134
00:09:01,028 --> 00:09:05,985
得到直觀, 但
人們通常引用這種結果

135
00:09:05,985 --> 00:09:11,223
來解釋
深度表達的價值

136
00:09:11,223 --> 00:09:13,600
現在, 除了這個理由

137
00:09:13,600 --> 00:09:16,897
比較喜歡深度神經網路
老實說

138
00:09:16,897 --> 00:09:22,204
我想其它理由這個名詞深度
學習起飛只是因為品牌效應

139
00:09:22,204 --> 00:09:26,776
這個東西我們之前稱作神經
網路有很多層隱藏層，但

140
00:09:26,776 --> 00:09:31,198
這個名稱"深度學習" 
就是一個好品牌, 它是如此之深

141
00:09:31,198 --> 00:09:36,284
我想一旦一個名詞抓住了
將神經網路重新包裝或者

142
00:09:36,284 --> 00:09:39,622
將神經網路有很多
隱藏層重新包裝

143
00:09:39,622 --> 00:09:42,970
幫助捕捉大眾
的想像

144
00:09:42,970 --> 00:09:47,479
但對於品牌公關而言
深度網路做得很好

145
00:09:47,479 --> 00:09:51,342
有時候人們玩過火
堅持用一大堆隱藏層

146
00:09:51,342 --> 00:09:55,500
但當我開始新的問題時
我真的通常開始於

147
00:09:55,500 --> 00:09:58,803
羅吉斯迴歸分析
然後試用一個或是

148
00:09:58,803 --> 00:10:01,722
兩個隱藏層
用此當超參數

149
00:10:01,722 --> 00:10:05,731
用這個當作參數或者超參數
來調整為了試著找到

150
00:10:05,731 --> 00:10:07,935
正確的深度對於您的神經網路

151
00:10:07,935 --> 00:10:12,800
但過去幾年
有一種趨勢人們發現

152
00:10:12,800 --> 00:10:17,590
對於一些應用很深很深
神經網路也許

153
00:10:17,590 --> 00:10:22,264
數打的層級有時候
是最佳的模型對於那些問題

154
00:10:22,264 --> 00:10:27,605
所以這是一些直觀對於
為什麼深度學習似乎作用很好

155
00:10:27,605 --> 00:10:31,411
現在讓我們來看一下如何建置不只正向

156
00:10:31,411 --> 00:10:33,769
傳播, 還有反向傳播的機制