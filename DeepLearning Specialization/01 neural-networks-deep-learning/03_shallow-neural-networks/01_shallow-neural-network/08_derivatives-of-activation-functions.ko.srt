1
00:00:00,302 --> 00:00:04,340
여러분이 신경망을 위해서 후 
방향전파을 도입하는 경우

2
00:00:04,340 --> 00:00:07,813
여러분은 정말로 activation 함수의 
derivative나 기울기를 구해야 하는데요,

3
00:00:07,813 --> 00:00:11,333
이제 activation 함수의 선택권을 한번 보고

4
00:00:11,333 --> 00:00:14,025
이러한 함수의 기울기를 
어떻게 구하는지 살펴보겠습니다.

5
00:00:14,025 --> 00:00:17,239
여러분이 익숙한 시그모이드 함수인데요

6
00:00:17,239 --> 00:00:22,252
어떠한 z의 값에 대해, 
여기 z값은 어떤 기울기를 갖거나

7
00:00:22,252 --> 00:00:27,184
여기 조금한 선을 그리면, 
여기에 해당하는 derivative 값을 가질 수 있습니다.

8
00:00:27,184 --> 00:00:32,389
이 경우, 높이와 너비를 알 수 있는데요 
여기 삼각형에서 말이죠.

9
00:00:32,389 --> 00:00:39,823
그러면, g(z)가 시그모이드 함수인 
경우 기울기는 ddz g(z) 입니다.

10
00:00:39,823 --> 00:00:47,034
저희는 미적분을 통해 이 값이 g(x) 함수에서

11
00:00:47,034 --> 00:00:52,513
여러분이 미적분에 익숙하고 
derivative를 어떻게 구하는지 아시면,

12
00:00:52,513 --> 00:00:56,074
여기 시그모이드 함수에서 derivative를 구하면,

13
00:00:56,074 --> 00:01:00,300
여기 이 공삭과 동일하다는 것을 알 수 있습니다.

14
00:01:00,300 --> 00:01:02,380
다시 한번 여기서는 미적분 
단계를 진행하지 않을 것입니다.

15
00:01:02,380 --> 00:01:04,839
하지만 여러분이 미적분에 익숙하시면 언제든지

16
00:01:04,839 --> 00:01:08,465
잠시 비디오를 멈추시고 직접 
한번 계산해서 증명해보세요.

17
00:01:08,465 --> 00:01:16,583
그럼 여기는 g(z) 곱하기 1 빼기 g(z)인데요,

18
00:01:16,583 --> 00:01:20,923
이 표현이 말이 되는지 체크를 한번 해보겠습니다.

19
00:01:20,923 --> 00:01:26,343
처음으로, 만약 z가 매우 큰 값이면,

20
00:01:26,343 --> 00:01:30,315
이 경우, z가 10이라고 해보겠습니다.

21
00:01:30,315 --> 00:01:38,758
그러면 g(z)는 1과 가까울텐데요, 
그러면 왼쪽에 있는 공식은 저희에게 알려주는데요,

22
00:01:38,758 --> 00:01:42,979
ddz g(z) 는 1 곱하기 1빼기 1과 
가깝다는 것을 알려줄 것입니다.

23
00:01:42,979 --> 00:01:46,851
이것은 결국 0과 근접한 값인데요,

24
00:01:46,851 --> 00:01:51,267
이것은 틀리지 않습니다. 왜냐면 z가 매우 
큰값인 경우, 기울기가 0이기 때문입니다.

25
00:01:51,267 --> 00:01:58,885
반대로 z가 -10인 경우, 여기까지 가있다고 
하는 것입니다. 그러면 g(z)는 0과 가깝습니다.

26
00:01:58,885 --> 00:02:01,765
그러면 왼쪽에서는 dzz g(z) 가 g(z)와

27
00:02:01,765 --> 00:02:07,333
가까울 텐데요, g(z)는 0 곱하기 (1-0) 
인데요

28
00:02:07,333 --> 00:02:10,005
이 값은 역시 0과 근접합니다. 동일하게 맞죠.

29
00:02:10,005 --> 00:02:16,889
그리고 마지막으로 z가 0인 경우, 
g(z)는 2분의 1 인데요

30
00:02:16,889 --> 00:02:18,983
여기 시그모이드 함수인데요.

31
00:02:18,983 --> 00:02:24,040
그러면 derivative는 1 나누기 2 곱하기

32
00:02:24,040 --> 00:02:28,459
1 빼기 1/2 이고 이 값은 4분의 1입니다.

33
00:02:28,459 --> 00:02:33,135
이 값은 이 함수의 derivative 또는 기울기가 맞습니다.

34
00:02:33,135 --> 00:02:35,831
z가 0일때 말이죠.

35
00:02:35,831 --> 00:02:38,839
마지막으로 한가지 더 표기 관련하여 소개하자면,

36
00:02:38,839 --> 00:02:42,321
가끔은 여기 이것을 적는 것 대신해서

37
00:02:42,321 --> 00:02:46,087
짧게 derivative를 쓰는 방법은 g 프라임 z 입니다.

38
00:02:46,087 --> 00:02:52,167
그러면 g'(z)는 미적분에서 여기 대쉬를 
위에 붙히면 이것을 프라임이라고 하며,

39
00:02:52,167 --> 00:02:55,283
g 프라임 z 는 미적분에서

40
00:02:55,283 --> 00:03:00,455
g 함수의 derivative를 뜻합니다. 
입력값 z 변수에 대해서 말이죠.

41
00:03:00,455 --> 00:03:07,513
그리고 신경망에서는 a 가

42
00:03:07,513 --> 00:03:11,351
g(z)와 일치하고 이 값이

43
00:03:11,351 --> 00:03:17,145
또한 곱하기 (1-a)를 심플하게 정리됩니다.

44
00:03:17,145 --> 00:03:20,109
가끔씩 도입하는 과정에서

45
00:03:20,109 --> 00:03:25,256
이렇게 g'(z)=a 곱하기 1 빼기 a 를 보실텐데요,

46
00:03:25,256 --> 00:03:29,040
이것은 g 프라임이 observation인 경우를 뜻하는데요

47
00:03:29,040 --> 00:03:33,428
이 것은 derivative를 뜻하고. 그 값은 이것입니다.

48
00:03:33,428 --> 00:03:38,397
이 공식의 장점은 만약 a를 이미 구했을 때,

49
00:03:38,397 --> 00:03:43,078
여기 이 표현을 쓰면서, 굉장히 빨리 
이것의 기울기도 구할 수 있는 것입니다.

50
00:03:43,078 --> 00:03:45,038
g 프라임 값을 말이죠.

51
00:03:45,038 --> 00:03:47,632
여기까지 내용이 시그모이드 
activation 함수의 내용이였는데요.

52
00:03:47,632 --> 00:03:51,172
이제 tan h activation 함수를 보겠습니다.

53
00:03:51,172 --> 00:03:56,311
이전에 봤던 것과 비슷하게, ddz g(z)의

54
00:03:56,311 --> 00:04:01,155
정의는 g(z)에서 그 값이 z일때의 기울기를 쯧합니다.

55
00:04:01,155 --> 00:04:07,747
그 값이 z일때의 기울기를 쯧합니다. 
그리고 미적분학을 알면,

56
00:04:07,747 --> 00:04:13,066
이것의 derivative값을 구하여

57
00:04:13,066 --> 00:04:17,359
공식을 간단하세 표현하면, 
이렇게 간단하게 표현되는데요,

58
00:04:20,813 --> 00:04:23,997
그리고 이전에 배운대로

59
00:04:23,997 --> 00:04:27,019
이것을 g 프라임 z로 부르겠습니다.

60
00:04:27,019 --> 00:04:30,941
원하시면 이 공식이 말이 
되는지 체크하셔도 좋습니다.

61
00:04:30,941 --> 00:04:37,005
예를 들어 z 가 10인 경우 tan h (z)는 1과 가까울 것입니다.

62
00:04:37,005 --> 00:04:41,309
이것은 1 에서 -1로 가는데요,

63
00:04:41,309 --> 00:04:45,183
그러면 g'(z)는 이 공식에 의하면,

64
00:04:45,183 --> 00:04:48,148
1-1의 제곱이므로 그 값이 0과 가깝게 됩니다. 
이것은 z가 큰 경우입니다.

65
00:04:48,148 --> 00:04:53,930
1-1의 제곱이므로 그 값이 0과 가깝게 됩니다. 
이것은 z가 큰 경우입니다.

66
00:04:53,930 --> 00:04:58,760
반대로 만약 z의 값이 매우 작다면 z가 -10이라면

67
00:04:58,760 --> 00:05:02,440
tan h(z) 는 -1 과 가까울 것입니다.

68
00:05:02,440 --> 00:05:07,871
그러므로 g'(z) 는 1 빼기 -1 의 제곱입니다.

69
00:05:07,871 --> 00:05:12,792
그러므로 1 - 1 과 가까운 것인데요 
그러면 마찬가지로 그값은 0이 됩니다.

70
00:05:12,792 --> 00:05:18,421
마지막으로 z가 0인 경우인데요, 
그러면 tan h(z)=0 인데요,

71
00:05:18,421 --> 00:05:22,165
이 경우에는 기울기가 1입니다.

72
00:05:22,165 --> 00:05:26,433
이 부분은 z의 값이 0일때의 기울기 점을 나타냅니다.

73
00:05:26,433 --> 00:05:33,001
요약하자면, 만약 a 가 g(z)라면, 
만약 a=tan h (z),

74
00:05:33,001 --> 00:05:38,300
그러면 derivative g 프라임 z 는 1- 제곱인데요

75
00:05:38,300 --> 00:05:42,000
만약 a의 값을 이미 계산한 경우에는

76
00:05:42,000 --> 00:05:46,522
이 공식을 사용해서 재빨리 
derivative의 값을 구할 수 있습니다.

77
00:05:46,522 --> 00:05:49,618
마지막으로 여기서 ReLu 함수와

78
00:05:49,618 --> 00:05:51,338
Leaky ReLu 함수의 derivative를 
찾는 방법을 말씀 드리겠습니다.

79
00:05:51,338 --> 00:05:57,866
g(z)=max (0, z)의 값에 대해서

80
00:05:57,866 --> 00:06:03,330
이것의 기울기는 만약 z가 0보다 작으면

81
00:06:03,330 --> 00:06:09,066
그 값은 0입니다. 그리고 만약 z의 
값이 0보다 큰 경우, 기울기 값이 1이 됩니다.

82
00:06:09,066 --> 00:06:15,498
그리고 undefined인데요 엄밀히 이야기하면 
정의 할 수 없는 undefined 입니다.

83
00:06:15,498 --> 00:06:18,339
z가 정확히 0일 경우에 말이죠.

84
00:06:18,339 --> 00:06:21,398
이것을 소프트웨어에 도입시키는 경우, 
수학적으로 100퍼센트 올바르지 않을 수 있습니다.

85
00:06:21,398 --> 00:06:25,571
하지만 z의 값이 0인 부분에

86
00:06:25,571 --> 00:06:30,501
대해서 기울기 값을 정확히 1로 
지정하거나 또는 0으로 지정해도 됩니다.

87
00:06:30,501 --> 00:06:31,781
대해서 기울기 값을 정확히 1로 
지정하거나 또는 0으로 지정해도 됩니다.

88
00:06:31,781 --> 00:06:33,578
최적화 전문가라면

89
00:06:33,578 --> 00:06:37,109
엄밀히 이야기하면 g 프라임은 그러면 
g(z) 함수에 대해 sub gradient 함수 가 됩니다.

90
00:06:37,109 --> 00:06:41,360
그렇기 때문에 기울기 강하가 계속 작동할 것입니다.

91
00:06:41,360 --> 00:06:47,327
이것을 생각하는 방법은, 
z가 정확히 0이 되는 확률이 너무나도

92
00:06:47,327 --> 00:06:52,805
작은 확률이기 때문에 별로 상관이 없다고 
생각하면 됩니다. z가 0일때 어떤 derivative값을

93
00:06:52,805 --> 00:06:54,303
설정하는지 별로 상관 없다는 관점인 것입니다.

94
00:06:54,303 --> 00:06:59,155
실제로는, 사람들이 z의 derivative에 
대해서 이렇게 도입합니다.

95
00:06:59,155 --> 00:07:03,712
마지막으로 여러분이 직접 
Leaky ReLu activation 함수에서 트레이닝 하는 경우에,

96
00:07:03,712 --> 00:07:06,882
마지막으로 여러분이 직접 
Leaky ReLu activation 함수에서 트레이닝 하는 경우에,

97
00:07:06,882 --> 00:07:12,244
g(z)는 max (0.01z, z)가 될텐데요,

98
00:07:12,244 --> 00:07:15,247
그러므로 g 프라임 z는

99
00:07:15,247 --> 00:07:19,074
만약 z값이 0보다 작을때 0.01일 텐데요,

100
00:07:19,074 --> 00:07:20,477
0.01일 텐데요, 0보다 클 경우,

101
00:07:20,477 --> 00:07:26,403
0보다 클 경우, 1일 것입니다.

102
00:07:26,403 --> 00:07:31,479
다시 한번, 기울기는 z의 값이 
0일 때는 정의되지 않습니다.

103
00:07:31,479 --> 00:07:38,353
그러므로 코드를 도입해서 기울기의 값 또는 g'(z)의 값이

104
00:07:38,353 --> 00:07:41,499
z의 값이 0인 경우의 기울기로 반영하면 코드가 잘 작동할 것입니다.

105
00:07:41,499 --> 00:07:45,594
이제 이런 공식으로 무장되었으니, 여러분은

106
00:07:45,594 --> 00:07:48,400
activation 함수의 기울기를 구하실 수 있을 것입니다.

107
00:07:48,400 --> 00:07:50,553
이제 기본 블럭이 되는 내용을 다루었으니

108
00:07:50,553 --> 00:07:54,502
여러분은 기울기 강하를 신경망에서 
도입하는 방법을 보실 준비가 되셨을 것입니다.

109
00:07:54,502 --> 00:07:57,309
다음 비디오를 통해서 보도록 하겠습니다.