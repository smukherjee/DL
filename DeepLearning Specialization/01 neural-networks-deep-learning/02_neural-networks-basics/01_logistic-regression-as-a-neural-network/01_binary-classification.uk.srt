1
00:00:00,920 --> 00:00:02,860
Привіт і з поверненням.

2
00:00:02,860 --> 00:00:08,860
На цьому тижні ми збираємось поговорити про основи
 програмування нейронних мереж (НМ).

3
00:00:08,860 --> 00:00:11,990
Виявляється, що для реалізації НМ

4
00:00:11,990 --> 00:00:16,260
існує кілька дуже важливих методик.

5
00:00:16,260 --> 00:00:21,150
Наприклад, маючи тренувальний набір з m тренувальних зразків,

6
00:00:21,150 --> 00:00:25,110
ми, вірогідно, використаємо цикл for для

7
00:00:25,110 --> 00:00:28,240
покрокової обробки своїх m тренувальних зразків.

8
00:00:28,240 --> 00:00:31,260
Проте, в той же час, при реалізації НМ

9
00:00:31,260 --> 00:00:34,540
нам захочеться обробити весь тренувальний набір

10
00:00:34,540 --> 00:00:39,040
без явного використання циклу for для всього тренувального набору.

11
00:00:39,040 --> 00:00:42,940
Тож ти побачиш як це зробити на цьому тижні.

12
00:00:42,940 --> 00:00:47,700
Інша методика - це те, що,
 під час побудови алгоритму обчислень нашої мережі,

13
00:00:47,700 --> 00:00:51,670
нам, зазвичай, потрібні "прямий шлях" або "пряме поширення"

14
00:00:51,670 --> 00:00:56,100
і подальший "зворотній шлях" або "зворотнє поширення".

15
00:00:56,100 --> 00:01:00,010
Тож на цьому тижні, ти також дізнаєшся чому

16
00:01:00,010 --> 00:01:04,830
алгоритм обчислень для навчання НМ використовує

17
00:01:04,830 --> 00:01:08,010
окремо пряме і зворотнє поширення.

18
00:01:09,100 --> 00:01:12,620
На цьому тижні я хочу описати ці методики за допомогою

19
00:01:12,620 --> 00:01:16,170
логістичної регресії (ЛР), щоб їх легше було зрозуміти.

20
00:01:16,170 --> 00:01:19,970
Проте, навіть якщо ти вже чув/ла раніше про ЛР, думаю, що

21
00:01:19,970 --> 00:01:23,845
і для тебе знайдуться цікаві ідеї на цьому тижні.

22
00:01:23,845 --> 00:01:25,815
Тож почнемо.

23
00:01:25,815 --> 00:01:30,605
ЛР - алгоритм двійкової класифікації.

24
00:01:30,605 --> 00:01:33,145
Тож давай почнемо з визначення задачі.

25
00:01:33,145 --> 00:01:36,925
Ось приклад задачі двійкової класифікації.

26
00:01:36,925 --> 00:01:41,545
Наприклад, ти маєш на вході ось таке зображення і

27
00:01:41,545 --> 00:01:47,260
хочеш отримати мітку "кіт"

28
00:01:47,260 --> 00:01:52,140
(в цьому випадку на виході буде 1) або "не кіт" 
(в цьому випадку на виході буде 0).

29
00:01:52,140 --> 00:01:57,740
Ми будемо використовувати y для позначення вихідної мітки.

30
00:01:57,740 --> 00:02:01,550
Давай розглянемо як зображення представлене в комп'ютері.

31
00:02:01,550 --> 00:02:05,680
Щоб зберігати зображення, комп'ютер зберігає три окремі матриці,

32
00:02:05,680 --> 00:02:09,890
що відповідають червоному, зеленому і синьому 
колірним каналам цього зображення.

33
00:02:10,990 --> 00:02:15,900
Тож, якщо наше вхідне зображення має розмір 64*64 пікселів,

34
00:02:15,900 --> 00:02:21,700
то ми матимемо три 64*64 матриці,

35
00:02:21,700 --> 00:02:27,230
що відповідають червоним, зеленим і синім значенням 
інтенсивності пікселів нашого зображення.

36
00:02:27,230 --> 00:02:31,290
Хоча на цьому маленькому слайді я намалював значно менші матриці.

37
00:02:31,290 --> 00:02:35,320
Це насправді матриці 5*4, а не 64*64.

38
00:02:35,320 --> 00:02:41,640
Тож, щоб перетворити ці значення інтенсивності пікселів 
у вектор ознак, ми

39
00:02:41,640 --> 00:02:48,000
розгорнемо всі ці значення у вхідний вектор ознак x.

40
00:02:48,000 --> 00:02:53,782
А щоб розгорнути всі ці значення інтенсивності 
пікселів у вектор ознак,

41
00:02:53,782 --> 00:02:59,580
ми визначити вектор ознак x, 
що відповідає цьому зображенню, ось так.

42
00:02:59,580 --> 00:03:03,960
Ми просто візьмемо всі ці значення 255, 231 і т.д.

43
00:03:03,960 --> 00:03:10,827
255, 231 і т.д., поки не перерахуємо всі червоні пікселі.

44
00:03:10,827 --> 00:03:15,737
Десь далі - 255, 134, 255, 134 і т.д.

45
00:03:15,737 --> 00:03:20,952
доки не отримаємо довгий вектор ознак, що міститиме всі червоні,

46
00:03:20,952 --> 00:03:25,570
зелені та сині значення інтенсивності пікселів цього зображення.

47
00:03:25,570 --> 00:03:31,043
Тож, якщо це зображення має розмір 64*64, то сумарний розмір

48
00:03:31,043 --> 00:03:36,401
цього вектора x буде 64*64*3,

49
00:03:36,401 --> 00:03:41,320
тому що це сумарна кількість чисел, які ми маємо у всіх цих матрицях.

50
00:03:41,320 --> 00:03:44,097
Що, в цьому випадку, перетворюється на 12 288.

51
00:03:44,097 --> 00:03:47,330
Це те, що ми отримаємо, перемноживши ці числа.

52
00:03:47,330 --> 00:03:51,870
Тож ми будемо використовувати nₓ = 12 288,

53
00:03:51,870 --> 00:03:55,080
щоб позначити розмір вхідного вектора ознак x.

54
00:03:55,080 --> 00:03:59,280
А також інколи, для скорочення, 
я буду також використовувати маленьку n,

55
00:03:59,280 --> 00:04:02,720
для позначення розміру вхідного вектора ознак.

56
00:04:02,720 --> 00:04:07,510
Тож у двійковій класифікації наша мета - 
натренувати класифікатор, який може приймати на вхід

57
00:04:07,510 --> 00:04:10,760
зображення, представлене цим вектором ознак x,

58
00:04:10,760 --> 00:04:15,460
і передбачити відповідну мітку y: 1 або 0,

59
00:04:15,460 --> 00:04:19,000
тобто, "кіт" це зображення чи "не кіт".

60
00:04:19,000 --> 00:04:21,560
Тепер давай приймемо деякі позначення, які ми будемо

61
00:04:21,560 --> 00:04:23,820
використовувати в цьому курсі.

62
00:04:23,820 --> 00:04:29,453
Одиничний тренувальний зразок представлений парою (x, y),

63
00:04:29,453 --> 00:04:34,446
де x - вектор ознак розміром x,

64
00:04:34,446 --> 00:04:39,320
а y - мітка (0 або 1).

65
00:04:39,320 --> 00:04:44,550
Наш тренувальний набір міститиме 
маленьку m тренувальних зразків.

66
00:04:44,550 --> 00:04:50,320
Тож наш тренувальний набір записуватиметься (x₁, y₁), тобто вхідні і

67
00:04:50,320 --> 00:04:55,370
вихідні дані нашого 1-ого тренувального зразка, (x₂, y₂) для

68
00:04:55,370 --> 00:05:01,980
2-ого тренувального зразка, і так до (xₘ, yₘ), 
тобто останнього нашого тренувального зразка.

69
00:05:01,980 --> 00:05:05,650
І все це разом - це весь наш тренувальний набір.

70
00:05:05,650 --> 00:05:10,170
Тож я буду використовувати маленьку m 
для позначення кількості тренувальних зразків.

71
00:05:10,170 --> 00:05:14,418
Також інколи, щоб підкреслити, що це - кількість 
тренувальних зразків, я писатиму

72
00:05:14,418 --> 00:05:16,437
m з нижнім індексом train.

73
00:05:16,437 --> 00:05:18,692
А коли ми говоритимемо про тестувальний набір,

74
00:05:18,692 --> 00:05:24,430
ми інколи використовуватимемо m з нижнім індексом test, 
щоб позначити кількість тестувальних зразків.

75
00:05:24,430 --> 00:05:27,430
Тож це - кількість тестувальних екземплярів.

76
00:05:27,430 --> 00:05:33,440
І нарешті, щоб компактно позначити всі тренувальні зразки,

77
00:05:33,440 --> 00:05:36,840
ми введемо матрицю велике X,

78
00:05:36,840 --> 00:05:41,592
яка являє собою складені в стовпчики

79
00:05:41,592 --> 00:05:44,568
x₁, x₂ і т.д. наш тренувальний набір.

80
00:05:44,568 --> 00:05:49,958
Тож ми беремо x₁ і записуємо його як 1-ий стовпчик матриці,

81
00:05:49,958 --> 00:05:54,798
x₂ і записуємо як 2-ий стовпчик і т.д. до xₘ.

82
00:05:54,798 --> 00:05:58,000
Це і буде матрицею велике X.

83
00:05:58,000 --> 00:06:03,005
Тож ця матриця X матиме m стовпців, де m - це кількість тренувальних

84
00:06:03,005 --> 00:06:08,665
зразків, а кількість рядків, або висота, цієї матриці - це nₓ.

85
00:06:08,665 --> 00:06:14,400
Май на увазі, що в інших випадках тобі може зустрітись матриця велике X,

86
00:06:14,400 --> 00:06:19,390
в якій тренувальні зразки будуть зберігатись в рядках. Ось так.

87
00:06:19,390 --> 00:06:23,940
Транспоноване x₁ і так до транспонованого xₘ.

88
00:06:23,940 --> 00:06:27,704
Виявляється, що коли ми реалізуємо НМ, використовуючи

89
00:06:27,704 --> 00:06:32,218
позначення зліва (згідно конвенції), реалізація буде значно простішою.

90
00:06:32,218 --> 00:06:37,171
Резюмуємо. X - матриця розміром nₓ*m. І,

91
00:06:37,171 --> 00:06:40,404
якщо ми реалізуємо це на Python,

92
00:06:40,404 --> 00:06:45,362
то побачимо x.shape() (це команда Python для

93
00:06:45,362 --> 00:06:50,325
знаходження розміру матриці), що повертає (nₓ, m).

94
00:06:50,325 --> 00:06:53,255
Це означає, що розмір матриці - nₓ*m.

95
00:06:53,255 --> 00:06:58,785
Тож ось так складаються вхідні тренувальні зразки x в матрицю.

96
00:06:58,785 --> 00:07:01,315
Як щодо вихідних міток y?

97
00:07:01,315 --> 00:07:04,815
Виходить, що для спрощення реалізації НМ

98
00:07:04,815 --> 00:07:10,030
буде зручно і y записати у стовпчик.

99
00:07:10,030 --> 00:07:14,650
Тож нехай велике Y дорівнює y₁, y₂

100
00:07:14,650 --> 00:07:18,580
і до yₘ. Ось так.

101
00:07:18,580 --> 00:07:24,980
Тож Y тут буде матрицею розміром 1*m.

102
00:07:24,980 --> 00:07:30,530
І знову ж, на Python, Y.shape() буде (1, m).

103
00:07:30,530 --> 00:07:34,810
Це означає матрицю розміру 1*m.

104
00:07:34,810 --> 00:07:39,660
Реалізуючи свою нову НМ пізніше 
в рамках цього курсу, ти побачиш, що буде зручно

105
00:07:39,660 --> 00:07:43,630
діяти згідно конвенції, тобто всі дані, 
пов'язані з різними тренувальними зразками

106
00:07:43,630 --> 00:07:48,580
(під даними я маю на увазі X, або Y, 
або інші величини, які ти побачиш пізніше),

107
00:07:48,580 --> 00:07:49,900
все необхідне -

108
00:07:49,900 --> 00:07:52,990
дані, пов'язані з різними тренувальними зразками -

109
00:07:52,990 --> 00:07:57,430
складати стовпчиками, як ми це робили для X і Y.

110
00:07:58,450 --> 00:08:01,380
Тож це - позначення, які ми використовуватимемо 
для ЛР і

111
00:08:01,380 --> 00:08:04,060
НМ далі в цьому курсі.

112
00:08:04,060 --> 00:08:07,430
Якщо ти забудеш що означає якесь позначення, наприклад, m

113
00:08:07,430 --> 00:08:08,300
чи n,

114
00:08:08,300 --> 00:08:12,630
чи будь-що інше, ми також опублікували на сайті курсу 
інструкцію з позначеннями,

115
00:08:12,630 --> 00:08:17,430
якою ти можеш користуватись, щоб швидко знайти 
що означає будь-яке позначення.

116
00:08:17,430 --> 00:08:20,890
Тож давай перейдемо до наступного відео, де ми почнемо розбирати ЛР

117
00:08:20,890 --> 00:08:23,190
використовуючи ці позначення.