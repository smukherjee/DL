Вітаю на 4-ому тижні курсу! До тепер ти побачив/ла пряме і зворотне поширення в контексті нейронної мережі (НМ) з 1-им прихованим шаром 
та логістичну регресію (ЛР). Ти вивчив/ла векторизацію і коли важливо ініціалізувати коефіцієнти в випадкові значення. Якщо ти виконав/ла минулотижневу домашню роботу, 
то вже встиг/ла застосувати і оцінити як це працює. Тож на даний момент ти вже побачив/ла більшість того, 
що потрібно для реалізації глибокої НМ. На цьому тижні ми візьмемо всі ці ідеї і складемо докупи так, щоб можна було реалізувати глибоку НМ. Через те, що цього тижня програмувальне завдання дуже велике і потребуватиме більше роботи, я зробив відео цього тижня коротшими, 
щоб ти міг/могла подивитись їх трохи швидше і потім мав/ла більше часу на велику програмувальну вправу вкінці, яка, надіюсь, закінчиться побудовою глибокої НМ, якою ти пишатимешся. Тож що таке глибока НМ? Ти вже бачив/ла це зображення ЛР, а також бачив/ла НМ з одним прихованим шаром. Ось приклади НМ з 2-ома прихованими шарами і НМ з 5-ма прихованими шарами, які ніби кажуть, що ЛР дуже "мілка" модель, а оця модель - набагато "глибша". "Мілка" чи "глибока" залежить від багатьох факторів. Тож НМ з 1-им прихованих шаром буде двошаровою НМ. Згадай, що коли ми рахуємо шари НМ, 
ми не враховуємо вхідний шар, ми рахуємо лише приховані шари і вихідний шар. Тож це буде - 2-шарова НМ, все ще досить мілка, проте, вже не така як ЛР. Технічно, ЛР - 1-шарова НМ. Проте, за останні кілька років в спільноті ШІ [штучний інтелект] в машинному навчанні (МН) 
зрозуміли, що існують функції, які можна навчати в дуже глибоких НМ і практично неможливо в мілкіших. Хоча для кожної окремої задачі 
практично неможливо точно передбачити наскільки глибоку НМ потрібно побудувати, проте, буде розумно спробувати ЛР, 1-, а потім 2-шарову НМ. Взагалі, кількість прихованих шарів 
 можна розглянути як додатковий гіперпараметр, в який можна підставити різні числа і оцінити його вплив на даних для перехресної перевірки 
або розробницькому наборі. Ми поговоримо про це детальніше пізніше. Давай зараз розглянемо позначення, 
які ми застосовуємо для опису глибоких НМ. Ось 1, 2, 3, 4-шарова НМ з 3-ома прихованими шарами. Кількість вузлів в прихованих шарах - 5, 5, 3 і 1 вихідний вузол. Тож для позначення ми використовуватимемо велике L, яким позначатимемо кількість шарів НМ. Тож в цьому випадку L=4, тобто кількості шарів. Також ми використовуватимемо n[l] для позначення кількості вузлів або обчислювальних одиниць в шарі під номером мале l. Тож, якщо ми проіндексуємо оце як 0-ий шар, оце - як 1-ий шар, оце - як 2-ий шар, оце - як 4-ий шар, то матимемо, наприклад, що n[1] (тобто оце, 1-ий прихований шар) буде =5, 
бо ми маємо тут 5 прихованих вузлів. Для оцього ми маємо, що n[2] (кількість вузлів в 2-ому прихованому шарі) також =5. n[3]=3 і n[4] (а воно буде також n[L], тобто відображати кількість вихідних вузлів) буде =1, бо велике L=4. Також матимемо, що для вхідного вузла n[0], який також =n[x], буде =3. Тож такі позначення ми будемо використовувати, 
щоб позначати скільки вузлів ми маємо в окремому шарі. Для кожного вузла l ми використовуватимемо a[l], щоб позначати активатори в шарі l. Пізніше ми побачимо, що в прямому поширенні 
ми будемо обчислювати a[l] як функцію активації g(z[l]). І функції теж дамо індекс l. А також w[l], щоб позначити коефіцієнти обчислюваного z[l] в шарі l. Аналогічно, - b[l] для обчислення z[l]. І, нарешті, останнє позначення. Вхідні ознаки, які називаються x, є також активаторами 0-ого шару. Тож a[0]=x. А активатори останнього шару - a[L]=ŷ. Тож a з верхнім індексом L в квадратних дужках = 
передбачуваному виходу або передбаченням ŷ НМ. Отже, тепер ти знаєш як виглядає глибока НМ і які позначення ми використовуємо 
для опису і обчислень всередині НМ. Я знаю, що дав багато нових позначень в цьому відео, 
проте, якщо ти якось забудеш, що позначає якийсь знак, 
то ми також опублікували на сайті курсу список позначень або путівник позначень, 
який можна використовувати для пошуку значення символів. Далі я хотів би описати як виглядає пряме поширення в цьому типі НМ. Давай перейдемо до наступного відео.