1
00:00:00,360 --> 00:00:04,530
前回のビデオでは ニューラルネットワークで
１つのトレーニングサンプルから予測を計算する方法を

2
00:00:04,530 --> 00:00:06,610
見ました

3
00:00:06,610 --> 00:00:11,520
このビデオでは 複数のトレーニングサンプルを
ベクトル化する方法を紹介します

4
00:00:11,520 --> 00:00:15,350
結果はロジステック回帰のときに見たものと
ほぼ同じものになります

5
00:00:15,350 --> 00:00:19,050
それぞれのトレーニングサンプルを
それぞれの列に重ねていくことで

6
00:00:19,050 --> 00:00:23,630
先ほどのビデオでの式を
少し変えると使うことができます

7
00:00:23,630 --> 00:00:27,860
ニューラルネットワークに
ほぼ同時に

8
00:00:27,860 --> 00:00:32,340
全てのサンプルに対しての出力を
計算させることができます

9
00:00:32,340 --> 00:00:35,080
どのようにするのか
見ていきましょう

10
00:00:35,080 --> 00:00:40,192
この４つの式は前回のビデオで使った式で
z[1]、a[1]、z[2]、a[2]の計算法を

11
00:00:40,192 --> 00:00:41,348
表わしています

12
00:00:41,348 --> 00:00:46,867
これを使うと
ベクトルx を入力したときに

13
00:00:46,867 --> 00:00:53,810
単一のサンプルのa[2]=yハットを
生成することができます

14
00:00:54,920 --> 00:01:00,050
m 個のサンプルがある時には
この処理を繰り返します

15
00:01:00,050 --> 00:01:01,870
つまり最初のサンプルの

16
00:01:01,870 --> 00:01:06,600
x(1)を使って

17
00:01:06,600 --> 00:01:11,062
最初のサンプルの予測値のyハット(1)を
計算します

18
00:01:11,062 --> 00:01:16,537
x(2)も同じ式を使い
yハット(2)を生成します

19
00:01:16,537 --> 00:01:23,050
x(m) のところまで続けて行き
yハット(m)を生成します

20
00:01:23,050 --> 00:01:28,349
すべての活性化関数の表記をするために

21
00:01:28,349 --> 00:01:31,669
a[2](1)と書き

22
00:01:31,669 --> 00:01:36,676
a[2](2)と書き

23
00:01:36,676 --> 00:01:40,640
a[2](m)と書きます

24
00:01:40,640 --> 00:01:46,830
この表記a[2](i)は

25
00:01:46,830 --> 00:01:52,520
(i)がi個目のサンプルを指し

26
00:01:52,520 --> 00:01:57,220
(2)が２層目を指します

27
00:01:58,530 --> 00:02:02,460
このようにして[]と()の添え字は
使われます

28
00:02:04,170 --> 00:02:07,920
これらが意味するのは
ベクトル化されていない実装の場合で

29
00:02:07,920 --> 00:02:11,000
全てのトレーニングサンプルに対して
予測値を計算する場合には

30
00:02:11,000 --> 00:02:15,630
for i=1 to mをし

31
00:02:15,630 --> 00:02:18,260
これらの４つの式を
実装する必要があります

32
00:02:18,260 --> 00:02:24,162
z[1](i)

33
00:02:24,162 --> 00:02:30,064
=W[1]x(i)+b[1]

34
00:02:30,064 --> 00:02:38,253
a[1](i)はシグモイドz[1](i)で

35
00:02:38,253 --> 00:02:43,683
z[2](i)=W[2]a[1](i)+b[2]です

36
00:02:43,683 --> 00:02:50,099
そして

37
00:02:50,099 --> 00:02:56,686
a[2](i)はシグモイドz[2](i)です

38
00:02:56,686 --> 00:03:03,172
これらの４つの式に対して
トレーニングサンプルに

39
00:03:03,172 --> 00:03:08,788
依存している全ての変数に
(i)を付けます

40
00:03:08,788 --> 00:03:12,612
つまり(i)を
x、Z、a に付けます

41
00:03:12,612 --> 00:03:18,570
こうすると m個のトレーニングサンプルの出力を
計算できます

42
00:03:18,570 --> 00:03:23,930
ここで行いたいのは
この計算をベクトル化してfor ループを排除します

43
00:03:23,930 --> 00:03:27,680
ところで 私があまりに多くの線形代数の基礎を
出しているように感じても

44
00:03:27,680 --> 00:03:31,170
深層学習時代には
これを正しく実装できることが

45
00:03:31,170 --> 00:03:34,580
重要なことだと分かってきます

46
00:03:34,580 --> 00:03:38,160
私たちはこのコースでの表記方法について
非常に注意を払って選びました

47
00:03:38,160 --> 00:03:41,460
ベクトル化のステップを
できるだけ簡単にすることを目指しました

48
00:03:41,460 --> 00:03:46,140
この多くの基礎を見ていくことで
これらのアルゴリズムの正しい実装を

49
00:03:46,140 --> 00:03:49,750
より早く動作させられるようになる手助けが
できればと思います

50
00:03:51,060 --> 00:03:56,210
ではこのコード全体を
コピーして次のスライドに入れて

51
00:03:56,210 --> 00:03:57,880
どうベクトル化するかを
見ていきましょう

52
00:03:59,130 --> 00:04:02,154
ここに前のスライドからの
m個のトレーニングサンプルを

53
00:04:02,154 --> 00:04:04,324
順次処理するfor ループがあります

54
00:04:04,324 --> 00:04:09,769
定義を思い出してください

55
00:04:09,769 --> 00:04:16,860
行列Xは トレーニングサンプルが
列に重ねられたものでした

56
00:04:16,860 --> 00:04:20,180
トレーニングサンプルを
列に並べたものです

57
00:04:20,180 --> 00:04:23,220
nxとm次元の配列の

58
00:04:23,220 --> 00:04:27,860
行列になります

59
00:04:29,198 --> 00:04:32,630
このfor ループを
ベクトル化した実装を行うために

60
00:04:32,630 --> 00:04:35,760
必要なことの肝心な部分を
教えようとしています

61
00:04:35,760 --> 00:04:41,394
ここで行う必要があるのは

62
00:04:41,394 --> 00:04:46,035
Z[1]=w[1]X+b[1]を計算することです

63
00:04:46,035 --> 00:04:50,692
A[1]=シグモイドのz[1]

64
00:04:50,692 --> 00:04:56,157
Z[2]=w[2]A[1]+b[2]で計算でき

65
00:04:56,157 --> 00:05:01,348
最後に

66
00:05:01,348 --> 00:05:10,100
A[2]=シグモイドのz[2]

67
00:05:10,100 --> 00:05:16,440
例示をするなら
小文字のベクトルx から

68
00:05:16,440 --> 00:05:23,480
それぞれ列に重ねていくことで
大文字の行列Xまで来ました

69
00:05:23,480 --> 00:05:28,494
これをzに対しても行うと

70
00:05:28,494 --> 00:05:33,509
z[1](1)、z[1](2)のように続けて

71
00:05:33,509 --> 00:05:40,290
これはすべて列ベクトルで
z[1](m)まで行きます

72
00:05:40,290 --> 00:05:46,270
このz[1](i)でもmの全てを
列に重ねることで

73
00:05:46,270 --> 00:05:50,045
Z[1]の行列ができます

74
00:05:50,045 --> 00:05:55,299
同じようにこのa[1](i)の量は

75
00:05:55,299 --> 00:06:00,957
a[1](1)、a[1](2)

76
00:06:00,957 --> 00:06:06,980
そしてa[1](m)までを
列に重ねると

77
00:06:06,980 --> 00:06:11,610
小文字のxから大文字のXを算出し
小文字のzから大文字のZを

78
00:06:11,610 --> 00:06:13,280
算出したのと同じように

79
00:06:13,280 --> 00:06:20,920
ベクトル小文字のa から
A[1]を算出します

80
00:06:20,920 --> 00:06:26,685
同様にZ[2]とA[2]も
計算できます

81
00:06:26,685 --> 00:06:30,141
これらもベクトルを使って

82
00:06:30,141 --> 00:06:32,016
横に重ねることで
取得できます

83
00:06:32,016 --> 00:06:37,326
これについても
横に重ねることでできます

84
00:06:37,326 --> 00:06:40,840
Z[2]とA[2]が得られます

85
00:06:40,840 --> 00:06:44,042
この表記が持っているもので
考えると役立つのは

86
00:06:44,042 --> 00:06:47,391
このZやAなどの行列は

87
00:06:47,391 --> 00:06:51,420
トレーニングサンプルに対して
横に番号付けを行います

88
00:06:51,420 --> 00:06:55,631
これが横の番号付けが
トレーニングサンプルに対応する理由です

89
00:06:55,631 --> 00:06:59,730
左から右に行くと
トレーニングデータを順次見ていることになります

90
00:06:59,730 --> 00:07:04,617
縦には この垂直の番号付けは
ニューラルネットワークのそれぞれのノードに

91
00:07:04,617 --> 00:07:06,130
対応します

92
00:07:06,130 --> 00:07:11,077
例えば このノードは

93
00:07:11,077 --> 00:07:16,554
行列のこの一番上の角の値は
最初のトレーニングサンプルの

94
00:07:16,554 --> 00:07:21,633
隠れ層の最初のユニットに対応します

95
00:07:21,633 --> 00:07:25,812
ひとつ下の値は最初のトレーニングサンプルの
隠れ層の２つ目のユニットに

96
00:07:25,812 --> 00:07:27,525
対応します

97
00:07:27,525 --> 00:07:31,505
そして最初のトレーニングサンプルの
隠れ層の３個目のユニットと続きます

98
00:07:31,505 --> 00:07:37,540
下へと見ていくときは
隠れ層のユニットの番号付けを索引していきます

99
00:07:39,670 --> 00:07:42,564
一方 横へと見ていくときは
隠れ層の最初のユニットで最初のトレーニングサンプルから

100
00:07:42,564 --> 00:07:45,450
２個目のサンプルへと見ていき

101
00:07:45,450 --> 00:07:48,240
３個目そして
次々と続いていき

102
00:07:48,240 --> 00:07:53,718
ここのノードが対応するのが
隠れ層の最初のユニットかつ

103
00:07:53,718 --> 00:07:59,030
最後でそしてm個目のトレーニングサンプルの
活性化のところまで続きます

104
00:08:00,760 --> 00:08:07,663
まとめると 行列Aの水平側は
トレーニングサンプルに対応していて

105
00:08:10,150 --> 00:08:14,195
垂直側は行列Aのそれぞれの索引は

106
00:08:14,195 --> 00:08:17,589
それぞれの隠れ層のユニットに
対応しています

107
00:08:22,342 --> 00:08:26,870
同じ感覚が行列Zでも行列Xでも
当てはまります

108
00:08:26,870 --> 00:08:31,840
水平がそれぞれのトレーニングサンプルに対応し

109
00:08:31,840 --> 00:08:36,227
垂直がニューラルネットワークの入力層での
ユニットまた同時に

110
00:08:36,227 --> 00:08:41,180
それぞれの入力特徴量に
対応しています

111
00:08:42,750 --> 00:08:46,600
これらの式に関して ベクトル化してニューラルネットワークを
実装する方法が分かったかと思います

112
00:08:46,600 --> 00:08:51,320
複数のサンプルに対しての
ベクトル化です

113
00:08:51,320 --> 00:08:55,130
次のビデオでは
この方法がこのような種類のベクトル化について

114
00:08:55,130 --> 00:08:59,070
正しい実装なのかを
正当化する理由を紹介します

115
00:08:59,070 --> 00:09:03,468
正当化がロジステック回帰で見たものと
同じようなものだと判明します

116
00:09:03,468 --> 00:09:05,300
次のビデオに進みましょう