1
00:00:00,000 --> 00:00:02,520
حسنًا، أعتقد أن هذا الفيديو سيكون مثيرًا.

2
00:00:02,520 --> 00:00:04,635
في هذا الفيديو، سترون كيفية تنفيذ

3
00:00:04,635 --> 00:00:08,490
انحدار التدرج للشبكة العصبية
بطبقة مخفية واحدة لديكم.

4
00:00:08,490 --> 00:00:12,090
في هذا الفيديو، سأمنحكم فقط
المعادلات التي تحتاجون إلى تنفيذها

5
00:00:12,090 --> 00:00:16,245
لتفعيل الانتشار الخلفي
أو انحدار التدرج،

6
00:00:16,245 --> 00:00:18,555
وبعد ذلك في الفيديو الذي يليه،

7
00:00:18,555 --> 00:00:20,940
سأوضح لكم الأسباب التي تجعل

8
00:00:20,940 --> 00:00:24,150
هذه المعادلات بالذات هي
المعادلات الدقيقة،

9
00:00:24,150 --> 00:00:28,320
والمعادلات الصحيحة لحساب
التدرجات التي تحتاجون إليها لشبكتكم العصبية.

10
00:00:28,320 --> 00:00:29,910
لذا فإن الشبكة العصبية،

11
00:00:29,910 --> 00:00:31,875
التي تحتوي على طبقة مخفية واحدة حتى الآن،

12
00:00:31,875 --> 00:00:34,964
ستحتوي على المتغيرات W1

13
00:00:34,964 --> 00:00:39,285
وB1 وW2 وB2.

14
00:00:39,285 --> 00:00:40,800
لذلك، كتذكير،

15
00:00:40,800 --> 00:00:48,150
إذا كان لديكم NX أو بدلاً من ذلك ميزات الإدخال N0،

16
00:00:48,150 --> 00:00:51,090
والوحدات المخفية N1،

17
00:00:51,090 --> 00:00:57,260
ووحدات الإخراج N2 في الأمثلة لدينا.

18
00:00:57,260 --> 00:00:59,690
حتى الآن لدي فقط N2 يساوي 1،

19
00:00:59,690 --> 00:01:05,720
وستكون المصفوفة W1 هي N1، N0.

20
00:01:05,720 --> 00:01:08,870
B1 ستكون المتجه ذو الأبعاد N1،

21
00:01:08,870 --> 00:01:12,770
لذا، يمكننا كتابة ذلك كـ N1
في مصفوفة أحادية الأبعاد،

22
00:01:12,770 --> 00:01:14,120
ومتجه العمود بالفعل.

23
00:01:14,120 --> 00:01:18,395
ستكون أبعاد W2 هي N2، N1،

24
00:01:18,395 --> 00:01:25,485
وسيكون بُعد B2 هو N2، 1.

25
00:01:25,485 --> 00:01:28,925
حسنًا، رأينا فقط حتى الآن
أمثلة يكون خلالها N2 يساوي 1،

26
00:01:28,925 --> 00:01:32,180
حيث تكون لديكم وحدة مخفية واحدة فقط.

27
00:01:32,180 --> 00:01:39,405
لذا، لديكم أيضًا دالة تكلفة
لشبكة عصبية.

28
00:01:39,405 --> 00:01:43,370
حتى الآن، سأفترض
أنكم تقومون بالتصنيف الثنائي.

29
00:01:43,370 --> 00:01:45,110
وفي تلك الحالة،

30
00:01:45,110 --> 00:01:50,600
تكلفة المتغيرات الخاصة بكم
كما يلي ستكون

31
00:01:50,600 --> 00:01:56,520
1 على M في متوسط دالة الخسارة هذه.

32
00:01:56,520 --> 00:02:02,580
إذًا، L هنا هي الخسارة عندما
تتوقع الشبكة العصبية لديكم Y hat، حسنًا.

33
00:02:02,580 --> 00:02:06,750
هذا حقًا A2 عندما تكون
تسمية التدرج تساوي Y.

34
00:02:06,750 --> 00:02:08,560
إذا كنتم تقومون بإجراء التصنيف الثنائي،

35
00:02:08,560 --> 00:02:13,310
فإن دالة الخسارة ستكون تحديدًا ما
تستخدمونه للانحدار اللوجستي في وقت سابق.

36
00:02:13,310 --> 00:02:15,890
لذا، لتدريب متغيرات الخوارزمية لديكم،

37
00:02:15,890 --> 00:02:19,705
فإنكم تحتاجون إلى تنفيذ انحدار التدرج.

38
00:02:19,705 --> 00:02:21,570
عند تدريب شبكة عصبية،

39
00:02:21,570 --> 00:02:26,435
يكون من المهم تهيئة المتغيرات
عشوائيًا بدلاً من جميع الأصفار.

40
00:02:26,435 --> 00:02:28,340
سنرى لاحقًا سبب هذه الحالة،

41
00:02:28,340 --> 00:02:31,110
لكن بعد تهيئة
المتغير إلى شيء ما،

42
00:02:31,110 --> 00:02:34,555
كل حلقة أو انحدار تدرج
مع التوقعات المحسوبة.

43
00:02:34,555 --> 00:02:38,270
لذا، فأنتم تحسبون Y hat I بشكل أساسي،

44
00:02:38,270 --> 00:02:41,765
لـ I يساوي 1 إلى M، مثلاً.

45
00:02:41,765 --> 00:02:44,450
ثم، تحتاجون إلى حساب المشتقات.

46
00:02:44,450 --> 00:02:47,750
وتحتاجون إلى حساب DW1،

47
00:02:47,750 --> 00:02:54,279
وهذا هو المشتقة لدالة التكلفة
مع مراعاة المتغير W1،

48
00:02:54,279 --> 00:02:56,499
يمكنكم حساب متغير آخر،

49
00:02:56,499 --> 00:02:58,375
وسأطلق عليه DB1،

50
00:02:58,375 --> 00:03:02,260
وهو مشتقة أو انحدار
دالة التكلفة لديكم

51
00:03:02,260 --> 00:03:06,190
مع مراعاة المتغير B1 وما إلى ذلك.

52
00:03:06,190 --> 00:03:09,685
وبالمثل للمتغيرات الأخرى W2 وB2.

53
00:03:09,685 --> 00:03:17,775
وأخيرًا، تحديث انحدار التدرج
سيكون عبارة عن تحديث W1 كـ W1 ناقص ألفا.

54
00:03:17,775 --> 00:03:21,150
معدل التعلم مضروبًا في D, W1.

55
00:03:21,150 --> 00:03:26,310
سيتم تحديث B1 إلى B1 ناقص معدل التعلم

56
00:03:26,310 --> 00:03:32,280
مضروبًا في DB1، وبالمثل لـ W2 وB2.

57
00:03:32,280 --> 00:03:35,560
أحيانًا، أستخدم نقطتين يساوي
وأحيانًا يساوي،

58
00:03:35,560 --> 00:03:37,630
أي رمز يعمل جيدًا.

59
00:03:37,630 --> 00:03:40,790
لذا، سيكون هذا
تكرارًا للانحدار التدريجي،

60
00:03:40,790 --> 00:03:42,580
وتكررون ذلك مرات عديدة

61
00:03:42,580 --> 00:03:45,100
حتى تبدو المتغيرات
وكأنها تتقارب.

62
00:03:45,100 --> 00:03:46,300
لذا، في الفيديوهات السابقة،

63
00:03:46,300 --> 00:03:49,055
تحدثنا حول كيفية
حساب التوقعات،

64
00:03:49,055 --> 00:03:50,400
كيفية حساب المخرجات،

65
00:03:50,400 --> 00:03:52,960
ورأينا كيفية القيام بذلك
من خلال طريقة تنطوي على التحويل إلى متجه أيضًا.

66
00:03:52,960 --> 00:03:57,820
لذا، فإن الأمر الرئيسي هو معرفة كيفية حساب
مصطلحات المشتقات الجزئية هذه،

67
00:03:57,820 --> 00:04:03,010
DW1 وDB1 وكذلك
المشتقات DW2 وDB2.

68
00:04:03,010 --> 00:04:06,730
لذا ما أريد القيام به هو إعطاءكم

69
00:04:06,730 --> 00:04:11,050
المعادلات التي تحتاجون إليها
لحساب هذه المشتقات.

70
00:04:11,050 --> 00:04:15,130
وسأقوم بتأجيل ذلك إلى الفيديو التالي،
وهو فيديو اختياري

71
00:04:15,130 --> 00:04:19,030
للتعمق أكثر فيما يتعلق بكيفية
التوصل إلى هذه الصيغ.

72
00:04:19,030 --> 00:04:25,360
لذا، دعوني فقط
ألخص معادلات الانتشار الأمامي.

73
00:04:25,360 --> 00:04:32,510
لذا، لديكم Z1 يساوي W1X زائد B1،

74
00:04:32,510 --> 00:04:42,560
ثم A1 يساوي دالة التنشيط
في هذه الطبقة المطبقة حسب العناصر كـ Z1،

75
00:04:42,560 --> 00:04:46,610
ثم Z2 يساوي W2،

76
00:04:46,610 --> 00:04:52,595
A1 زائد V2، ثم أخيرًا،

77
00:04:52,595 --> 00:04:55,295
مثل كل المتغيرات المحولة إلى متجهات على مستوى مجموعتكم التدريبية، حسنًا؟

78
00:04:55,295 --> 00:05:00,580
A2 يساوي G2 (Z2)‎.

79
00:05:00,580 --> 00:05:03,605
ومرة أخرى، إذا كنا نفترض الآن
أننا نقوم بالتصنيف الثنائي،

80
00:05:03,605 --> 00:05:07,120
فإن دالة التنشيط هذه،
يجب أن تكون الدالة السينية حقًا،

81
00:05:07,120 --> 00:05:08,995
نفس الشيء لهذه النهاية العصبية.

82
00:05:08,995 --> 00:05:11,900
فهذا هو الانتشار الأمامي أو

83
00:05:11,900 --> 00:05:14,690
الاتجاه من اليسار إلى اليمين للحساب لشبكتكم العصبية.

84
00:05:14,690 --> 00:05:16,730
بعد ذلك، دعونا نحسب المشتقات.

85
00:05:16,730 --> 00:05:21,725
إذًا، هذه هي خطوة الانتشار الخلفي،

86
00:05:21,725 --> 00:05:30,900
ثم أحسب DZ2 يساوي A2
ناقص تدرج Y،

87
00:05:30,900 --> 00:05:33,130
وفقط كتذكير،

88
00:05:33,130 --> 00:05:35,240
تكون كل هذه المتغيرات محولة إلى متجهات عبر الأمثلة.

89
00:05:35,240 --> 00:05:38,540
إذًا، المصفوفة Y هي هذه المصفوفة 1 على

90
00:05:38,540 --> 00:05:44,600
M التي تدرج كل أمثلة M
الخاصة بكم مكدسة أفقيًا.

91
00:05:44,600 --> 00:05:50,599
ثم تبين أن DW2 تساوي ذلك،

92
00:05:50,599 --> 00:05:54,920
وفي الواقع، هذه المعادلات الثلاث الأولى

93
00:05:54,920 --> 00:06:00,630
مماثلة تمامًا للانحدار التدريجي
المرتبط بالانحدار اللوجستي.

94
00:06:00,910 --> 00:06:03,170
X تساوي 1،

95
00:06:03,170 --> 00:06:08,635
فاصلة، keep dims=true.

96
00:06:08,635 --> 00:06:13,600
لنتعرف على تفاصيل بسيطة وهي أن np.sum هو

97
00:06:13,600 --> 00:06:18,700
أمر Python NumPy للجمع
على مستوى بُعد واحد بالمصفوفة.

98
00:06:18,700 --> 00:06:21,450
في هذه الحالة، يتم الجمع أفقيًا،

99
00:06:21,450 --> 00:06:25,645
وما تفعله keepdims هو
أنها تمنع بايثون من

100
00:06:25,645 --> 00:06:30,750
إخراج واحدة من هذه المصفوفات
المرحة المصنفة رقم واحد، حسنًا؟

101
00:06:30,750 --> 00:06:33,525
حيث كانت الأبعاد N فاصلة.

102
00:06:33,525 --> 00:06:36,045
لذلك من خلال وجود keepdims = true،

103
00:06:36,045 --> 00:06:43,210
فإن هذا يضمن أن بايثون ستخرج
لـ DB متجهًا N, 1.

104
00:06:43,210 --> 00:06:47,145
في الواقع، أعتقد أن ذلك من الناحية التقنية سيكون N2, 1.

105
00:06:47,145 --> 00:06:49,680
في هذه الحالة، سيكون رقمًا تلو الآخر،

106
00:06:49,680 --> 00:06:51,795
وربما لا يكون مهمًا،

107
00:06:51,795 --> 00:06:55,350
ولكن لاحقًا، سنرى عندما يكون ذلك مهمًا فعلاً.

108
00:06:55,350 --> 00:06:59,825
لذا، فإن ما فعلناه حتى الآن
هو مماثل جدًا للانحدار اللوجستي.

109
00:06:59,825 --> 00:07:04,260
ولكن الآن بينما تواصلوا
تشغيل الانتشار الخلفي،

110
00:07:04,260 --> 00:07:05,790
ستحسبون ذلك،

111
00:07:05,790 --> 00:07:16,340
DZ2 مضروبًا في G1 '(Z1)‎.

112
00:07:16,340 --> 00:07:19,190
لذا فإن هذه الكمية G1' هي

113
00:07:19,190 --> 00:07:23,945
المشتقة الخاص بما إذا كانت دالة التنشيط
التي تستخدمونها للطبقة المخفية،

114
00:07:23,945 --> 00:07:25,750
ولطبقة الإخراج،

115
00:07:25,750 --> 00:07:29,470
أفترض أنكم تنفذون التصنيف
الثنائي من خلال الدالة السينية.

116
00:07:29,470 --> 00:07:32,630
لذلك، فإن هذا مضمن بالفعل
في هذه الصيغة لـ DZ2،

117
00:07:32,630 --> 00:07:37,735
وفي هذه المرة هذا المنتج يعتمد على العناصر.

118
00:07:37,735 --> 00:07:45,650
لذلك، ستكون هذه عبارة عن
مصفوفة N1, M، وسيكون هذا

119
00:07:45,650 --> 00:07:51,545
المشتقة التي تعتمد على العناصر أيضًا
عبارة عن مصفوفة N1, N،

120
00:07:51,545 --> 00:07:55,910
وبالتالي سيكون في هذه المرة هناك
ناتجًا يعتمد على العناصر للمصفوفتين.

121
00:07:55,910 --> 00:08:00,950
وأخيرًا، DW1 يساوي هذا،

122
00:08:00,950 --> 00:08:07,010
وDB1 يساوي هذا،

123
00:08:07,010 --> 00:08:14,930
وp.sum DZ1، المحور

124
00:08:14,930 --> 00:08:20,820
يساوي 1، keepdims = true.

125
00:08:20,820 --> 00:08:26,455
لذا، بينما في السابق ربما كانت
أهمية keepdims أقل إذا كان N2 يساوي 1.

126
00:08:26,455 --> 00:08:29,475
فإن النتيجة هي واحد تلو الآخر،
وهو مجرد عدد حقيقي.

127
00:08:29,475 --> 00:08:36,330
هنا، DB1 ستكون المتجه N1, 1،

128
00:08:36,330 --> 00:08:39,180
لذا، فأنتم تريدون بايثون، وتريدون Np.sons.

129
00:08:39,180 --> 00:08:43,990
سأضع شيئًا من هذا البعد بدلاً من
مصفوفة مرحة من التصنيف الأول

130
00:08:43,990 --> 00:08:49,720
لهذا البعد والذي قد ينتهي به الحال
من خلال الخطأ في بعض حسابات البيانات.

131
00:08:49,720 --> 00:08:52,914
وستكون الطريقة الأخرى هي
عدم الاحتفاظ بالمتغيرات،

132
00:08:52,914 --> 00:09:01,470
ولكن تتم إعادة تشكيل إخراج NP.sum
إلى هذا البُعد بوضوح،

133
00:09:01,470 --> 00:09:04,665
وهو ما تود أن تحتوي عليه DB.

134
00:09:04,665 --> 00:09:09,655
حسنًا، فقد كان هذا هو الانتشار الأمامي
في أربع معادلات على ما أعتقد،

135
00:09:09,655 --> 00:09:13,400
والانتشار الخلفي في ست معادلات على ما أعتقد.

136
00:09:13,400 --> 00:09:15,590
أعرف أنني قمت بتدوين هذه المعادلات،

137
00:09:15,590 --> 00:09:17,990
ولكن في الفيديو الاختياري الآخر،

138
00:09:17,990 --> 00:09:20,690
سوف نقوم بتوضيح بعض الأمور لكيفية

139
00:09:20,690 --> 00:09:24,730
اشتقاق المعادلات الست لخوارزمية
الانتشار الخلفي.

140
00:09:24,730 --> 00:09:26,455
لكم مطلق الحرية سواء أردتم مشاهدته أم لا.

141
00:09:26,455 --> 00:09:28,875
ولكن على أي حال،
إذا قمتم بتطبيق هذه الخوارزميات،

142
00:09:28,875 --> 00:09:33,255
فسيكون لديكم تطبيق صحيح
للانتشار الأمامي والانتشار الخلفي.

143
00:09:33,255 --> 00:09:38,120
وسيكون بإمكانكم حساب المشتقات
التي تحتاجون إليها لتطبيق انحدار التدرج،

144
00:09:38,120 --> 00:09:40,360
لتعلم متغيرات الشبكة العصبية الخاصة بكم.

145
00:09:40,360 --> 00:09:43,190
ويمكن تطبيق هذه الخوارزمية

146
00:09:43,190 --> 00:09:46,000
وجعلها ملائمة للعمل بدون
الفهم العميق لحساب التفاضل والتكامل.

147
00:09:46,000 --> 00:09:49,505
ويقوم بذلك الكثير من
ممارسي التعلم العميق الناجحين.

148
00:09:49,505 --> 00:09:50,975
ولكن، إذا أردتم،

149
00:09:50,975 --> 00:09:52,505
يمكنكم أيضًا مشاهدة الفيديو التالي،

150
00:09:52,505 --> 00:09:57,680
للتمتع بفهم أكبر للاشتقاق
الخاص بهذه المعادلات.