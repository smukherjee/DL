이번 비디오에서는, 신경망 네트워크네서
예측수치를 계산하는 방법을 배웠습니다. 싱글 트레이닝 샘플에 대해서 말이죠. 이번 비디오에서는, 여러개의 트레이닝 샘플에서
벡터화시키는 방법을 배우겠습니다. 로지스특 회귀분석에서 본 결과와
비슷할텐데요, 다른 트레이닝 샘플을 매트릭스 줄별로 쌓아서 이전 비디오에서 이용한 공식을 다시
이용 가능한데요, 이전 공식을 살짝 변형시켜,
신경망이 모든 샘플에 대해 결과값을 산출할 수 있도록 해줍니다. 어떻게 하는지 한번 상세 내용을 보겠습니다. 이전 비디오에서 z1, a1, z2, a2의 값을 구하는 방법에 대한 4개 공식이였는데요, 벡터 x 입력 특성에 따라서, 이것을 이용해서, a2 = ŷ인 것을 생성할 수 있습니다.
하나의 트레이닝 샘플에 대해서 말이죠. 여러분이 m 트레이닝 샘플이 있는 경우,
반복적으로 진행하는데요, 첫번째 샘플의 경우, x 위첨자 소괄호 1로 ŷ을 계산합니다.
첫번째 트레이닝 샘플에 대한 예상수치입니다. x2 를 이용해서
ŷ 2를 예상합니다. 이렇게 x(m)까지 내려가
ŷ m을 구합니다. activation 함수로 표기하기 위해, 이것을 a[2](1)으로 표기하겠습니다. 그리고 이것은 a[2](2)이구요, 이것은 a(2)(m)입니다. 여기서 a[2](i)로 표기되는데요, 소괄호 i는 트레이닝 샘플 i를 뜻합니다. 대괄호 2는 2번 층을 나타내구요. 이렇게 대괄호와 소괄호가 사용되는 것입니다. unvectorized 도입이 있는 경우에는, 모든 트레이닝 샘플에 대해서
그 예측값을 계산하고 싶으면, i가 1에서 m까지 인 경우 말이죠, 그러면 여기 4개의 공식을 도입합니다.
아시겠죠? z[1](i)를 W(1) x(i) + b[1]와 일치하게 해줍니다. a[1](i) 는 z[1](1)의 시그모이드입니다. z[2](i) 는 w[2]a[1](i) + b[2] 입니다. 그리고, z2i는 w2a1i 더하기
b2이구요 a[2](i) 는 z[2](i)의 시그모이드 입니다. 그러면 여기 4개 공식에
트레이닝 샘플에 의존하는 모든 변수에 위첨자 소괄호를 더하는 것이라고 생각하면 됩니다. 여기 위첨자와 소괄호 i 를
x, z, a에 더합니다. 여러분이 모든 결과값을 m트레이닝 샘플에 대해
계산하고 싶으면 여기 전체 값을 벡터화시킵니다. 제가 만약에 이런 선형 수학 부분이 많이 나온다고 생각이 드시면,
알아두실 부분은, 딥러닝에서 이러한 부분이 올바른 값이 되도록하는 것이
중요하다는 것입니다. 그리고 이 코스에서는
표기를 신중히 합니다. 이 벡터화를 최대한 쉽게 만들기 위해서 말이죠. 이런 핵심부분은 여러분의 알고리즘이 잘 작동할 수 있도록
빨리 도입되는데 도움을 줄 것입니다. 자 그럼, 여기 코드부분을
다음 슬라이드에 모두 복사하겠습니다. 그리고나서 벡터화시키는 방법을 살펴보겠습니다. 이전 슬라이드에 보이는 부분인데요, 4개의 트레이닝 샘플에 m개의
트레이닝 샘플과 연결되는 부분입니다. 다시 한번 보겠습니다.
x 매트릭스를 여기 이 줄들에 쌓은 트레이닝 샘플이라고 했죠. 그러면 트레이닝 샘플을 모두 여기 줄에 쌓도록 합니다. 그러면 이것은 n이 되구요, 정확히 얘기하면 nx이죠, 그리고 m이죠,
이것이 매트릭스의 다이멘션이 됩니다. 핵심부분 펀치라인을 얘기하겠습니다.
여기 for loop의 벡터화된 도입을 위해 어떤 것을 도입해야하는지에 관한 내용 말이죠. 여러분이 계산해야 하는 것은, Z[1] = W[1] X + b[1] 그리고 A[1]= z[1] 의 시그모이드 입니다.. 그리고 z[2] = w[2]입니다 A[1] + b[2] 이고, A[2] 는 Z[2]의 시그모이드입니다. 유사한 점은, 저희가 소문자 x의 벡터에서 대문자 x 매트릭스로 갔는데요,
다른 줄별로 소문자 x의 값을 쌓아서 말이죠. z도 이와 같이 똑같이 진행하면,
예를 들어, z[1](i), z[1](2), 등등 세로줄의 벡터인데요,
z[1](m) 까지 말이죠. 맞죠. 여기 이 첫번째 것과 동일한데 모두 m까지를 나타낸 것인데요. 그러면 이 것은 매트릭스 z1이 됩니다. 비슷하게 a에도 적용할 수 있습니다. a[1](1), a[1](2) 등등 이렇게 말이죠. a[1](m) 까지해서 줄별로 쌓아진 것을 나타낼 수 있습니다. 그러면 저희가 소문자 x에서 대문자 X로 간 것과 같이, 소문자 z에서 대문자 Z가 된 것과 같이 말이죠, 이것은 소문자 a벡터에서, 대문자 A1이 됩니다. 여기 A1과 같이 말이죠.
또 비슷하게, z[2] 와 a[2]는 여기 이 벡터들을 쌓아서 얻어지는데요, 가로로 쌓아서 말이죠. 그리고 여기 이런 벡터들을 가로로 쌓아서 말이죠. Z[2] 와 E[2]를 얻기까지 말이죠. 여러분이 생각하는데 도움을 줄만한 표기에 대한 내용은,
여기 Z와 A와 같은 매트릭스는 트레이닝 샘플들을 가로로 인덱싱할 것인데요, 그렇기 때문에 가로 인덱스가 다른 트레이닝 샘플을
나타내는 것입니다. 만약 왼쪽에서 오른쪽을 가는 경우,
트레이닝 세트를 스케닝하는 것과 마찬가지입니다. 세로로는 여기 세로 인덱스가
여러분의 신경망에 있는 여러개의 노드를 나타냅니다. 예를 들어, 여기 이 노드는,
가장 위에 있는데요, 가장 위 코너에 있는 매트릭스 값은 첫번때 샘플에서 숨겨진 유닛의 activation을
나타냅니다. 그 다음 값은 두번째 첫번째 샘플에서
두번째 숨겨진 유닛의 activation을 나타냅니다. 그리고 첫번째 샘플의 세번째 숨겨진 유닛
도 이렇게 나타나겠죠. 이렇게 스캔을 해보면, 이것이
숨겨진 유닛에 해당하는 인덱스 숫자이구요 반대로 가로로 움직이는 경우,
첫번째 트레이닝 샘플의
첫번째 숨겨진 유닛에서, 두번째 트레이닝 샘플의 첫번째
숨겨진 유닛으로 이렇게 세번째 트레이닝 샘플 등등 말이죠. 여기 이 노드가
마지막 트레이닝 샘플의 첫번째 숨겨진 유닛의 activation까지 나타날때까지 n번째 트레이닝 샘플로
이어집니다. 가로로는 A매트릭스가 n개의
트레이닝 샘플로 이어지는데요, 세로로는 A 메트릭스의 인덱스 가 숨겨진 유닛의 숫자를 나타냅니다. z 매트릭스도 동일한 직관이 적용됩니다. x 와 마찬가지고 가로에서는
n개의 트레이닝 샘플을 나타내듯이 말이죠. 세로로는 입력 특성의 숫자가
나타나구요. 이 숫자는 신경망의 입력 층과는 다릅니다. 이제 여기서 배운 공식을 통해서 벡터화를 통해 복수의 샘플이 있는 경우
벡터화 시키는 방법을 배웠는데요, 다음 비디오에서는 어떻게 이런 공식이 정의되는 것인지 조금 더 자세히
벡터화에 대한 정확한 도입에 대한 정의를
다루겠습니다. 공식의 타당성은 지금 본 내용과 그리 크게 다르진 않을 것입니다. 다음 비디오로 넘어가겠습니다.