1
00:00:00,000 --> 00:00:01,350
Dans la précédente vidéo,

2
00:00:01,350 --> 00:00:05,065
vous avez vu les bases de la construction 
 d'un réseau de neurones.

3
00:00:05,065 --> 00:00:07,696
Chaque couche est composée d'une étape 
 de propagation directe, vers l'avant

4
00:00:07,696 --> 00:00:09,975
et d'une étape de rétropropagation,
 vers l'arrière.

5
00:00:09,975 --> 00:00:12,370
Nous allons voir comment 
implémenter ces étapes.

6
00:00:12,370 --> 00:00:14,230
Commençons avec la propagation directe.

7
00:00:14,230 --> 00:00:20,045
Rappelons que nous allons avoir 
a[l-1] en entrée et a[l] en sortie

8
00:00:20,045 --> 00:00:21,795
et nous mettrons z[l] en cache.

9
00:00:21,795 --> 00:00:24,615
Et nous venons de dire que
 du point de vue de l'implémentation,

10
00:00:24,615 --> 00:00:28,145
nous mettrons en cache
 W[l] et b[l] également,

11
00:00:28,145 --> 00:00:31,585
juste pour faciliter l'appel des fonctions
 dans l'exercice de programmation.

12
00:00:31,585 --> 00:00:35,438
Les équations devraient
 vous rappeler quelque chose.

13
00:00:35,438 --> 00:00:46,813
Pour implémenter la fonction vers l'avant, 
on fait juste ceci égal à W[l] * a[l-1] + b[l],

14
00:00:46,813 --> 00:00:53,620
et puis a[l] = la fonction
 d'activation appliquée à z.

15
00:00:53,620 --> 00:00:57,250
Et si vous voulez 
l'implémentation vectorisée,

16
00:00:57,250 --> 00:01:06,218
alors c'est juste ça fois A[l-1] + b,

17
00:01:06,218 --> 00:01:09,930
b utilisant le broadcasting de Python.

18
00:01:09,930 --> 00:01:15,296
Et A[l] = g appliqué élément par élément à Z.

19
00:01:15,296 --> 00:01:20,128
Et, souvenez vous sur le diagramme,
 pour l'étape directe,

20
00:01:20,128 --> 00:01:22,565
quand nous avions cette chaîne de boîtes,

21
00:01:22,565 --> 00:01:26,775
vous initialisez ça en mettant en entrée A[0],

22
00:01:26,775 --> 00:01:29,305
qui est égal à X.

23
00:01:29,305 --> 00:01:31,440
Vous initialisez ça,

24
00:01:31,440 --> 00:01:33,380
quelle est l'entrée de la première ?
 C'est a[0],

25
00:01:33,380 --> 00:01:40,207
c'est à dire les caractéristiques d'entrées,
 soit pour un exemple d'apprentissage

26
00:01:40,207 --> 00:01:42,255
si vous faites un exemple à la fois,

27
00:01:42,255 --> 00:01:45,800
ou A[0] majuscule, 
le set d'apprentissage entier,

28
00:01:45,800 --> 00:01:48,215
si vous traitez tout 
le set d'apprentissage d'un coup.

29
00:01:48,215 --> 00:01:51,990
Donc, c’est l’entrée de la première fonction 
vers l'avant de la chaîne,

30
00:01:51,990 --> 00:01:53,970
et puis simplement répéter
 cela vous permet de

31
00:01:53,970 --> 00:01:57,085
calculer la propagation directe, 
de gauche à droite.

32
00:01:57,085 --> 00:02:00,110
Ensuite, nous allons parler de l’étape
 de rétropropagation.

33
00:02:00,110 --> 00:02:03,585
Ici, votre but est d’entrer da[l],

34
00:02:03,585 --> 00:02:08,040
et d'avoir en sortie da[l-1], dW[l] et db,

35
00:02:08,040 --> 00:02:16,240
Laissez moi écrire
 les différentes étape des calculs.

36
00:02:16,240 --> 00:02:23,866
dz[l] est égal au produit élément
 par élément de da[l] fois g[l]' de z[l]

37
00:02:23,866 --> 00:02:27,405
et ensuite on calcule les dérivées

38
00:02:27,405 --> 00:02:34,420
dW[l] = dz[l] fois a[l - 1]

39
00:02:34,420 --> 00:02:37,445
Je ne mets pas explicitement ceci
 en cache, mais en fait

40
00:02:37,445 --> 00:02:39,114
on en a besoin aussi.

41
00:02:39,114 --> 00:02:47,560
Et puis, db [l] = dz [l] et enfin,

42
00:02:47,560 --> 00:02:59,089
da [l-1] = W[l] _transpose fois dz [l], OK ?

43
00:02:59,089 --> 00:03:02,310
Et, je ne veux pas passer par 
les calculs détaillés des dérivées,

44
00:03:02,310 --> 00:03:06,605
mais si vous prenez la définition de da 
et que vous la remplacez là-dedans,

45
00:03:06,605 --> 00:03:10,260
vous obtenez la même formule que 
nous avons eue la semaine précédente,

46
00:03:10,260 --> 00:03:16,617
pour le calcul de dz[l] 
en fonction du précédent dz[l].

47
00:03:16,617 --> 00:03:18,135
Si je remplace juste ça ici,

48
00:03:18,135 --> 00:03:33,817
vous vous retrouvez avec dz [l] = 
W[l+1]_transpose * (dz[l+1] * g[l]'(z [l]) ).

49
00:03:33,817 --> 00:03:36,165
Et je sais qu'il semble y avoir 
beaucoup d'algèbre,

50
00:03:36,165 --> 00:03:38,360
vous pouvez vérifier vous même que

51
00:03:38,360 --> 00:03:40,820
c'est bien l'équation que nous avions écrite

52
00:03:40,820 --> 00:03:43,055
pour la rétropropagation
 la semaine dernière, quand nous

53
00:03:43,055 --> 00:03:45,930
faisions un réseau de neurones
 avec une seule couche cachée.

54
00:03:45,930 --> 00:03:48,602
Rappelez vous bien, ce * est 
un produit élément par élément,

55
00:03:48,602 --> 00:03:54,950
et donc vous n'avez besoin que de
 ces quatre équations pour implémenter 
votre fonction vers l'arrière.

56
00:03:54,950 --> 00:03:58,735
Et puis finalement, je vais écrire 
 la version vectorisée.

57
00:03:58,735 --> 00:04:04,135
La première ligne devient

58
00:04:04,135 --> 00:04:11,045
dZ[l] = dA[l] * g [l]' (Z[l])

59
00:04:11,045 --> 00:04:13,060
Rien de surprenant.

60
00:04:13,060 --> 00:04:23,715
dW [l] devient 1/m dZ[l] fois
 transposée de A[l-1], puis,

61
00:04:23,715 --> 00:04:30,844
db [l] devient 1/m np.sum (dZ [l], 

62
00:04:30,844 --> 00:04:37,970
axis = 1, keepdims = True)

63
00:04:37,970 --> 00:04:44,095
Nous avons parlé de l’utilisation de np.sum 
dans la semaine précédente pour calculer db.

64
00:04:44,095 --> 00:04:56,155
Et puis finalement, 
dA [l-1] = W [l] _transpose fois dZ [l].

65
00:04:56,155 --> 00:05:02,435
Cela nous permet d'avoir 
en entrée cette quantité da

66
00:05:02,435 --> 00:05:07,950
et en sortie dW[l], db[l],

67
00:05:07,950 --> 00:05:10,100
les dérivées dont vous avez besoin,

68
00:05:10,100 --> 00:05:16,022
ainsi que da[l-1], comme ceci.

69
00:05:16,022 --> 00:05:18,905
Voilà donc comment vous implémentez
 la fonction vers l’arrière.

70
00:05:18,905 --> 00:05:20,560
Bien, pour résumer,

71
00:05:20,560 --> 00:05:23,585
on prend l’entrée X,

72
00:05:23,585 --> 00:05:25,060
vous avez la première couche,

73
00:05:25,060 --> 00:05:28,445
qui utilise peut-être 
une fonction d’activation ReLU,

74
00:05:28,445 --> 00:05:30,570
vous allez à la deuxième couche,

75
00:05:30,570 --> 00:05:33,340
qui a peut-être autre
 fonction d’activation ReLU.

76
00:05:33,340 --> 00:05:35,175
On passe à la troisième couche,
 qui a peut-être 

77
00:05:35,175 --> 00:05:39,595
une fonction d’activation sigmoïde,
 si vous faites de la classification binaire,

78
00:05:39,595 --> 00:05:41,875
et cela renvoie ŷ.

79
00:05:41,875 --> 00:05:43,890
Et puis, avec ŷ,

80
00:05:43,890 --> 00:05:46,265
vous pouvez calculer la perte,

81
00:05:46,265 --> 00:05:49,685
et cela vous permet de démarrer
 votre itération vers l'arrière.

82
00:05:49,685 --> 00:05:51,775
Je vais dessiner les flèches d'abord, ok ?

83
00:05:51,775 --> 00:05:54,320
Pour éviter de changer 
de stylos tout le temps.

84
00:05:54,320 --> 00:06:03,430
Ensuite, vous avez la rétropropagation,
 pour calculer les dérivées, pour calculer

85
00:06:03,430 --> 00:06:16,063
dW[3], db[3], dW[2], db[2], dW[1], db[1],

86
00:06:16,063 --> 00:06:18,865
et dans le processus, vous calculez

87
00:06:18,865 --> 00:06:24,820
le cache va transférer z[1], z[2], z[3],

88
00:06:24,820 --> 00:06:32,250
et vous passez vers l'arrière da[2] et da[1].

89
00:06:32,250 --> 00:06:34,730
On pourrait calculer da[0],

90
00:06:34,730 --> 00:06:35,880
mais nous n’utiliserons pas ça.

91
00:06:35,880 --> 00:06:37,935
Alors vous pouvez simplement ignorer ça.

92
00:06:37,935 --> 00:06:40,785
Voici comment implémenter la propagation 
vers l'avant et la rétropropagation

93
00:06:40,785 --> 00:06:44,040
pour un réseau de neurones à trois couches.

94
00:06:44,040 --> 00:06:46,140
Il y a juste un dernier détail,
 que je n'ai pas mentionné,

95
00:06:46,140 --> 00:06:48,735
qui est que pour la récursion vers l'avant,

96
00:06:48,735 --> 00:06:52,420
nous initialisons avec
 les données d’entrée X.

97
00:06:52,420 --> 00:06:54,090
Et pour la récursion vers l'arrière ?

98
00:06:54,090 --> 00:06:59,235
Eh bien, il s’avère que da[l],

99
00:06:59,235 --> 00:07:01,065
lorsque vous utilisez
 la régression logistique,

100
00:07:01,065 --> 00:07:02,977
quand vous faites de
 la classification binaire,

101
00:07:02,977 --> 00:07:09,685
est égal à -y/a + (1-y)/(1-a).

102
00:07:09,685 --> 00:07:12,575
Donc il s’avère que la dérivée
 de la fonction de perte,

103
00:07:12,575 --> 00:07:14,180
par rapport à la sortie,

104
00:07:14,180 --> 00:07:17,475
par rapport à ŷ, 
on peut montrer que c'est ça.

105
00:07:17,475 --> 00:07:19,105
Si vous savez calculer les dérivées,

106
00:07:19,105 --> 00:07:21,328
si vous prenez la fonction de perte L,

107
00:07:21,328 --> 00:07:24,150
et que vous la dérivez par rapport
 à ŷ, par rapport à a,

108
00:07:24,150 --> 00:07:26,505
vous pouvez montrer que 
vous obtenez cette formule.

109
00:07:26,505 --> 00:07:31,350
C’est elle que vous devez utiliser pour da
 pour la dernière couche, L majuscule.

110
00:07:31,350 --> 00:07:35,715
Et bien sûr, pour une
 implémentation vectorisée,

111
00:07:35,715 --> 00:07:38,351
alors vous initialisez 
la récursion vers l'arrière

112
00:07:38,351 --> 00:07:43,706
pas avec cela, mais 
avec dA pour la couche L,

113
00:07:43,706 --> 00:07:48,465
qui va être la même chose
 pour les différents exemples,

114
00:07:48,465 --> 00:07:54,011
-y/a pour le premier 
exemple d'apprentissage,

115
00:07:54,011 --> 00:07:55,285
+(1-y)/(1-a) pour le premier 
exemple d'apprentissage,

116
00:07:55,285 --> 00:07:58,153
+(1-y)/(1-a) pour le premier 
exemple d'apprentissage,

117
00:07:58,153 --> 00:08:05,185
trois petits points, jusqu'au
 m-ième exemple d'apprentissage.

118
00:08:05,185 --> 00:08:09,423
Voilà donc comment vous 
implémentez la version vectorisée.

119
00:08:09,423 --> 00:08:13,055
Voilà comment vous initialisez la
 version vectorisée de la rétropropagation.

120
00:08:13,055 --> 00:08:16,100
Bien, vous avez vu maintenant 
les blocs de construction de base

121
00:08:16,100 --> 00:08:20,030
des propagations vers l'avant
 et vers l'arrière,

122
00:08:20,030 --> 00:08:22,340
Maintenant,
 si vous implémentez ces équations,

123
00:08:22,340 --> 00:08:24,530
vous aurez
 une implémentation correcte de la

124
00:08:24,530 --> 00:08:27,640
propagation vers l'avant et vers l'arrière pour 
avoir les dérivées dont vous avez besoin.

125
00:08:27,640 --> 00:08:29,660
Vous pouvez penser "ça fait 
beaucoup d’équations,

126
00:08:29,660 --> 00:08:32,080
Je suis un peu confus, je ne suis pas sûr
 que je voie comment cela fonctionne."

127
00:08:32,080 --> 00:08:34,645
Et si vous vous sentez comme ça, 
mon conseil est,

128
00:08:34,645 --> 00:08:37,205
Quand vous ferez l'exercice de 
programmation de cette semaine,

129
00:08:37,205 --> 00:08:40,175
vous pourrez implémenter ça vous-même,

130
00:08:40,175 --> 00:08:42,020
et ce sera beaucoup plus concret.

131
00:08:42,020 --> 00:08:43,805
Et je sais qu'il y a beaucoup d’équations,

132
00:08:43,805 --> 00:08:46,265
et peut-être que certaines équations
 vous semblent obscures,

133
00:08:46,265 --> 00:08:49,055
mais si vous faites les dérivées

134
00:08:49,055 --> 00:08:50,905
et l’algèbre linéaire, ce qui n’est pas facile,

135
00:08:50,905 --> 00:08:52,430
n’hésitez pas à essayer,

136
00:08:52,430 --> 00:08:56,390
mais c’est en fait une des dérivations les plus
 difficiles en apprentissage automatique.

137
00:08:56,390 --> 00:08:57,950
Les équations deviennent complexes,

138
00:08:57,950 --> 00:09:02,685
les équations pour calculer les dérivées, 
en particulier pour la rétropropagation.

139
00:09:02,685 --> 00:09:04,750
Mais encore une fois, si cela 
vous semble un peu abstrait,

140
00:09:04,750 --> 00:09:06,400
un peu mystérieux,

141
00:09:06,400 --> 00:09:09,108
je vous conseille d'aller faire 
l'exercice de programmation

142
00:09:09,108 --> 00:09:11,465
qui rendra les choses un peu plus concrètes.

143
00:09:11,465 --> 00:09:14,120
Et je dois aussi dire que, même aujourd'hui,

144
00:09:14,120 --> 00:09:16,805
quand j’implémente un algorithme
 d’apprentissage, parfois,

145
00:09:16,805 --> 00:09:18,000
même moi je suis surpris quand

146
00:09:18,000 --> 00:09:21,170
mon implémentation de l'algorithme 
d'apprentissage fonctionne, parce que

147
00:09:21,170 --> 00:09:25,670
beaucoup de la complexité de l'apprentissage 
automatique vient des données
 plutôt que du code.

148
00:09:25,670 --> 00:09:27,095
Alors parfois, vous pensez que vous

149
00:09:27,095 --> 00:09:28,685
n'avez implémenté que
 quelques lignes de code,

150
00:09:28,685 --> 00:09:30,110
vous n'êtes pas trop sûr de ce que ça a fait,

151
00:09:30,110 --> 00:09:31,625
mais cela fonctionne presque
 comme par magie,

152
00:09:31,625 --> 00:09:35,296
parce que beaucoup de la magie n’est pas
 réellement dans le code que vous écrivez,

153
00:09:35,296 --> 00:09:37,090
qui n’est souvent pas très long.

154
00:09:37,090 --> 00:09:38,705
Il n’est pas vraiment simple,

155
00:09:38,705 --> 00:09:40,730
mais ce n’est pas dix mille,

156
00:09:40,730 --> 00:09:42,115
cent mille lignes de code,

157
00:09:42,115 --> 00:09:44,750
mais vous y entrez tellement 
de données que parfois,

158
00:09:44,750 --> 00:09:46,850
même si j’ai travaillé en apprentissage
 automatique pendant longtemps,

159
00:09:46,850 --> 00:09:49,400
parfois, ça me surprend encore un peu
 quand mon algorithme d’apprentissage

160
00:09:49,400 --> 00:09:53,274
fonctionne parce que beaucoup
 de la complexité de
 votre algorithme d’apprentissage

161
00:09:53,274 --> 00:09:55,970
provient des données plutôt 
que de la nécessité

162
00:09:55,970 --> 00:10:01,020
d'écrire des milliers et
 des milliers de lignes de code.

163
00:10:01,020 --> 00:10:05,935
Très bien. Donc voici comment implémenter 
des réseaux de neurones profonds.

164
00:10:05,935 --> 00:10:10,325
Et encore une fois, ça va devenir
 plus concret quand vous aurez
 fait l’exercice de programmation.

165
00:10:10,325 --> 00:10:14,220
Avant de passer à la suite, 
dans la prochaine vidéo,

166
00:10:14,220 --> 00:10:17,480
Je veux parler des hyper paramètres 
et des paramètres.

167
00:10:17,480 --> 00:10:19,681
Il s’avère que quand vous entraînez
 des réseaux profonds,

168
00:10:19,681 --> 00:10:22,225
savoir bien organiser vos hyper paramètres

169
00:10:22,225 --> 00:10:25,400
vous aidera à être plus efficace 
dans l’élaboration de vos réseaux.

170
00:10:25,400 --> 00:10:29,000
Dans la vidéo suivante, nous allons voir
 ce que cela signifie exactement.