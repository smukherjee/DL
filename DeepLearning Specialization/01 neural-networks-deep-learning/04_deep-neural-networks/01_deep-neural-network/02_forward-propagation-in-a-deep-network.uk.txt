В останньому відео ми описали що таке глибока l-шарова нейронна мережа (НМ). А також поговорили про позначення, які використовуються для опису таких НМ. В цьому відео ти побачиш як застосовувати пряме поширення в глибоких НМ. Як завжди, давай спочатку розглянемо як виглядає пряме поширення для окремого тренувального зразка x, а пізніше поговоримо про векторизовану версію, де будемо застосовувати пряме поширення для всього тренувального набору зараз. Маючи один тренувальний зразок x, ось як можна обчислити активатори 1-ого шару. Тож для 1-ого шару ми обчислюємо z[1]=w[1]x+b[1]. Тож w[1] і b[1] - це параметри, які впливають на активатори 1-ого шару. Ось 1-ий шар НМ. А далі ми обчислюємо активатори для цього шару, що =g(z[1]). Якою буде функція g залежить від того, в якому шарі ми знаходимось. Тож індекс тут теж повинен позначати функцію активації 1-ого шару. Зробивши це, ми обчислимо активатори 1-ого шару. Як щодо 2-ого шару? Оцього шару. Тут ми обчислимо z[2]=w[2]a[1]+b[2]. Тож активатором 2-ого шару буде вихід 1-ого шару, тобто оця величина плюс вектор зсуву для 2-ого шару. Далі a[2] =функції активації, застосованій до z[2]. Добре. Тож це був 2-ий шар. І т.д. аж до вихідного шару, а це - 4-ий шар, де ми матимемо z[4]= параметри шару помножені на активатори попереднього шару плюс вектор зсуву і, аналогічно, a[4]=g[4](z[4]). Ось як ми обчислюємо наше передбачення ŷ. Варто відмітити, що x тут також =a[0], тому що вектор вхідних ознак x є також активаторами 0-ого шару. Тож ми можемо закреслити x і написати замість нього a[0]. Тепер, як бачимо, всі ці рівняння дуже схожі. Правильно? Тож можемо написати загальне рівняння: z[l]= w[l]a[l-1]+b[l]; і тепер активатори для цього шару будуть функцією активації від значень z. Тож це - загальні рівняння прямого поширення. Отже, ми зробили все це для окремого тренувального зразка. Як щодо векторизованої версії для всього тренувального набору одночасно? Рівняння виглядатимуть дуже схоже. Для 1-ого шару матимемо велика Z[1]=w[1] помножене на велике X плюс b[1]. І A[1]=g[1](Z[1]). Правильно? Май на увазі, що X=A[0]. Тобто це просто тренувальні зразки складені в стовпці. І ми можемо просто взяти і закреслити X і написати замість нього A[0]. І далі для наступного шару матимемо щось схоже. Z[2]=w[2]A[1]+B[2] і A[2]=g[2](Z[2]). Ми просто беремо оці вектори z та a і складаємо їх по горизонталі. Тож ось вектор z для 1-ого тренувального зразка, вектор z для 2-ого тренувального зразка і т.д. до m-ого тренувального зразка. Тож складаємо їх в стовпці і називаємо велике Z. Аналогічно для великої A, як і для великої X, всі тренувальні зразки є векторами-стовпцями, складеними зліва-направо. І потім, знову ж, ми обчислюватимемо ŷ, що =g[4](z[4]), що в свою чергу, =A[4] і це будуть передбачення для всіх тренувальних зразків складених по горизонталі. Щоб узагальнити наші позначення, я зміню оці позначення вгорі. Тепер ми можемо замінити малі z та a їхніми великими візаві. (Хоча Z вже виглядає як велика). І, таким чином, ми отримуємо векторизовану версію прямого поширення, що обробляє одразу весь тренувальний набір, бо ж A[0] - це X. Тепер. Якщо ми глянемо на цю реалізацію векторизації, то нам здасться, що тут мав би бути цикл for. Правильно? Тут мало б бути щось типу "for l=1..4". Для l від 1 до великої L. Тож ми б мали обчислити активатори для 1-ого шару, потім для 2-ого, потім для 3-ого і потім для 4-ого шару. Тож тут, схоже, має бути цикл for. Я знаю, що коли ми реалізуємо нашу НМ, ми, зазвичай, хочемо позбутись явних циклів for. Але це - єдине місце, де я не думаю, що є якийсь спосіб реалізувати це без явного циклу for. Тож, коли ми реалізуємо пряме поширення, це абсолютно нормально мати цикл for для обчислення активаторів 1-ого шару, потім 2-ого, потім 3-ого і потім 4-ого шару. Ніхто не знає і я не думаю, що є хоч якийсь спосіб зробити це без циклу for, який йтиме від 1 до великої L, тобто від 1 до загальної кількості шарів в нашій НМ. Тож в цьому місці абсолютно нормально мати явний цикл for. Тож це все щодо позначень глибоких НМ і щодо того, як реалізувати пряме поширення в цих НМ. Якщо рівняння, які ми щойно бачили, здаються дещо знайомими, то це тому, що ми брали рівняння дуже схожі на ті, що ми бачили в НМ з одним прихованим шаром і просто повторювали їх більше разів. Тож, як виявляється, ми реалізували глибоку НМ. Одним зі способів збільшити шанси здійснити реалізацію без помилок - це обдумати (дуже системно і обережно) розміри матриць, з якими ми працюємо. Коли я намагаюсь виправити помилки в своєму коді, я часто беру аркуш паперу і просто ретельно записую розміри матриць, з якими працюю. Давай подивимось як ми можемо це робити в наступному відео.