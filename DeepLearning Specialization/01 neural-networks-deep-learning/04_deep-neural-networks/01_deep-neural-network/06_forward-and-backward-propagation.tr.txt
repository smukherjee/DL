Bir önceki videoda derin sinir ağını uygulamanın 
basit yapılarını gördünüz. İleri yayılım her katmanda
 bir basamaktır ve buna karşılık bir geri yayılım 
basamağı vardır Hadi bu basamakları 
nasıl uygulayacağız görelim İleri yayılım ile başlıyoruz. Hatırlayın ki bunun girdi 
a[L-1] ve çıktı a[l] ile ne ilişkisi var ve ön bellekteki z[L] ile Ve kodlamadaki bakış açısını az önce açıkladık belki ön bellekde w[L] ve
 b[L] de birlikte tutulabilir, bu, problem çözümü sırasında 
fonksiyonları hızlıca çağırmaya yarar. Ve dahası bu iş için kullanılan
 denklemler 
 tanıdık görünmelidir İleri yayılımı kodlamanın bir yolu budur 
 w[L] ile aL] çarpımına 
b[L] eklenmesine eşittir, ve sonra a[L] aktivasyon 
fonksiyonunun 
z'ye uygulanmasına eşittir. Eğer bu notasyonu açıklamasını vektörize
 yapmak isterseniz A[L-1] ile W[L] çarpımına b[L] eklenmesidir. B'yi eklemek hiper yayınlamaktır ve A[L], G'nin z'deki her eleman üzerine dağılımıdır. Hatırladığınız üzere, ileri basamak için olan diyagramda ileri giden kutu zincirleri vardı dolayısıyla bunu başlatıp, a[o]'ı besliyorsunuz ve bu X'e eşit oluyor. Yani bunu başlatınca İlk baştakinin girdisi nedir? Bu gerçekten a[0]'dır, tek eğitim örneğinde bu girdi özellikleridir. Bir anda tek örnek yapıyorsan veya A[0], tüm eğitim seti, eğer tüm eğitim setini işliyorsan, Bu zincirdeki ilk dört fonksiyonun girdisidir ve bunu tekrar etmek size ileri yayılımı soldan sağa hesaplama kolaylığı sağlar. Sonraki adımda, geri yayılım basamağı hakkında konuşalım, Hedefiniz girdi olan da[L], ve çıktı da[L-1] ve dW[L] ve db. İzin verin tüm bunları hesaplamada ihtiyaç duyacağınız 
 tüm basamakları yazayım, 
dz[L] = da[L] g[L] ile z[L]'nin elemanlarının çarpımı ve türevleri hesaplamak için dW[l] = dz[l] * a[l - 1]. Bunu açıkça ön belleğe atmadım ama sonuçta buna da ihtiyacınız var. Ve sonra db[l] = dz[l] sonuçta da[l-1] = w[l]_transpoz * dz[l], tamam mı? Ve bunun nasıl türediği ile ilgili detaylı 
açıklama yapmayacağım, ancak bu tanımı alıp burada uygularsanız sonrasında önceki hafta elde ettiğimiz 
aynı formülü elde edersiniz. dz[L]'yi önceki dz[L]'nin bir fonksiyonu olarak 
nasıl hesaplayacağımıza gelirsek, aslında Burada uygularsam, sonuçta dz[l] = w[l+1]_transpoz dz[l+1] * g[l]` z[l]. çıkar. Biliyorum bu çok fazla cebir gibi görünüyor, bu konude kendiniz için sağlama yapabilirsiniz Bu bizim yazdığımız denklemdir 
. geçen hafta geri yayılım için 
 ne zaman biz‑ tek saklı katmanlı bir yapay sinir ağı yaparken hatırlatmam gerekir ki şu an bu elemanların çarpımıdır ve geri yayılımı hesaplamak için bu dört denkleme ihtiyaç vardır. Son olarak bunların vektör halini de yazacağım, Birinci satır dz[l] = dA[l] olacak, g[l]` of z[l].'nin elemanlarının çarpımı. Belki burada bir sürpriz yok, dW[l] 1/m, dz[l] * a[l-1]_transpoz olur ve sonra db[l]  1/m np.sum dz[l],(numpy ile toplama) olur. sonra eksen = 1 olur, boyutlar sabit, 
keepdims = true Geçtiğimiz hafta np.sum üzerinde konuştuk,
 şimdi bunu db'yi hesaplamakta kullanacağız. Ve sonra nihayet, dA [l-1] W [l] _transpoz * dz [l]. Sonuçta bu durum girdi olarak bu değeri 
vermenize olanak sağlar, tam burada ve çıktı da[L-1] ve dW[L] ve db. ihtiyaç duyulan türevler dA yanı sıra [l-1], değil mi? Aşağıdaki gibi. bu geri fonksiyonu nasıl hesapladığımızı gösterir. Yani özetlemek gerekirse, X girdisini alıp ilk katman olabilir belki bu katmanın ReLu aktivasyon fonksiyonu olabilir Ve sonra ikinci katmanda belki diğer ReLU aktivasyon fonksiyonu var üçüncü çüncü katmana gider, ve bunun çıktısı y_hatdb[l] becomes 1/m np.sum dz[l] yaklaşımı. ve bu y_hat verir. Ve bu y_hat'i kullanarak, kayıp fonksiyonunu hesaplayabilirsiniz. bundan sonra geri iterasyona başlamak mümkün olur. Önce okları çizeceğim, tamam mı? Çok fazla kalem değiştirmeme gerek kalmayacak. Elimizde geri yayılımı hesaplamak için türevleri hesaplamalıyız, dW[3], db[3], dW[2], db[2], dW[1], db[1], ve bu süreçte hesaplıyor olacağız Sanırım, önbellekte z [1], [2] z, z [3] trasferi olacak, ve burada duraklayalım, geriye doğru da [2] da [1]. Bu da [0], hesaplar ama bunu kullanmayacağız. Sonuçta bunu yok sayabilirsiniz, tamam mı? Artık ileri ve geri yayılım nasıl uygulanır, üç katmanlı yapay sinir ağı için biliyorsunuz. Şimdi size daha önce söylemediğim
 son bir detaydan bahsedeyim bu ileri öz yinelemedir, girdi olan veri X ile başlatacağız. Geri öz-yineleme nasıl olacak? Şey, Görünüşe göre o da [l], lojistik regresyon yaparken, ikili sınıflama yaparken y eşittir / + 1-y/1-a. Bu yüzden, kayıp fonksiyonunun türevi çıktıya göre y_hat'e göre, bu şekilde gösterilir
 ve bu da böyledir. eğer calculus bilgisine aşinaysanız L kayıp fonksiyonuna bir bakın, ve sonra a ve y_hat'e göre türevlerini alın, Bu formülü nasıl elde ettiğinizi gösterebilirsiniz. Son katman için büyük harf L için da'nin formülü bu şekildedir. Ve tabi ki elinizde vektör halindeki uygulama olmuş olsaydı, sonra geri öz yinelemeyi başlatmak için bu değil dA (büyük A ile) L. katman için bu farklı örnekteki aynı şey olacaktır. ilk eğitim örneği üzerinde, 1-y, ilk eğitim örneği için, 1-a üzerinde, ilk eğitim örneği için 
(bu nasıl bir tekrar aşkı!) en sondaki örneğe kadar, sonuçta 1-a[m] Vektör halini de nasıl uygulayacağınızı anlattık. Vektörize geri yayılımın nasıl başlatılacağını gösterdik. Şimdiye kadar basit temel taşlarını gördünüz, hem ileri hem de geri yayılım algoritmalarının. Şimdi bu denklemleri kodlarsanız/uygularsanız doğru kodlamayı elde edersiniz. ileri yayılım ve geri yayılım için türevler tüm ihtiyaç duyduğunuz şeylerdir. Çok fazla denklem olduğunu düşünebilirsiniz, ki haklısınız Kafam karışık, bunlar nasıl işliyor anlayamıyorum diyebilirsiniz. Eğer böyle hissediyorsanız benim önerim, bu haftanın kodlama ödevine başladığınızda kendiniz bu denklemleri kendiniz kodlayabileceksiniz ve bu çok daha somut olacak. Ve biliyorum çok fazla denklem var, ve belki de bazı denklemler tam mantıklı gelmeyecek, ama calculus üzerinde çalışırsanız ve lineer cebir çalışırsanız, ki bunlar kolay olmayacaktır en azından denemeye özen gösterin Ama aslında makine öğrenmesindeki en zor türev işlemlerini içerir. Denklemler çığ gibi büyüyor, veya türev almak için kalkulus denklemleri özellikle geri yayılım üzerinde Ama yine de, bu size soyut gelebilir, biraz gizemliymiş gibi olabilir, benim önerim, ilk ödevi yaptığınızda biraz daha somut gelecektir. Buna rağmen, bugün bile söylemeliyim ki ben bir öğrenme algoritmasını kodlarken, bazen ben bile şaşırabiliyorum kodlamalarım çalıştığı zaman, çünkü çok fazla makine öğrenmesi karmaşıklığı kodların satırlarından ziyade,
 verinin yapısından gelir, Bazen siz de çok az kod yazıyor gibi hissedebilirsiniz ne yaptığınızdan emin olamayabilirsiniz, ama bu neredeyse sihirli bir şekilde çalışır. tüm bu sihir yüzünden çalışır, yazdığınız kod parçası değil ki bu çoğunlukla çok uzun değildir. Çok da basit değildir ama on bin yüz bin satırlarca kod değildir. Ama bazen çok fazla veriyle beslersen, ben bile makine öğrenmesiyle ilgili
 çok uzun zamandır çalışmama rağmen Bazen, hala beni şaşırtıyor biraz benim öğrenme algoritmamın 
bu kadar karmaşıklıkla
 başa çıkarak çalışabilmesi ki bu karmaşa veriden gelir, mutlaka binlerce binlerce kod satırı yazmaktan ziyade Anlaşıldı. Derin sinir ağlarını nasıl
 uygulanacağını gördünüz. Ve yine ödevdeki ilk örneği yaparken 
herşey daha da somutlaşacak anlayacaksınız. İlerlemeden önce, önümüzdeki videoda, Hiper parametre ve parametre 
kavramlarını konuşmak istiyorum. Derin ağları eğitirken sonuçta, hiper parametreleri iyice düzenleyebilmek ağı geliştirirken daha etkili olabilmenizi sağlar. Sonraki videoda tam olarak bu ne demektir konuşacağız.