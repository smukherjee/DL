1
00:00:00,920 --> 00:00:02,860
欢迎回来

2
00:00:02,860 --> 00:00:08,860
本周我们将会重温神经网络编程的基础知识

3
00:00:08,860 --> 00:00:11,990
我们发现当你实现一个神经网络的时候

4
00:00:11,990 --> 00:00:16,260
一些技巧是非常重要的

5
00:00:16,260 --> 00:00:21,150
比如如果你有一个含有m个训练样本的训练集

6
00:00:21,150 --> 00:00:25,110
你可能习惯于用一个for循环

7
00:00:25,110 --> 00:00:28,240
来处理这m个训练样本

8
00:00:28,240 --> 00:00:31,260
但当你实现一个神经网络的时候

9
00:00:31,260 --> 00:00:34,540
你一般想训练整个训练集

10
00:00:34,540 --> 00:00:39,040
但不显式使用for循环来遍历整个训练集

11
00:00:39,040 --> 00:00:42,940
那么，你将在本周的资料中看到如何实现这一点

12
00:00:42,940 --> 00:00:47,700
另外，当你在你的网络中组织计算的时候

13
00:00:47,700 --> 00:00:51,670
经常使用所谓的前向传播

14
00:00:51,670 --> 00:00:56,100
以及所谓的反向传播

15
00:00:56,100 --> 00:01:00,010
所以在本周的资料中
你还将了解到为什么在训练神经网络时

16
00:01:00,010 --> 00:01:04,830
计算可以被正向转播组织为一次前向传播过程

17
00:01:04,830 --> 00:01:08,010
以及一次反向传播过程

18
00:01:09,100 --> 00:01:12,620
在本周的材料中
我想使用逻辑回归算法作为例子

19
00:01:12,620 --> 00:01:16,170
以使其更容易理解

20
00:01:16,170 --> 00:01:19,970
即使你之前已经了解过逻辑回归算法
我想在这周的材料中

21
00:01:19,970 --> 00:01:23,845
你仍然会收获一些新的有趣的想法

22
00:01:23,845 --> 00:01:25,815
所以让我们开始吧

23
00:01:25,815 --> 00:01:30,605
逻辑回归是一个二元分类算法

24
00:01:30,605 --> 00:01:33,145
所以让我们从这样一个问题开始

25
00:01:33,145 --> 00:01:36,925
这里有一个二元分类问题的例子

26
00:01:36,925 --> 00:01:41,545
你也许有一张输入图片 像这样

27
00:01:41,545 --> 00:01:47,260
你希望算法输出一个0或1标签
指明图上是不是猫

28
00:01:47,260 --> 00:01:52,140
输出1说明是猫，输出0说明不是猫

29
00:01:52,140 --> 00:01:57,740
在这里，我们用符号y来表示输出标签

30
00:01:57,740 --> 00:02:01,550
让我们先来看一看图像在计算机里是怎么表示的

31
00:02:01,550 --> 00:02:05,680
为了在计算机里保存一幅（彩色）图像
计算机要存储3个独立的矩阵

32
00:02:05,680 --> 00:02:09,890
分别对应着图像的红、绿、蓝3个颜色通道

33
00:02:10,990 --> 00:02:15,900
所以，如果你的图像大小是64x64

34
00:02:15,900 --> 00:02:21,700
那么你会有3个64x64大小的实数矩阵

35
00:02:21,700 --> 00:02:27,230
分别代表图像的红、绿、蓝3个颜色通道

36
00:02:27,230 --> 00:02:31,290
尽管我在幻灯片上画的矩阵比实际小的多

37
00:02:31,290 --> 00:02:35,320
大小只是5x4，而不是64x64

38
00:02:35,320 --> 00:02:41,640
所以，为了用向量表示这些像素矩阵

39
00:02:41,640 --> 00:02:48,000
我们将把这些矩阵中的像素值展开为一个向量x
作为算法的输入

40
00:02:48,000 --> 00:02:53,782
为了将这些像素值展开成一个向量

41
00:02:53,782 --> 00:02:59,580
我们所做的就是按下面方法
定义一个与该图像相对应的向量

42
00:02:59,580 --> 00:03:03,960
我们直接将255、231.....等这些像素值取出
放入向量x中

43
00:03:03,960 --> 00:03:10,827
直到红色通道的像素值全部取出放入到向量x中

44
00:03:10,827 --> 00:03:15,737
接着是255、134、255...等代表绿色通道的像素值
最后是红色蓝色通道的像素值

45
00:03:15,737 --> 00:03:20,952
最终，我们会得到一个非常长的向量

46
00:03:20,952 --> 00:03:25,570
向量中列出了图像中红、绿、蓝三个通道的像素值

47
00:03:25,570 --> 00:03:31,043
如果图像的大小是64x64

48
00:03:31,043 --> 00:03:36,401
那么该向量的维度将会是64x64x3

49
00:03:36,401 --> 00:03:41,320
因为这就是图像的矩阵包含的元素总数

50
00:03:41,320 --> 00:03:44,097
在这个例子中，向量的总长度为12288

51
00:03:44,097 --> 00:03:47,330
也就是64x64x3的乘积

52
00:03:47,330 --> 00:03:51,870
并且，我们将用n(x) = 12288

53
00:03:51,870 --> 00:03:55,080
来表示输入特征向量x的维度

54
00:03:55,080 --> 00:03:59,280
并且，有时候为了简化，我也会直接用小写n

55
00:03:59,280 --> 00:04:02,720
来表示输入特征向量的维度

56
00:04:02,720 --> 00:04:07,510
所以，在二元分类问题中
我们的目标是学习到这样的一个分类器：

57
00:04:07,510 --> 00:04:10,760
我们输入一幅以特征向量x表示的图像

58
00:04:10,760 --> 00:04:15,460
然后预测对应的输出y是1还是0

59
00:04:15,460 --> 00:04:19,000
即，这幅图上是猫还是不是猫

60
00:04:19,000 --> 00:04:21,560
先让我们来罗列出一些符号

61
00:04:21,560 --> 00:04:23,820
这些符号将会在余下课程中陆续用到

62
00:04:23,820 --> 00:04:29,453
单个样本由一对（x,y）表示

63
00:04:29,453 --> 00:04:34,446
其中，x是一个n(x)维的特征向量

64
00:04:34,446 --> 00:04:39,320
y是标签，取值为0或1

65
00:04:39,320 --> 00:04:44,550
训练集包含m个训练样本

66
00:04:44,550 --> 00:04:50,320
所以训练集将被写成(x(1),y(1))

67
00:04:50,320 --> 00:04:55,370
表示第一个样本的输入和输出

68
00:04:55,370 --> 00:05:01,980
(x(2),y(2))代表第二个样本的输入和输出

69
00:05:01,980 --> 00:05:05,650
直到(x(m),y(m))，表示最后一个样本输入和输出

70
00:05:05,650 --> 00:05:10,170
因此，我将使用小写m代表训练集的样本总数

71
00:05:10,170 --> 00:05:14,418
并且，有时为了强调是表示训练集的样本总数

72
00:05:14,418 --> 00:05:16,437
我会写成M=M(train)

73
00:05:16,437 --> 00:05:18,692
当是测试集时

74
00:05:18,692 --> 00:05:24,430
我有时会用m(test)来表示测试集的样本总数

75
00:05:24,430 --> 00:05:27,430
因此，m(test)等于测试集的样本总数

76
00:05:27,430 --> 00:05:33,440
最后，为了将所有的训练样本写成更加紧凑的形式

77
00:05:33,440 --> 00:05:36,840
我们将定义一个矩阵，用大写X表示

78
00:05:36,840 --> 00:05:41,592
X定义如下：将训练集中的输入x(1)、x(2)....x(m)取出

79
00:05:41,592 --> 00:05:44,568
按列排列到矩阵X中

80
00:05:44,568 --> 00:05:49,958
因此，我们将x(1)放在矩阵的第一列中

81
00:05:49,958 --> 00:05:54,798
x(2)放在矩阵的第二列中，直到x(m)

82
00:05:54,798 --> 00:05:58,000
这样，便得到了矩阵X

83
00:05:58,000 --> 00:06:03,005
因此，矩阵M将有m列，m即训练样本的总数

84
00:06:03,005 --> 00:06:08,665
同时矩阵X有n(x)行

85
00:06:08,665 --> 00:06:14,400
请注意，在一些其他课程中，你可能会看到

86
00:06:14,400 --> 00:06:19,390
矩阵X的定义是将训练样本的输入按行排列的
像这样

87
00:06:19,390 --> 00:06:23,940
x1的转置到x(m)的转置

88
00:06:23,940 --> 00:06:27,704
事实证明，在神经网络中

89
00:06:27,704 --> 00:06:32,218
使用左边的表示方法，运用时会相对简单

90
00:06:32,218 --> 00:06:37,171
概括一下，X是一个n(x)乘m维的矩阵

91
00:06:37,171 --> 00:06:40,404
在Python中

92
00:06:40,404 --> 00:06:45,362
你会看到X.shape命令
这是Python里

93
00:06:45,362 --> 00:06:50,325
得到矩阵形状的命令的输出是n(x)，m

94
00:06:50,325 --> 00:06:53,255
这表示X是一个n(x)乘m维的矩阵

95
00:06:53,255 --> 00:06:58,785
以上就是你如何组织训练样本的输入x到矩阵X

96
00:06:58,785 --> 00:07:01,315
那么如何组织输出标签Y呢

97
00:07:01,315 --> 00:07:04,815
事实证明要使你应用神经网络时更简单

98
00:07:04,815 --> 00:07:10,030
你同样应该讲y按列排列

99
00:07:10,030 --> 00:07:14,650
因此，Y等于[ y(1) y(2) ... y(m) ]

100
00:07:14,650 --> 00:07:18,580
直到y(m)

101
00:07:18,580 --> 00:07:24,980
Y是一个1xm大小的矩阵

102
00:07:24,980 --> 00:07:30,530
同样的，使用Python命令Y.shape，将会输出(1，m)

103
00:07:30,530 --> 00:07:34,810
表示Y是一个1xm大小的矩阵

104
00:07:34,810 --> 00:07:39,660
在本课程之后，当你实现你自己的神经网络时

105
00:07:39,660 --> 00:07:43,630
你会发现一个非常有用的惯例：混合不同的训练样本数据

106
00:07:43,630 --> 00:07:48,580
在这里数据是指单个x或y，或者之后其他的数目数据

107
00:07:48,580 --> 00:07:49,900
混合不同训练样本的数据

108
00:07:49,900 --> 00:07:52,990
并将它们按列排列

109
00:07:52,990 --> 00:07:57,430
像我们之前对x和y做的那样

110
00:07:58,450 --> 00:08:01,380
以上就是我们之后会在回归算法

111
00:08:01,380 --> 00:08:04,060
和神经网络算中使用到的一些符号

112
00:08:04,060 --> 00:08:07,430
如果你忘记了某个符号的意义
比如M代表什么

113
00:08:07,430 --> 00:08:08,300
N代表什么

114
00:08:08,300 --> 00:08:12,630
或者其他什么的
我们已经在课程网站贴出了符号指南

115
00:08:12,630 --> 00:08:17,430
你可以用它快速的查找特定符号的意义

116
00:08:17,430 --> 00:08:20,890
就这样，让我们开始进入下一节课

117
00:08:20,890 --> 00:08:23,190
着手实现逻辑回归算法
GTC字幕组翻译