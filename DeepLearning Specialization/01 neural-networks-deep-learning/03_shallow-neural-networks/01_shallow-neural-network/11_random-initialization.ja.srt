1
00:00:00,000 --> 00:00:01,619
ニューラルネットワークを訓練するとき

2
00:00:01,619 --> 00:00:03,955
重みをランダムに初期化することは
大切です

3
00:00:03,955 --> 00:00:08,426
ロジスティック回帰では
重みを０に初期化して大丈夫でしたが

4
00:00:08,426 --> 00:00:12,258
ニューラルネットワークで
重みをすべて０にして

5
00:00:12,258 --> 00:00:14,706
最急降下法を適用しても
うまくいきません

6
00:00:14,706 --> 00:00:15,289
そのわけを見ていきましょう

7
00:00:15,289 --> 00:00:20,173
２つの特徴量の入力があります

8
00:00:20,173 --> 00:00:25,953
なのでn[0]=2で
二つの隠れ層があるのでn[1]=2ですね

9
00:00:25,953 --> 00:00:31,547
すると 隠れ層のための行列W[1]は

10
00:00:31,547 --> 00:00:35,373
2×2になります

11
00:00:35,373 --> 00:00:41,230
試しに全て０に初期化してみましょう
[0 0 0 0]という2×2の行列です

12
00:00:41,230 --> 00:00:45,531
そしてb[1]も[0 0]とします

13
00:00:45,531 --> 00:00:50,788
バイアス項bを０に初期化するのは
実は問題ありません

14
00:00:50,788 --> 00:00:54,240
しかしWを全て０にするのは問題です

15
00:00:54,240 --> 00:00:59,625
なぜ問題なのかというと

16
00:00:59,625 --> 00:01:05,522
全てのサンプルに対して

17
00:01:05,522 --> 00:01:09,253
a1[1]とa2[1]が同じですよね？

18
00:01:09,253 --> 00:01:12,613
この活性化とこの活性化が
同じだということです

19
00:01:12,613 --> 00:01:17,170
なぜなら 隠れ層の二つのユニットは
全く同じ関数を計算しているからです

20
00:01:17,170 --> 00:01:21,810
それから逆伝播を行うと

21
00:01:21,810 --> 00:01:24,478
dz1[1]とdz2[1]も同じで

22
00:01:24,478 --> 00:01:30,165
対称になってしまいますね

23
00:01:30,165 --> 00:01:33,720
隠れ層のユニットが二つとも
同じように初期化されるのです

24
00:01:33,720 --> 00:01:36,080
ここでは 出力されるときの重みも

25
00:01:36,080 --> 00:01:39,851
同じであると想定しています

26
00:01:39,851 --> 00:01:45,122
W[2]も[0 0]だということです

27
00:01:45,122 --> 00:01:48,691
このようにニューラルネットワークを
初期化すると

28
00:01:48,691 --> 00:01:53,590
隠れ層のこのユニットとこのユニットが
全く同じになります

29
00:01:53,590 --> 00:01:57,011
完璧に対称であるとも言えます

30
00:01:57,011 --> 00:02:01,687
全く同じ関数を計算している
という意味です

31
00:02:01,687 --> 00:02:03,765
帰納法から

32
00:02:03,765 --> 00:02:08,064
どの訓練の繰り返しの後でも

33
00:02:08,064 --> 00:02:11,272
二つのユニットは全く同じ関数を
計算しているといえます

34
00:02:11,272 --> 00:02:17,521
すると dWはこのように

35
00:02:17,521 --> 00:02:20,681
全ての行が同じ値である
行列になります

36
00:02:20,681 --> 00:02:23,318
重みを更新すると

37
00:02:23,318 --> 00:02:30,163
W[1]はW[1]-α×dWに
更新されます

38
00:02:30,163 --> 00:02:33,740
すると 全ての繰り返しの後で

39
00:02:33,740 --> 00:02:37,616
一行目が二行目と同じであることが
わかるでしょう

40
00:02:37,616 --> 00:02:41,487
帰納法から次のようなことがわかります

41
00:02:41,487 --> 00:02:44,688
Wの値を全て０に初期化すると

42
00:02:44,688 --> 00:02:49,164
隠れ層の二つのユニットが
同じ関数の計算から始まるので

43
00:02:49,164 --> 00:02:53,541
そして二つともが出力ユニットに
同じ影響を与えるので

44
00:02:53,541 --> 00:02:57,542
一回目の繰り返しの後でも

45
00:02:57,542 --> 00:03:00,273
二つの隠れユニットは対称なままです

46
00:03:00,273 --> 00:03:04,507
なので帰納法を用いると
２回目 ３回目の繰り返しの後も

47
00:03:04,507 --> 00:03:07,013
いくらニューラルネットワークを
訓練しても

48
00:03:07,013 --> 00:03:10,373
二つの隠れユニットは 全く同じ関数を
計算し続けていることがわかります

49
00:03:10,373 --> 00:03:15,212
この場合 隠れユニットを
一つ以上持つ意味がなくなってしまいます

50
00:03:15,212 --> 00:03:17,692
全部が同じことを計算している
だけだからです

51
00:03:17,692 --> 00:03:22,378
もちろんもっと大きな
ニューラルネットワークで

52
00:03:22,378 --> 00:03:24,972
３つの特徴量があって
たくさんの隠れユニットがあっても

53
00:03:24,972 --> 00:03:29,239
このニューラルネットワークと
同じことが言えるでしょう

54
00:03:29,239 --> 00:03:34,107
全ての重みを０に初期化すると

55
00:03:34,107 --> 00:03:37,103
全ての隠れユニットが
対称になってしまうのです

56
00:03:37,103 --> 00:03:40,603
そしてどれだけ最急降下法を実行しても

57
00:03:40,603 --> 00:03:44,037
全く同じ関数を計算し続けるでしょう

58
00:03:44,037 --> 00:03:48,785
異なる隠れユニットには
異なる関数を計算してほしいので

59
00:03:48,785 --> 00:03:52,835
これではうまくいきません

60
00:03:52,835 --> 00:03:57,748
解決法は パラメータをランダムに
初期化することです

61
00:03:57,748 --> 00:03:58,677
実際には次のようなことをします

62
00:03:58,677 --> 00:04:04,053
W1=np.random.randnと
設定します

63
00:04:04,053 --> 00:04:07,037
これでガウス分布の乱数が生成されます
(2,2)のサイズです

64
00:04:07,037 --> 00:04:12,358
そして大抵は とても小さい数字
0.01などを掛けます

65
00:04:12,358 --> 00:04:14,951
つまりとても小さい乱数に
初期化することになります

66
00:04:14,951 --> 00:04:20,590
bは対称になっても
問題ではない つまり

67
00:04:20,590 --> 00:04:24,735
対称性の破れの問題と呼ばれるものを
持たないとわかっています

68
00:04:24,735 --> 00:04:29,370
そのため bはただの０に初期化して
大丈夫です

69
00:04:29,370 --> 00:04:32,166
Wがランダムに初期化されている限り

70
00:04:32,166 --> 00:04:36,769
それぞれの隠れユニットが
別の計算をしている状態で始められるからです

71
00:04:36,769 --> 00:04:40,912
すると もう対称性の破れの問題に
悩まされることはありません

72
00:04:40,912 --> 00:04:43,795
同様にW[2]もランダムに初期化しましょう

73
00:04:43,795 --> 00:04:48,858
b[2]は０で初期化して構いません

74
00:04:48,858 --> 00:04:55,321
さて この定数はどこから来てなぜ0.01なのか
疑問に思っているかもしれません

75
00:04:55,321 --> 00:04:58,478
なぜ100や1000ではないのでしょう？

76
00:04:58,478 --> 00:05:02,313
それは 普通は重みを
とても小さいランダムな値に

77
00:05:02,313 --> 00:05:05,763
初期化するのが好まれるからです

78
00:05:05,763 --> 00:05:10,443
もしtanhやシグモイド関数や
他のシグモイドを

79
00:05:10,443 --> 00:05:14,047
適用するとき
たとえそれが出力層だけであっても

80
00:05:14,047 --> 00:05:17,922
もし重みが大きすぎたら

81
00:05:17,922 --> 00:05:23,967
活性化の値を計算したとき

82
00:05:23,967 --> 00:05:28,621
z[1]=W1x+bで

83
00:05:28,621 --> 00:05:34,094
a[1]がz[1]に活性化関数を
適用したものですね

84
00:05:34,094 --> 00:05:39,097
なので もしwがとても大きければ

85
00:05:39,097 --> 00:05:44,235
zのうちいくつかは
とても大きいか小さい値になりますよね

86
00:05:44,235 --> 00:05:49,789
するとtanhやシグモイド関数の
平らな部分にきてしまうので

87
00:05:49,789 --> 00:05:55,699
勾配が小さくなるため

88
00:05:55,699 --> 00:05:58,302
最急降下法は遅くなってしまいます

89
00:05:58,302 --> 00:05:59,730
つまり 学習が遅くなるのです

90
00:05:59,730 --> 00:06:04,133
おさらいすると wが大きすぎれば

91
00:06:04,133 --> 00:06:08,633
訓練の初めの方であっても
zはとても大きくなってしまい

92
00:06:08,633 --> 00:06:13,525
tanhやシグモイド活性化関数が
飽和したような状態になるため

93
00:06:13,525 --> 00:06:15,418
学習が遅くなってしまいます

94
00:06:15,418 --> 00:06:17,231
シグモイドやtanh活性化関数を
ニューラルネットワークで

95
00:06:17,231 --> 00:06:22,149
全く使わないなら
それほど問題ではありませんが

96
00:06:22,149 --> 00:06:26,506
二項分類をするなら
出力ユニットはシグモイド関数なので

97
00:06:26,506 --> 00:06:30,806
パラメータの初期状態は
大きくない方が良いです

98
00:06:30,806 --> 00:06:35,435
こういうわけで0.01や
他の小さい数字を

99
00:06:35,435 --> 00:06:36,872
掛けることが
合理的だといえるのです

100
00:06:36,872 --> 00:06:38,536
そしてW[2]についても同じですね

101
00:06:38,536 --> 00:06:44,295
これがrandom.randnとなって

102
00:06:44,295 --> 00:06:49,545
この例では1×2の行列ですよね
それ掛ける0.01です

103
00:06:49,545 --> 00:06:51,404
ここの「s」を忘れていました

104
00:06:51,404 --> 00:07:00,085
最後に 0.01より良い定数も
あり得ます

105
00:07:00,085 --> 00:07:04,304
隠れ層が一つのみの
ニューラルネットワークを訓練しているとき

106
00:07:04,304 --> 00:07:09,129
隠れ層の多くない 比較的浅い
ニューラルネットワークなので

107
00:07:09,129 --> 00:07:12,392
0.01に設定しても大丈夫でしょう

108
00:07:12,392 --> 00:07:15,705
しかしすごく深いニューラルネットワークを
訓練する場合

109
00:07:15,705 --> 00:07:19,294
0.01ではない定数を
選びたくなることもあるかもしれません

110
00:07:19,294 --> 00:07:23,642
次の週には なぜ0.01ではない
定数を選びたくなることが

111
00:07:23,642 --> 00:07:27,925
あるのかをお話ししたいと思います

112
00:07:27,925 --> 00:07:32,008
どちらにせよ これはある程度
小さい値となります

113
00:07:32,008 --> 00:07:34,584
今週の動画は以上です

114
00:07:34,584 --> 00:07:38,348
隠れ層が一つの
ニューラルネットワークの作り方や

115
00:07:38,348 --> 00:07:42,430
パラメータの初期化の仕方
順伝播を使った予測の方法

116
00:07:42,430 --> 00:07:45,445
そして誤差逆伝播法を使った
微分の求め方や

117
00:07:45,445 --> 00:07:46,275
最急降下法の実装の仕方が
わかりましたね

118
00:07:46,275 --> 00:07:48,654
では きっと今週のクイズと

119
00:07:48,654 --> 00:07:51,166
プログラミング課題ができると思います

120
00:07:51,166 --> 00:07:52,143
頑張ってくださいね

121
00:07:52,143 --> 00:07:54,802
プログラミング課題を楽しんでもらえれば
嬉しいです

122
00:07:54,802 --> 00:07:57,728
それでは四週目で会えるのを
心待ちにしています