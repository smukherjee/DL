1
00:00:00,000 --> 00:00:01,530
У попередньому відео

2
00:00:01,530 --> 00:00:06,885
ми бачили як для тренувальних зразків, 
складених по горизонталі в матрицю X,

3
00:00:06,885 --> 00:00:11,158
можна вивести векторизовану реалізацію 
прямого поширення для нейронної мережі (НМ).

4
00:00:11,158 --> 00:00:14,760
Давай дамо дещо ширше обґрунтування чому рівняння, 
які ми записали,

5
00:00:14,760 --> 00:00:19,775
є правильною реалізацією векторизації для багатьох прикладів.

6
00:00:19,775 --> 00:00:25,590
Тож давай розглянемо частину обчислень прямого поширення 
на кількох прикладах.

7
00:00:25,590 --> 00:00:27,645
Скажімо, що для першого тренувального зразка

8
00:00:27,645 --> 00:00:29,130
ми вивели

9
00:00:29,130 --> 00:00:38,970
ось це: z¹⁽¹⁾=w¹x⁽¹⁾+b¹; для другого тренувального зразка -

10
00:00:38,970 --> 00:00:49,310
ось це: z¹⁽²⁾=w¹x⁽²⁾+b¹; потім

11
00:00:49,310 --> 00:00:50,900
для третього тренувального зразка -

12
00:00:50,900 --> 00:00:56,064
ось це: z¹⁽³⁾=w¹x⁽³⁾+b¹.

13
00:00:56,064 --> 00:01:00,930
Тож, щоб трохи спростити пояснення, 
я проігнорую b на цьому слайді.

14
00:01:00,930 --> 00:01:08,395
Тож, скажімо (щоб трохи спростити обґрунтування), b=0.

15
00:01:08,395 --> 00:01:11,140
Проте, аргумент, що залишається буде працювати

16
00:01:11,140 --> 00:01:14,320
практично так само як і коли b ненульове.

17
00:01:14,320 --> 00:01:17,610
Ми просто трохи спростимо описання на цьому слайді.

18
00:01:17,610 --> 00:01:21,110
Тож, w[1] буде деякою матрицею.

19
00:01:21,110 --> 00:01:25,625
Ця матриця має деяку кількість рядків.

20
00:01:25,625 --> 00:01:28,296
Тож, якщо глянути на ці вирази,

21
00:01:28,296 --> 00:01:30,070
ми маємо

22
00:01:30,070 --> 00:01:40,021
добуток w[1]x⁽¹⁾, що дає в результаті вектор-стовпець. 
Намалюємо його ось так.

23
00:01:40,021 --> 00:01:47,420
Аналогічно, отут

24
00:01:47,420 --> 00:01:54,730
ми маємо

25
00:01:54,730 --> 00:02:00,460
добуток w[1]x⁽²⁾, що також дає в результаті вектор-стовпець.

26
00:02:00,460 --> 00:02:03,250
Це буде z[1]⁽²⁾.

27
00:02:03,250 --> 00:02:06,730
І, нарешті, отут

28
00:02:06,730 --> 00:02:12,315
ми маємо добуток w[1]x⁽³⁾,

29
00:02:12,315 --> 00:02:19,530
що дає в результаті третій вектор-стовпець, оце z[1]⁽³⁾.

30
00:02:19,530 --> 00:02:25,250
Тепер, якщо взяти тренувальний набір велике X,

31
00:02:25,250 --> 00:02:31,475
який ми формуємо, складаючи докупи наші тренувальні зразки.

32
00:02:31,475 --> 00:02:37,010
Тож матриця велике X формується вектором x⁽¹⁾

33
00:02:37,010 --> 00:02:43,430
складеним по вертикалі з x⁽²⁾, а потім з x⁽³⁾.

34
00:02:43,430 --> 00:02:46,250
Це у випадку якщо ми маємо лише 3 тренувальні зразки.

35
00:02:46,250 --> 00:02:50,371
Коли їх більше, то їх можна просто продовжити складати по горизонталі.

36
00:02:50,371 --> 00:02:57,790
І тепер, якщо помножити цю матрицю X на W, то,

37
00:02:57,790 --> 00:03:00,190
якщо згадати як множити матриці,

38
00:03:00,190 --> 00:03:02,680
ми побачимо, що 1-ий стовпець буде

39
00:03:02,680 --> 00:03:06,313
складатись з тих же величин, які я намалював отут фіолетовим,

40
00:03:06,313 --> 00:03:10,930
2-ий стовпець міститиме оці 4 величини,

41
00:03:10,930 --> 00:03:16,612
а 3-ій міститиме оці оранжеві величини.

42
00:03:16,612 --> 00:03:19,480
Ось як, виявляється, це буде виглядати.

43
00:03:19,480 --> 00:03:27,740
І, звичайно, це = z[1]⁽¹⁾, представленому як

44
00:03:27,740 --> 00:03:37,185
вектор-стовпець, z[1]⁽²⁾, представленому як вектор-стовпець, z[1]⁽³⁾,

45
00:03:37,185 --> 00:03:39,273
також представленому як вектор-стовпець.

46
00:03:39,273 --> 00:03:41,100
Це коли ми маємо 3 тренувальні зразки.

47
00:03:41,100 --> 00:03:44,255
Якщо ми маємо більше зразків, то тут буде більше стовпців.

48
00:03:44,255 --> 00:03:51,220
І це лише матриця велике Z[1].

49
00:03:51,220 --> 00:03:55,230
Тож, надіюсь, це дає обґрунтування чому

50
00:03:55,230 --> 00:04:02,830
до цього ми мали w[1]x⁽ⁱ⁾=z[1]⁽ⁱ⁾

51
00:04:02,830 --> 00:04:08,310
коли розглядали окремий тренувальний зразок.

52
00:04:08,310 --> 00:04:12,565
А коли складаємо окремі тренувальні зразки в стовпці,

53
00:04:12,565 --> 00:04:15,250
то в результаті, який ми отримуємо,

54
00:04:15,250 --> 00:04:18,725
z, відповідно, теж складені стовпцями.

55
00:04:18,725 --> 00:04:24,565
І я вже не буду показувати, але ти можеш переконатись 
з допомогою посіву [broadcasting] Python,

56
00:04:24,565 --> 00:04:26,245
що якщо ми додамо

57
00:04:26,245 --> 00:04:30,534
оці величини b, то результат залишиться правильним.

58
00:04:30,534 --> 00:04:34,540
І, справді, ми отримаємо посів Python,

59
00:04:34,540 --> 00:04:41,790
b⁽ⁱ⁾ додасться до кожного стовпця матриці.

60
00:04:41,790 --> 00:04:48,220
Тож на цьому слайді я лише обґрунтував, що

61
00:04:48,220 --> 00:04:51,980
Z[1]=W[1]X+b[1]

62
00:04:51,980 --> 00:04:54,020
є правильною векторизацією

63
00:04:54,020 --> 00:04:57,493
першого кроку з 4-ох, які ми мали на попередньому слайді.

64
00:04:57,493 --> 00:04:59,990
І, виявляється, що подібним чином можна

65
00:04:59,990 --> 00:05:02,660
показати, що інші кроки можна обчислити, користуючись

66
00:05:02,660 --> 00:05:08,105
схожою логікою: якщо скласти вхідні дані в стовпці, 
то, розв'язавши результуюче

67
00:05:08,105 --> 00:05:11,510
рівняння, ми отримаємо вихідні дані відповідно складені в стовпці.

68
00:05:11,510 --> 00:05:14,970
І, наостанок, давай резюмуємо про що ми говорили в цьому відео.

69
00:05:14,970 --> 00:05:16,520
Якщо ми маємо таку НМ,

70
00:05:16,520 --> 00:05:21,693
ми сказали, що нам потрібно обчислити ось це, 
щоб реалізувати пряме поширення

71
00:05:21,693 --> 00:05:27,693
по черзі для кожного тренувального зразка при i=1 до m. 
Також ми сказали,

72
00:05:27,693 --> 00:05:34,100
що потрібно скласти тренувальні зразки в стовпці, ось так, 
для кожної величини Z[1],

73
00:05:34,100 --> 00:05:38,265
A[1], Z[2], A[2]. Ось так потрібно скласти у відповідні стовпці.

74
00:05:38,265 --> 00:05:43,820
Тут приклад для A[1], проте, він правильний і для Z[1],

75
00:05:43,820 --> 00:05:46,975
A[1], Z[2] і A[2].

76
00:05:46,975 --> 00:05:51,090
Потім на попередньому слайді ми показали,

77
00:05:51,090 --> 00:05:58,785
що оцей рядок дозволяє 
векторизувати обчислення для всіх m зразків одночасно.

78
00:05:58,785 --> 00:06:00,555
І, виявляється, що, опираючись на це,

79
00:06:00,555 --> 00:06:03,880
можна показати, що інші рядки

80
00:06:03,880 --> 00:06:08,811
є правильною векторизацією всіх 4-ох рядків коду.

81
00:06:08,811 --> 00:06:10,675
І просто нагадаю, що

82
00:06:10,675 --> 00:06:18,960
через те що X також =A[0], тому що, пам'ятаємо,

83
00:06:18,960 --> 00:06:27,980
вхідний вектор ознак x дорівнював a[0]. Тож x⁽ⁱ⁾=a[0]⁽ⁱ⁾.

84
00:06:27,980 --> 00:06:30,870
І, справді, є певна симетрія між

85
00:06:30,870 --> 00:06:34,110
цими рівняннями. Це рівняння може бути

86
00:06:34,110 --> 00:06:41,790
записане як Z[1]=W[1]A[0]+b[1].

87
00:06:41,790 --> 00:06:45,680
Тож, як бачиш, оця пара рівнянь і оця пара

88
00:06:45,680 --> 00:06:51,805
рівнянь дійсно виглядають дуже схоже, 
їхні індекси відрізняються лише на 1.

89
00:06:51,805 --> 00:06:55,880
Тож це ніби показує, що різні шари НМ

90
00:06:55,880 --> 00:07:00,585
роблять практично одні і ті ж обчислення знову і знову.

91
00:07:00,585 --> 00:07:04,220
Тут ми маємо 2-шарову НМ. А коли ми перейдемо до

92
00:07:04,220 --> 00:07:08,475
набагато глибших НМ на наступному тижні,

93
00:07:08,475 --> 00:07:11,670
ти побачиш, що навіть глибші НМ, по суті, містять

94
00:07:11,670 --> 00:07:16,215
оці 2 кроки і просто обчислюють їх набагато більше разів 
ніж ти бачиш тут.

95
00:07:16,215 --> 00:07:21,255
Тож отак ти можеш векторизувати НМ 
для багатьох тренувальних зразків.

96
00:07:21,255 --> 00:07:25,590
Далі. До цього часу ми використовували 
сигмоїдальну функцію для всієї НМ.

97
00:07:25,590 --> 00:07:27,925
Але, виявляється, це не найкращий підхід.

98
00:07:27,925 --> 00:07:29,675
В наступному відео давай дещо глибше

99
00:07:29,675 --> 00:07:32,450
розглянемо як можна використовувати, так звані,

100
00:07:32,450 --> 00:07:37,190
функції активації, 
для яких сигмоїда - лише один з можливих варіантів.