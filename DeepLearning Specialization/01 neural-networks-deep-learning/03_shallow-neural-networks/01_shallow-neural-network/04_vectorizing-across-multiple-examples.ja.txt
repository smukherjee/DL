前回のビデオでは ニューラルネットワークで
１つのトレーニングサンプルから予測を計算する方法を 見ました このビデオでは 複数のトレーニングサンプルを
ベクトル化する方法を紹介します 結果はロジステック回帰のときに見たものと
ほぼ同じものになります それぞれのトレーニングサンプルを
それぞれの列に重ねていくことで 先ほどのビデオでの式を
少し変えると使うことができます ニューラルネットワークに
ほぼ同時に 全てのサンプルに対しての出力を
計算させることができます どのようにするのか
見ていきましょう この４つの式は前回のビデオで使った式で
z[1]、a[1]、z[2]、a[2]の計算法を 表わしています これを使うと
ベクトルx を入力したときに 単一のサンプルのa[2]=yハットを
生成することができます m 個のサンプルがある時には
この処理を繰り返します つまり最初のサンプルの x(1)を使って 最初のサンプルの予測値のyハット(1)を
計算します x(2)も同じ式を使い
yハット(2)を生成します x(m) のところまで続けて行き
yハット(m)を生成します すべての活性化関数の表記をするために a[2](1)と書き a[2](2)と書き a[2](m)と書きます この表記a[2](i)は (i)がi個目のサンプルを指し (2)が２層目を指します このようにして[]と()の添え字は
使われます これらが意味するのは
ベクトル化されていない実装の場合で 全てのトレーニングサンプルに対して
予測値を計算する場合には for i=1 to mをし これらの４つの式を
実装する必要があります z[1](i) =W[1]x(i)+b[1] a[1](i)はシグモイドz[1](i)で z[2](i)=W[2]a[1](i)+b[2]です そして a[2](i)はシグモイドz[2](i)です これらの４つの式に対して
トレーニングサンプルに 依存している全ての変数に
(i)を付けます つまり(i)を
x、Z、a に付けます こうすると m個のトレーニングサンプルの出力を
計算できます ここで行いたいのは
この計算をベクトル化してfor ループを排除します ところで 私があまりに多くの線形代数の基礎を
出しているように感じても 深層学習時代には
これを正しく実装できることが 重要なことだと分かってきます 私たちはこのコースでの表記方法について
非常に注意を払って選びました ベクトル化のステップを
できるだけ簡単にすることを目指しました この多くの基礎を見ていくことで
これらのアルゴリズムの正しい実装を より早く動作させられるようになる手助けが
できればと思います ではこのコード全体を
コピーして次のスライドに入れて どうベクトル化するかを
見ていきましょう ここに前のスライドからの
m個のトレーニングサンプルを 順次処理するfor ループがあります 定義を思い出してください 行列Xは トレーニングサンプルが
列に重ねられたものでした トレーニングサンプルを
列に並べたものです nxとm次元の配列の 行列になります このfor ループを
ベクトル化した実装を行うために 必要なことの肝心な部分を
教えようとしています ここで行う必要があるのは Z[1]=w[1]X+b[1]を計算することです A[1]=シグモイドのz[1] Z[2]=w[2]A[1]+b[2]で計算でき 最後に A[2]=シグモイドのz[2] 例示をするなら
小文字のベクトルx から それぞれ列に重ねていくことで
大文字の行列Xまで来ました これをzに対しても行うと z[1](1)、z[1](2)のように続けて これはすべて列ベクトルで
z[1](m)まで行きます このz[1](i)でもmの全てを
列に重ねることで Z[1]の行列ができます 同じようにこのa[1](i)の量は a[1](1)、a[1](2) そしてa[1](m)までを
列に重ねると 小文字のxから大文字のXを算出し
小文字のzから大文字のZを 算出したのと同じように ベクトル小文字のa から
A[1]を算出します 同様にZ[2]とA[2]も
計算できます これらもベクトルを使って 横に重ねることで
取得できます これについても
横に重ねることでできます Z[2]とA[2]が得られます この表記が持っているもので
考えると役立つのは このZやAなどの行列は トレーニングサンプルに対して
横に番号付けを行います これが横の番号付けが
トレーニングサンプルに対応する理由です 左から右に行くと
トレーニングデータを順次見ていることになります 縦には この垂直の番号付けは
ニューラルネットワークのそれぞれのノードに 対応します 例えば このノードは 行列のこの一番上の角の値は
最初のトレーニングサンプルの 隠れ層の最初のユニットに対応します ひとつ下の値は最初のトレーニングサンプルの
隠れ層の２つ目のユニットに 対応します そして最初のトレーニングサンプルの
隠れ層の３個目のユニットと続きます 下へと見ていくときは
隠れ層のユニットの番号付けを索引していきます 一方 横へと見ていくときは
隠れ層の最初のユニットで最初のトレーニングサンプルから ２個目のサンプルへと見ていき ３個目そして
次々と続いていき ここのノードが対応するのが
隠れ層の最初のユニットかつ 最後でそしてm個目のトレーニングサンプルの
活性化のところまで続きます まとめると 行列Aの水平側は
トレーニングサンプルに対応していて 垂直側は行列Aのそれぞれの索引は それぞれの隠れ層のユニットに
対応しています 同じ感覚が行列Zでも行列Xでも
当てはまります 水平がそれぞれのトレーニングサンプルに対応し 垂直がニューラルネットワークの入力層での
ユニットまた同時に それぞれの入力特徴量に
対応しています これらの式に関して ベクトル化してニューラルネットワークを
実装する方法が分かったかと思います 複数のサンプルに対しての
ベクトル化です 次のビデオでは
この方法がこのような種類のベクトル化について 正しい実装なのかを
正当化する理由を紹介します 正当化がロジステック回帰で見たものと
同じようなものだと判明します 次のビデオに進みましょう