У попередньому відео ти бачив/ла структурні блоки реалізації глибокої нейронної мережі (НМ): крок прямого поширення для кожного шару і і відповідний крок зворотного поширення. Давай подивимось як їх реалізувати на практиці. Почнемо з прямого поширення. Згадаймо, що тут вводиться a[l-1], а виводиться a[l] і кеш z[l]. І ми казали, що з точки зору реалізації добре б кешувати також і w[l] з b[l], просто щоб простіше обчислювати функції в програмувальній вправі. А рівняння для цього нам вже знайомі. Для реалізації прямої функції вони ось такі: z[l]=w[l]a[l-1]+b[l] і a[l]= активаційній функції g від z. І якщо ми хочемо векторизувати реалізацію, то це буде Z[l]=W[l]A[l-1]+b (b буде додаватись з допомогою посіву Python) і A[l]= g поелементно від Z[l]. Згадай, що в Діаграмі [обчислень] прямого кроку ми мали отакий ланцюг квадратів спрямованих вперед, які ми ініціалізували введенням A[0], що =X. Тож це ми ініціалізували. А чим є вхідні дані в 1-ий квадрат? Це - a[0], тобто вхідні ознаки або одного тренувального зразка, якщо ми обробляємо 1 зразок, або велике A[0], тобто увесь тренувальний набір, якщо ми обробляємо увесь тренувальний набір. Тож це - вхідні дані для 1-ої прямої функції в ланцюгу. А потім просте повторення дозволяє нам обчислити пряме поширення зліва-направо. Далі давай поговоримо про крок зворотного поширення. Тут наша ціль - ввести da[l], а вивести - da[l-1], dw[l] і db[l]. Давай запишемо кроки, які нам потрібні для обчислення цих величин. dz[l]=da[l] помножене поелементно на похідну g[l]'(z[l]). Потім обчислюємо похідні dw[l]=dz[l]a[l-1]. Я не зберігаю це в кеші, але, виявляється, що воно теж потрібно. [?] Далі db[l]=dz[l]. І, нарешті, da[l-1]=w[l]ᵀdz[l]. Правильно? Я не хочу заглиблюватись в деталі цього виведення, але, виявляється, якщо взяти оце визначення da і підставити сюди, то ми отримаємо ту ж формулу, що мали на попередньому тижні. Тобто будемо обчислювати dz[l] як функцію від попереднього dz[l]. 
Фактично, якщо ми підставимо оце сюди, то отримаємо dz[l]=w[l+1]ᵀdz[l+1]g[l]'(z[l]). Я розумію, що здається, що тут багато алгебри. Насправді, ти можеш самостійно перевірити, що це - рівняння, яке ми записали для зворотного поширення на минулому тижні. Коли ми реалізуємо НМ з єдиним прихованим шаром. І просто нагадаю, що це - поелементне множення. Тож все, що нам потрібно - це ось ці 4 рівняння. 
І ми зможемо реалізувати зворотну функцію. І, врешті-решт, запишемо векторизовану версію. Перший рядок буде dZ[l]=dA[l] помножене поелементно на g[l]'(z[l]). Ну тут нічого незвичайного. dW[l] стане 1/m помножене на dZ[l] помножене на A[l-1]ᵀ, а db[l] стане 1/m помножене на np.sum(dZ[l], потім axis=1, keepdims=True). Ми говорили про використання np.sum() 
для обчислення db на минулому тижні. І, нарешті, dA[l-1]=W[l]ᵀdZ[l]. Тож це дозволяє нам ввести оцю величину - da - сюди і вивести dW[l], db[l] - похідні, які нам потрібні - і вивести da[l-1] і dA[l-1]. Правильно? Ось так. Тож отак ми реалізуємо зворотну функцію. І щоб резюмувати. Беремо вхідне X. Швидше за все, ми маємо 1-ий шар, припустимо, з ВЛВ [ReLU] в ролі активаційної функції. Потім йдемо до 2-ого шару, який, припустимо, використовує ще одну ВЛВ [ReLU] 
в ролі активаційної функції. Йдемо до 3-ого шару, який, припустимо, має сигмоїду в ролі активаційної функції, 
припустимо, ми робимо двійкову класифікацію. І отримуємо ŷ. А потім, використовуючи ŷ, ми можемо обчислити Втрату. А це дозволяє нам почати зворотну ітерацію. Я намалюю спочатку стрілки, добре? Просто не хочу дуже часто змінювати колір курсору. І тут ми матимемо зворотне поширення, що обчислює похідні dW[3], db[3], dW[2], db[2], dW[1], db[1]. І по дорозі ми обчислюватимемо... Ага. Буде передаватись кеш z[1], z[2], z[3]. А тут ми передаємо назад da[2] і da[1]. Тут може бути обчислено і da[0], але ми його не використовуватимемо. Тож можемо просто його відкинути. Правильно? Тож отак ми реалізуємо пряме і зворотне поширення для 3-шарової НМ. Тепер остання деталь, яку я не згадав. Пряму рекурсію ми ініціалізуватимемо вхідними даними X. А як щодо зворотної рекурсії? Що ж, виявляється, що da[l] при використанні логістичної регресії (ЛР), при виконанні двійкової класифікації, =-(y/a)+(1-y)/(1-a). Тож, виявляється, що похідна функції Втрати по відношенню до вихідних даних, по відношенню до ŷ, може бути представлена отак. Якщо ти знайомий/а з диф.численням, якщо візьмеш функцію Втрати L і візьмеш похідні по ŷ або a, то побачиш, що отримаєш оцю формулу. Тож це - формула, яку ми повинні використовувати 
для останнього шару da[велике L]. І, звичайно, якщо виконувати векторизовану реалізацію, то ініціалізація зворотної рекурсії буде не оцим, а - dA[L], що буде тим же, але для окремого зразка. -y⁽¹⁾/a⁽¹⁾ для 1-ого тренувального зразка плюс (1-y⁽¹⁾) для 1-ого тренувального зразка поділити на (1-a⁽¹⁾) для 1-ого тренувального зразка. І т.д. до останнього тренувального зразка m. Тож отак ми реалізуємо векторизовану версію. Отак ми ініціалізуємо векторизовану версію зворотного поширення. Отож, тепер ти побачив/ла основні структурні блоки як прямого, так і зворотного поширення. Тепер, якщо ти реалізуєш ці рівняння, то отримаєш правильну реалізацію прямого і зворотного поширень і отримаєш необхідні похідні. Можливо, ти думаєш, що тут дуже багато рівнянь і дещо збентежений/а, бо не впевнений/а, 
що повністю зрозумів/ла як це працює. Тож якщо ти так себе почуваєш, я раджу, почати робити програмувальну вправу. Ти зможеш реалізувати це самостійно. Там все буде більш конкретно. Я розумію, що рівнянь дуже багато і, можливо, деякі рівняння не повністю зрозумілі, але якщо ти пропрацюєш диф.числення і лінійну алгебру (що не так вже й легко)... Спробуй, хоч це й справді одне з найбільш 
складних виведень в машинному навчанні (МН). Дійсно, рівняння наростають лавиноподібно. Принаймі, диф.рівняння для обчислення похідних, 
особливо при зворотному поширенні. Проте, знову ж, якщо тобі це видається абстрактним, дещо містичним, то я раджу зробити програмувальну вправу. Тоді воно набуде конкретики. Також мушу сказати, що навіть зараз, коли я реалізую навчальний алгоритм, інколи навіть я дивуюсь, коли моя реалізація навчального алгоритму працює. 
Це все тому, що складність МН більше залежить від даних ніж від рядків коду. Тож інколи здається, що ти реалізуєш кілька рядків коду, не повністю розумієш, що вони роблять, але вони мало не магічно працюють. Це тому, що більшість магії знаходиться 
не в шматку коду, який ми пишемо, бо він дуже часто не такий вже й великий. Він не те щоб простий, але це не десятки тисяч чи сотні тисяч рядків коду. Просто ми вводимо так багато даних, що інколи навіть я, який вже довго працює в МН, інколи і я дещо дивуюсь, коли мій навчальний алгоритм працює. 
Бо більша частина складності навчального алгоритму залежить від даних, а не від необхідності писати тисячі-тисячі рядків коду. Добре. Тож отак ми реалізуємо глибоку НМ. І, знову ж, конкретики додасться, 
коли ти зробиш програмувальну вправу. Перед тим як піти далі, я хочу обговорити в наступному відео гіперпараметри і параметри. Виявляється, що при тренуванні глибоких НМ, уміння добре налаштувати гіперпараметри допоможе тобі більш ефективно розробляти НМ. В наступному відео давай поговоримо що це значить.