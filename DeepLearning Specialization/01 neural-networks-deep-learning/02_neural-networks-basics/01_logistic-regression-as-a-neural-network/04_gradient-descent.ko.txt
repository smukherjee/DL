로지스틱 회귀분석 모델을 보았는데요, 싱글 트레이닝 example에서 얼마나 잘하는지를 측정하는 비용함수를 보았었는데요, 또한, 파라티터 w와 b가 전체 트레이닝 세트에서 얼마나 잘하는지를 측정하는 비용함수도 같이 봤습니다. 이제는 기울기 강하 알고리즘을 어떻게 이용하여 w와 b를 트레이닝세트에서 트레이닝 또는 
학습할 수 있는지 배워보도록 하겠습니다. 복습하자면, 이것이 어디서 많이 눈이 익숙한 로지스틱 
회귀분석 알고리즘이죠. 2번째 줄에는 J라는 비용함수가 있습니다. 이 함수는 파라티터 w와 b에 대한 함수입니다. 그리고 이것은 평균값으로 정의되죠. 그러므로 1 나누기 m 곱하기 이 loss함수의 합입니다. 그렇게해서 loss함수는 각각의 트레이닝 example에 대해서
알고리즘에 결과값 ŷ(i)이 얼마나 잘 쌓거나 ground true label y(i)과 
얼마나 잘 비교되는지 측정합니다. 전체 공식은 오른쪽에 펼쳐져있습니다. 자 그래서, 비용함수는 트레이닝세트에서 파라티터 w와 b가 얼마나 잘 하는지를
측정하는데요 w와 b라는 파라티터를 배우기 위해서는, w와b가 비용함수 J(w, b)를 최소화시키는 값을
찾는게 자연스러운 단계이겠죠? 이것이 gradien descent의 모습입니다. 이 표에서는 가로축이 공간 매개 변수 w와 b를 나타냅니다. 실제로 w는 다 큰 다이멘션일 수 있지만 표에 나타내기 위해 w와 b가 실수라고하겠습니다. J(w, b, ) 비용함수는 그러면 이 가로축 w와 b 표면 어딘가에 있을 것입니다. 표면의 높이는 특정지점에서 J(w, b)의 값을 가르키는데요 저희는 J라는 비용함수가 최소값이 되는 w와 b의 값을 구해야 합니다. 알고보면, 이 J 비용함수는 볼록함수입니다. 하나의 그릇 모양이라고 할 수 있는데요, 
이것이 볼록함수이고, 이런 여러개의 local이 있는 비볼록 함수와는 다르죠. 이렇게 정의된 비용함수 J(w, b)는 볼록하고, 그렇기 때문에 이와 같은 비용함수 J를 이용하는 것입니다. 로지스틱 회귀분석에서 말이죠. 파라티터의 적합한 값을 찾으려면, w와 b를 최초값으로 초기화해야 합니다. 이 빨간 점이 상징하듯이 말이죠. 로지스틱 회귀분석에서는 거의 모든 
초기화 방법이 잘 구현됩니다. 일반적으로 0으로 초기화시키는데요, 무작위로 초기화 시키는 방법도 효율적입니다. 그렇지만 대부분의 사람은 보통 이러한 방법은
잘 사용하지 않죠. 이 함수가 볼록함수이기 때문에
어느 지점에서 초기화하더라도 똑같은 지점에 도달하거나 거의 비슷한 점에
도달할 것입니다. 기울기 강하 가 하는 것은, 
처음 시작점에서 시작해서 가장 기울기가 높은 내리막길 방향으로 이동합니다. 그렇기 때문에 기울기 강하 이후로는
이 점에 도착할 수도 있습니다. 그 이유는, 기울기 강하 가 가장 경사가 높은 내리막길
방향으로 향하기 때문입니다. 최대한 빠른 속도로 말이죠. 이것이 기울기 강하 의 반복업무 중 하나입니다. 기울기 강하를 2번 진행한 이후, 
이곳으로 넘어갈 수도 있습니다. 3번째에는 여기로 말이죠, 이것이 이제는 표에 가려질텐데요, 결과적으로 이상적으로는 전역최적점으로 모이거나
근접한 지점에 도달할 것입니다. 이 그림은 기울기 강하 알고리즘을 나타내고 있습니다. 조금 더 상세히 적어보겠습니다. 조금 더 쉽게 나타내기위해
J(w)라는 함수가 있다고 해보겠습니다. 이 값을 최소화시키고 싶은데요, 
이렇게 생겼다고 해보겠습니다. 더 쉽게 그리리 위해서, 
일단은 b를 무시하고, 다차원이 아닌 1차원적인 표로 만들겠습니다. 기울기 강하 가 이렇게해서, 해당 update를 계속 반복적으로 진행할 것입니다. w값을 갖고 업데이트할 것입니다. 콜론표시로 w를 업데이트함을 표시할텐데요, w를 w 빼기 알파로하고, 이것은 derivative dJ(w)/dw입니다. 이것을 알고리즘이 합쳐질때까지 반복하겠습니다. 알아둘 점은, 여기 알파는 학습 속도를 뜻하고, 한번에 얼마만큼 기울기 강하를 진행할 수 있는지 
조절해줍니다. 학습 속도인 알파를 어떻게 선택하는지는 나중에 자세히
이야기하겠습니다. 2번째로, 여기 이 값은 derivative입니다. 이것은 업데이트 또는 w에 얼마나 변화를 줄지 여부를 알려주는 값입니다. 기울기 강하를 도입하기 위한 코드를 만들기 시작하면 dw라는 변수가 derivative를 나타내도록 규칙을 만들 것입니다. 그렇게해서 코드를 만드는 경우, w : 는 w 빼기 알파 곱하기 dw 라고 적어볼 텐데요 dw를 변수 이름으로 만들어서 이 derivative 항을 뜻하게 만듭니다. 자 이제 해당 기울기 강하 업데이트가 말이 되도록
만들겠습니다. w가 여기 있었다고 하죠. 그러면 비용함수 J(w)에서 이 지점에 있습니다. 기억할 것은, derivative의 정의가 특정 지점에서 함수의 기울기라는 것입니다. 함수의 기울기는 높이 나누기 너비, 맞죠? 이 작은 삼각형에서 말이죠, 
기울기는 J(w) 에서 이 탄젠트 지점이 됩니다. 그렇게해서, derivative는 양수입니다. W는 w 빼기 학습 속도 곱하기 derivative 값만큼 업데이트 
되는 것입니다. derivative값은 양수이고, 
그렇기 때문에 w에서 빼게되는 것인데요 그러므로 왼쪽으로 진행합니다. 만약 이 큰 w값으로 시작했다고하면 기울기 강하는 알고리즘이 천천히
파라티터를 줄이도록 할 것입니다. 다른 예제로, 만약 w가 여기 있었다고 하면 이 지점에서는 dJ/dw의 기울기는 
음수일 것입니다. 이 경우, 기울기 강하 업데이트는 알파 곱하기
음수의 값을 뺄 것입니다. 결과적으로 w를 천천히 증가시킬텐데요, 
w를, 기울기 강하 의 반복을 통해 점점 더 크게 만드는 효과가 있습니다. 그러므로 왼쪽에서 초기화를 진행하던 오른쪽에서 진행하던, 기울기 강하는
이 전역 최소값을 향해 움직일 것입니다. 여러분이 derivative이나 미적분학에 익숙해 있지 
않으신 경우이거나, dJ(w)/dw가 뜻하는 바가 무엇인지 잘 모르시더라도
너무 걱정하지 마십시요. derivative에 대해서는 다음 비디오에서 더욱 자세히 다루겠습니다. 미적분학을 잘 아시면, 신경망이 어떻게 작동하는지 잘 이해하실 수 있을 것입니다. 하지만, 미적분학에 익숙하지 않더라도 다음 비디오를 통해, derivative와 미적분학에 대한 직관적인 부분을 다뤄서 신경망을 잘 다룰 수 있도록 하겠습니다. 일단은 이런 항이 기울기를 뜻한다는 것을 이해하시고, 현재 매개 변수 값에서의 함수 기울기를 찾고 가장 높은 기울기에서, 비용함수 J인 경우, 어느 방향으로 이동할지 여부를 알아내는 것입니다. 오로지 w가 파라티터인 경우, 
J(s)에 대한 기울기 강하를 적었습니다. 로지스틱 회귀분석에서는 
비용함수가 w와 b라는 파라티터에 대한 함수입니다. 이 경우, 기울기 강하의 inner loop는, 
이와 같이 생긴 것인데요, 반복해야하는 이 것은
아래와 같습니다. w를 w 빼기 학습 속도 곱하기 w에 대한 J(w, b) derivative로 업데이트 합니다. b는 b빼기 학습속도 곱하기 b에 대한 비용함수의 derivative로 b를
업데이트 합니다. 그래서 이 밑에 있는 2개의 공식이 
실제로 도입하는 업데이트입니다. 따로 말씀 드릴 부분이 있는데요, 
미적분학에서 쓰는 표기가 어떤 분들에게 조금 헷갈릴 수 있는데요. 미적분학을 이해하는 것이 굉장히 중요하다고
할 수 없지만 이런 것을 보면, 너무 많이 생각하지 않으시길 바랍니다. 여기서 보이는 미적분학에서 쓰는 항은 이렇게 쓰는데요, 이 구부려진 모양입니다. 이 심볼은, 이렇게 멋지게 쓰인 것은 소문자 d이구요 조금 특화된 글꼴로 쓰는데 
이러한 표기를 보면, J(w, b)의 기울기라는 것입니다. w방향으로 J(w, b)함수가 얼마나 기울고 있는지를 나타냅니다. 미적분학에서의 규칙이
아주 타당하다고는 생각되지 않지만, 이 규칙이 오히려 더 헷갈리게 만드는 것 같은데요 만약 J가 2개 또는 그 이상의 변수로 이루어져 있을 시, 소문자 d를 쓰는 대신에 
이 우습게 생긴 심볼을 사용합니다. 이것을 partial derivative 심볼이라고 하는데요. 일단 신경쓰지 마십시요. 그리고 J가 만약 한개의 변수를 가진 함수라면
소문자 d를 사용합니다. 그렇기 때문에 이 우습게 생긴 partial derivative 심볼을 사용하는 경우와 소문자 d를 위와 같이 사용하는 경우의 차이는 J 함수가 2개 이상의 변수를 가지고 있는지의 차이입니다. 그런 경우, 이 partial derivative 심볼을 쓰고, 1개의 변수가 있는 경우, 소문자 d를 사용합니다. 이런 것이 바로 미적분학에서 쓰이는 조금은 웃긴 표기법인데요. 오히려 더 헷갈리게 만드는 것 같습니다. partial derivative 심볼를 보시면 단순히 함수의 기울기가 이 변수들 중 하나에 대하여 계산된다고 생각하면 됩니다. 비슷하게, 앞서 다룬 미적분학 표기법을 동일하게 적용합니다, 그 이유는 여기서는 J가 2개의 입력값이 있기 때문이죠. 이 밑에 있는 부분은 이 partial derivative 심볼과 같이
쓰여야 합니다. 이 것은 거의 똑같은데요, 소문자 d가 의미하는 것과
거의 비슷합니다. 마지막으로 이것을 코드에 도입시킬 때는, 이 양이, 즉 w를 업데이트하는 정도의 양을, dw 변수로 코드에서는 표기될텐데요 이 값 맞죠? b를 업데이트하고 싶은 정도는 코드에서의 db 변수로 표기될 것입니다. 자 이렇게 gradiend descent를 도입할 수 있는 것입니다. 만약 여러분이 미적분학을 수년동안
접하지 않으셨다면, 지금 본인의 현재 편안한 정도의 레벨보다
미적분학에서 더 많은 derivative를 다룬다고 생각하실텐데요, 만약 여러분이 이렇게 느끼시면 걱정하지 마십시요. 다음 비디오에서는 derivative에 대한 조금 더 직관적인 이해를 
돕도록 하겠습니다. 미적분학에 대한 깊은 수학적인 지식없이도 직관적인 미적분학의 이해만으로도 신경망을 조금 더 효율적으로 만들 수 있을 것입니다. 다음 비디오로 넘어가서 derivatice에 대해 조금 더 이야기해보도록 하겠습니다.