1
00:00:00,006 --> 00:00:04,743
이전 비디오에서는 신걸 숨겨진 레이어 
신경망이 어떻게 생겼는지 보았는데요,

2
00:00:04,743 --> 00:00:08,175
이번 비디오에서는 정확히 어떻게 신경망들이 결과값을

3
00:00:08,175 --> 00:00:10,361
산출해내는지 한번 보도록 하겠습니다.

4
00:00:10,361 --> 00:00:15,533
그 절차는 마치 로제스틱 회귀분석방식과 
비슷한데요, 반복적으로 진행이 됩니다.

5
00:00:15,533 --> 00:00:16,423
한번 자세히 살펴볼까요.

6
00:00:16,423 --> 00:00:19,364
이것이 two layer 신경망이 생긴 모양인데요,

7
00:00:19,364 --> 00:00:23,973
어떻게 결과값을 산출하는지 
한번 자세히 살표보도록하죠.

8
00:00:23,973 --> 00:00:26,653
이전에 이야기 했지만, 로지스틱 회귀분석은,

9
00:00:26,653 --> 00:00:31,035
여기 이 동그란 이미지에서 2가지 절차로 이루어지는데요,

10
00:00:31,035 --> 00:00:34,498
첫번째 단계로는 Z의 값을 이와 같이 구하고

11
00:00:34,498 --> 00:00:37,754
두번째로는, Z의 시그모이드 함수로 activation을 산출합니다.

12
00:00:37,754 --> 00:00:40,536
신경망에서는 이러한 
절차를 여러번 진행 하는 것입니다.

13
00:00:40,536 --> 00:00:43,953
한번 숨겨진 레이어 에 있는 한개의 노드를 예로 시작해볼까요

14
00:00:43,953 --> 00:00:46,320
그럼 숨겨진 레이어에 있는 첫번때 노드를 보겠습니다.

15
00:00:46,320 --> 00:00:48,079
다른 노드들은 일단 잘 안보이게 하겠습니다.

16
00:00:48,079 --> 00:00:50,820
왼쪽의 로지스틱 회귀분석과 비슷하게

17
00:00:50,820 --> 00:00:54,391
여기 이 숨겨진 레이어의 노드는 
2가지의 계산 단계가 있는데요.

18
00:00:54,391 --> 00:00:58,418
첫번째는, 여기 이 노드 반쪽에서 왼쪽으로 생각하십시요.

19
00:00:58,418 --> 00:01:02,754
이 것은 z는 w transpose x 더하기 B이구요

20
00:01:02,754 --> 00:01:08,253
표기는 여기서와 같이 첫번째 
숨겨진 레이어에서 속하는 양을 나타내는데요,

21
00:01:08,253 --> 00:01:13,458
그렇기 때문에 이렇게 대괄호들이 많이 쓰였습니다. 
이것은 그리고 숨겨진 레이어의 첫번째 노드입니다.

22
00:01:13,458 --> 00:01:16,597
그렇기 때문에 위첨자에 1로 표시하는 것입니다.

23
00:01:16,597 --> 00:01:18,424
처음에는 이렇게 나타내고

24
00:01:18,424 --> 00:01:24,419
두번째 절차는 a1 은 z1의 시그모이드 함수를 계산합니다.

25
00:01:24,419 --> 00:01:29,013
그러므로 z와 a 에 대해 공식 표기는 여기서

26
00:01:29,013 --> 00:01:33,770
a, l , i 위첨자 대괄호 l은 층의 번호를 나타내고

27
00:01:33,770 --> 00:01:37,720
여기 i 첨자는 층에 있는노드를 표현합니다.

28
00:01:37,720 --> 00:01:42,344
그럼 저희가 볼 것은 1번 층인데요, 
즉 숨겨진 레이어에서 첫번째 노드입니다.

29
00:01:42,344 --> 00:01:45,878
그렇기 때문에 위첨자와 아래첨자가 모두 1이였던 것입니다.

30
00:01:45,878 --> 00:01:49,965
여기 동그라미에서 첫번째 노드는

31
00:01:49,965 --> 00:01:52,579
이 2가지 계산 절차를 진행하는 것임을 표현해줍니다.

32
00:01:52,579 --> 00:01:58,399
자 그럼 이제 네트워크의 2번째 노드를 보겠습니다.
숨겨진 레이어에서의 2번째 노드 말이죠.

33
00:01:58,399 --> 00:02:01,482
마찬가지로 로지스틱 회귀분석을 
나타내는 왼쪽의 그림과 같이,

34
00:02:01,482 --> 00:02:04,781
여기 조금한 동그라미가 2가지 계싼 절차를 나타내죠.

35
00:02:04,781 --> 00:02:08,733
첫번째는 z값을 계산하는건데요, 
이것은 아직 1번 층입니다.

36
00:02:08,733 --> 00:02:12,996
이제 2번째 노드는 w = transpose x 더하기 v입니다.

37
00:02:12,996 --> 00:02:17,880
그리고 a 1, 2 = z의 시그모이드 1, 2입니다. 
여러분이 언제든 비디오를 멈추셔서지

38
00:02:17,880 --> 00:02:23,071
확인하셔도 되는데요, 
위첨자와 아래첨자 표기가 위의 보라색으로

39
00:02:23,071 --> 00:02:28,453
적었던 내용과 동일한 원리라는 
것을 다시 한번 확인할 수 있습니다.

40
00:02:28,453 --> 00:02:32,831
신경망에 있는 위의 2개 숨겨진 유닛에 대해 알아봤는데요,

41
00:02:32,831 --> 00:02:36,940
유닛 3가 4도 마찬가지로 비슷한 계산법이 적용됩니다.

42
00:02:36,940 --> 00:02:39,778
그럼 이제 여기 이 공식과

43
00:02:39,778 --> 00:02:44,169
여기 이 공식을 다음 슬라이드에 복사해보겠습니다.

44
00:02:44,169 --> 00:02:48,921
여기 네트워크가 보이구요, 여기는 쳣번째 
공식을 나타내고 있구요, 여기는 2번째 공식입니다.

45
00:02:48,921 --> 00:02:54,050
이 2개는 저희가 첫번째와 두번째 
숨겨진 유닛에서계산했던 공식이죠.

46
00:02:54,050 --> 00:02:59,022
그런 다름에 여러분이 이와 같이 
비슷하게 세번째와 네번째도 공식을 적용하면,

47
00:02:59,022 --> 00:03:02,093
세번째와 네번째 숨겨진 유닛에 대해 말이죠, 
다음과 같이 말이죠.

48
00:03:02,093 --> 00:03:06,550
이 표기를 확실하게 하기 위해 한번 보겠습니다. 
이것은 w11 인데요,

49
00:03:06,550 --> 00:03:09,430
이것은 vector transpose 곱하기 x입니다.

50
00:03:09,430 --> 00:03:13,460
여기 위첨자 t가 나타내는 것입니다.
이것은 vector transpose입니다.

51
00:03:13,460 --> 00:03:17,585
여러분도 짐작하셨겠지만, 
이것을 신경망 네트워크에 도입하는 경우에,

52
00:03:17,585 --> 00:03:20,209
이것을 4개의 루프로 하는 것은 
비효율적이라 생각하실 것입니다.

53
00:03:20,209 --> 00:03:25,174
그렇기 때문에 이 4개의 
공식을 갖고 벡터화시킬 것입니다.

54
00:03:25,174 --> 00:03:29,348
그러면 저는 먼저 z를 벡터로 
산출하는 방법을 보여드릴 것입니다.

55
00:03:29,348 --> 00:03:30,859
이렇게 하실 수 있습니다.

56
00:03:30,859 --> 00:03:34,843
일단 여기 W들을 가지고 매트릭스에 쌓도록 하겠습니다.

57
00:03:34,843 --> 00:03:38,767
그러면 여기와 같이 w 1 1 transpose 와 같은 세로 벡터인데요

58
00:03:38,767 --> 00:03:42,231
이렇게 줄의 형식 벡터로 나타낼 수 있습니다.

59
00:03:42,231 --> 00:03:48,494
이어서 w 1 2 transpose , 
w 1 3 transpose w 1 4 transpose와 같습니다.

60
00:03:48,494 --> 00:03:54,499
이렇게 4개의 벡터를 합쳐서 메트릭스가 나옵니다.

61
00:03:54,499 --> 00:03:59,204
이것을 생각하는 방식은 보이는 것과 
같이 4개의 로지스틱 회귀분석법이 있죠.

62
00:03:59,204 --> 00:04:03,913
각각의 로지스틱 회귀분석 은 
연계되는 매개변수 w가 있죠.

63
00:04:03,913 --> 00:04:06,535
그리고 이런 4개의 벡터를 쌓으면서 결과적으로

64
00:04:06,535 --> 00:04:08,842
4 X 3 매트릭스가 생깁니다.

65
00:04:08,842 --> 00:04:14,281
그러면 이 매트릭스를 가지고 입력 특성 값인

66
00:04:14,281 --> 00:04:19,806
x1, x2, x3와 곱하면, 매트릭스가 곱해지는 과정에 의해

67
00:04:19,806 --> 00:04:24,546
w1 1 transpose x, w 2 1 transpose x, w 3 1 transpose x,

68
00:04:24,546 --> 00:04:30,995
w 4 1 transpose x 가 나옵니다. 
그러면 이제 b들을 알아보도록 하겠습니다.

69
00:04:30,995 --> 00:04:35,997
그러면 여기 벡터들에 더하는데요, b11 b12

70
00:04:35,997 --> 00:04:40,811
b13 in b14 그러면 이것은 이것과 같은데요,

71
00:04:40,811 --> 00:04:45,654
b11 b12 b13 b14 그러면

72
00:04:45,654 --> 00:04:50,579
여기 보이듯이 각각의 결과값에 대한 4줄은,

73
00:04:50,579 --> 00:04:55,772
여기 각각의 양과 위와 같이 똑같이 일치합니다.

74
00:04:55,772 --> 00:05:00,899
다시 말해, 여기 이 것이 그러므로 z1 1,

75
00:05:00,899 --> 00:05:05,303
z1 2, z1 3, z 1 4와 일치하게 됩니다.

76
00:05:05,303 --> 00:05:10,289
여기에서 정의된 것터럼 말이죠. 
그리고 놀라시진 않겠지만

77
00:05:10,289 --> 00:05:15,097
이 전체값을 저희는 z1이라고 부르겠습니다. 
이 값은 각각의 개인 값을 세로 벡터에 쌓아서 생깁니다.

78
00:05:15,097 --> 00:05:19,524
벡터화를 시키는 경우에, 
여러분께 도움이 될만한 경험에 의거한 규칙은,

79
00:05:19,524 --> 00:05:23,966
이것을 제대로 접근하도록 도움을 줄만한 방법은, 
이렇게 층별로 다른 노드가 있는경우, 세로로 쌓을텐데요,

80
00:05:23,966 --> 00:05:27,656
그렇기 때문에 z11에서 z14까지 있는 경우,

81
00:05:27,656 --> 00:05:31,852
이렇게 다른 노드들이숨겨진 레이어에서

82
00:05:31,852 --> 00:05:36,481
이 4개의 값을 세로로 쌓았습니다. 
결과적으로 벡터 z1을 구성하기 위해서 말이죠,

83
00:05:36,481 --> 00:05:40,457
한가지 표기를 더 보자면, 여기 이 4 X 3 매트릭스는

84
00:05:40,457 --> 00:05:45,233
소문자 w11, w12, 등등을 쌓아서 생긴 것인데요,

85
00:05:45,233 --> 00:05:49,860
이 매트릭스를 대문자 W 1이라고 부를 것입니다.
 이 벡터는그리고 비슷하게

86
00:05:49,860 --> 00:05:54,623
B 위첨자 대괄호 1이라고 부를 것입니다.

87
00:05:54,623 --> 00:05:59,584
이제 그러면 저희는 벡터 매트릭스 
표기를 통해서 Z의 값을 계산했는데요,

88
00:05:59,584 --> 00:06:03,535
저희가 마지막으로 해야하는 것은 
여기 이 a의 값을 계산하는 것입니다.

89
00:06:03,535 --> 00:06:08,195
놀랍지 않게도 a1을 여기 이런 activation 값들, a11에서

90
00:06:08,195 --> 00:06:13,019
a14 까지의 값들을 쌓아서 정의할 것입니다.

91
00:06:13,019 --> 00:06:18,202
그러므로 여기 4개의 값을 가져서 쌓도록 합니다.

92
00:06:18,202 --> 00:06:21,122
a1이라는 벡터에 말이죠.
그러면 이것은 z1의 시그모이드가 될 것입니다.

93
00:06:21,122 --> 00:06:25,794
이것은 이제 시그모이드 함수의 도입인데요,

94
00:06:25,794 --> 00:06:30,761
이 것은 z의 4가지 요소를 갖고 
시그모이드 함수에 적용되는 것입니다.

95
00:06:30,761 --> 00:06:36,750
element wise가 적용되는 것이죠.
저희는 z1이 w1 곱하기
벡터 x

96
00:06:36,750 --> 00:06:41,883
더하기 벡터x 더하기 벡터 b1이라는 것을 알아냈는데요.
그리고 a1은 시그모이드 곱하기 z1입니다.

97
00:06:41,883 --> 00:06:47,321
이것을 다음 슬라이드에 복사하겠습니다 
입력값 x에 대해 신경망의 첫번째 층에

98
00:06:47,321 --> 00:06:52,156
z1은 w1 곱하기 X 더하기 B 1인데요,

99
00:06:52,156 --> 00:06:56,286
a1은 z1의 시그모이드 이구요,

100
00:06:56,286 --> 00:07:01,526
이것의 다이멘션은

101
00:07:01,526 --> 00:07:06,563
4 x 1 = 이것은 4 x 3 매트릭스이고, 
곱하기 3 x 1벡터 더하기 4 x 1 벡터인 b가 더하잽니다.

102
00:07:06,563 --> 00:07:11,297
그리고 이것은 이것과 다이멘션이 동일하게 4 x 1입니다. 
그리고 기억하시겠지만,

103
00:07:11,297 --> 00:07:16,793
x는 0이라고 했죠. ŷ이 a2인 것과 같이 말이죠.

104
00:07:16,793 --> 00:07:21,560
그렇기 때문에 원하면 이 x값을 a0로 대입해도 됩니다.

105
00:07:21,560 --> 00:07:25,417
입력특성 x 벡터에 가명이기 때문에 대입해도 됩니다.

106
00:07:25,417 --> 00:07:30,968
비슷한 derivation 방법으로 다음 층의 표기도

107
00:07:30,968 --> 00:07:35,972
비슷하게 나타낼 수 있는데요, 결과값이 하는 것은,

108
00:07:35,972 --> 00:07:40,770
w2와 b2와 연계가 될텐데요,

109
00:07:40,770 --> 00:07:44,549
이런 매개 변수의 경우, 
이것은 1x 4매트릭스가 될 것입니다.

110
00:07:44,549 --> 00:07:47,529
그리고 b2는 (1, 1) 실수가 됩니다

111
00:07:47,529 --> 00:07:51,982
그렇기 때문에 z2 가 (1, 1) 
실수인 매트릭스가 될 것입니다.

112
00:07:51,982 --> 00:07:57,267
이것은 1, 4가 될 것이고, 이것은 4, 1 이였죠. 
더하기 b2는 1, 1입니다.

113
00:07:57,267 --> 00:08:02,397
그렇기 때문에 이것은 또한 실수가 됩니다.

114
00:08:02,397 --> 00:08:07,787
여기 이 결과값을 로지스틱 회귀분석 방식으로 생각하면, 
w와 b의 매개 변수를 갖는 것 말이죠.

115
00:08:07,787 --> 00:08:12,517
W는 실제로 w[2] transpose와 유사한 역할을합니다.

116
00:08:12,517 --> 00:08:16,675
W는 W transpose와 같 b는 b2와 같습니다.

117
00:08:16,675 --> 00:08:21,665
네트워크의 이 왼쪽에 있는 
부분을 일단 무시하겠습니다.

118
00:08:21,665 --> 00:08:26,434
그러면 여기 이 마지막 결과값은 
로지스틱 회귀분석방식과 매우 유사한데요,

119
00:08:26,434 --> 00:08:30,010
단지 w와 b로 매개 변수를 갖는 대신에,

120
00:08:30,010 --> 00:08:35,784
변수를 w2와 b2로 쓰는 것입니다.
1 x 4 다이멘션과 1 x 1 다이멘션으로 말이죠.

121
00:08:35,784 --> 00:08:39,765
로지스틱 회귀분석이 결과값을

122
00:08:39,765 --> 00:08:44,620
도입하기 위해서는 또는 예측하기 위해서는 
Z를 w transpose x 더하기 b로 나타냅니다.

123
00:08:44,620 --> 00:08:51,143
그리고 ŷ은 a이고, z의 시그모이드입니다.

124
00:08:51,143 --> 00:08:55,499
만약 숨겨진 레이어가 1개인 
신경망의 경우, 해야하는 것은,

125
00:08:55,499 --> 00:09:00,131
이런 결과값을 갖기 위해서 말이죠,
여기 이 4개의 공식을 도입합니다.

126
00:09:00,131 --> 00:09:04,902
이것을 결과값을 도출하기 위한 
벡터화 도입이라고 생각하면 쉽습니다.

127
00:09:04,902 --> 00:09:09,329
여기 숨겨진 레이어의 로지스틱 
회귀분석 유닛에 대해 말이죠.

128
00:09:09,329 --> 00:09:13,867
이것이 하는 역할이 그렇구요, 이것은 
결과값 층의 regression인데요, 이것이 하는 것입니다.

129
00:09:13,867 --> 00:09:18,401
이러한 표현이 이해가 됐었으면 
좋겠습니다. 기억하실 것은,

130
00:09:18,401 --> 00:09:22,001
이런 신경망의 결과값을 계산하기 
위해서는 여기 4개 라인의 코드만 필요합니다.

131
00:09:22,001 --> 00:09:25,706
1개의 입력 특성 벡터값이 주어지면, 
4개의 코드를 활용하여,

132
00:09:25,706 --> 00:09:30,278
4개의 코드를 활용하여, 계산할 수 있습니다.

133
00:09:30,278 --> 00:09:34,575
로지스틱 회귀분석과 비슷하게, 복수의 트레이닝 
샘플을 가지고 벡터화하고 싶을 것입니다.

134
00:09:34,575 --> 00:09:39,002
가지고 벡터화하고 싶을 것입니다.
이런 경우, 트레이닝 예시들을 줄별로

135
00:09:39,002 --> 00:09:43,653
트레이닝 시킴으로써 매트릭스를 
이용해서 말이죠, 아니면 약간의 변형을 주어,

136
00:09:43,653 --> 00:09:47,396
로지스틱 회귀분석에서 본 것과 같이,

137
00:09:47,396 --> 00:09:50,514
신경망의 결과값을 예시 별로 계산하는 것이 아니라

138
00:09:50,514 --> 00:09:55,114
전체 트레이닝 세트에서 결과값을 찾을 수 있습니다.

139
00:09:55,114 --> 00:09:57,939
이 내용은 다음 비디오에서 다루겠습니다.