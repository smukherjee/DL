Привіт і з поверненням. На цьому тижні ми збираємось поговорити про основи
 програмування нейронних мереж (НМ). Виявляється, що для реалізації НМ існує кілька дуже важливих методик. Наприклад, маючи тренувальний набір з m тренувальних зразків, ми, вірогідно, використаємо цикл for для покрокової обробки своїх m тренувальних зразків. Проте, в той же час, при реалізації НМ нам захочеться обробити весь тренувальний набір без явного використання циклу for для всього тренувального набору. Тож ти побачиш як це зробити на цьому тижні. Інша методика - це те, що,
 під час побудови алгоритму обчислень нашої мережі, нам, зазвичай, потрібні "прямий шлях" або "пряме поширення" і подальший "зворотній шлях" або "зворотнє поширення". Тож на цьому тижні, ти також дізнаєшся чому алгоритм обчислень для навчання НМ використовує окремо пряме і зворотнє поширення. На цьому тижні я хочу описати ці методики за допомогою логістичної регресії (ЛР), щоб їх легше було зрозуміти. Проте, навіть якщо ти вже чув/ла раніше про ЛР, думаю, що і для тебе знайдуться цікаві ідеї на цьому тижні. Тож почнемо. ЛР - алгоритм двійкової класифікації. Тож давай почнемо з визначення задачі. Ось приклад задачі двійкової класифікації. Наприклад, ти маєш на вході ось таке зображення і хочеш отримати мітку "кіт" (в цьому випадку на виході буде 1) або "не кіт" 
(в цьому випадку на виході буде 0). Ми будемо використовувати y для позначення вихідної мітки. Давай розглянемо як зображення представлене в комп'ютері. Щоб зберігати зображення, комп'ютер зберігає три окремі матриці, що відповідають червоному, зеленому і синьому 
колірним каналам цього зображення. Тож, якщо наше вхідне зображення має розмір 64*64 пікселів, то ми матимемо три 64*64 матриці, що відповідають червоним, зеленим і синім значенням 
інтенсивності пікселів нашого зображення. Хоча на цьому маленькому слайді я намалював значно менші матриці. Це насправді матриці 5*4, а не 64*64. Тож, щоб перетворити ці значення інтенсивності пікселів 
у вектор ознак, ми розгорнемо всі ці значення у вхідний вектор ознак x. А щоб розгорнути всі ці значення інтенсивності 
пікселів у вектор ознак, ми визначити вектор ознак x, 
що відповідає цьому зображенню, ось так. Ми просто візьмемо всі ці значення 255, 231 і т.д. 255, 231 і т.д., поки не перерахуємо всі червоні пікселі. Десь далі - 255, 134, 255, 134 і т.д. доки не отримаємо довгий вектор ознак, що міститиме всі червоні, зелені та сині значення інтенсивності пікселів цього зображення. Тож, якщо це зображення має розмір 64*64, то сумарний розмір цього вектора x буде 64*64*3, тому що це сумарна кількість чисел, які ми маємо у всіх цих матрицях. Що, в цьому випадку, перетворюється на 12 288. Це те, що ми отримаємо, перемноживши ці числа. Тож ми будемо використовувати nₓ = 12 288, щоб позначити розмір вхідного вектора ознак x. А також інколи, для скорочення, 
я буду також використовувати маленьку n, для позначення розміру вхідного вектора ознак. Тож у двійковій класифікації наша мета - 
натренувати класифікатор, який може приймати на вхід зображення, представлене цим вектором ознак x, і передбачити відповідну мітку y: 1 або 0, тобто, "кіт" це зображення чи "не кіт". Тепер давай приймемо деякі позначення, які ми будемо використовувати в цьому курсі. Одиничний тренувальний зразок представлений парою (x, y), де x - вектор ознак розміром x, а y - мітка (0 або 1). Наш тренувальний набір міститиме 
маленьку m тренувальних зразків. Тож наш тренувальний набір записуватиметься (x₁, y₁), тобто вхідні і вихідні дані нашого 1-ого тренувального зразка, (x₂, y₂) для 2-ого тренувального зразка, і так до (xₘ, yₘ), 
тобто останнього нашого тренувального зразка. І все це разом - це весь наш тренувальний набір. Тож я буду використовувати маленьку m 
для позначення кількості тренувальних зразків. Також інколи, щоб підкреслити, що це - кількість 
тренувальних зразків, я писатиму m з нижнім індексом train. А коли ми говоритимемо про тестувальний набір, ми інколи використовуватимемо m з нижнім індексом test, 
щоб позначити кількість тестувальних зразків. Тож це - кількість тестувальних екземплярів. І нарешті, щоб компактно позначити всі тренувальні зразки, ми введемо матрицю велике X, яка являє собою складені в стовпчики x₁, x₂ і т.д. наш тренувальний набір. Тож ми беремо x₁ і записуємо його як 1-ий стовпчик матриці, x₂ і записуємо як 2-ий стовпчик і т.д. до xₘ. Це і буде матрицею велике X. Тож ця матриця X матиме m стовпців, де m - це кількість тренувальних зразків, а кількість рядків, або висота, цієї матриці - це nₓ. Май на увазі, що в інших випадках тобі може зустрітись матриця велике X, в якій тренувальні зразки будуть зберігатись в рядках. Ось так. Транспоноване x₁ і так до транспонованого xₘ. Виявляється, що коли ми реалізуємо НМ, використовуючи позначення зліва (згідно конвенції), реалізація буде значно простішою. Резюмуємо. X - матриця розміром nₓ*m. І, якщо ми реалізуємо це на Python, то побачимо x.shape() (це команда Python для знаходження розміру матриці), що повертає (nₓ, m). Це означає, що розмір матриці - nₓ*m. Тож ось так складаються вхідні тренувальні зразки x в матрицю. Як щодо вихідних міток y? Виходить, що для спрощення реалізації НМ буде зручно і y записати у стовпчик. Тож нехай велике Y дорівнює y₁, y₂ і до yₘ. Ось так. Тож Y тут буде матрицею розміром 1*m. І знову ж, на Python, Y.shape() буде (1, m). Це означає матрицю розміру 1*m. Реалізуючи свою нову НМ пізніше 
в рамках цього курсу, ти побачиш, що буде зручно діяти згідно конвенції, тобто всі дані, 
пов'язані з різними тренувальними зразками (під даними я маю на увазі X, або Y, 
або інші величини, які ти побачиш пізніше), все необхідне - дані, пов'язані з різними тренувальними зразками - складати стовпчиками, як ми це робили для X і Y. Тож це - позначення, які ми використовуватимемо 
для ЛР і НМ далі в цьому курсі. Якщо ти забудеш що означає якесь позначення, наприклад, m чи n, чи будь-що інше, ми також опублікували на сайті курсу 
інструкцію з позначеннями, якою ти можеш користуватись, щоб швидко знайти 
що означає будь-яке позначення. Тож давай перейдемо до наступного відео, де ми почнемо розбирати ЛР використовуючи ці позначення.