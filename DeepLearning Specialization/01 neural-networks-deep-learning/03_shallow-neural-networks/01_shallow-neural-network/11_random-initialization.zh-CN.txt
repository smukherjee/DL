当你开始训练神经网络时 将权重参数进行随机初始化非常重要 在逻辑回归的问题中
把权重参数初始化为零是可行的 但把神经网络的权重参数全部初始化为零
并使用梯度下降 将无法获得预期的效果 让我们来看看为什么 这里有两个输入样本参数 因此n[0]等于2 还有两个隐藏单元
因此n[1]也等于2 所以与隐藏层关联的权重w[1] 是一个2x2的矩阵 现在我们将这个矩阵的初始值都设为0 同样我们将矩阵b[1]的值也都初始化为零 偏离度矩阵b[1]的初始值都是0
不会影响最终结果 但是将权重参数矩阵w[1]初始值都设为零
会引起某些问题 这样的初始权重参数会导致 无论使用什么样的样本进行训练 a[1]1与a[1]2始终是相同的 这第一个激活函数和这第二个激活函数将是完全一致的 因为这些隐藏神经元在进行完全相同的计算工作 当你进行反向传播的计算时 由于对称问题 这些隐藏单元将会在同样的条件下被初始化 最终导致z[1]1的导数和dz[1]2的导数也不会有差别 同样的 我假设输出的权重也是相同的 所以输出权重参数矩阵w[2]也等于[0,0] 但如果你使用这种方法来初始化神经网络 那么上面这个隐藏单元和下面这个隐藏单元也是相同的 它们实现的是完全相同的功能 有时候我们也称这是“对称”的 我们来归纳一下这个结果 经过每一次训练迭代 你将会得到两个 实现完全相同功能的隐藏单元 在之前的视频中W的导数将会是一个矩阵 大概看上去是这样 每一行都是相同的 然后我们进行一次权重更新 当你在实际操作时 w[1]将被更新成w[1]-α*dw 而你将会发现 经过每一次迭代后 w[1]的第一行与第二行是相同的 所以根据上述信息来归纳 我们可以得到一个证明结果 如果你在神经网络中
将所有权重参数矩阵w的值初始化为零 由于两个隐藏单元肩负着相同的计算功能 并且也将同样的影响作用在输出神经元上 经过一次迭代后 依然会得到相同的结果 这两个隐藏神经元依然是“对称”的 同样推导下去 经过两次迭代
三次迭代 以及更多次迭代 无论你将这个神经网络训练多久 这两个隐藏单元仍然在使用同样的功能进行运算 在这个例子中 由于隐藏单元实现的都是相同的功能 所以我们使用一个就够了 在更大的神经网络中 假设我们有三个输入特征值 以及非常多的隐藏单元 一个类似的结论也同样成立 就像我现在画的这样 我不会画出所有的连接线 如果你依然将权重参数矩阵初始化为零
那么无论你 运行梯度下降多长时间
所有的隐藏神经元都将是“对称”的 它们依然将运行在完全相同的功能下 而这并不能给我们带来任何帮助 因为我们希望两个不同的隐藏单元能实现不同的功能 因此只有进行随机初始化能够解决这样的问题 让我们看看如何在Python中操作 我们通常使用w[1]=np.random.randn((2,2))*0.01 这样的写法来对这个2*2的矩阵进行随机初始化 并乘上一个非常小的数 比如0.01 这样操作后 你已经将权重参数矩阵赋予了非常小的随机初始值 然后对于b来说 b并不会由于初始值为零而产生对称问题 或称之为对称失效问题 所以使用b[i]=np.zeros((2,1))将b矩阵初始值设为零 好了 现在权重参数矩阵w已经完成了随机初始化 不同的隐藏单元会承担不同的计算工作 我们也不会再遇到类似前面说的对称失效问题了 然后我们可以用同样的方法来将w[2]进行随机初始化 b[2]也依然可以初始化为零 好了 现在你想知道的可能是
为什么使用这个常量 为什么是0.01 为什么我们不把它设置为100或1000？ 主要原因是 我们通常比较喜欢使用 非常非常小的随机初始值 而当你使用tanh或者sigmoid的激活函数时 或者在输出层使用了sigmoid函数 如果步长过大 那么当你要计算激活值的时候 你应该还记得z[1]=w[1]*x+b[1] 而a[1]=g[1]*(z[1]) 所以 当w非常大的时候 z（的绝对值）也相应的会非常大<br />或者说z可能是一个非常大的数或是一个非常小的数 在这样的情况下 你可能最终会发现图上 tanh和sigmoid函数中 这些相对平坦的部分<br />梯度的斜率非常小 这意味着梯度下降会非常缓慢 所以整个学习过程也会变得尤为缓慢 概括一下 如果w过大 你很容易在开始时就 得出一个非常大的z 而这会导致你的tanh和sigmoid激活函数学习进度缓慢 无法实现预期功能 如果在你的神经网络中未使用 任何sigmoid或者tanh激活函数 这种情况可能不明显 但是如果你使用二分类 并且你的输出神经元 使用了sigmoid函数 那么你不会希望初始参数过大 所以这就是为什么我们说在上述公式中需要乘以0.01 或者其他比较小的数值 对权重参数矩阵w2来说也适用 我们可以表示成 w[2]=np.random.randn((1,2))*0.01 噢上面应该是zeros 这里还有另外一个结论分享给大家
有时候会有比0.01更为合适的数值 当你在训练一个仅含一个隐藏层的神经网络时 显而易见 0.01这个数值在类似于这样不含过多隐藏层的 浅层神经网络中是非常合适的 但当你要训练一个非常非常复杂的深度神经网络时 我们会使用一个不同的数值 在下周的课程中我们会简单说一下 如何根据不同的情况
来挑一个与0.01不同的数值 无论如何
通常情况我们的计算结果都会是一个相对小的数值 所以这周我们就先讲到这里 从这期的教学视频中
你应该学到了如何设计一个 含有单独隐藏层的神经网络
参数初始化 使用前向传播进行预测 以及在梯度下降时使用反向传播中 涉及的导数计算 所以现在你应该能完成课后测试和 本周的编程练习 祝你好运 我希望各位能喜欢我的课程
在其中学到知识 也希望在后几期课程中能继续见到各位 翻译 | 审阅
Cousera Global Translator Community