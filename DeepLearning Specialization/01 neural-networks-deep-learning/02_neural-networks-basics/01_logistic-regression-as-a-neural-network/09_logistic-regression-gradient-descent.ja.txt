おかえりなさい。 このビデオでは 微分を計算する方法を話します ロジステック回帰の勾配降下法を
実装できるようにするためです 覚えてほしいのは何を実装するかです つまり鍵となる数式で
ロジステック回帰での 勾配降下法を
実装するのに必要な数式です このビデオでは計算グラフを使って
この計算を行おうと思います ロジステック回帰の勾配降下法を
算出する目的には 多少労力を使い過ぎなことは認めますが この方法で説明を
徐々にしていき これらの考えを理解してもらえるようになり 完全なニューラルネットワークについて話す頃には
より理解しやすくなるようにしたいのです それではロジステック回帰の勾配降下法に
飛び込んでいきましょう 簡単に振り返ると
ロジステック回帰は次のようなものだと設定しました 予測のyハットは
このように定義され zはこの上のようになり もし１データの場合に焦点を当てるか
１点のデータに関しては 損失は次のように 定義されます a はロジステック回帰の出力で y は正解のラベルです ではこのデータの時の
これの計算グラフを書きましょう 特徴はX1 X2 の２つしかないと想定します z を計算するためには w1 w2 の入力が必要です そして特徴x1 x2 以外にも
b が必要です 計算グラフでは
これらが z を計算するのに使われます w1x1+w2x2+b で これを四角で囲みます そしてyハットを計算します a=σ(z)で これが計算グラフの次のステップです そして最後に L(a,y) を計算しますが
式はここには書き移しません ロジステック回帰では
目指すのはパラメーターの wやbを変更して
損失を減らすことです １つのサンプルに対しての損失を
計算するような 順誤差伝播法での計算方法を
記述していました 今回は微分を逆方向に計算していく方法を
話していきましょう これは奇麗に書き直した図です この損失に関しての微分を計算するのが
目的なので 逆方向に戻るときに
最初に行うのは このa に関する損失の微分を
計算します ここに少し書き加えます このコードでは da とこの変数を書きます 微積分学を知っていれば これが-y/a+(1-y)/1-aと
書けるでしょう もし微積分学を知っていれば 損失の式をつかって 小文字a の変数に対する微分を
計算できて この式を作れます もし微積分学を知らなくても
気にしないでください このコースを通して
必要なときには 微分の式を
お知らせします 微積分学のエキスパートだったら 微積分学を使って
前のスライドの損失の式を見て a に関する微分を
導いてみることをお勧めします 十分に微積分学を知らなくても
気にしないでください da の量と
a に関する最終的な出力変数を 計算し終わったら 逆方向に行くことができます dz を導けます これはPython コードの変数名ですが このdz は損失の微分のdLと合わせて dL/dz になります 損失についてはa y を
明示的に引数として書いても構いません どちらの表記方法も
同じように許容されます この式は
a-y と表現できます 微積分学をよく知っている人たちのために
少し追加でお知らせしますが よく知らなくても
気にしないでください このdL dz は dL/da da/dz と
表現できます da/dz は a(1-a)とできます dL/da は先ほど
このようになると確認しました この２つの量を使うと dL/da の確認した項と
da/dz の確認した項を使って この２つを乗算すると a-y のように単純化させて
表現することができます こう算出しました これこそが連鎖律で
この形は避けられるものです 微積分学に詳しければ
どうぞ全体の流れを計算してみてください でも詳しくない場合には
dz=a-y さえ知っておけばよいです あなたのために
この微積分の計算を行ってあります 誤差逆伝播法の最終ステップは
逆方向に進んで w とb を
いくら変化する必要があるかを計算します w1 に関する微分は
"dw1"と呼んで表現できます "dw1" =x1 dz です 同様にw2 についてのdw2は x2 dz です db=dz です １つのデータに対してだけ
勾配降下法を行いたい場合には このようにすることができます この式を使ってdz を計算して これらのdw1 dw2 db の式を使って この更新を行います w1はw1ひく 学習率のαかけるdw1です w2 も同様に更新できます b:=b-αdbです これが１つのデータに対しての
勾配降下法を行う１つの段階です １つの教師データの時に どうやって微分を計算し
ロジステック回帰の勾配降下法を実装するかを見てきました ロジステック回帰のモデルを学習させるには １つの教師データではなく
m個の教師データを与えるでしょう 見ての通り、ここある。 １つの教師データだけからではなく 教師データ全体から
学習させるように 適用する方法を
見ていきましょう