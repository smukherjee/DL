Коли ми реалізуємо зворотне поширення 
для нейронної мережі (НМ), то потрібно вміти обчислювати нахил або похідну функції активації. Тож давай розглянемо як вибрати функцію активації і як обчислити нахил цієї функції. Ось вже знайома сигмоїдальна функція активації. Для будь-якого значення z, скажімо оцього, ця функція матиме деякий нахил або деяку похідну, 
що відповідатиме (якщо намалювати тут маленьку лінію) відношенню висоти до ширини 
оцього маленького трикутника. 
[протилежного катета до прилеглого] Тож якщо g(z) - сигмоїда, то нахил функції =(d/dz)*g(z). Або, як ми знаємо з диференціального числення 
(диф.числення), - нахил g(z) в точці z. Якщо ти знайомий/а з диф.численням і знаєш як брати похідні, то знаєш, якщо брати похідну сигмоїди, то можна показати, що ми отримаємо оцю формулу. Знову ж таки, я не буду показувати кроки диф.числення. Але якщо ти знайомий/а з диф.численням, не вагайся поставити відео на паузу і довести це для себе. Тож, це =g(z)*(1-g(z)). Давай переконаємось в правдивості цієї формули. По-перше, якщо z дуже велике, скажімо z=10, то g(z) буде близьким до 1 і, згідно формули зліва, (d/dz)*g(z) буде близьким до g(z), що =1*(1-1), що, в результаті, буде близьким до 0. І це справді вірно, бо коли z дуже велике, нахил наближається до 0. І, навпаки, якщо z=-10 (знаходиться отут), то g(z) наближається до 0. Тож, згідно формули зліва, (d/dz)*g(z) буде наближатись до g(z), що =0*(1-0), тобто дуже близьке до 0, що теж правильно. Нарешті, якщо z=0, то g(z)=1/2. Ось ця точка сигмоїди. І, отже, похідна =(1/2)*(1-1/2), що =1/4. І це, дійсно, виявляється правильним значенням похідної або нахилу цієї функції при z=0. Також, представлю ще деякі позначення. Інколи, замість того, щоб писати оце, похідна скорочено позначається g штрих (z). Тож g'(z) (в диф.численні ця риска називається штрихом)... Тож g'(z) - це скорочене позначення похідної функції g по змінній z в диф.численні. В НМ ми маємо a=g(z), = оцьому. Тоді ця формула також спрощується до a(1-a). Інколи в реалізації ти можеш побачити щось типу g'(z)=a(1-a). То це лише позначення того, що g', що означає похідну, = оцьому отут. Перевага цієї формули в тому, що коли вже обчислено значення a, то, використавши це рівняння, 
можна також швидко обчислити значення нахилу g'. Добре. Це була сигмоїдальна функція активації. Давай тепер розглянемо функцію активації th 
[гіперболічний тангенс]. Аналогічно як і до цього, визначення (d/dz)*g(z) буде нахилом g(z) в точці z. Якщо глянути на формулу функції гіперболічного тангенсу і, знаючи диф.числення, то можна взяти похідну і показати, що вона спрощується до цієї формули. Використавши скорочене позначення, як і раніше, ми будемо також називати це g штрих від z. Тож, якщо хочеш, можеш перевірити цю формулу на правдивість. Наприклад, якщо z=10, то th(z) наближатиметься до 1. Оце знаходиться в межах від +1 до -1. Тож g'(z), згідно формули, буде 1-1², тож буде 0. Тож при дуже великому z нахил наближатиметься до 0. Навпаки, при дуже маленькому z, скажімо z=-10, th(z) наближатиметься до -1, а g'(z) наближатиметься до 1-(-1)², а це буде 1-1, тобто наближатиметься до 0. І, нарешті, якщо z=0, то th(z)=0. І тоді нахил буде =1, що й справді так, якщо z=0. Тож резюмуємо. Якщо a=g(z), а в даному випадку це th(z), то похідна g'(z)=1-a². Тож ще раз, якщо ми вже обчислили значення a, то можна використати цю формулу, щоб обчислити також і похідну. Врешті-решт, розглянемо як обчислити похідну 
для випрямленого лінійного вузла (ВЛВ) і нещільного ВЛВ як функцій активації. Для ВЛВ g(z)=max(0, z). Тож похідна, виявляється, буде =0, якщо z<0, і =1, якщо z>0. Технічно похідна є невизначеною для z=0. При програмній реалізації це, можливо, не на 100% математично правильно. Але працює як слід, при z точно =0, якщо ми прирівняємо його до 0 чи до 1. Це, типу, не має значення. Якщо ти експерт з оптимізації, то технічно g' стає так званим субградієнтом функції активації g(z), тому Градієнтний спуск працюватиме. Але можна також вважати, що ймовірність, 
що z буде =0,000000..., тобто дуже малим, 
що практично не матиме значення до чого ми прирівняємо похідну при z=0. Тож на практиці реалізація похідної відбувається ось так. І, нарешті, якщо ми тренуємо НМ з нещільним ВЛВ як функцією активації, то g(z) буде =max(0.01z, z). І, отже, g'(z) буде =0,01, якщо z<0, і =1, якщо z>0. Тож, ще раз. Технічно, градієнт невизначений коли z точно =0. Але якщо в коді реалізації ми визначимо похідну або призначимо g' або 0,01 або 1 (це несуттєво) при z точно =0. То код працюватиме. Тож, маючи ці формули, ти вже можеш обчислювати нахили або похідні функцій активації. Тепер, маючи цей будівельний блок, ти готовий/а побачити реалізацію Градієнтного блоку для НМ. Давай перейдемо до наступного відео, щоб це побачити.