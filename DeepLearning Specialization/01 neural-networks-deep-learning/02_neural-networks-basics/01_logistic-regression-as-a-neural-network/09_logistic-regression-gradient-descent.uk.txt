Вітаю з поверненням! У цьому відео ми поговоримо як обчислювати похідні, щоб реалізувати Градієнтний спуск для логістичної регресії (ЛР). Що потрібно винести з цього відео - що має бути реалізовано - це ключові рівняння, що потрібні для реалізації Градієнтного спуску для ЛР. Але в цьому відео я не буду робити обчислення в Діаграмі обчислень. Мушу відмітити, що використання Діаграми обчислень дещо зайве для виведення Градієнтного спуску для ЛР. Проте, хочу почати пояснювати таким чином, щоб ти міг/ла ознайомитись з основними ідеями, які, надіюсь, стануть зрозумілішими 
коли ми будемо говорити про повноцінні нейронні мережі (НМ). Тож, відштовхуючись від цього, 
давай заглибимось в Градієнтний спуск для ЛР. Згадаймо, що ми записуємо ЛР наступним чином. Передбачення ŷ записуються ось так, де z дорівнює ось цьому. Якщо ми розглянемо лише 1 зразок зараз, то Втрата по відношенню до цього зразка визначається ось так. Де a - результат ЛР, а y - емпіричні (Ground Truth) мітки. Тож давай запишемо це в Діаграмі обчислень. Наприклад, скажімо, ми маємо лише 2 характеристики x₁ та x₂. Щоб обчислити z нам потрібно, крім значень характеристик x₁ та x₂, додати на вхід також w₁, w₂ і b. Ці величини в Діаграмі обчислень використовуються для обчислення z, яке = w₁x₁+w₂x₂+b. Обведемо це прямокутником. А потім ми обчислюємо ŷ або a=σ(z). Це наступний крок в Діаграмі обчислень. І, нарешті, ми обчислюємо L(a, y). Я не буду копіювати ще раз формулу. Тож, в ЛР ми хочемо коригувати параметри w і b, щоб зменшити цю Втрату. Ми описали кроки прямого поширення, що показують як обчислити Втрату для окремого тренувального зразка Тепер давай поговоримо як піти у зворотному напрямку, 
щоб обчислити похідні. Візьмемо цю діаграму, так як ми хочемо обчислити похідні відносно цієї Втрати. Перше, що потрібно зробити у зворотному напрямку - це обчислити похідну Втрати по відношенню до оцього сценарію, по змінній a. Тож, в коді, ми будемо позначати цю змінну просто da. Якщо ти знайомий/а з диф.численням, то можеш вивести, що це =-y/a+(1-y)/(1-a). Щоб це зробити потрібно взяти формулу Втрати і, якщо ти знайомий/а з диф.численням, обчислити похідну відносно змінної позначеної маленькою a. Ти отримаєш цю формулу. Проте, якщо ти не знайомий/а з диф.численням, не переймайся. Ми будемо давати формули похідних і все, що необхідно, в цьому курсі. Тож, якщо ти експерт з диф.числення, то заохочую тебе взяти формулу Втрати з попереднього слайду і спробувати обчислити похідну по a, 
використовуючи диф.числення. Але якщо ти недостатньо знаєш диф.числення, не хвилюйся. Тепер, коли ми обчислили рівняння da, тобто похідну або остаточну змінну по a, ми можемо продовжити зворотний рух. Виявляється, що dz можна показати як (це назва змінної в Python) похідну Втрати по z. Або ж замість L ми можемо записати Втрату, включаючи a та y як параметри. Обидва позначення прийнятні. Ми можемо побачити, що це =a-y. Кілька коментарів для експертів з диф.числення. Якщо ти не експерт з диф.числення, не переймайся. Виявляється, що dL/dz може бути записано як dL/da помножене на da/dz. І, виявляється, що da/dz буде a(1-a), а dL/da, як ми вже побачили, - ось цим. Тож якщо ми візьмемо ці 2 рівняння: dL/da, що = оцьому, і da/dz, що = оцьому, і перемножимо їх, то можна спростити цей добуток до a-y. Тож ось так це виводиться. Це і є ланцюговим правилом, 
яке скоротило вираз до наведеної форми. Зрозуміло? Тож не вагайся вивести це самостійно, 
якщо знаєш диф.числення. Якщо ж не знаєш, то достатньо розуміти, що dz обчислюється як a-y, а диф.числення ми зробили за тебе. І останній крок зворотного поширення - повернутись на початок і визначити наскільки потрібно змінити w і b. В цьому випадку, можемо побачити, що похідна по w₁ (в коді ми її називаємо dw1) =x₁*dz. І, аналогічно, dw2, яка показує наскільки потрібно змінити w₂, =x₂*dz. А b, вибачте, db=dz. Тож, якщо ти хочеш реалізувати Градієнтний спуск 
відносно цього одного зразка, то потрібно зробити наступне. Потрібно з допомогою оцієї формули обчислити dz, а потім з допомогою оцих формул обчислити dw1, dw2 і db, а потім провести оці коригування: w₁ коригується як w₁ мінус навчальний темп α, помножений на dw1; w₂ коригується аналогічно, а b коригується як навчальний темп, помножений на db. Тож це буде один крок обчислень (бо стосується одного зразка). Тож ти побачив/ла як обчислювати похідні і застосовувати Градієнтний спуск для ЛР 
для одного тренувального зразка. Проте, коли ми тренуємо модель ЛР, ми маємо не один тренувальний зразок, 
а тренувальний набір з m тренувальних зразків. У наступному відео давай розглянемо як ці ідеї можна застосувати для навчання не лише одного зразка, а для всього тренувального набору.