deeplearning.ai द्वारा इस कोर्स के हिस्से के रूप में, आशा है न केवल आपको पढ़ाना तकनीकी 
विचार डीप लर्निंग के बल्कि आपका परिचय भी करवाना कुछ लोगों से, 
कुछ नायकों से डीप लर्निंग के. लोग जिन्होंने आविष्कार किया बहुत से इन सुझावों का जो आप सीखेंगे 
इस कोर्स में या इस विशेषज्ञता में. इन वीडियो में, मैं आशा करता हूँ पूछने का 
डीप लर्निंग के इन नेताओं से देने के लिए आपको कैरियर सलाह कि 
कैसे आप घुस सकते हैं डीप लर्निंग में, कैसे आप कर सकते हैं अनुसंधान या 
ढूँढ सकते हैं नौकरी डीप लर्निंग में. इस इंटर्व्यू श्रृंखला के पहले के रूप में, मुझे ख़ुशी है पेश करते हुए आपको 
एक इंटर्व्यू जेफ़्फ़्री हिंटॉन के साथ. जेफ़ स्वागत है, और शुक्रिया यह इंटर्व्यू करने के लिए 
deeplearning.ai के साथ. >> मुझे निमंत्रित करने के लिए धन्यवाद. >> मुझे लगता है इस पोईँट पर आपने, 
इस ग्रह पर किसी अन्य से ज़्यादा अविष्कार किए हैं बहुत से विचार/सुझाव डीप लर्निंग के पीछे. और बहुत से लोग आपको डीप लर्निंग 
का गॉडफादर बुला रहे हैं. हालांकि यह नहीं था कुछ मिनट पहले तक जब हम 
बातें कर रहे थे, जब मुझे एहसास हुआ आप सोचते हैं मैं हूँ पहला आपको वह बुलाने वाला, 
जिसे करने से मुझे बहुत ख़ुशी है. लेकिन मैं जो पूछना चाहता हूँ है कि बहुत से लोग 
जानते हैं आपको एक दिव्य चरित्र के रूप में. मैं पूछना चाहता हूं आपकी निजी कहानी 
दिव्य चरित्र के पीछे. तो कैसे आप जुड़े, पीछे वापिस जाते हुए,
 कैसे आप जुड़े AI और मशीन लर्निंग और न्यूरल नेटवर्क से? > > तो जब मैं हाई स्कूल में था, 
मेरा एक सहपाठी था जो हमेशा बेहतर था सभी चीज़ों में, 
वह एक प्रतिभाशाली गणितज्ञ था. और वह एक दिन स्कूल में आया और बोला
, क्या तुम जानते हो मस्तिष्क होलोग्राम का 
उपयोग करता है? और मुझे लगता है वह था लगभग 1966 में, 
और मैंने कहा, कुछ क्या होता है एक होलोग्राम? और उसने बताया कि एक होलोग्राम में आप 
इसे आधा काट कर सकते हैं, और आपको फिर भी पूरी तस्वीर मिल सकती है. और कि मस्तिष्क की यादों को पूरे मस्तिष्क में 
वितरित किया जा सकता है. और इसलिए मुझे लगता है कि उसने लैश्ली के
 प्रयोगों के बारे में पढ़ा था, जहां आप एक चूहे के मस्तिष्क के टुकड़े काटते हो और जानते हो कि यह बहुत कठिन है ढूँढना एक टुकड़ा 
जहाँ यह संग्रह करता है एक ख़ास याद. तो वह था जिसने मेरी रुचि पैदा की कि कैसे 
मस्तिष्क संग्रह करता है यादें. और फिर जब मैं विश्वविद्यालय गया, मैंने शुरू किया फिजियोलॉजी और 
भौतिकी के अध्ययन से. मुझे लगता है कि जब मैं कैंब्रिज में था, मैं केवल अकेला ग्रैजूएट था फिजियोलॉजी 
और भौतिकी करने वाला. और फिर मैंने वह छोड़ दिया और कोशिश की फ़िलासफ़ी करने की, क्योंकि 
मैंने सोचा कि वह शायद मुझे दे अधिक जानकारी. लेकिन वह मुझे लग रहा था वास्तव में कुछ कम भेद करने के तरीक़ों में जब 
उन्होंने कुछ झूठ कहा. और तो फिर मैंने मनोविज्ञान करना शुरू किया. और मनोविज्ञान में उनके बहुत, बहुत सरल सिद्धांत थे, 
और मुझे ऐसा लग रहा था यह निराशाजनक रूप से अपर्याप्त है समझाने 
के लिए कि मस्तिष्क क्या कर रहा था. तो फिर मैंने कुछ अवकाश ले लिया और 
एक बढ़ई बन गया. और फिर मैंने तय किया कि मैं AI की कोशिश करूंगा, 
और एडिनबर्ग के पास गया, पढ़ने के लिए AI लैंगर हिगिंज़ के साथ. और उसने न्यूरल नेटवर्क पर बहुत 
अच्छा काम किया था, और और उसने हाल ही में त्याग दिये थे न्यूरल नेटवर्क्स, 
और बहुत प्रभावित था विनोगार्ड कि थीसिस से. तो जब मैं आया उसने सोचा कि मैं एक तरह से 
कर रहा हूँ यह पुराने जमाने का काम, और मुझे सांकेतिक AI पर शुरू करना चाहिए. और हमारे बहुत से झगड़े हुए उस बारे में, 
लेकिन मैं सिर्फ़ करता रहा जिसमें मुझे विश्वास था. >> और फिर क्या? > > मुझे अंततः PhD मिल गयी AI में, और 
फिर मुझे ब्रिटेन में कोई नौकरी नहीं मिली. लेकिन मैंने यह बहुत अच्छा विज्ञापन देखा स्लोन फ़ेलोशिप के लिए कैलिफोर्निया में, 
और कामयाब हुआ पाने में उनमें से एक. और मैं कैलिफोर्निया चला गया, 
और वहाँ सब कुछ अलग था. तो ब्रिटेन में, न्यूरल नेट्स माने जाते थे 
एक तरह से मूर्खतापूर्ण, और कैलिफोर्निया में, डॉन नॉर्मन और डेविड रूमेलहार्ट बहुत ग्रहणशील थे न्यूरल नेट्स के लिए. यह पहली बार था जब मैं था कहीं जहाँ सोचना 
कि मस्तिष्क कैसे काम करता है, और सोचना कि कैसे वह मनोविज्ञान से 
संबंधित हो सकता है, एक बहुत ही सकारात्मक रूप में देखा जाता था. और यह वहाँ बहुत आनंददायक था, ख़ासकर डेविड रूमेलहार्ट के साथ 
मिलकर काम करना शानदार था. > > अच्छा, बढ़िया.
तो यह था जब आप UCSD में थे, और आप और रूमेलहार्ट, लगभग कब, 1982, ने ख़त्म किया लिखना सेमिनल बैकप्रॉप पेपर, सही है? > > वास्तव में, यह उससे अधिक पेचीदा है. > > क्या हुआ? > > मुझे लगता है, 1982 के शुरू में, डेविड रूमेलहार्ट और मैंने और रॉन विलियम्स ने, एक साथ बनाया बैकप्रॉप एलगोरिद्म, यह मुख्य रूप से डेविड रूमेलहार्ट का आइडिया था. हमें बाद में पता चला कि कई अन्य लोगों ने इसको बनाया था. डेविड पार्कर ने बनाया था, इसे शायद हमारे बाद, 
लेकिन हमारे प्रकाशित करने से पहले. पॉल वेरबोस ने प्रकाशित किया था पहले, 
काफ़ी साल पहले, लेकिन किसी ने उस पर ज़्यादा ध्यान नहीं दिया. और कई अन्य लोग थे जिन्होंने विकसित किए थे 
बहुत समान एलगोरिद्म्स, यह स्पष्ट नहीं है कि बैकप्रॉप से क्या तात्पर्य है. लेकिन इस्तेमाल करना चेन रूल का पाने के लिए 
डेरिवेटिव्स एक नया आइडिया नहीं था. >> अच्छा, आप क्यों सोचते हैं कि वह 
आपका पेपर था जिसने सहायता की समुदाय की बहुत अधिक जुड़ने के लिए बैकप्रॉप से? ऐसा लगता है कि आपके पेपर ने लाया एक 
मोड़ स्वीकृत करने में यह एल्गोरिद्म, जिसने भी इसे स्वीकार किया. >> तो हम कर पाए एक पेपर नेचर में 1986 में. और मैंने काफ़ी राजनीतिक काम किया पेपर 
को स्वीकृत कराने के लिए. मैंने जाना कि एक रैफ़्री शायद होना था स्टूअर्ट सदरलैंड, जो ब्रिटेन में एक प्रसिद्ध मनोवैज्ञानिक था. और मैं गया उससे बात करने लम्बे समय के लिए, और समझाया उसे सीधे कि क्या चल रहा था. और वह बहुत प्रभावित हुए इस तथ्य से कि हमने दिखाया कि बैकप्रॉप सीख सकता था 
शब्दों के निरूपण. और आप देख सकते थे वे निरूपण, जो हैं छोटे वेक्टर्स, और आप समझ सकते थे मतलब प्रत्येक फ़ीचर का. तो हम उसे वास्तव में ट्रेन कर सकते थे फ़ैमिली ट्री 
के शब्दों के छोटे ट्रिपल्स पर, जैसे मेरी की माँ मदर विक्टोरिया है. और आप देंगे इसे पहले दो शब्द, और 
इसे प्रिडिक्ट करना होगा आख़िरी शब्द. और आपके इसे ट्रेन करने के बाद, आप देख सकते थे सभी प्रकार के फ़ीचर 
निरूपण में अलग-अलग शब्दों के. जैसे व्यक्ति की राष्ट्रीयता वहाँ, वे किस पीढ़ी के थे, परिवार के पेड़ की कौन सी 
शाखा में वे थे, और इसी प्रकार आगे. यही था कि स्टुअर्ट सदरलैंड वास्तव में 
इससे इतना प्रभावित हुआ, और मुझे लगता है इसीलिए पेपर स्वीकार कर लिया गया. >> बहुत शुरुआत के शब्द एंबेड्डिंग्स, 
और आप अभी से देख पा रहे हैं सीखे हुए फ़ीचर्ज़ सिमैनटिक अर्थों के उभरते हुए ट्रेनिंग अल्गोरिद्म से. > > हां, तो एक मनोवैज्ञानिक के दृष्टिकोण से, 
क्या दिलचस्प था कि इसने एकीकृत किया दो एकदम भिन्न क़िस्म के विचारों को कि ज्ञान क्या है. तो वहां था एक पुराने मनोवैज्ञानिक का दृष्टिकोण 
कि एक धारणा है सिर्फ़ एक बड़ा बंडल फ़ीचर्ज़ का, और उसके लिए बहुत से सबूत हैं. और फिर वहाँ था AI दृष्टिकोण उस समय का, 
जो है एक नियमानुरूप दृष्टिकोण. जो था कि एक धारणा है कि यह कैसे अन्य 
धारणायों से सम्बंधित होता है. और एक धारणा को कैप्चर करने के लिए, आपको कुछ 
करना होगा जैसे एक ग्राफ़ संरचना या शायद एक सिमैंटिक नेट. और जो इस बैक प्रॉपगेशन उदाहरण 
ने दिखाया था, आप दे सकते हैं इसे जानकारी जो जा सकती है एक ग्राफ संरचना में, 
या इस मामले में एक परिवार के पेड़ में. और यह बदल सकता है उस जानकारी को
 फ़ीचर्ज़ में इस तरह से कि यह तब इस्तेमाल कर सकता है फ़ीचर्ज़ डिराइव करने के लिए 
नई तर्कयुक्त जानकारी, यानि कि सामान्यीकरण लेकिन महत्वपूर्ण बात थी कि यह आगे पीछे 
ग्राफ़ संरचना में या परिवार के पेड़ की ट्री संरचना के निरूपण में, और एक निरूपण लोगों का एक बड़े फ़ीचर वेक्टर्ज़ में. और वास्तव में उस ग्राफ़ की तरह की संरचना से 
आप पा सकते थे फ़ीचर्ज़ वेक्टर्स में, और फ़ीचर वेक्टर्ज़ से, आप पा सकते थे 
और ज्यादा ग्राफ़ की तरह की संरचना. >> तो यह है 1986? 90 के दशक के शुरू में, बेंजीयो ने दिखाया
 कि आप वास्तव में वास्तविक डेटा ले सकते हैं, आप ले सकते हैं अंग्रेज़ी टेक्स्ट, 
और अप्लाई कर सकते हैं समान तकनीक वहाँ, और पा सकते हैं एंबेड्डिंग्स असली शब्दों के लिए अंग्रेज़ी टेक्स्ट से, 
और उससे बहुत लोग प्रभावित हुए. >> मुझे लगता है हाल ही में हम काफ़ी बात कर रहे हैं कैसे
 द्रुतगामी कम्प्यूटर्स जैसे GPU और सुपरकम्प्यूटर जो चलायमान कर रहे हैं डीप लर्निंग. मैंने नहीं सोचा था कि पीछे 1986 और 90 के दशक के 
शुरू में, ऐसा लगता है जैसे बीच में आपके और बेंजीयो के यह पहले ही शुरुआत थी 
इस प्रवृति की. > > हां, यह एक बहुत बड़ा बढ़ाव था. 1986 में, मैं इस्तेमाल कर रहा था एक लिस्ट मशीन 
जो कम थी एक दसवें भाग से एक मेगा फ़्लॉप के. और लगभग 1993 या क़रीब तब तक, 
लोगों को मिल रहे थे दस मेगा फ़्लॉप्स. >> अच्छा. >> तो वह था एक फ़ैक्टर 100 का, वह था समय जहाँ प्रयोग में आसान था, क्योंकि कम्प्यूटर्स अधिक द्रुतगामी हो रहे थे. >> पिछले कई दशकों में, आपने आविष्कार किया है इतने सारे न्यूरल नेटवर्क्स और डीप लर्निंग के हिस्सों का. मैं वास्तव में उत्सुक हूँ, सभी चीज़ों के 
बारे में जिनका अपने अविष्कार किया है, आप उनमें से किस बारे में सबसे अधिक उत्साहित हैं
 आज? >> तो मैं सोचता हूँ कि सबसे बढ़िया है काम जो 
मैं करता हूँ टेरी सेजनोवसकी के साथ बोल्टज़मैन मशीन्स पर. तो हमने जाना कि वहाँ था यह बहुत ही सरल लर्निंग अल्गोरिद्म जो 
अप्लाई करने पर बहुत ही घने रूप से जुड़े नेट्स पर जहाँ आप देख सकते हैं 
कुछ नोड्ज़. तो यह सीखेगा हिडन रेप्रेज़ेंटेशन्स और यह था
 एक बहुत ही सरल अल्गोरिद्म. और यह लगता था उस तरह का काम जो 
आप कर सकते हैं मस्तिष्क में क्योंकि प्रत्येक सिनैप्स को जानने की ज़रूरत है आचरण की दो न्यूरॉन्स के जिनसे वह सीधे जुड़ा है. और जो जानकारी प्रचारित की गई थी वह समान थी. वहाँ दो अलग चरण थे, जिन्हें हमने कहा वेक और स्लीप. लेकिन दो भिन्न चरणों में, आप प्रचारित कर रहे थे जानकारी समान ढंग से. जबकि कुछ बैक प्रॉपगेशन जैसे में, 
वहाँ है एक फ़ॉर्वर्ड पास और एक बैक्वर्ड पास, और वे काम करते हैं भिन्न ढंग से. वे भेज रहे हैं भिन्न प्रकार के सिग्नलज़. तो मैं सोचता हूँ वह है सबसे बढ़िया चीज़. और बहुत सालों तक, वह लगता था एक कुतूहल जैसे, क्योंकि ऐसा लगता था जैसे यह बहुत धीमी थी. लेकिन बाद में, मैंने छोड़ा थोड़ा बहुत सुंदरता को, 
और इसने शुरू किया मुझे सेटल करना और इस्तेमाल किया केवल एक 
इटरेशन, एक कुछ सरलतर नेट में. और उससे मिली प्रतिबंधित बोल्ट्ज़मैन मशीन्स, जिन्होंने वास्तव में व्यवहार में प्रभावी ढंग से काम किया. तो नेटफ़्लिक्स प्रतियोगिता में, उदाहरण के लिए, प्रतिबंधित बोल्ट्जमैन मशीने थी एक घटक विजेता प्रविष्टि में. >> और वास्तव में, हाल के पुनरुत्थान में न्यूरल नेटवर्क के और डीप लर्निंग के, 2007 में शुरू होते हुए, 
थी प्रतिबंधित / सीमित बोल्ट्ज़मैन मशीन, और अप्रतिबंधित बोल्ट्जमैन मशीन का काम जो 
आप और आपकी प्रयोगशाला ने किया. >> वह एक और काम है जिससे मैं बहुत ख़ुश हूँ, विचार कि आप ट्रेन कर सकते हैं आपकी प्रतिबंधित 
बोल्ट्ज़मैन मशीन, जिसमें सिर्फ़ थी एक लेयर हिडन फ़ीचर्ज़ की और 
आप सीख सकते थे एक लेयर फ़ीचर्ज़ की. और फिर आप उन फ़ीचर्ज़ को ले सकते थे डेटा की तरह 
और कर सकते थे इसे दोबारा, और फिर आप उन फ़ीचर्ज़ को ले सकते थे डेटा की तरह 
और कर सकते थे इसे दोबारा, उतनी बार जितनी बार आप चाहते थे. तो वह अच्छा था, उसने व्यवहार में काम किया. और फिर UY टे को एहसास हुआ कि पूरी चीज़ को 
एक मॉडल के रूप में लिया जा सकता है, लेकिन यह मॉडल एक अजीब तरह का था. यह एक मॉडल था जहां शीर्ष पर आपके पास 
एक प्रतिबंधित बोल्ट्जमैन मशीन थी, लेकिन उसके नीचे आपके पास था एक सिग्मोईड बिलीफ़ नेट 
जो था कुछ जिसका कई साल पहले आविष्कार हुआ था. तो यह एक डायरेक्टेड मॉडल था और जो हम बना पाए थे ट्रेन करके इन प्रतिबंधित बोल्ट्ज़मैन मशीन्स को था एक कार्यक्षम ढंग इन्फ़र करने का 
सिग्मोईड बिलीफ़ नेट्स में. तो, उस समय के आसपास, वहाँ लोग थे काम करते हुए न्यूरल नेट्स पर, 
जो इस्तेमाल कर रहे थे घने रूप से जुड़े नेट्स को लेकिन उनके पास नहीं था कोई अच्छा ढंग करने का 
प्रोबबिलिसिटिक इमप्रिंट्स का. और लोग थे जो काम कर रहे थे ग्राफ़ मॉडल्स पर, 
मेरे बच्चों के विपरीत, जो इन्फ़र कर पा रहे थे सही ढंग से, 
लेकिन केवल विरले रुप से जुड़े नेट्स में. और हम जो दिखा पाए था तरीक़ा लर्न करने का इन डीप बिलीफ़ नेट्स को ताकि वहाँ है एक अनुमानित फ़ॉर्म 
इन्फ़्रेन्स की जो बहुत द्रुत है, और यह देता है सिर्फ़ एक फ़ॉर्वर्ड पास 
और वह था एक बहुत ही सुंदर परिणाम. और आप गारंटी दे सकते थे कि हर बार जब आप लर्न करते है
 वह अतिरिक्त लेयर फ़ीचर्ज़ की वहां एक बैंड था, हर बार जब आप एक नई लेयर सीखते हैं, 
आपको मिलता है एक नया बैंड, और नए बैंड हमेशा पुराने बैंड की तुलना में बेहतर था. >> भिन्नता बैंड्ज़, दिखाते हुए जैसे आप जोड़ते हैं लेयर्स. हां, मुझे वह वीडियो याद है. >> तो वह थी दूसरी चीज़ जिसके बारे में मैं वास्तव में उत्साहित था. और मुझे लगता है तीसरी चीज़ थी काम जो 
मैंने किया वेरीएशनल विधियों पर. और ऐसा हुआ कि सांख्यिकीय लोगों ने किया था
 उसी तरह का काम पहले. लेकिन हमें उसके बारे में पता नहीं था. तो हम कामयाब रहे बनाने में EM को बेहतर काम बहुत बेहतर दिखा कर कि 
आपको नहीं करने की आवश्यकता 
एक सटीक E स्टेप. आप कर सकते हैं एक अनुमानित E स्टेप. और EM था एक बड़ा अल्गोरिद्म सांख्यिकीय में. और हमने इसका एक बड़ा सामान्यीकरण दिखाया था. और विशेष रूप से, 1993 में, शायद वैन कैम्प के साथ. मैंने लिखा एक पेपर, जो मैं सोचता हूँ, 
था पहला वेरीएशनल बेस पेपर, जहाँ हमने दिखाया कि आप वास्तव में कर सकते हैं 
बेज़ीयन लर्निंग का एक वर्ज़न जो था कहीं अधिक सुविधाजनक, 
अनुमानित करके ट्रू पोस्टीरीयर एक गॉशियन से. और आप कर सकते थे वह एक न्यूरल नेट में. और मैं उस से बहुत उत्साहित था. >> ठीक है. वह. सही है. हाँ, मुझे लगता है मुझे याद हैं वे सब पेपर्स. नील और हिंटॉन, अप्रोक्सिमेट EM पेपर, 
काफ़ी घंटे व्यतीत किए उसे पढ़ने में. और मैं सोचता हूँ कुछ उनमें से अल्गोरिद्म्स जो 
इस्तेमाल करते हैं आजकल, या कुछ उनमें से अल्गोरिद्म्स जो इस्तेमाल करते हैं 
बहुत से लोग लगभग प्रतिदिन, हैं वे चीज़ें जैसे ड्रॉपआउट्स, मुझे लगता है
 ReLU ऐक्टिवेशनस भी आपके ग्रूप से आए? >> हां और नहीं. तो अन्य लोगों ने सोचा है 
रेक्टिफ़ायड लिनीयर यूनिट्स के बारे में. और हमने वास्तव में किया प्रतिबंधित बोल्ट्ज़मैन 
मशीन्स के साथ, दिखाते हुए कि एक ReLU था लगभग बिल्कुल समान एक 
लॉजिकल यूनिट्स के एक पूरे स्टैक के जैसे. और वह थी एक चीज़ जिसने मदद की 
ReLU को ऊपर आने में. >> मैं उस बारे में बहुत उत्सुक था. ReLU पेपर में था बहुत सा गणित दिखाते हुए 
कि यह फ़ंक्शन किया जा सकता है अनुमानित इस 
बहुत जटिल फ़ॉर्म्युला से. क्या अपने किया वह गणित 
ताकि आपका पेपर स्वीकृत हो 
एक ऐकडेमिक कॉन्फ़्रेन्स में, यह उस सब गणित ने वास्तव में प्रभावित किया 
विकास मैक्स 0 और x का? >> वह था एक केस जहाँ वास्तव में गणित था 
महत्वपूर्ण आइडिया के विकास के लिए. तो मैं जानता था 
रेक्टिफ़ायड लिनीयर यूनिट्स के बारे में, 
ज़ाहिर है, और मैं जानता था लॉजिस्टिक यूनिट्स के बारे में. बोल्टज़मैन मशीन्स पर काम की वजह से, सारा मूल काम हुआ था लॉजिस्टिक यूनिट्स से. और इसलिए सवाल था, क्या लर्निंग अल्गोरिद्म काम कर सकता है
 किसी में रेक्टिफ़ायड लिनीयर यूनिट्स के साथ? और दिखाने से कि रेक्टिफ़ायड लिनीयर यूनिट्स हैं 
लगभग बिल्कुल समान एक स्टैक के लॉजिस्टिक यूनिट्स के, हमने दिखाया कि पूरा गणित लगेगा. >> ठीक है. और इसने प्रेरणा दी आज के लिए, सैकड़ों लोग 
इस्तेमाल करते हैं ReLU और और यह काम करता है बिना >> हाँ. बिना अनिवार्य रूप से समझे वही प्रेरणा. >> हाँ, एक बात मैंने बाद में नोटिस की
 जब मैं गया गूगल पर. शायद 2014 में, मैंने दिया एक व्याख्यान गूगल में 
ReLU इस्तेमाल करने पर और आयडेंटिटी मेट्रिक्स को इनिशियलाइज करने पर. क्योंकि ReLUs के बारे में अच्छी बात यह है कि 
अगर आप दोहराते रहते हैं हिडन लेयर्स को और आप इनिशीयलाइज करते हैं 
आयडेंटिटी से, यह सिर्फ़ कॉपी करता है पैटर्न नीचे की लेयर का. और इसलिए मैं दिखा रहा था कि
 आप ट्रेन कर सकते हैं 
नेटवर्क्स को 300 हिडन लेयर्स से और आप उन्हें ट्रेन कर सकते हैं वास्तव में कुशलता से यदि
 आप इनिशियलाइज करते हैं उनकी आयडेंटिटी से. लेकिन मैंने उस पर और काम नहीं किया और 
मुझे वास्तव में खेद है उसमें न लगे रहने का. हमने प्रकाशित किया एक पेपर क्वोक ली के साथ 
दिखाते हुए कि आप इनिशियलाइज कर सकते हैं दिखाते हुए कि आप इनिशियलाइज कर सकते हैं 
 रेकरिंगनेस उस तरह से. लेकिन मुझे काम करना चाहिए था उस पर 
आगे क्योंकि बाद में ये रेसिडुयल नेटवर्क्स हैं वास्तव में उस तरह की चीज़. >> पिछले कुछ वर्षों में मैंने सुना है आप मस्तिष्क के 
बारे में बहुत बात करते हैं. मैंने सुना है आपको बात करते हुए 
बैकप्रॉप और मस्तिष्क के सम्बंध की. उस पर अपने वर्तमान विचार क्या हैं? >> मैं वास्तव में अभी उस पर 
एक पेपर पर काम कर रहा हूं. मुझे लगता है, मेरा मुख्य विचार है यह. यदि ऐसा हुआ कि बैकप्रॉप है वास्तव में बढ़िया 
अल्गोरिद्म लर्निंग करने के लिए. तब यक़ीनन क्रम-विकास समझ सकता है
 कैसे उसे इम्प्लमेंट करना है. मेरा मतलब है आपके पास कोशिकाएँ हैं 
जो बदल सकती हैं आँखों या दाँतों में. अब, अगर कोशिकाएँ वह कर सकती हैं, 
वे यकीनन इम्प्लमेंट कर सकते है बैक प्रॉपगेशन और शायद इसके लिए इस विशाल चयनात्मक दबाव में. तो मुझे लगता है कि न्यूरो साययंटिस्ट का विचार 
कि यह मुमकिन नहीं लग रहा है सिर्फ़ मूर्खतापूर्ण है. इसका कुछ सूक्ष्म कार्यान्वयन / इम्प्लमेंटेशन हो सकता है. और मुझे लगता है कि मस्तिष्क में शायद कुछ है 
जो वास्तव में हो सकता है सटीक न हो बैक प्रॉपगेशन के, लेकिन यह काफ़ी कुछ इसके जैसा है. और पिछले कुछ वर्षों में, मैंने कई विचार बनाएँ हैं 
कि यह कैसे काम कर सकता है. तो 1987 में, जे मैक क्लिलैंड के साथ काम करते हुए, मैंने बनाया रीसर्क्यलेशन अल्गोरिद्म, जहां विचार है आप जानकारी भेजें एक लूप में. और आप इसे बनाने की कोशिश करें ताकि चीज़ें न बदलें जैसे जानकारी जाती है इस लूप में. तो सरलतम वर्ज़न होगा कि आपके पास हैं 
इनपुट यूनिट्स और हिडन यूनिट्स, और भेजते हैं जानकारी इनपुट से हिडन और 
फिर वापिस इनपुट को, और फिर वापिस हिडन को और 
फिर वापिस इनपुट को और इसी प्रकार आगे. और क्या आप चाहते हैं, आप ट्रेन चाहते हैं 
एक ऑटोएनकोडर, लेकिन आप इसे ट्रेन करना चाहते हैं बिना किए 
बैकप्रॉपगेशन. तो आप सिर्फ़ इसे ट्रेन करते हैं छुटकारा पाने की कोशिश में 
सभी भिन्नताओं से गतिविधियों में. तो विचार है कि लर्निंग रूल सिनैप्स का है कि बदलें वेट प्रपॉर्शन 
प्रीसिनैप्टिक इनपुट पर और पोस्ट सिनैप्टिक इनपुट पर बदलाव की दर के अनुपात में लेकिन रीसर्क्यलेशन में, आप बनाने की कोशिश कर रहे हैं 
पोस्ट सिनैप्टिक इनपुट, आप बनाना चाह रहे हैं पुराने को अच्छा 
और नए को ख़राब, तो आप बदल रहे हैं उस दिशा में. हमने बनाया यह अल्गोरिद्म न्यूरोसाययंटिस्ट्स के बनाने से पहले स्पाइक-टाइमिंग-डिपेंडेंट प्लैस्टिसिटी. स्पाइक-टाइमिंग-डिपेंडेंट प्लैस्टिसिटी है 
वास्तव में वही अल्गोरिद्म लेकिन विपरीत दिशा में, जहाँ नई चीज़ है अच्छी और 
पुरानी चीज़ है ख़राब लर्निंग रूल में. तो आप बदल रहे हैं वेट प्रपॉर्शन प्रीसिनैप्टिक ऐक्टिविटी गुणा नई प्रीसिनैप्टिक ऐक्टिविटी घटा पुरानी पर. बाद में 2007 में मुझे एहसास हुआ, कि अगर अपने लिया एक स्टैक प्रतिबंधित बोल्ट्जमैन मशीनों का और आपने उसे ट्रेन किया. उसे ट्रेन करने के बाद आपके पास थी बिलकुल सही परिस्थितियाँ इम्प्लमेंट करने के लिए बैकप्रॉपगेशन सिर्फ़ रीकन्स्ट्रक्ट करके. यदि आप देखते रीकन्स्ट्रक्शन समय, वह रीकन्स्ट्रक्शन समय वास्तव में आपको बताएगा डेरिवेटिव डिस्क्रमिनेटिव पर्फ़ॉर्मन्स का. और पहली डीप लर्निंग वर्क्शाप में NIPS में 
2007 में, मैंने दिया एक व्याख्यान उस बारे में. जिसे लगभग पूरी तरह नजरअंदाज कर दिया गया. बाद में, जोशुया बेंजीयो, ने लिया वह विचार और उसने काम किया उस पर वास्तव में और अधिक. और मैं ख़ुद भी उस पर और काम कर रहा हूँ. और मैं सोचता हूँ यह विचार कि यदि आपके पास है 
एक स्टैक ऑटोएनकोडर्स का, तब आप पा सकते हैं डेरिवेटिव्स भेज कर ऐक्टिविटी बैकवर्ड्स 
और पा सकते हैं रीकन्स्ट्रक्शन एरर, है एक वास्तव में दिलचस्प विचार और शायद 
हो सकता है जैसे मस्तिष्क इसे करता है. एक अन्य विषय जो मैं जानता हूँ आपने पूरा सोचा है 
और मैंने सुना है आप अभी भी उस पर काम कर रहे हैं कि कैसे बर्ताव करना है 
मल्टिपल टाइम स्केल्ज़ से डीप लर्निंग में. तो, क्या आप उस पर अपने विचार 
साँझा करना चाहेंगे? >> हां, तो वास्तव में, वह मेरे 
पहले साल की बात है ग्रैजूएशन में. पहला व्याख्यान जो मैंने कभी दिया था वह था 
जिसे मैं कहता हूँ इस्तेमाल करना फ़ास्ट वेट्स 
के बारे में. तो वेट्स जो अनुरूप बनते हैं तेज़ी से 
लेकिन क्षय होते हैं तेज़ी से. और इसलिए रख सकते हैं शॉर्ट टर्म मेमरी. और मैंने दिखाया एक बहुत सरल सिस्टम में 
1973 में कि आप कर सकते हैं असल रिकर्शन उन वेट्स से. और असल रिकर्शन से मेरा क्या मतलब है कि 
न्यूरॉन्स जो इस्तेमाल होते हैं चीज़ों के निरूपण में, दोबारा इस्तेमाल होते हैं 
निरूपण में चीज़ों के रिकर्सिव कोर में. और वेट्स जो इस्तेमाल होते है नॉलेज निरूपण में दोबारा इस्तेमाल होते हैं रिकर्सिव कोर में. और इसलिए वह उठाता है सवाल कि कब आप 
पॉप आउट करते हैं आपका रिकर्सिव कोर, कैसे आप याद रखते हैं क्या था जब चीज़ों के 
बीच में थे? कहाँ है वह मेमरी? क्योंकि आपने इस्तेमाल किए न्यूरॉन्स 
रिकर्सिव कोर के लिए. और जवाब है कि आप रख सकते हैं 
वह मेमरी फ़ास्ट वेट्स में, और और आप दोबारा पा सकते हैं 
एक्टिव स्टेट न्यूरॉन्स की उन फ़ास्ट वेट्स से. और अभी हाल ही में जिमी बा के साथ 
काम करते हुए, हमने वास्तव में बनाया एक पेपर उस पर इस्तेमाल 
करते हुए फ़ास्ट वेट्स उस तरह के रिकर्शन के लिए. >> ठीक है. > > तो वह था काफी बड़ा गैप. पहला मॉडल 1973 में प्रकाशित किया गया था और फिर जिमी बा का मॉडल था 2015 में, मुझे लगता है 
या 2016 में. तो यह लगभग 40 साल बाद है. >> और, मुझे लगता है, एक और आइडिया जो 
मैं सोचता हूँ, कुछ साल पहले, पांच साल से अधिक, मुझे लगता है कि कैप्सूल है, 
आप क्या सोचते हैं उसके बारे में? ठीक है, तो मैं वापिस हूँ उस स्थिति में जिसमें 
मुझे आदत है होने की. जो है कि मेरे पास है यह आइडिया जिसमें मुझे सच में 
विश्वास है और किसी और को उसमें विश्वास नहीं है. और मैं प्रस्तुत करता हूँ पेपर उस बारे में और 
उन्हें ख़ारिज कर दिया जाता है. लेकिन मुझे सच में इस विचार में विश्वास है 
और मैं उस पर ज़ोर देता रहूँगा. तो यह टिका है, वहाँ कुछ प्रमुख विचार है. एक है कि कैसे आप दर्शाते हैं मल्टी-डिमेन्शनल 
एंटिटीज़, और आप दर्शा सकते हैं मल्टी डिमेन्शनल एंटिटीज़ सिर्फ़ 
थोड़ी बैक डोर ऐक्टिविटीज़ से. जब तक आप जानते हैं वहां उनमें से कोई एक है. तो आइडिया है कि इमिज के प्रत्येक क्षेत्र में, 
आप मान लेंगे कि वहाँ ज़्यादा से ज़्यादा, एक तरह का एक ही फ़ीचर है. और फिर आप इस्तेमाल करेंगे कुछ न्यूरॉन्स समूह, और उनकी ऐक्टिविटीज़ दर्शाएगीं विभिन्न पहलू 
उस फ़ीचर के, जैसे कि उस क्षेत्र में क्या हैं वास्तव में 
उसके x और y कोऑर्डिनट्स. कौन सी ऑरीएंटेशन पर है वह? कितनी गति से यह चल रहा है? इसका क्या रंग है? कितना स्पष्ट है यह? और उस तरह की चीज़ें. तो आप इस्तेमाल कर सकते हैं कुछ न्यूरॉन्स 
दर्शाने के लिए विभिन्न आयाम उसी चीज़ के. बशर्ते उनमें से केवल एक ही हो. वह है एक बहुत विभिन्न तरीक़ा दर्शाने का जो हम आमतौर पर करते हैं न्यूरल नेट्स में. आमतौर पर न्यूरल नेट्स में, 
हमारे पास होती है एक बहुत बड़ी लेयर, और सारे यूनिट्स चलते हैं और करते हैं 
जो भी वे करते हैं. लेकिन आप नहीं सोचते बंडल करने का 
उन्हें छोटे समूहों में जो दर्शाते हैं विभिन्न आयाम उसी चीज़ के. तो मुझे लगता है हमारे पास होना चाहिए 
यह अतिरिक्त संरचना. और अन्य विचार जो इसके साथ आता है. >> तो इसका मतलब है कि डिस्ट्रिब्यूटेड रेप्रेज़ेंटेशन में, आप विभाजित करते हैं रेप्रेज़ेंटेशन को. >> हाँ. 
 >> विभिन्न सबसेट्स में. >>हाँ. 
>> दर्शाने के लिए, ठीक है, बजाय- मैं उन प्रत्येक सबसेट्स को कहता हूँ एक कैप्सुल. >> ठीक है. >> आइडिया यह है कि एक कैप्सूल दर्शा सकता है 
एक इन्स्टन्स एक फ़ीचर का, लेकिन केवल एक. और यह दर्शाता है सभी विभिन्न पहलू 
उस फ़ीचर के, यह है एक फ़ीचर जिसके है बहुत से पहलु तुलना में एक आम न्यूरॉन और एक आम न्यूरल नेट के, 
जिसमें है केवल एक स्केलर प्रॉपर्टी. >> जी हाँ
 हाँ. >> और फिर आप क्या कर सकते हैं कि यदि 
आपके पास है वह, आप कर सकते हैं जो आम न्यूरल नेट्स काफ़ी ख़राब करते हैं, जो है कि 
आप कर सकते हैं जिसे मैं कहता हूँ 
रूटिंग बाए एग्रिमेंट. तो चलिए मान लेते हैं आप सेग्मेंटेशन 
करना चाहते हैं और और आपके पास है कुछ जो मुँह हो सकता है 
और कुछ और जो शायद नाक हो सकता है. और आप जानना चाहते हैं कि आपको रखना चाहिए 
उन्हें एक साथ बनाने के लिए एक चीज़. तो आइडिया में होना चाहिए एक कैप्सूल एक मुँह जिसमें हैं पैरामीटर्स मुँह के. और आपके पास है कैप्सूल एक नाक के लिए 
जिसमें है पैरामीटर्स नाक के. और फिर समझने के लिए कि क्या 
उन्हें एक साथ रखा जाए या नहीं, आप उनमें से प्रत्येक से वोट करवाते हैं 
कि क्या पैरामीटर्स होने चाहिए एक चेहरे के. अब अगर मुंह और नाक सही 
स्पेश्यल रिलेशनशिप में हो, वे सहमत होंगे. तो जब आपको मिलते हैं दो कैप्सूल एक लेवल में 
जो वोट कर रहे हैं पैरामीटर्स के समान सेट के लिए ऊपर के अगले लेवल के लिए, आप मान सकते हैं 
कि वे शायद सही हैं, क्योंकि सहमत होना एक उच्च डिमेन्शनल स्पेस 
में काफ़ी असंभावित है. वह है एक बहुत विभिन्न तरीक़ा फ़िल्टर करने का, उससे जो हम आमतौर पर 
न्यूरल नेट्स में इस्तेमाल करते हैं. तो मुझे लगता है यह रूटिंग बाए एग्रिमेंट 
होगा बहुत महत्वपूर्ण बेहतर जनरलाइज करवाने के लिए
 न्यूरल नेट्स को सीमित डेटा से. मुझे लगता है यह बहुत अच्छा करेगा दृष्टिकोण में
 बदलाव आने पर, सेगमेंटेशन करने में बहुत अच्छा करेगा. और मैं उम्मीद कर रहा हूँ यह होगा
 सांख्यिकिय रूप से 
अधिक कुशल तुलना में जो हम वर्तमान में न्यूरल नेट्स में करते हैं. जो है, यदि आप नियंत्रण करना चाहते है दृष्टिकोण में
 बदलाव पर, आप सिर्फ़ दें इसे सारे बदलाव दृष्टिकोण में 
और ट्रेनिंग करें उन सभी पर. >> अच्छा, ठीक है, तो बजाय करने के 
केवल फ़ीड फ़ॉर्वर्ड सूपर्वायज़्ड लर्निंग, आप लर्न कर सकते हैं इसे किसी भिन्न ढंग से. >> ठीक है, मेरी अभी भी योजना है इसे करने की 
सूपर्वायज़्ड लर्निंग के साथ, लेकिन फ़ॉर्वर्ड पास के मेकैनिक्स काफ़ी भिन्न हैं. यह नहीं है एक शुद्ध फ़ॉर्वर्ड पास उस मायने में कि
 वहाँ थोड़ी बहुत इटरेशन्स हो रही हैं, जहाँ आप सोचते हैं 
आपको मिला है एक मुँह 
और आप सोचते हैं आपको मिला है एक नाक. और इस्तेमाल करते हैं थोड़ा इटरेशन का 
तय करने के लिए क्या उन्हें सच में साथ जाना चाहिए बनाने के 
लिए एक चेहरा. और आप कर सकते हैं बैक प्रॉप्स उस इटरेशन से. तो आप यह कर सकते हैं थोड़ा अंतर करने के लिए, और हम उस पर काम कर रहे हैं 
अब मेरे ग्रूप में टोरोंटो में. तो अब मेरे पास है एक छोटी गूगल टीम टोरोंटो में,
 ब्रेन टीम का एक हिस्सा. और वह है जिसके बारे में मैं अब उत्साहित हूँ. >> अच्छा, बढ़िया, हाँ. तत्पर हैं उस पेपर के लिए जब वह आएगा. >> हाँ, यदि यह आता है [हँसी]. >> आपने कई दशकों से डीप लर्निंग में काम किया है. मैं वास्तव में बहुत उत्सुक हूं, कैसे अपनी सोच, आपकी समझ AI की बदली इन सालों में? >> तो मुझे लगता है बहुत सा मेरा बौद्धिक इतिहास है 
बैक प्रॉपगेशन के इर्द गिर्द, और कैसे इस्तेमाल करना है बैक प्रॉपगेशन, 
कैसे इस्तेमाल करना है इसकी ताक़त का. तो शुरू में, 80 के दशक के मध्य में, 
हम इसका उपयोग कर रहे थे डिस्क्रमिनेटिव लर्निंग के लिए और यह
 अच्छा चल रहा था. मैंने तब तय किया, 90 के दशक के शुरू में, कि वास्तव में लोगों द्वारा अधिकांश लर्निंग होगी 
अनसूपर्वायज़्ड लर्निंग. और मुझे बहुत दिलचस्पी पैदा हुई 
अनसुपरवाईज्ड लर्निंग में, और वह था जब मैंने शुरू किया काम करना 
वेक-स्लीप अल्गोरिद्म जैसी चीज़ों पर. >> और उस समय आपकी टिप्पणी ने वास्तव में 
प्रभावित किया मेरी सोच को भी. तो जब मैं अग्रणी था गूगल ब्रेन में,
 हमारे पहले प्रोजेक्ट ने काफ़ी किया काम अनसूपर्वायज़्ड लर्निंग पर 
आपके प्रभाव की वजह से. >> ठीक है, और शायद मैंने आपको गुमराह किया हो. क्योंकि अंतत:, मैं सोचता हूँ अनसूपर्वायज़्ड लर्निंग होगी 
बहुत महत्वपूर्ण. लेकिन आपको वास्तविकता का सामना करना है. और जिसने काम किया पिछले क़रीब 
दस सालों में है सूपर्वायज़्ड लर्निंग. डिस्क्रिमिनेटिव ट्रेनिंग, जहाँ आपके पास हैं लेबल्स, या आप प्रिडिक्ट करना चाह रहे हैं 
अगली चीज़ सिरीज़ में, 
तो वह लेबल की तरह काम करती है. और उसने अविश्वसनीय रूप से अच्छा काम किया है. मुझे अभी भी विश्वास है कि अनसूपर्वायज़्ड लर्निंग होगा 
महत्वपूर्ण, और चीज़ें काम करेंगी अविश्वसनीय रूप से ज्यादा बेहतर 
जब हम उससे सहित ढंग से काम करवा पाएँगे, लेकिन हमने अभी तक नहीं किया है. >> हाँ, मैं सोचता हूँ बहुत से वरिष्ठ लोग डीप लर्निंग में, मुझे मिला कर, बहुत उत्साहित रहते हैं इस बारे में. सिर्फ़ इतना कि हम में से किसी को अभी कोई 
आइडिया नहीं है कि कैसे करना है इसे. शायद आपको है, मुझे नहीं लगता कि मुझे है. >> वेरिएशनल ऑटोएनकोडर है जहाँ आप इस्तेमाल
 करते हैं रीपैरामीटराइज़ेशन ट्रिक. मुझे एक बहुत अच्छे विचार की तरह लगा. और जैनेरेटिव ऐड्वर्सेरीयल नेट्स भी 
मुझे एक अच्छा विचार लगा. मुझे लगता है जैनेरेटिव ऐड्वर्सेरीयल नेट्स हैं एक तरह से सबसे बड़ा आइडिया डीप लर्निंग में 
जो वास्तव में नया है. मैं उम्मीद कर रहा हूँ कि मैं बना सकूँ कैप्सूल्स 
जो सफल हैं, लेकिन अभी जैनेरेटिव ऐड्वर्सेरीयल नेट्स, 
मैं सोचता हूँ एक बड़ी सफलता हैं. >>क्या हुआ स्पार्सिटी और स्लो फ़ीचर्ज़ का, जो थे दो अन्य सिद्धांत बनाने के लिए 
अनसूपर्वायज़्ड मॉडल्स? मैंने कभी स्पार्सिटी पर उतना काम नहीं 
किया जितना आपने, दोस्त. लेकिन स्लो फ़ीचर्ज़, मुझे लगता है, एक गलती है. आपको नहीं कहना चाहिए स्लो. मूल विचार सही है, लेकिन आपको नहीं जाना चाहिए 
फ़ीचर्ज़ पर जो बदलते नहीं हैं. आपको जाना चाहिए फ़ीचर्ज़ पर 
जो बदलते हैं पूर्वकथनीय रूप से. तो यहाँ है एक तरह से मूल सिद्धांत कि 
कैसे आपको कुछ भी मॉडल करना चाहिए. आप अपने माप लें, और आप 
अप्लाई कर रहे हैं नॉन-लिनीअर ट्रैन्स्फ़र्मेशन्स आपके मापों को जब तक 
आपको नहीं मिलता एक रेप्रेज़ेंटेशन एक स्टेट वेक्टर जैसे 
जहाँ ऐक्शन लिनीअर है. तो आप सिर्फ़ दिखावा नहीं करते कि यह लिनीअर है
 जैसे आप करते हैं आम फ़िल्टर्ज़ के साथ. लेकिन आप वास्तव में ढूँढते हैं एक ट्रैन्स्फ़र्मेशन 
दिखने वाले से अंतर्निहित वेरिएबल्स तक जहाँ लिनीअर ऑपरेशनस, जैसे मेट्रिक्स गुणक अंतर्निहित वेरिएबल्स पर, 
काम करेंगे. तो उदाहरण के लिए, यदि आप
 बदलना चाहते हैं दृष्टिकोण, आप बनाना चाहते हैं इमिज एक अन्य दृष्टिकोण से, जो आपको करना चाहिए कि जाएँ 
पिक्सल्स से कोऑर्डिनेट्स पर. और एक बार जब आप पहुँच जाते हैं
 कोऑर्डिनेट रेप्रेज़ेंटेशन पर, जो है एक तरह का काम जो मैं 
उम्मीद करता हूँ कैप्सूल्स करेंगे. आप तब कर सकते हैं एक मेट्रिक्स गुणक 
बदलने के लिए दृष्टिकोण, और फिर आप मैप कर सकते हैं वापिस इसे पिक्सल्स पर. >> ठीक है, इसलिए आपने वह सब किया. >> मुझे लगता है कि यह एक बहुत, 
बहुत सामान्य सिद्धांत है. >> यही कारण है कि आपने चेहरे संश्लेषण 
पर वह सब काम किया है, है न? जहाँ आप लेते हैं एक चेहरा और कम्प्रेस करते हैं इसे 
एक बहुत छोटी डिमेन्शन के वेक्टर में, और इसलिए आप छेड़-छाड़ कर सकते हैं उसके साथ और 
पा सकते हैं अन्य चेहरे. >> मेरा एक छात्र था जिसने उस पर काम किया, 
मैंने ख़ुद उस पर ज़्यादा काम नहीं किया. >> अब मुझे यकीन है कि आपको अब भी 
हर समय पूछा जाता है, अगर कोई डीप लर्निंग में जाना चाहता है, 
उन्हें क्या करना चाहिए? तो आपकी क्या सलाह है? मुझे यक़ीन है कि आपने दी है बहुत सी सलाह 
लोगों को एक एक करके, लेकिन सम्पूर्ण विश्व के श्रोताओं को 
जो देख रहे हैं यह वीडियो. आपकी उनके लिए क्या सलाह है 
डीप लर्निंग में जाने के लिए? >> ठीक है, तो मेरी सलाह है एक तरह से 
साहित्य पढ़ें, लेकिन बहुत ज्यादा नहीं. तो यह है सलाह जो मुझे मिली मेरे सलाहकार से, 
जो काफ़ी अलग है जो बहुत लोग कहते हैं. अधिकांश लोग कहते हैं कि आपको कई साल लगाने 
चाहिए साहित्य पढ़ने में और फिर आपको शुरू करना चाहिए काम 
आपके अपने आइडिया पर. और वह कुछ शोधकर्ताओं के लिए सच हो सकता है, 
लेकिन रचनात्मक शोधकर्ताओं के लिए मुझे लगता है आप क्या करना चाहते हैं कि पढ़ें थोड़ा साहित्य. और नोटिस कुछ जो आप सोचते हैं 
सब लोग ग़लत कर रहे हैं. मैं उस अर्थ में विपरीत हूँ. आप देखते हैं इसे और यह बस सही नहीं लगता. और फिर समझते हैं कैसे इसे सही करना है. और फिर लोग आपको कहेंगे कि वह अच्छा नहीं हैं, 
लेकिन आप उसपर बने रहें. और लोगों को उस पर बनाए रखने के लिए 
मेरे पास एक बढ़िया सिद्धांत है, जो है कि या आपका अंतर्ज्ञान अच्छा है या नहीं है. यदि आपका अंतर्ज्ञान अच्छा है 
आपको उस पर चलना चाहिए और अंतत: आप सफल होंगे. यदि आपका अंतर्ज्ञान अच्छा नहीं है, कोई फर्क नहीं 
पड़ता कि आप क्या करते हैं. >> ठीक है [हँसी]. प्रेरणादायक सलाह, निश्चित रूप से लेनी चाहिए. >> आपको अपने अंतर्ज्ञान पर भरोसा भी करना चाहिए. उन पर भरोसा न करने में कोई फ़ायदा नहीं है. >> अच्छा, हाँ. मैं आमतौर पर लोगों को न केवल 
पढ़ने की सलाह देता हूँ, बल्कि प्रकाशित पेपर्स को 
दोबारा करने को कहता हूँ. और शायद उससे सीमित हो जाता है कि 
आप कितने कर पाते हैं, क्योंकि दोबारा उन परिणामों को ला पाने में 
काफ़ी समय लगता है. हाँ, यह सच है जब आप दोबारा करने की 
कोशिश कर रहे हैं एक प्रकाशित पेपर, आपको समझ आती है छोटी-छोटी चालें 
इसे काम करवाने के लिए. अन्य सलाह जो मेरे पास है कि कभी प्रोग्रामिंग 
बंद मत करो. क्योंकि यदि आप किसी छात्र को कुछ 
करने के लिए देते हैं और वे कमज़ोर छात्र हैं वे वापिस आएँगे और कहेंगे, यह नहीं चला. और कारण कि यह नहीं चला होगा एक 
छोटा निर्णय जो उन्होंने लिया, जो उन्होंने नहीं समझा कि महत्वपूर्ण है. और यदि आप देते हैं इसे एक अच्छे छात्र को, 
जैसे U Y Tay उदाहरण के लिए, आप उसे दे सकते हैं कुछ भी और वह वापिस 
आकर कहेगा काम हो गया. मुझे याद है ऐसा करना एक बार, और मैंने कहा 
रुको एक मिनट UY, हमारी पिछली बातचीत के बाद, मुझे समझ आया कि यह काम नहीं करेगा, 
निम्नलिखित कारणों की वजह से. और उसने कहा, हाँ, मुझे उसी समय समझ आ गया, 
तो मैंने मान लिया कि आपका वह मतलब नहीं था. >> [हँसी], अच्छा, हाँ, वह बढ़िया है, हाँ. चलो देखते है, कोई अन्य सलाह लोगों के लिए जो जाना चाहते हैं 
AI और डीप लर्निंग में. >> मुझे लगता है कि मूल रूप से आप उतना पढ़े कि 
अंतर्ज्ञान का विकास शुरू कर सकें. और फिर, अपने अंतर्ज्ञान पर भरोसा करें 
और चले उस पर, और ज़्यादा चिंता न करें यदि सब लोग 
कहते हैं यह बकवास है. >> और मुझे लगता है यह जानने का कोई 
तरीक़ा नहीं है कि अन्य लोग सही हैं या ग़लत जब वे कहते हैं यह बकवास है, 
लेकिन आपको सिर्फ़ उस पर चलना चलना चाहिए 
और फिर जानना चाहिए. >> लेकिन एक चीज़ है, जो है, 
यदि आप सोचते हैं यह एक बहुत ही बढ़िया विचार है, और अन्य लोग आपको कहते हैं 
यह बिल्कुल बकवास है, तब आप जानते हैं आप वाक़ई कुछ कर रहे हैं. तो एक उदाहरण उसका है कि जब रैम्पार्ट और मैंने 
पहले बनाए वेरीएशनल मेथड, मैंने भेजी मैल समझाते हुए इसे एक पुराने छात्र को 
जिसका नाम था पीटर ब्राउन, जिसे EM के बारे में काफ़ी पता था. और उसने दिखाया इसे लोगों को जो 
उसके साथ काम करते थे, डेल पीएट्रो भाई, जो जुड़वाँ थे, शायद. और उसने मुझे बाद में बताया उन्होंने जो कहा, 
और उन्होंने कहा, या यह आदमी नशे में है, या यह सिर्फ़ मूर्ख है, तो उन्होंने वास्तव में सोचा कि वह बकवास था. अब, यह हो सकता है थोड़ा उस वजह से जैसे मैंने समझाया इसे, क्योकिं मैंने समझाया इसे सहज शब्दों में. लेकिन जब आपके पास है जो आप सोचते हैं 
एक बढ़िया आइडिया है और अन्य लोग सोचते हैं उसे एकदम बकवास, 
तब वह संकेत है एक सच में अच्छे आइडिया का. >> अच्छा, शोध के विषय, नए ग्रैजूएट छात्रों को कैप्सूल पर 
काम करना चाहिए और शायद अनसुपरवाईज्ड लर्निंग, कोई अन्य? >> नए स्नातक छात्रों के लिए एक अच्छी सलाह है, देखो कि क्या आप ढूँढ सकते हैं एक एडवाईज़र 
जिसके आपके जैसे विचार हैं. क्योंकि यदि आप काम करते हैं 
जो एडवाईज़र के काफ़ी क़रीब है, आपको मिलेगी बहुत सी अच्छी सलाह और 
समय आपके एडवाईज़र से. यदि आप काम करते हैं जिसमें आपके 
एडवाईज़र को कोई रुचि नहीं है, जो आपको मिलेगा, आपको मिलेगी कुछ सलाह, 
लेकिन वह कुछ ज़्यादा उपयोगी नहीं होगी. >> ठीक है, आख़िरी सलाह शिक्षार्थियों के लिए, आपको कैसा लगता है लोगों का 
PhD प्रोग्राम में प्रवेश लेना? तुलना में जुड़ना एक चोटी की कम्पनी से, 
या एक चोटी के अनुसंधान ग्रूप से? >> हां, यह जटिल है, मुझे लगता है अभी, क्या हो रहा है, वहाँ नहीं हैं पर्याप्त शिक्षविद डीप लर्निंग में प्रशिक्षित 
शिक्षित करने के लिए सभी लोगों को जिन्हें हमें विश्वविद्यालयों में शिक्षित करने की जरूरत है. वहाँ नहीं है पर्याप्त फ़ैकल्टी, लेकिन मुझे लगता है कि यह अस्थाई होगा. मुझे लगता है कि क्या हुआ है, 
ज्यादातर विभाग बहुत धीरे से समझ पा रहे हैं एक तरह की क्रांति जो चल रही है. मैं एक तरह से सहमत हूँ आपके साथ
 कि यह कुछ एक दूसरी औद्योगिक क्रांति 
जैसे नहीं है, लेकिन यह लगभग है उस पैमाने पर कुछ है. और एक बहुत बड़ा परिवर्तन आ रहा है, मूलत: इसलिए कि हमारे सम्बंध 
कम्प्यूटर्स के साथ बदल गए हैं. बजाय उनको प्रोग्राम करने के, हम 
अब उन्हें दिखाते हैं, और वे इसे समझते हैं. वह है एक बहुत ही भिन्न ढंग इस्तेमाल करने 
का कम्प्यूटर्स को, और कम्प्यूटर साइयन्स डिपार्टमेंट्स बने हैं कम्प्यूटर्स को 
प्रोग्राम करने के आइडिया पर. और वे नहीं समझते कि एक तरह से, यह दिखाना कम्प्यूटर्स को होगा उतना ही बड़ा 
जितना कम्प्यूटर्स को प्रोग्राम करना. सिवाय कि वे नहीं समझते कि आधे लोग 
डिपार्टमेंट में होने चाहिए लोग जो करवा पाए कम्प्यूटर्स से काम उनको दिखा कर. तो मेरा डिपार्टमेंट इस को स्वीकार नहीं करता कि 
इसमें होने चाहिए बहुत से लोग इसे करते हुए. वे सोचते हैं उनके पास हैं कुछ, शायद कुछ और, 
लेकिन बहुत ज़्यादा नहीं. और उस स्थिति में, आपको याद कराना होगा बड़ी कम्पनी को करने के लिए ट्रेनिंग. तो गूगल अब ट्रेन करता है लोगों को 
 हम कहते हैं ब्रेन रेज़िडेन्स. मुझे लगता है विश्वविद्यालय अंततः कर पाएँगे. >> अच्छा, ठीक है, वास्तव में बहुत से 
छात्रों ने यह समझ लिया है. चोटी के 50 प्रोग्राम्स में से बहुत से, आधे से ज़्यादा 
आवेदक वास्तव में चाह रहे है काम करना दिखाने पर बजाय प्रोग्राम करने पर. हाँ, वास्तव में, श्रेय देने के लिए जहाँ उचित है, जबकि deeplearning.ai बना रहा है 
एक डीप लर्निंग स्पेशलाइज़ेशन. जहाँ तक मैं जानता हूँ, उनका पहल डीप लर्निंग 
MOOC था वास्तव में आपका पढ़ाया कौरसेरा पर, 2012 में, भी. और कुछ हद तक हैरत की बात है, वह था जब आपने पहले प्रकाशित किया था RMS अल्गोरिद्म, 
जो है एक रफ़ भी. >> ठीक है, हाँ, जैसा आप जानते हैं, वह था क्योंकि 
आपने मुझे आमंत्रित किया था MOOC करने के लिए. और तब जब मुझे संदेह था करने में, आप मुझे 
उत्साहित करते रहे करने के लिए उसे, तो वह बहुत अच्छा हुआ कि मैंने वह किया, 
हालाँकि बहुत काम करना पड़ा. >> हाँ, और धन्यवाद वह करने के लिए, 
मुझे याद है आपका मुझे शिकायत करना, कितना काम था वह. और आपका रात भर जागना, 
लेकिन मैं सोचता हूँ बहुत से शिक्षार्थियों को लाभ हुआ आपके पहले MOOC से, 
तो मैं आपका बहुत आभारी हूँ उसके लिए. तो. वह अच्छा है, हाँ. >> हाँ, कुछ वर्षों में, मैंने देखा है आपको उलझते हुए 
बहस में AI के पैरडायम्स, और क्या वहाँ कोई परिवर्तन है AI के पैरडायम्स में. क्या हैं आपके, क्या आप उस पर अपने
 विचार साँझा कर सकते हैं? >> हाँ ख़ुशी से, तो मैं सोचता हूँ कि शुरुआती दिनों में, 
50 के दशक में, लोग जैसे वॉन नियुमन और ट्यूरिंग ने 
नहीं विश्वास किया सिम्बॉलिक AI में, वे कहीं अधिक प्रेरित थे मस्तिष्क से. दुर्भाग्य से, उन
 दोनों की बहुत छोटी उम्र में मृत्यु हो गई, 
और उनकी आवाज सुनी नहीं जा सकी. और AI के शुरुआती दिनों में, लोग पूरी तरह आश्वस्त थे कि रेप्रेज़ेंटेशनज़ 
जिनकी आपको आवश्यकता है इंटेलिजेंस के लिए, थे किसी तरह के 
सिम्बॉलिक इक्स्प्रेशन्स. जैसे कि शुद्ध लॉजिक, जहाँ आप कर सकते थे
 नॉन-मोनोटोनिक चीज़ें, और न बहुत कुछ लॉजिक, लेकिन कुछ लॉजिक जैसे, 
और कि सार इंटेलिजेन्स का था तर्क. अब क्या हुआ है, वहां एक पूरी तरह से अलग दृश्य है, जो है कि एक सोच क्या है, है एक बड़ा 
वेक्टर न्यूरल ऐक्टिविटी का, तो तुलना करें उसकी एक सोच जो है 
एक सिम्बॉलिक इक्स्प्रेशन. और मैं सोचता हूँ कि लोग जिन्होंने सोचा कि सोच थे 
सिम्बॉलिक इक्स्प्रेशन्स सिर्फ़ की एक बड़ी ग़लती. जो अंदर आता है वह है एक स्ट्रिंग शब्दों की, 
और जो बाहर आता है वह है एक स्ट्रिंग शब्दों की. और उस वजह से, शब्दों की स्ट्रिंग्स हैं प्रकट ढंग 
रेप्रेज़ेंट करने के चीज़ों को. तो उन्होंने सोचा जो है मध्य में 
वह थी एक स्ट्रिंग शब्दों की, या कुछ शब्दों की स्ट्रिंग जैसे. और मैं सोचता हूँ कि जो है मध्य में वह बिल्कुल नहीं थी 
शब्दों की एक स्ट्रिंग जैसे. मैं सोचता हूँ कि आइडिया कि सोच होनी चाहिए 
किसी तरह की भाषा में उतना मूर्खतापूर्ण है जितना आइडिया कि समझ कि लेआउट एक 
स्थानिक दृश्य का होने चाहिए पिक्सल्स में, पिक्सल्स आ जाते हैं. और अगर हम, अगर हमारे पास होता एक 
डॉट मेट्रिक्स प्रिंटर मशीन से जुड़ा, तब पिक्सल्स आ जाएँगे, 
लेकिन जो मध्य में है वह नहीं है पिक्सल्स. और इसलिए मैं सोचता हूँ सोच हैं सिर्फ़ 
ये बड़े वेक्टर्स, और उन बड़े वैक्टर्स में कॉज़ल शक्ति है. वे अन्य बड़े वेक्टर्स का कारण बनते हैं, और वह है बिल्कुल विपरीत मानक AI दृष्टिकोण के 
कि सोच हैं सिम्बॉलिक इक्स्प्रेशन्स. >> ठीक है, बढ़िया, मुझे लगता है AI निश्चित रूप से इस नए 
दृष्टिकोण के दौर में आ रही है आजकल. >>इसमें से कुछ, मुझे लगता है कि बहुत से लोग अभी भी 
सोचते हैं कि सोच होने चाहिए 
सिम्बॉलिक इक्स्प्रेशन्स. >> बहुत शुक्रिया यह इंटर्व्यू करने के लिए. यह बहुत अच्छा था सुनना कैसे 
डीप लर्निंग विकसित हुई है सालों में, तथा कैसे आप अभी भी इसे भविष्य की ओर 
ले जा रहे हैं, तो धन्यवाद, जेफ़. ठीक है, आपका धन्यवाद मुझे यह अवसर देने के लिए. शुक्रिया