1
00:00:00,000 --> 00:00:01,530
前回の動画では、

2
00:00:01,530 --> 00:00:04,227
ロジスティック回帰モデルについて理解しました。

3
00:00:04,227 --> 00:00:07,526
ロジスティック回帰モデルのパラメータWとbを訓練するためには、

4
00:00:07,526 --> 00:00:10,570
コスト関数を定義する必要があります。

5
00:00:10,570 --> 00:00:14,430
では、ロジスティック回帰の学習に使えるコスト関数について確認していきましょう。

6
00:00:14,430 --> 00:00:18,195
まとめると、これが前回のスライドから抜粋した式です。

7
00:00:18,195 --> 00:00:20,792
出力結果yハットは

8
00:00:20,792 --> 00:00:24,690
wT(転置)x+bにシグモイド関数を適用したもので、zのシグモイド関数の定義はこちらです。

9
00:00:24,690 --> 00:00:27,600
ここで、モデルのパラメータを学習するために

10
00:00:27,600 --> 00:00:31,200
m個のサンプルを含む教師データのセットが与えられています。そして、

11
00:00:31,200 --> 00:00:34,060
その教師データに含まれる出力値をほぼ得られるような

12
00:00:34,060 --> 00:00:37,781
パラメータWとbを求めるものとします。

13
00:00:37,781 --> 00:00:40,225
教師データから得られる予測値は、

14
00:00:40,225 --> 00:00:43,260
ここではyハット(i)と表記し、同様に

15
00:00:43,260 --> 00:00:47,720
教師データの中に含まれる正解値を表すラベルをy_iと記載します。

16
00:00:47,720 --> 00:00:52,110
式の初めの方をもう少し詳しく確認してみると、

17
00:00:52,110 --> 00:00:56,205
初めのyハットは、ある教師サンプルxに対して先ほど定義したものですが、

18
00:00:56,205 --> 00:01:00,930
もちろん各々の教師サンプルに対して定義するならば、

19
00:01:00,930 --> 00:01:03,240
各サンプルを区別するために、

20
00:01:03,240 --> 00:01:07,710
インデックスの丸かっこを上付き添字にして使用します。

21
00:01:07,710 --> 00:01:12,870
i番目の教師サンプルに対する予測値、つまりｙハット(i)は、

22
00:01:12,870 --> 00:01:18,835
シグモイド関数をWの転置行列と教師サンプルの入力値x(i)

23
00:01:18,835 --> 00:01:25,905
の積にbを加えた値に適用すれば得られます。ここで、z(i)は次のように定義することもできます。

24
00:01:25,905 --> 00:01:30,110
z(i)=wT・x(i)+b。

25
00:01:30,110 --> 00:01:31,350
この講座全体を通して、

26
00:01:31,350 --> 00:01:33,966
私たちは、これまでに述べた表記法を使うつもりです。つまり、

27
00:01:33,966 --> 00:01:41,605
丸かっこiの上付き添字は、

28
00:01:41,605 --> 00:01:47,615
教師データのi番目、i番目のサンプルに関連付けられた

29
00:01:47,615 --> 00:01:50,885
x,y,zあるいはその他の変数を示します。

30
00:01:50,885 --> 00:01:54,840
上付き添字のかっこ(i)の意味の説明は以上です。

31
00:01:54,840 --> 00:01:57,630
ではこれから、どのような損失関数または誤差関数を使えば

32
00:01:57,630 --> 00:02:01,315
私たちのアルゴリズムがうまく評価できるのか一緒に見てみましょう。

33
00:02:01,315 --> 00:02:06,015
一つできることと言えば、損失を次のように定義することです。アルゴリズムが出力する値を

34
00:02:06,015 --> 00:02:12,320
yハット、真の値を示すラベルをyとすると、損失が二乗誤差かあるいはその半分となるようにすればよいでしょう。

35
00:02:12,320 --> 00:02:14,975
このように定義できそうなことが分かりましたが、

36
00:02:14,975 --> 00:02:17,670
ロジスティック回帰において、一般的にはその定義は使われていません。

37
00:02:17,670 --> 00:02:21,000
何故なら、いざパラメータを学習させようとすると、

38
00:02:21,000 --> 00:02:25,682
後で述べるような非凸な最適化問題を解くことになるからです。

39
00:02:25,682 --> 00:02:30,105
すると、結果的には複数の局所解を持つ最適化問題を扱うこととなり、

40
00:02:30,105 --> 00:02:33,285
勾配降下法では一つの大域解を見つけることができないでしょう。

41
00:02:33,285 --> 00:02:35,580
もしたった今話したいくつかのコメントが理解できなくても

42
00:02:35,580 --> 00:02:38,320
心配しないでください。後の動画でその話を扱う予定です。

43
00:02:38,320 --> 00:02:40,990
ただ、持ち合わせるべきは以下のような洞察だと思います。

44
00:02:40,990 --> 00:02:44,620
損失関数と呼ばれる関数Lは、

45
00:02:44,620 --> 00:02:51,265
正解のラベルがyのときに実際の出力値yハットがどれほど正しいかを測るために定義する必要があります。

46
00:02:51,265 --> 00:02:54,345
二乗誤差を採用することは合理的な気がしますが、

47
00:02:54,345 --> 00:02:58,160
勾配降下法がうまく動かない場合はそうではありません。

48
00:02:58,160 --> 00:03:00,500
従って、ロジスティック回帰において実際には

49
00:03:00,500 --> 00:03:05,695
二乗誤差と似たような役割を持つ別の損失関数を使います。

50
00:03:05,695 --> 00:03:08,910
その損失関数を使えば、私たちは凸な最適化問題を得られ、

51
00:03:08,910 --> 00:03:13,530
後の動画で見られるように、より簡単な方法で最適化することができるのです。

52
00:03:13,530 --> 00:03:17,310
ロジスティック回帰では実際には、

53
00:03:17,310 --> 00:03:21,795
以下のような損失関数を使います。今ここで書き下すと、

54
00:03:21,795 --> 00:03:31,740
-ylog(yハット)+(1-y)(1-log(yハット))

55
00:03:31,740 --> 00:03:34,600
となります。

56
00:03:34,600 --> 00:03:38,785
なぜこの損失関数が理に適うのか、直観的に説明してみましょう。

57
00:03:38,785 --> 00:03:41,285
留意するべき点は、

58
00:03:41,285 --> 00:03:45,820
もし二乗誤差を使うならば、私たちはその誤差をなるべく小さくしようとすることです。

59
00:03:45,820 --> 00:03:48,680
すなわち、このロジスティック回帰の損失関数についても

60
00:03:48,680 --> 00:03:51,495
同様に私たちはその値をなるべく小さくしようとするでしょう。

61
00:03:51,495 --> 00:03:53,508
何故このことが理に適うのか理解するため、

62
00:03:53,508 --> 00:03:55,260
次に示す2つのケースを考えてみてください。

63
00:03:55,260 --> 00:03:56,570
一つめのケースは、

64
00:03:56,570 --> 00:03:59,430
yが1の場合で、そのとき損失関数

65
00:03:59,430 --> 00:04:05,415
(yハット,y)は丁度マイナス符号で書けますから、

66
00:04:05,415 --> 00:04:08,735
-log(yハット)となります。

67
00:04:08,735 --> 00:04:10,770
もしyが1と等しいなら、yが1なので

68
00:04:10,770 --> 00:04:14,070
2番目の項(1-y)は0となるからです。

69
00:04:14,070 --> 00:04:19,880
つまり、もしyが1ならば、なるべくlog(yハット)を大きくしようとします。

70
00:04:19,880 --> 00:04:26,040
log(yハット)を大きくしたいということは、

71
00:04:26,040 --> 00:04:32,935
yハットをなるべく大きくしたいということを意味しますね。

72
00:04:32,935 --> 00:04:35,170
しかし、yハットはご存知の通り、

73
00:04:35,170 --> 00:04:38,440
シグモイド関数であり、1よりも大きくなることはあり得ません。

74
00:04:38,440 --> 00:04:41,850
すると、もしyが1と等しいならば、

75
00:04:41,850 --> 00:04:44,050
yハットをなるべく大きくしたい一方、

76
00:04:44,050 --> 00:04:48,220
その値が1を超えることは決してなく、要するになるべく1に近づけようとしていることになります。

77
00:04:48,220 --> 00:04:50,740
もう一方のケースは、yが0の場合です。

78
00:04:50,740 --> 00:04:55,375
もしyが0と等しいなら、yは0であるから損失関数の第1項目は0になります。

79
00:04:55,375 --> 00:05:01,290
そうすると損失関数は2つ目の項で定まります。

80
00:05:01,290 --> 00:05:07,210
つまり損失関数は-log(1-yハット)となり、

81
00:05:07,210 --> 00:05:11,480
学習させる段階で、損失関数を小さくしようとするならば、

82
00:05:11,480 --> 00:05:19,450
log(1-yハット)を大きくすることを意味します。

83
00:05:19,450 --> 00:05:22,050
なぜなら、そこにマイナス符号があるからです。

84
00:05:22,050 --> 00:05:24,660
そして同じような理由から、結論として、

85
00:05:24,660 --> 00:05:30,870
この損失関数はyハットをなるべく小さくしようとしていることになります。

86
00:05:30,870 --> 00:05:34,320
そして再び、yハットは0と1の間でなければならないことから、

87
00:05:34,320 --> 00:05:38,155
もしyが0と等しいならば、

88
00:05:38,155 --> 00:05:43,790
yハットがなるべく0に近づくように損失関数のパラメータが変化することになります。

89
00:05:43,790 --> 00:05:48,305
ここで、yが1と等しい時は

90
00:05:48,305 --> 00:05:52,950
yハットを大きくしようとし、yが0ならばyハットを小さくしようとするような関数は数多く存在します。

91
00:05:52,950 --> 00:05:55,150
ちょうど緑色で記載した部分により、

92
00:05:55,150 --> 00:05:59,920
少し厳密ではないですが先ほどの損失関数を与えれば良い根拠が分かりました。

93
00:05:59,920 --> 00:06:03,970
この後のオプション講座では、

94
00:06:03,970 --> 00:06:08,500
ロジスティック回帰でこのような特殊な式の損失関数を好んで使う理由を、より厳密に示すことになります。

95
00:06:08,500 --> 00:06:13,630
最後に、損失関数は、ひとつの教師サンプルについて定義されていました。

96
00:06:13,630 --> 00:06:16,760
サンプル一つ分に対してどれほど良い具合に学習できているのかを測っています。

97
00:06:16,760 --> 00:06:21,148
ここで、コスト関数と呼ばれるものをここで定義してみましょう。

98
00:06:21,148 --> 00:06:24,690
コスト関数は、教師データのデータセット全体に対してうまく学習できているのかを測ります。

99
00:06:24,690 --> 00:06:28,660
なので、コスト関数Jは、

100
00:06:28,660 --> 00:06:33,130
パラメータWとbに対して適用されますが、

101
00:06:33,130 --> 00:06:43,270
各々の教師サンプルに対する損失関数の総和を1/m倍した平均値となるはずです。

102
00:06:43,270 --> 00:06:45,435
一方、ここでyハットはもちろん

103
00:06:45,435 --> 00:06:49,570
ロジスティック回帰アルゴリズムによる予測結果であり、ご存知の通り、

104
00:06:49,570 --> 00:06:52,430
パラメータWとbの特別なセットを利用してます。

105
00:06:52,430 --> 00:06:54,480
そしてこの式をそのまま展開すると、

106
00:06:54,480 --> 00:06:58,010
マイナスに1/mをかけて

107
00:06:58,010 --> 00:07:03,550
シグマi=1からm、シグマの中はJの定義式となります。

108
00:07:03,550 --> 00:07:07,530
なので、y(i)log(yハット)(i)

109
00:07:07,530 --> 00:07:14,530
プラス(1-y(i))log(1-yハット(i))

110
00:07:14,530 --> 00:07:17,880
おそらく、私は角括弧をここで書くでしょう。

111
00:07:17,880 --> 00:07:20,945
そしてマイナス符号は丸ごと外に配置します。

112
00:07:20,945 --> 00:07:23,665
私が用語定義として使うつもりなのが、

113
00:07:23,665 --> 00:07:29,120
損失関数は、ちょうどひとつの教師サンプルに対して適用するということ。

114
00:07:29,120 --> 00:07:33,010
そしてコスト関数は全てのパラメータに関するコストであるということ。

115
00:07:33,010 --> 00:07:36,115
なので、ロジスティック回帰モデルの学習では、

116
00:07:36,115 --> 00:07:38,980
私たちは一番下に記載したように、全ての損失Jに渡るコストを最小化する

117
00:07:38,980 --> 00:07:43,475
パラメータWとbを見つけることになります。

118
00:07:43,475 --> 00:07:48,040
ここまで、ロジスティック回帰アルゴリズム、

119
00:07:48,040 --> 00:07:50,770
教師データに対する損失関数、そして

120
00:07:50,770 --> 00:07:54,190
アルゴリズムの全パラメータに渡るコスト関数の組み立て方を見てきました。

121
00:07:54,190 --> 00:07:59,485
実はロジスティック回帰は、とても、とても小さなニューラルネットワークとして捉えることができるのです。

122
00:07:59,485 --> 00:08:01,905
次の動画では、私たちは

123
00:08:01,905 --> 00:08:04,965
ニューラルネットワークで何を行っているか直観的に理解するための考察に入りたいと思います。

124
00:08:04,965 --> 00:08:08,230
なので是非とも次の動画へ進んでください。

125
00:08:08,230 --> 00:08:11,630
ロジスティック回帰をどのようにしてとても小さなニューラルネットワークとみなすのかお話します。