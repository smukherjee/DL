1
00:00:00,000 --> 00:00:03,475
この動画では、ロジスティック回帰について
学びます。

2
00:00:03,475 --> 00:00:07,080
これは、教師あり学習で出力のラベルyが

3
00:00:07,080 --> 00:00:10,690
0か1のどちらかのとき、
つまり二項分類の問題で

4
00:00:10,690 --> 00:00:13,600
使われるアルゴリズムです。

5
00:00:13,600 --> 00:00:18,350
猫の画像かそうでないかを判断したい
画像に対応する

6
00:00:18,350 --> 00:00:23,150
特徴ベクトルxの入力があるとしましょう。

7
00:00:23,150 --> 00:00:26,525
予測を出力できる
アルゴリズムが欲しいですね。

8
00:00:26,525 --> 00:00:28,254
そのyの予測のことを、

9
00:00:28,254 --> 00:00:31,130
y hatと表します。

10
00:00:31,130 --> 00:00:35,896
より形式的に言うと、y hatとは、
特徴ベクトルxが与えられたときに

11
00:00:35,896 --> 00:00:40,630
yが1である可能性であってほしいです。

12
00:00:40,630 --> 00:00:43,880
前の動画で見たように

13
00:00:43,880 --> 00:00:45,530
xが画像だとすると、

14
00:00:45,530 --> 00:00:47,300
その画像が猫である可能性を

15
00:00:47,300 --> 00:00:49,820
y hatに教えてもらいたいわけです。

16
00:00:49,820 --> 00:00:53,420
前の動画で言ったように、

17
00:00:53,420 --> 00:00:56,960
xはnx次元のベクトルです。

18
00:00:56,960 --> 00:01:02,000
すると、ロジスティック回帰のパラメータは

19
00:01:02,000 --> 00:01:07,745
xと同じようにnx次元のベクトルであるwと、

20
00:01:07,745 --> 00:01:11,670
ただの実数であるbということになります。

21
00:01:11,670 --> 00:01:16,055
入力xとパラメータw,bがあるとき、

22
00:01:16,055 --> 00:01:20,595
どうやってy hatを求めるのでしょうか。

23
00:01:20,595 --> 00:01:22,970
うまくいかないですが
やってみることができることとして、

24
00:01:22,970 --> 00:01:27,590
y hatを
wの転置かけるxプラスbにしてみること、

25
00:01:27,590 --> 00:01:33,045
つまり入力xの線形の
関数にしてみることです。

26
00:01:33,045 --> 00:01:37,145
実際、線形回帰を行う場合には
これを使います。

27
00:01:37,145 --> 00:01:41,345
しかし、二項分類においては
このアルゴリズムはあまり良くありません。

28
00:01:41,345 --> 00:01:45,575
なぜなら、
y hatはy=1である可能性でなければならず、

29
00:01:45,575 --> 00:01:50,480
それは0と1の間でなければ
ならないからです。

30
00:01:50,480 --> 00:01:54,697
この式ではそうなりそうにありません、

31
00:01:54,697 --> 00:01:58,475
なぜならwの転置かけるxはすごく大きくなるか
負になることすらありえます。

32
00:01:58,475 --> 00:02:00,905
そのような値では、0と1の間になってほしい

33
00:02:00,905 --> 00:02:03,620
可能性として意味を成しません。

34
00:02:03,620 --> 00:02:07,670
なので、ロジスティック回帰では

35
00:02:07,670 --> 00:02:12,050
y hatはこの値にシグモイド関数を
適用した値となります。

36
00:02:12,050 --> 00:02:14,850
シグモイド関数とはこのようなものです。

37
00:02:14,850 --> 00:02:24,000
水平方向の軸をzとすると、
zについてのシグモイド関数はこうなります。

38
00:02:24,000 --> 00:02:28,050
0から1までなめらかに進みます。

39
00:02:28,050 --> 00:02:30,120
軸にラベルを書きますね。

40
00:02:30,120 --> 00:02:34,915
ここが0で、0.5の位置で
垂直方向の軸と交わります。

41
00:02:34,915 --> 00:02:41,305
これがzについてのシグモイド関数で、
zをこの値を表すのに使います。

42
00:02:41,305 --> 00:02:43,020
wの転置かけるxプラスbです。

43
00:02:43,020 --> 00:02:46,230
シグモイド関数の公式はこれです。

44
00:02:46,230 --> 00:02:49,380
実数であるzについてのシグモイド関数は、

45
00:02:49,380 --> 00:02:52,510
1+e^(-z)分の1です。

46
00:02:52,510 --> 00:02:54,695
ここから、いくつかのことがわかります。

47
00:02:54,695 --> 00:03:01,255
もしzがとても大きければ、
e^(-z)は0に近くなります。

48
00:03:01,255 --> 00:03:03,420
なので、zについてのシグモイドは

49
00:03:03,420 --> 00:03:07,255
(1+0に限りなく近いもの)分の1となります。

50
00:03:07,255 --> 00:03:11,280
なぜなら、eの(マイナスに大きい数字)乗というのは
0に近くなるからです。

51
00:03:11,280 --> 00:03:13,505
これは1に近くなります。

52
00:03:13,505 --> 00:03:16,255
実際、左のグラフを見ると、

53
00:03:16,255 --> 00:03:20,475
zがとても大きいと
zについてのシグモイドは1に近くなります。

54
00:03:20,475 --> 00:03:24,105
反対に、zがとても小さいと、

55
00:03:24,105 --> 00:03:28,970
もしくはとても大きな負の数字だと、

56
00:03:29,180 --> 00:03:39,640
zについてのシグモイドは
1+e^(-z)分の1で、

57
00:03:39,640 --> 00:03:42,565
これが巨大な数字になります。

58
00:03:42,565 --> 00:03:47,944
(1+巨大な数字)分の1と考えると、

59
00:03:47,944 --> 00:03:54,473
これは、

60
00:03:54,473 --> 00:03:56,570
0に近くなります。

61
00:03:56,570 --> 00:04:00,325
実際、zがとても大きな負の数字になると、

62
00:04:00,325 --> 00:04:03,505
zについてのシグモイドは0に近くなります。

63
00:04:03,505 --> 00:04:06,070
ロジスティック回帰を実装するとき、

64
00:04:06,070 --> 00:04:10,350
あなたがすることは、
y hatがy=1である可能性の良い予測になるような

65
00:04:10,350 --> 00:04:15,220
パラメータwとbを学習することです。

66
00:04:15,220 --> 00:04:18,955
次に進む前に、表記法について少し説明します。

67
00:04:18,955 --> 00:04:20,830
ニューラルネットワークを
プログラムするとき、

68
00:04:20,830 --> 00:04:26,855
大抵パラメータwとパラメータbは
別々にしておきます。

69
00:04:26,855 --> 00:04:30,000
ここでは、bは切片項に対応します。

70
00:04:30,000 --> 00:04:31,295
他のコースでは、

71
00:04:31,295 --> 00:04:35,110
これらを別の方法で扱っている
表記を見たことがあるかもしれません。

72
00:04:35,110 --> 00:04:42,205
ある慣例では、x0と呼ばれる余分な特徴を
定義し、1とすることがあります。

73
00:04:42,205 --> 00:04:47,250
その場合、xはnx+1次元です。

74
00:04:47,250 --> 00:04:53,865
すると、y hatはσ(θの転置)です。

75
00:04:53,865 --> 00:04:56,685
この表記の慣例では、

76
00:04:56,685 --> 00:05:00,510
θというベクトルパラメータがあります。

77
00:05:00,510 --> 00:05:03,175
θ0、θ1、θ2…と続き、

78
00:05:03,175 --> 00:05:09,520
θnxまであります。

79
00:05:09,520 --> 00:05:11,723
θ0はbのことで、

80
00:05:11,723 --> 00:05:13,663
これはただの実数です。

81
00:05:13,663 --> 00:05:18,505
θ1からθnxがwの働きをします。

82
00:05:18,505 --> 00:05:20,350
ニューラルネットワークを
実装するときには、

83
00:05:20,350 --> 00:05:26,145
bとwを別々のパラメータとして
扱った方が楽です。

84
00:05:26,145 --> 00:05:27,430
なので、このクラスでは、

85
00:05:27,430 --> 00:05:32,087
さっき赤色で書いた表記法は
使わないことにします。

86
00:05:32,087 --> 00:05:36,330
この表記法を他のクラスで見たことがなければ、
気にしないでください。

87
00:05:36,330 --> 00:05:39,610
この表記法を見たことがある人に
向けての話なので。

88
00:05:39,610 --> 00:05:43,730
ただ、このコースではこれらを使わない
ということを示しておきたかったのです。

89
00:05:43,730 --> 00:05:45,235
見たことがなければ、

90
00:05:45,235 --> 00:05:48,430
全然大切なことではないので
心配しないでください。

91
00:05:48,430 --> 00:05:52,465
さて、ロジスティック回帰のモデルが
どのようなものかを見てきました。

92
00:05:52,465 --> 00:05:57,140
次は、パラメータwとbを変えるために
コスト関数を定義しましょう。

93
00:05:57,140 --> 00:05:58,830
次のビデオでやってみましょう。