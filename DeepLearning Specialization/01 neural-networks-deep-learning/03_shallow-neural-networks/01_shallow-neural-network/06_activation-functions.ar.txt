عندما تقوم ببناء الشبكات العصبية الخاصة بك، من بين
الخيارات التي يجب أن تقوم باختيارها دالة التنشيط التي ترغب في استخدامها
في الطبقات المخفية بالإضافة إلى ماهية وحدات المخرجات
للشبكة العصبية الخاصة بك. حتى الآن، كنا نستخدم
دالة تنشيط sigmoid. إلا أنه في بعض الأحيان، هناك خيارات
قد تعمل بشكل أفضل بكثير. دعونا نلقي نظرة على بعض الخيارات. في خطوة الانتشار الرابعة
للشبكة العصبية لدينا هذه الخطوات الثلاثة حيث نستخدم
دالة sigmoid هنا. ودالة sigmoid يطلق عليها
دالة تنشيط. وهذه هي دالة sigmoid الشهيرة "a" يساوي واحد على واحد زائد
"e" لقيمة "z" السلبية. وبالتالي، في الحالة الأكثر عمومية
يمكن أن يكون لدينا دالة مختلفة "g" لـ "z"
والتي سوف أكتبها هنا حيث يمكن أن تكون "g" دالة غير خطية
لا تكون دالة sigmoid. وبالتالي، على سبيل المثال، تكون دالة
sigmoid بين صفر وواحد وهناك دالة تنشيط دائمًا ما تعمل
بشكل أفضل من دالة sigmoid وهي دالة ظل الزاوية أو دالة ظل
الزاوية الزائدية. فيكون هنا "z" وهنا "a"
وهناك "a" تساوي tanh(z) وهي تكون بين زائد واحد وناقص واحد. تكون صيغة دالة tanh هي "e" إلى "z" ناقص "e" لقيمة "z"
السلبية على مجموعها. ومن الناحية الحسابية في الواقع، هي عبارة عن
إصدار منقول من دالة sigmoid. تمامًا مثل دالة sigmoid مثل
هذه، ولكنها منقولة بحيث تعبر الآن نقطة صفر ومقياس "v"، وبالتالي
تكون 15 ناقص 1 وزائد 1. ويتضح أنه بالنسبة للوحدات المخفية
إذا جعلت الدالة "g" لـ "z" تساوي tanh(z) غالبًا ما يعمل ذلك بشكل أفضل
من دالة sigmoid لأن القيم بين زائد واحد وناقص 1
متوسط عمليات التنشيط التي تظهر في رأسك، ومتوسطها
أقرب إلى الصفر. وتمامًا كما تقوم في بعض الأحيان
بتدريب خوارزمية تعليم يمكن أن تقوم بتوسيط البيانات وجعل متوسط البيانات 0 باستخدام دالة
tanh بدلاً من دالة sigmoid. ويكون لذلك تأثير توسيط
للبيانات إلى حد ما بحيث يكون وسيط البيانات أقرب من الصفر
بدلاً من، ربما، 0.5. وذلك يجعل التعلم للطبقة التالية
أسهل قليلاً. وسوف نتحدث أكثر عن هذا الأمر في الدورة
التدريبية الثانية عندما نتحدث عن خوارزميات التحسين كذلك. إلا أنه من بين الحقائق الرئيسية أنني لم أعد أستخدم دالة التنشيط
sigmoid على الإطلاق. فدالة tanh غالبًا ودائمًا ما تكون
أفضل بشكل قاطع. والاستثناء الوحيد يتمثل في طبقة المخرجات
لأنه إذا كانت "y" تساوي 0 أو 1، فمن المنطقي أن تكون
قيمة y hat رقمًا وهو القيمة التي يتم إخراجها بين 0 و1
وليس بين ناقص 1 و1. إذًا، فإن الاستثناء الوحيد الذي يمكن أن
نستخدم فيه دالة تنشيط sigmoid هو عندما نستخدم التصنيف الثنائي وفي تلك الحالة، يمكنك استخدام دالة تنشيط
sigmoid لطبقة المخرجات. وبالتالي تكون قيمة "g" لـ "z" هنا تساوي سيجما "z" 2. وبالتالي ما تراه في هذا المثال هو الحالة التي
يمكن أن يكون لديك فيها دالة تنشيط tanh للطبقة المخفية، ودالة تنشيط sigmoid
لطبقة المخرجات. وبالتالي، يمكن أن تكون دوال إلغاء التنشيط
مختلفة للطبقات المختلفة. ويمكن أن نذكر أنه في بعض الأحيان تكون
دوال التنشيط مختلفة للطبقات المختلفة، ويمكن أن نستخدم الأقواس
المربعة العلوية هذه كذلك للإشارة إلى أن قيمة "g" في القوس المربع الأول
قد تكون مختلفة عن قيمة "g" للقوس المربع الثاني. ومرة أخرى، يشير القوس المربع العلوي
الأول إلى هذه الطبقة فيما يشير القوس المربع العلوي الثاني
إلى طبقة المخرجات. والآن، من بين سلبيات دالة sigmoid ودالة tanh أنه إذا كانت "z"
كبير للغاية أو صغيرة للغاية يصبح الانحدار أو المشتقة أو الميل
لهذه الدالة صغيرًا للغاية. وبالتالي، إذا كانت قيمة "z" صغيرة للغاية ينتهي الحال بميل الدالة
بحيث يكون قريبًا من الصفر. وبالتالي، يمكن أن يؤدي
ذلك إلى إبطاء الانحدار التدريجي. وبالتالي، هناك خير آخر وهو
شهير للغاية في تعلم الماكينات، ويطلق عليه اسم
وحدة التصحيح الخطية. وبالتالي، تبدو دالة القيمة مثل ذلك. وتكون الصيغة هي a = max(0,z). وبالتالي، يتكون المشتقة 1، طالما
كانت قيمة "z" موجبة. وتكون المشتقة أو الميل 0، عندما
تكون قيمة "z" سلبية. إذا كنت تقوم بتنفيذ ذلك فمن الناحية الفنية، لا تكون المشتقة محددة بشكل
جيد عندما تكون قيمة "z" صفرًا بالضبط. لكن، عندما تقوم بتنفيذ ذلك
في الكمبيوتر فإن الإجابة التي تحصل عليها بالضبط
هي أن "z" تساوي 0000000000000. وهي قيمة صغيرة للغاية، وبالتالي لا تحتاج
إلى القلق حيالها في واقع الممارسة العملية. ويمكن أن نتخيل قيمة المشتقة
عندما تكون "z" تساوي الصفر ويمكن أن نتخيل بأن قيمته
1 أو 0، وحينها تعمل بشكل جيد. وبالتالي تكون الحقيقة أنه لا يمكن
تمييزها وتلك هي الحقيقة، لذا إليكم بعض القواعد الرئيسية
لاختيار دوال التنشيط. إذا كانت قيمة المخرجات 0 و1، إذا كنت
تستخدم التصنيف الثنائي تكون دالة تنشيط sigmoid خيارًا طبيعيًا
للغاية لطبقة المخرجات. وبالتالي، بالنسبة لدوال ReLU
لكل الوحدات الأخرى أو وحدة التصحيح الخطية تكون بمثابة الخيار
الافتراضي لدالة التنشيط. وبالتالي، إذا لم تكن متأكدًا حيال ما
تستخدمه للطبقة المخفية لديك يمكن أن تستخدم دالة
تنشيط ReLU. فهي ما نرى أن أغلب الأشخاص
يستخدمونها في أغلب الأحوال. رغم أنه في بعض الأحيان يستخدم الأشخاص
كذلك دالة تنشيط tanh. من بين عيوب دالة ReLU أن
المشتقة تساوي الصفر عندما تكون قيمة "z" سلبية. في واقع الممارسة العملية، فإن ذلك يؤتي
ثماره بشكل جيد تمامًا. إلا أن هناك نسخة أخرى من دالة ReLU
يطلق عليها اسم leaky ReLU. وسوف أعطيكم الصيغة في
الشريحة التالية. ولكن، بدلاً من أن تكون صفرًا عندما تكون
قيمة "z" سلبية فإنها فقط تأخذ ميلاً بسيطًا مثل ذلك، وبالتالي
يطلق عليها اسم دالة leaky ReLU. وغالبًا ما تعمل بشكل أفضل من
دالة تنشيط ReLU رغم أنها لا تستخدم بنفس القدر في
واقع الممارسة العملية. رغم ذلك، أي دالة منهما يمكن أن تكون جيدة وإذا كان يلزم عليك الاختيار بينهما
فإنني دائمًا ما أستخدم دالة ReLU. وميزة كلا دالتي ReLU و
leaky ReLU أنه بالنسبة للكثير من المساحة لقيمة "Z"
، مشتقة دالة التنشيط يكون ميل دالة التنشيط مختلفًا
عن الصفر تمامًا. وبالتالي، في واقع الممارسة العملية
فإن استخدام دالة تنشيط ReLU تتعلم الشبكة العصبية الخاصة بك بشكل أسرع في
الغالب من استخدام دالة tanh أو استخدام دالة تنشيط sigmoid. والسبب الرئيسي أن تلك التأثيرات تكون أقل لميل الدالة نحو الصفر، مما يؤدي
إلى إبطاء التعليم. وأنا أدرك أنه بالنسبة لنصف نطاق "z"
يكون ميل دالة ReLU صفرًا لكن، في واقع الممارسة العملية، تكون قيمة "z"
أكبر من الصفر لعدد كافٍ من الوحدات المخفية لديك. وبالتالي، يمكن أن يكون التعليم سريعًا تمامًا
لأغلب أمثلة التدريب. لذا، دعونا نقوم بتلخيص الإيجابيات والسلبيات
لدوال التنشيط المختلفة بشكل سريع. هذه دالة تنشيط sigmoid. ما أقوله أنه يجب عدم استخدام هذه الدالة
على الإطلاق باستثناء طبقة المخرجات إذا كنت تقوم بإجراء التصنيف الثنائي، أو
ربما لا تقم باستخدام هذه الدالة إطلاقًا. والسبب وراء عدم استخدامي لهذه الدالة إطلاقًا
هو أن دالة tanh تعتبر أفضل كثيرًا بشكل قاطع. وتكون دالة تنشيط tanh هكذا. وبالتالي تكون دالة التنشيط الافتراضية والأكثر شيوعًا هي
دالة ReLU وهي هكذا. وبالتالي، إذا لم تكن متأكدًا من الدالة التي
يجب أن تستخدمها فاستخدم هذه الدالة ويمكنك كذلك تجربة دالة
leaky ReLU. حيث يمكن أن تكون القيمة (0.01 z, z). حسنًا؟ بالتالي، تكون قيمة "a"
الحد الأقصى لقيمة "z" عدد 0.01 مرة و"z"، وبالتالي يعطينا ذلك بعض
الانحناءات في الدالة. ويمكن أن تتساءل
لماذا هذا الثابت 0.01؟ حسنًا، يمكنك استخدام هذا المعامل الآخر
لخوارزمية التعلم. وبعض الأشخاص يقولون أن هذا يؤتي
ثماره بشكل أفضل. لكني بالكاد أرى أشخاصًا يقومون بذلك. لكن، إذا كنت ترغب في تجربة ذلك في
التطبيق الخاص بك يمكنك فعل ذلك بكل حرية. ويمكنك أن ترى كيف يعمل ذلك
ومدى كفاءته والالتزام به إذا كان
يعطيك نتائج جيدة. وبالتالي، أتمنى أن يمنحك ذلك بعض
المعلومات حول بعض خيارات دوال التنشيط التي يمكنك استخدامها في الشبكة
العصبية الخاصة بك. ومن بين الموضوعات التي سوف تراها في
التعلم العميق أنك في الغالب يكون لديك الكثير من الخيارات المختلفة فيما يتعلق بكيفية عمل
التعليمات البرمجية للشبكة العصبية الخاصة بك. والتي تتراوح بين عدد الوحدات المخفية
وتمر باختيار دالة التنشيط وصولاً إلى كيفية تهيئة الطرق والذي
سنراه بعد ذلك. هناك الكثير من الخيارات مثل ذلك. ويتضح أنه من الصعب في بعض الأحيان الحصول
على توجيهات جيدة لما يحل المشكلة لديك بأفضل
شكل ممكن. وبالتالي، طوال هذه الدورات التدريبية
سوف أستمر في إعلامكم بما أراه في المجال من ناحية ما
يكون أكثر شهرة أو أقل شهرة. ولكن، بالنسبة للتطبيق الخاص بك
والخصائص المميزة له من الصعب للغاية بشكل فعلي معرفة ما يؤتي
ثماره بأفضل شكل ممكن بالضبط مسبقًا. وبالتالي، من النصائح التي يمكن توجيهها أنك
إذا لم تكن متأكدًا حيال دالة التنشيط التي تعمل بأفضل شكل ممكن
فجربها جميعها، وقم بتقييمها اعتمادًا على مجموعة تحقق للمقاومة، أو مجموعة
تطوير، والتي سوف نتحدث عنها في وقت لاحق ونرى أيها يعمل بشكل أفضل
ثم استخدمها. وأعتقد أنه من خلال اختبار هذه الخيارات
المختلفة للتطبيق الخاص بك يمكن أن تكون أفضل في التدقيق المستقبلي لبنية
الشبكة العصبية الخاصة بك مقابل الخصائص المميزة للمشكلة الخاصة بك
بالإضافة إلى تطورات الخوارزميات. بدلاً من أن أقول لك أن تستخدم
دالة تنشيط ReLU دائمًا وعدم استخدام أي دالة أخرى. وقد يسري ذلك أو لا يسري
على أي مشكلة تعمل عليها سواء في المستقبل القريب أو في
المستقبل البعيد. حسنًا، لقد كان ذلك فيما
يتعلق باختيار دوال التنشيط ولقد رأيت دوال التنشيط
الأكثر شهرة. وهناك سؤال آخر يمكن أن
يراودك في بعض الأحيان وهو لماذا قد تحتاج إلى استخدام
دالة تنشيط في الأساس؟ لماذا لا نتوقف عن استخدامها؟ لذا، دعونا نتحدث عن ذلك في
الفيديو التالي، حيث سترى لماذا تحتاج الشبكات العصبية إلى بعض
أشكال دوال التنشيط غير الخطية.