1
00:00:00,360 --> 00:00:04,530
在上一节课中 我们看到了如何计算神经网络

2
00:00:04,530 --> 00:00:06,610
对于一个训练实例的预测

3
00:00:06,610 --> 00:00:11,520
在这节课中 我们会看到如何对多个训练实例矢量化

4
00:00:11,520 --> 00:00:15,350
这个结果会和我们之前看到的逻辑回归的结果很相似

5
00:00:15,350 --> 00:00:19,050
通过把不同的训练实例按列堆叠在一个矩阵里面

6
00:00:19,050 --> 00:00:23,630
我们就可以拿来之前的视频里的等式

7
00:00:23,630 --> 00:00:27,860
做一些很小的改动 使得神经网络在同时计算

8
00:00:27,860 --> 00:00:32,340
所有训练实例的预测

9
00:00:32,340 --> 00:00:35,080
那么我们来看一下具体怎么做

10
00:00:35,080 --> 00:00:40,192
这些是我们之前视频看到的4个等式

11
00:00:40,192 --> 00:00:41,348
用来计算z1 a1 z2 和a2 (如图示)

12
00:00:41,348 --> 00:00:46,867
这些等式告诉我们 给定一个特征矢量x (如图示)

13
00:00:46,867 --> 00:00:53,810
我们可以用它们来取得一个训练实例的a2=y^ (如图示)

14
00:00:54,920 --> 00:01:00,050
如果我们有m个训练实例 我们需要重复这个过程

15
00:01:00,050 --> 00:01:01,870
比如说 对于第一个训练实例

16
00:01:01,870 --> 00:01:06,600
给定x^(1) 来计算y^(1) (如图示)

17
00:01:06,600 --> 00:01:11,062
这是第一个训练实例的预测

18
00:01:11,062 --> 00:01:16,537
接下来用x^(2)来生成预测y^(2)（如图示）

19
00:01:16,537 --> 00:01:23,050
以此类推 直到用x^(m)来生成预测y^(m) (如图示)

20
00:01:23,050 --> 00:01:28,349
为了用激活函数的标记法

21
00:01:28,349 --> 00:01:31,669
我会(把y^(1))写作a^[2](1)（如图示)

22
00:01:31,669 --> 00:01:36,676
接下来是a^[2](2) (如图示)

23
00:01:36,676 --> 00:01:40,640
然后是a^[2](m) (如图示)

24
00:01:40,640 --> 00:01:46,830
这个记号法a^[2](i)

25
00:01:46,830 --> 00:01:52,520
这个圆括号代表第i个训练实例

26
00:01:52,520 --> 00:01:57,220
而这个方括号代表第2层

27
00:01:58,530 --> 00:02:02,460
这就是方括号和圆括号里的指数代表的意思

28
00:02:04,170 --> 00:02:07,920
所以如果你有一个未矢量化的实现

29
00:02:07,920 --> 00:02:11,000
当你想要计算所有训练实例的预测时

30
00:02:11,000 --> 00:02:15,630
你需要用一个循环 i从1到m (如图示)

31
00:02:15,630 --> 00:02:18,260
实现这4个等式

32
00:02:18,260 --> 00:02:24,162
需要让z^[1](i) (如图示)

33
00:02:24,162 --> 00:02:30,064
= W(1) x(i) + b[1]

34
00:02:30,064 --> 00:02:38,253
a[1](i) = sigma of z[1](1)

35
00:02:38,253 --> 00:02:43,683
z[2](i) = w[2]a[1](i)+ b[2]

36
00:02:43,683 --> 00:02:50,099
(如图示 念写的公式)

37
00:02:50,099 --> 00:02:56,686
(如图示 念写的公式)

38
00:02:56,686 --> 00:03:03,172
所以基本上就是上面这4个等式

39
00:03:03,172 --> 00:03:08,788
在所有训练实例的因变量上加上上角标(i)

40
00:03:08,788 --> 00:03:12,612
也就是在x z 和a上面加上上角标(i)

41
00:03:12,612 --> 00:03:18,570
来计算所有m个训练实例的输出

42
00:03:18,570 --> 00:03:23,930
我们想做的是矢量化整个计算 来去除这个for循环

43
00:03:23,930 --> 00:03:27,680
万一这部分看起来像是我在讲一些线性代数的繁琐细节

44
00:03:27,680 --> 00:03:31,170
实际上能够正确的实现这个

45
00:03:31,170 --> 00:03:34,580
在这个深度学习的纪元是很重要的

46
00:03:34,580 --> 00:03:38,160
而且我们在这门课里是非常仔细的选择标记法

47
00:03:38,160 --> 00:03:41,460
以便于矢量化的过程尽可能的容易

48
00:03:41,460 --> 00:03:46,140
所以我希望走进这些繁琐细节能够实际上帮助你们

49
00:03:46,140 --> 00:03:49,750
更快得到这些算法的正确实现

50
00:03:51,060 --> 00:03:56,210
那么就让我把这一部分的代码复制到下一页 (如图示)

51
00:03:56,210 --> 00:03:57,880
然后我们会看到如何把这部分矢量化

52
00:03:59,130 --> 00:04:02,154
这是我们前一页看到的

53
00:04:02,154 --> 00:04:04,324
for循环走过所有m个训练实例

54
00:04:04,324 --> 00:04:09,769
记得我们定义矩阵X

55
00:04:09,769 --> 00:04:16,860
是由训练实例按列堆叠组成的 (如图示)

56
00:04:16,860 --> 00:04:20,180
也就是说拿来训练实例 把它们按列堆叠

57
00:04:20,180 --> 00:04:23,220
所有这是一个维度为n

58
00:04:23,220 --> 00:04:27,860
或者说n_x乘以m的矩阵 (如图示)

59
00:04:29,198 --> 00:04:32,630
我就直接说重点 告诉你们需要做什么

60
00:04:32,630 --> 00:04:35,760
来得到这个for循环的矢量化实现

61
00:04:35,760 --> 00:04:41,394
实际上你们需要做的是计算

62
00:04:41,394 --> 00:04:46,035
Z[1] = W[1] X + b[1]

63
00:04:46,035 --> 00:04:50,692
(如图示 念写的公式)

64
00:04:50,692 --> 00:04:56,157
(如图示 念写的公式)

65
00:04:56,157 --> 00:05:01,348
(如图示 念写的公式)

66
00:05:01,348 --> 00:05:10,100
(如图示 念写的公式)

67
00:05:10,100 --> 00:05:16,440
所以如果你们想要相似对比 我们就是从小写的x们出发

68
00:05:16,440 --> 00:05:23,480
把小写的x们按列堆叠得到了大写的X矩阵

69
00:05:23,480 --> 00:05:28,494
如果我们对于z做同样的操作

70
00:05:28,494 --> 00:05:33,509
比如说 我们把 z^[1](1) z^[1](2) 等等 (如图示)

71
00:05:33,509 --> 00:05:40,290
注意这些也都是列矢量 直到z^[1](m) (如图示)

72
00:05:40,290 --> 00:05:46,270
这就是这第一个变量 (左上角z^[1](i)) 
所有m个按列堆叠起来的

73
00:05:46,270 --> 00:05:50,045
这就是矩阵Z^[1] (如图示)

74
00:05:50,045 --> 00:05:55,299
类似的 如果我们看这个变量 (左上角a^[1](i))

75
00:05:55,299 --> 00:06:00,957
把a^[1](1) a^[1](2) 等等 (如图示)

76
00:06:00,957 --> 00:06:06,980
直到a^[1](m) 把它们按列堆叠起来 (如图示)

77
00:06:06,980 --> 00:06:11,610
那么这个 就像我们从小写的x到大写的X

78
00:06:11,610 --> 00:06:13,280
从小写的z到大写的Z

79
00:06:13,280 --> 00:06:20,920
这里我们从小写的列矢量a们到大写的A^[1]矩阵

80
00:06:20,920 --> 00:06:26,685
(箭头指示上面等式里的A^[1]) 
对于Z^[2]和A^[2]是类似的 (如图示)

81
00:06:26,685 --> 00:06:30,141
它们也是通过把这些矢量

82
00:06:30,141 --> 00:06:32,016
横着堆叠起来

83
00:06:32,016 --> 00:06:37,326
也就是把这些矢量横着堆叠起来 (如图示)

84
00:06:37,326 --> 00:06:40,840
来得到大写的Z^[2]和A^[2]

85
00:06:40,840 --> 00:06:44,042
这个标记法有一个性质可能会有助于你们理解它

86
00:06:44,042 --> 00:06:47,391
这些矩阵 比如说Z和A

87
00:06:47,391 --> 00:06:51,420
横着来看我们的指数是训练实例

88
00:06:51,420 --> 00:06:55,631
这也就是为什么横的指数对应不同的训练实例

89
00:06:55,631 --> 00:06:59,730
当我们从左到右扫过时 我们扫描了整个训练集

90
00:06:59,730 --> 00:07:04,617
而竖着来看

91
00:07:04,617 --> 00:07:06,130
竖的指数对应着圣经网络里面的不同结点

92
00:07:06,130 --> 00:07:11,077
比如说 这个结点

93
00:07:11,077 --> 00:07:16,554
这个矩阵最左上角的值 (如图示) 对应着

94
00:07:16,554 --> 00:07:21,633
第一个训练实例的第一个隐藏单元的激活值

95
00:07:21,633 --> 00:07:25,812
它下面一个值对应着第二个隐藏单元

96
00:07:25,812 --> 00:07:27,525
对第一个训练实例的激活值

97
00:07:27,525 --> 00:07:31,505
然后是第三个隐藏单元对第一个训练实例的激活值
以此类推

98
00:07:31,505 --> 00:07:37,540
所以当我们向下扫的时候 我们对应的是隐藏单元号码

99
00:07:39,670 --> 00:07:42,564
而当我们横向扫过的时候 我们是从第一个隐藏单元

100
00:07:42,564 --> 00:07:45,450
对于第一个训练实例
GTC字幕组翻译

101
00:07:45,450 --> 00:07:48,240
到第一个隐藏单元对于第二个
第三个训练实例 等等

102
00:07:48,240 --> 00:07:53,718
直到这个结点 (如图示)

103
00:07:53,718 --> 00:07:59,030
对应第一个隐藏单元对最后第m个训练实例的激活值

104
00:08:00,760 --> 00:08:07,663
所以横向来看矩阵A代表不同训练实例

105
00:08:10,150 --> 00:08:14,195
而纵向来看不同的指数

106
00:08:14,195 --> 00:08:17,589
代表不同的隐藏单元

107
00:08:22,342 --> 00:08:26,870
我们可以把类似的直觉用于矩阵Z

108
00:08:26,870 --> 00:08:31,840
对于X也是一样 它横向来看是不同训练实例

109
00:08:31,840 --> 00:08:36,227
纵向来看是不同的输入特征

110
00:08:36,227 --> 00:08:41,180
这些输入特征其实也就是输入层的不同节点

111
00:08:42,750 --> 00:08:46,600
所以有了这些等式
你们现在知道了怎么实现神经网络矢量化

112
00:08:46,600 --> 00:08:51,320
也就是把多个实例矢量化

113
00:08:51,320 --> 00:08:55,130
在下节课中 我想要多给你们一些理由

114
00:08:55,130 --> 00:08:59,070
来解释为什么这是这种矢量化的一个正确实现

115
00:08:59,070 --> 00:09:03,468
其实这个理由和你们之前看到的逻辑回归的很相似

116
00:09:03,468 --> 00:09:05,300
我们下一节再见