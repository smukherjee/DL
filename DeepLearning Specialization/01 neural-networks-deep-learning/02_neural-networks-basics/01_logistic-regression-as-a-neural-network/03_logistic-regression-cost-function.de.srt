1
00:00:00,000 --> 00:00:01,530
Im vorherigen Video

2
00:00:01,530 --> 00:00:04,227
sahen sie das logistische Regressionsmodell.

3
00:00:04,227 --> 00:00:07,526
Um die Parameter w und b des logistischen Regressionsmodells zu trainieren,

4
00:00:07,526 --> 00:00:10,570
sollten Sie eine Kostenfunktion (= cost function) definieren.

5
00:00:10,570 --> 00:00:14,430
Schauen wir uns die Kostenfunktion an, wie wir sie für das Training der logistischen Regression nutzen können.

6
00:00:14,430 --> 00:00:18,195
Zur Wiederholung, dies ist was wir aus den vorherigen Folien definiert hatten.

7
00:00:18,195 --> 00:00:20,792
Somit wird die Ausgabe "y-Dach" gleich Sigmoid von

8
00:00:20,792 --> 00:00:24,690
w transponiert mal x plus b, wobei Sigmoid von z wie hier definiert ist.

9
00:00:24,690 --> 00:00:27,600
Um Parameter für ihr Modell zu lernen, seien ihnen ein

10
00:00:27,600 --> 00:00:31,200
Trainingsset mit m Trainingsbeispielen gegeben und es

11
00:00:31,200 --> 00:00:34,060
erscheint nur natürlich, dass sie daraus die Parameter w und b finden möchten,

12
00:00:34,060 --> 00:00:37,781
so dass zumindest für das Trainingsset die erhaltenen Ausgabe

13
00:00:37,781 --> 00:00:40,225
- die erhaltenen Vorhersagen aus dem Trainingsset,

14
00:00:40,225 --> 00:00:43,260
die wir nur als y-Dach hoch (i) schreiben - nahe bei

15
00:00:43,260 --> 00:00:47,720
den zugrundeliegenden Etiketten y hoch (i) aus dem Trainingsset liegen.

16
00:00:47,720 --> 00:00:52,110
Um etwas mehr Details in die Gleich oben einzufügen:

17
00:00:52,110 --> 00:00:56,205
Wir sagten, dass y-Dach, wie oben definiert, für ein Trainingsbeispiel x definiert ist

18
00:00:56,205 --> 00:01:00,930
und selbstverständlich benutzen wir für jedes Trainingsbeispiel

19
00:01:00,930 --> 00:01:03,240
in runden Klammern diese hochgestellte Indexierung

20
00:01:03,240 --> 00:01:07,710
um die einzelnen Beispiele zu unterscheiden.

21
00:01:07,710 --> 00:01:12,870
Ihre Prognose aus dem Trainingsbeispiel (i), es ist y-Dach (i), wird erhalten indem man

22
00:01:12,870 --> 00:01:18,835
die Sigmoidfunktion nimmt und sie auf w transponiert mal x hoch (i)

23
00:01:18,835 --> 00:01:25,905
- die Eingabe des Trainingsbeispiels - plus b anwendet; zudem kann man z (i)

24
00:01:25,905 --> 00:01:30,110
auch wie folgt definieren: z (i) ist gleich w transpose x (i) plus b.

25
00:01:30,110 --> 00:01:31,350
So, durch den gesamten Kurs

26
00:01:31,350 --> 00:01:33,966
werden wir diese Notationskonvention anwenden,

27
00:01:33,966 --> 00:01:41,605
dass das hochgestellte, geklammerte "i" Daten - wie x, y, z oder sonst etwas -

28
00:01:41,605 --> 00:01:47,615
referenziert, die mit dem i. Trainingsbeispiels verbunden sind;

29
00:01:47,615 --> 00:01:50,885
resp. mit dem i. Beispiel assoziieren.

30
00:01:50,885 --> 00:01:54,840
Das also bedeutet das hochgestellte "i" in Klammern.

31
00:01:54,840 --> 00:01:57,630
Nun, schauen wir uns an welche Verlustfunktion resp. Fehlerfunktion

32
00:01:57,630 --> 00:02:01,315
wir benutzen können, um zu messen wie gut unser Algorithmus ist.

33
00:02:01,315 --> 00:02:06,015
Eine Sache, die man tun könnte, wäre den Verlust (L) so zu definieren, dass wenn ihr Algorithmus "y-Dach" ausgibt

34
00:02:06,015 --> 00:02:12,320
und die wahre Bezeichnung "y" ist, man daraus möglicherweise den quadratischen oder halben quadratischen Fehler berechnet.

35
00:02:12,320 --> 00:02:14,975
Es zeigt sich jedoch, dass man dies tun könnte, aber bezüglich der

36
00:02:14,975 --> 00:02:17,670
logistischen Regression Menschen dies in der Regel so nicht tun.

37
00:02:17,670 --> 00:02:21,000
Und zwar, wenn man dazu kommt die Parameter lernen zu lassen,

38
00:02:21,000 --> 00:02:25,682
findet man heraus, dass das Optimierungsproblem, wir werden später darauf eingehen, nicht konvex wird.

39
00:02:25,682 --> 00:02:30,105
Das heißt also: Man gerät in ein Optimierungsproblem, das mehrere lokale Optima aufweist.

40
00:02:30,105 --> 00:02:33,285
Damit findet der Gradientenabstieg (= gradient descent) möglicherweise nicht das globale Optimum.

41
00:02:33,285 --> 00:02:35,580
Falls Sie die letzten paar Kommentare nicht verstanden haben,

42
00:02:35,580 --> 00:02:38,320
machen Sie sich nichts daraus, wir werden in einem späteren Video dazu kommen.

43
00:02:38,320 --> 00:02:40,990
Doch die Intuition, die Sie mitnehmen sollten, ist dass Sie diese

44
00:02:40,990 --> 00:02:44,620
Funktion "L", genannt "Verlustfunktion" (= loss function), eine Funktion ist, die Sie benötigen

45
00:02:44,620 --> 00:02:51,265
um zu messen wie gut ihre Ausgabe "y-Dach" ist, wenn die wahre Bezeichnung "y" beinhaltet.

46
00:02:51,265 --> 00:02:54,345
Obwohl die quadratische Fehlerberechnung als eine vernünftige Wahl erscheinen mag,

47
00:02:54,345 --> 00:02:58,160
verursacht sie doch, dass der Gradientenabstieg nicht gut funktioniert.

48
00:02:58,160 --> 00:03:00,500
Aus diesem Grunde definieren wir für die logistische Regression

49
00:03:00,500 --> 00:03:05,695
eine andere Verlustfunktion. Sie spielt eine ähnliche Rolle wie der

50
00:03:05,695 --> 00:03:08,910
"quadratische Fehler", ergibt aber ein konvexes Optimierungsproblem.

51
00:03:08,910 --> 00:03:13,530
Wie wir in einem späteren Video sehen werden, wird dies viel leichter zu optimieren sein.

52
00:03:13,530 --> 00:03:17,310
Was wir daher für die logistische Regression verwenden,

53
00:03:17,310 --> 00:03:21,795
ist die nachfolgende Verlustfunktion, die ich gleich an dieser Stelle aufführe:

54
00:03:21,795 --> 00:03:31,740
minus ((y mal log y-Dach) plus ((eins minus y) mal

55
00:03:31,740 --> 00:03:34,600
log(eins minus y-Dach))).

56
00:03:34,600 --> 00:03:38,785
Hier ein paar Intuitionen weshalb diese Verlustfunktion Sinn ergibt:

57
00:03:38,785 --> 00:03:41,285
Beachten Sie, dass wenn wir den quadratischen Fehler benutzen,

58
00:03:41,285 --> 00:03:45,820
möchte man dass der quadratische Fehler so klein wie möglich ist.

59
00:03:45,820 --> 00:03:48,680
Und mit dieser Verlustfunktion für die logistischen Regression

60
00:03:48,680 --> 00:03:51,495
wollen wir dies ebenfalls so klein wie möglich.

61
00:03:51,495 --> 00:03:53,508
Um zu verstehen weshalb dies sinnvoll ist,

62
00:03:53,508 --> 00:03:55,260
wollen wir die beiden Fälle anschauen:

63
00:03:55,260 --> 00:03:56,570
Im ersten Fall

64
00:03:56,570 --> 00:03:59,430
sagen wir y ist gleich eins, dann ist die Verlustfunktion "y-Dach"

65
00:03:59,430 --> 00:04:05,415
Komma "y" einfach nur dieser erste Term, mit diesem negativen Zeichen.

66
00:04:05,415 --> 00:04:08,735
Also dieses negative log von "y-Dach";

67
00:04:08,735 --> 00:04:10,770
sofern y gleich 1 ist. Weil, wenn y gleich 1 ist,

68
00:04:10,770 --> 00:04:14,070
dann wird der zweite Term 1 minus y gleich 0.

69
00:04:14,070 --> 00:04:19,880
Dies besagt also, wenn "y" gleich 1 ist, möchte man negativ log von "y-Dach" so groß wie

70
00:04:19,880 --> 00:04:26,040
möglich, so das bedeutet man möchte dass der log von "y-Dach" groß wird,

71
00:04:26,040 --> 00:04:32,935
so groß wie möglich und das bedeutet man möchte, dass "y-Dach" groß ist.

72
00:04:32,935 --> 00:04:35,170
Aber, weil - wie Sie wissen - "y-Dach" eine

73
00:04:35,170 --> 00:04:38,440
Sigmoidfunktion ist, kann dies nie größer als 1 werden.

74
00:04:38,440 --> 00:04:41,850
Also, dies besagt, wenn "y" gleich 1 ist, möchte man

75
00:04:41,850 --> 00:04:44,050
"y-Dach" so groß wie möglich. Aber, weil dies nie größer

76
00:04:44,050 --> 00:04:48,220
als 1 sein kann, sagen wir besser man möchte dass "y-Dach" ebenfalls nahe bei 1 liegt.

77
00:04:48,220 --> 00:04:50,740
Im anderen Fall, wenn y gleich 0 ist,

78
00:04:50,740 --> 00:04:55,375
Wenn y gleich 0 ist, dann wird dieser erste Term der Verlustfunktion ebenfalls 0;

79
00:04:55,375 --> 00:05:01,290
gerade weil y = 0 ist. Folglich definiert der zweite Term die Verlustfunktion.

80
00:05:01,290 --> 00:05:07,210
Damit wird die Verlustfunktion negativ log (1 minus "y-Dach")

81
00:05:07,210 --> 00:05:11,480
And daher, wenn Sie in ihrem Lernverfahren versuchen die Verlustfunktion klein zu machen,

82
00:05:11,480 --> 00:05:19,450
bedeutet dies Sie möchten, dass der log von 1 minus "y-Dach" groß wird

83
00:05:19,450 --> 00:05:22,050
und - weil hier ein negatives Zeichen steht - und

84
00:05:22,050 --> 00:05:24,660
durch ähnliche Begründungen kann man folgern,

85
00:05:24,660 --> 00:05:30,870
dass diese Verlustfunktion versucht "y-Dach" so klein wie möglich zu machen.

86
00:05:30,870 --> 00:05:34,320
Und nochmals, weil "y-Dach" zwischen 0 und 1 sein muss,

87
00:05:34,320 --> 00:05:38,155
besagt dies, dass wenn y gleich 0 ist, dass ihre Verlustfunktion

88
00:05:38,155 --> 00:05:43,790
die Parameter zwingt, "y-Dach" so nahe wie möglich an 0 zu bringen.

89
00:05:43,790 --> 00:05:48,305
Nun, es gibt viele Funktionen mit dem Rafidah Effekt, dass wenn y gleich 1 ist,

90
00:05:48,305 --> 00:05:52,950
wir versuchen "y-Dach" groß zu machen und wenn y gleich 0 ist, wir versuchen

91
00:05:52,950 --> 00:05:55,150
"y-Dach" klein zu machen. Wir haben soeben hier in grün

92
00:05:55,150 --> 00:05:59,920
eine etwas informelle Rechtfertigung für diese spezielle Verlustfunktion gegeben.

93
00:05:59,920 --> 00:06:03,970
Später werden wir ein optionales Video anbieten, das eine formalere Begründung gibt,

94
00:06:03,970 --> 00:06:08,500
weshalb wir in der logistischen Regression gerne die Verlustfunktion in dieser besonderen

95
00:06:08,500 --> 00:06:13,630
Form benutzen. Abschließend: Die Verlustfunktion wurde im Hinblick auf ein einzelnes

96
00:06:13,630 --> 00:06:16,760
Trainingsbeispiel definiert. Es misst, wie gut man dies bei einem einzelnen Trainingsbeispiel

97
00:06:16,760 --> 00:06:21,148
macht. Ich werde jetzt etwas definieren, was man die Kostenfunktion nennt, welche

98
00:06:21,148 --> 00:06:24,690
misst wie gut unser Tun in Bezug auf die gesamte Trainingsmenge ist.

99
00:06:24,690 --> 00:06:28,660
Die Kostenfunktion J, welche auf ihre

100
00:06:28,660 --> 00:06:33,130
Parameter "w" und "b" angewendet wird, wird der Durchschnitt 1 über "m" mal

101
00:06:33,130 --> 00:06:43,270
die Summe der Verlustfunktionen, die auf jedes einzelne der Trainingsbeispiele angewendet wurde.

102
00:06:43,270 --> 00:06:45,435
Wobei hier "y-Dach" selbstverständlich die ausgegebene

103
00:06:45,435 --> 00:06:49,570
Vorhersage durch den Algorithmus der logistischen Regression ist, wobei - wie Sie wissen -

104
00:06:49,570 --> 00:06:52,430
eine partikuläre Menge von Parametern w und b benutzt wird.

105
00:06:52,430 --> 00:06:54,480
Und, wenn wir dies alles berücksichtigen,

106
00:06:54,480 --> 00:06:58,010
wird dies negativ 1 dividiert durch m

107
00:06:58,010 --> 00:07:03,550
mal die Summe von "i" gleich "1" bis "m" der Definition der Verlustfunktion;

108
00:07:03,550 --> 00:07:07,530
wobei die y(i) log "y-Dach"(i)

109
00:07:07,530 --> 00:07:14,530
plus (1 minus y(i) mal log (1 minus "y-Dach"(i))).

110
00:07:14,530 --> 00:07:17,880
Ich denke, ich könnte hier eckige Klammern setzen.

111
00:07:17,880 --> 00:07:20,945
Man beachte, das Minuszeichen ist hier ausserhalb von allem anderen.

112
00:07:20,945 --> 00:07:23,665
Also, die Terminologie, welche ich benutzen werde, ist dass

113
00:07:23,665 --> 00:07:29,120
die Verlustfunktion wird nur auf ein einzelnes Trainingsbeispiel angewendet, wie hier

114
00:07:29,120 --> 00:07:33,010
und die Kostenfunktion sind die Kosten für Ihre Parameter.

115
00:07:33,010 --> 00:07:36,115
Um die Parameter w und b des logistischen Regressionsmodells zu trainieren,

116
00:07:36,115 --> 00:07:38,980
werden wir versuchen Parameter w und b so zu finden,

117
00:07:38,980 --> 00:07:43,475
dass die Kostenfunktion J über allem, wie unten beschrieben, minimiert wird.

118
00:07:43,475 --> 00:07:48,040
Alles in allem, haben Sie nun gesehen wie das Modell der Logistischen Regression,

119
00:07:48,040 --> 00:07:50,770
die Verlustfunktion des Trainingsbeispiels und die

120
00:07:50,770 --> 00:07:54,190
umfassende Kostenfunktion für die Parameter ihres Algorithmus aussieht.

121
00:07:54,190 --> 00:07:59,485
Es stellt sich heraus, dass die Logistische Regression als ein sehr, sehr kleines neuronales Netz

122
00:07:59,485 --> 00:08:01,905
betrachtet werden kann. Damit Sie ein Gefühl darüber erhalten können, wie neuronale Netze

123
00:08:01,905 --> 00:08:04,965
funktionieren, werden wir im nächsten Video darüber besprechen.

124
00:08:04,965 --> 00:08:08,230
Lasst uns zum nächsten Video gehen, das zeigt wie man die logistische Regression

125
00:08:08,230 --> 00:08:11,630
als ein sehr kleines neuronales Netzwerk betrachten kann.