前回の動画では、l個の層を持つ ディープニューラルネットワークとは何か、 そしてそのようなネットワークを
表現するために使う 表記法をお話ししました。 この動画では、
ディープネットワークにおいて どのように順伝播を行うのか見ていきます。 まずいつものように、
一つの教師データXについて 順伝播がどのように行われるか学び、 その後、全ての教師サンプルに対して 一気に順伝播を行うために ベクトル化する方法について お話しします。 一つの教師サンプルxが与えられたとき、 一つ目の層の活性化を計算する方法です。 一つ目の層では、 z[1]=W[1]x+b[1]です。 W[1]とb[1]は、 一層目の活性化に影響を与えるパラメータです。 これがニューラルネットワークの一層目です。 次に、この層の活性化を計算します。 g(z[1])ですね。 活性化関数gは 層によって違うので、 [1]というインデックスをつけて 一層目の活性化関数であることを示します。 さあ、一層目の活性化を 計算し終わりました。 二層目、この層はどうでしょう。 z[2]=W[2]a[1]+b[2]なので、 二層目の活性化は 重みの行列×一層目からの出力 この値ですね、 そこに二層目のバイアスベクトルを
足したものになります。 そしてa[2]は、
z[2]に活性化関数を適用したものです。 二層目は以上です。 ここからも同様に進めていって、 出力レイヤー、つまり四層目まで行きます。 z[4]は、 その層のためのパラメータ ×前の層からの活性化 +バイアスベクトル そして今回も、a[4]=g[4](z[4])です。 これが、予測される出力、
つまりy hatを計算する方法です。 一つ言っておきたいのは、 ここのxはa[0]と等しいということです。 なぜなら、入力ベクトルxは 0層目の活性化だからです。 なので、xを消してa[0]と書いてみます。 すると、これら全ての等式が 同じような形になります。 一般的なルールは、 z[l]=W[l]a[l-1]+b[l] そしてこの層の活性化は zの値に活性化関数が適用されたものです。 これが、一般的な順伝播の計算です。 これで、 単一の教師サンプルの話は終わりました。 では、全ての教師サンプルを
一気に処理するために ベクトル化したらどうなるでしょう。 式は今までと似たようなものです。 一層目においては、 大文字のZ[1]=W[1]X+b[1]で、 A[1]=g[1](Z[1])となります。 XはA[0]だということを 覚えておいてください。 これは、列ごとに教師サンプルが
並んでいるものです。 これを取って A[0]と書き換えます。 次の層も 同じような感じです。 Z[2]=W[2]A[1]+b[2] A[2]=g[2](Z[2]) ベクトルzやaを取ってきて 並べただけです。 これは1つ目のサンプルのzベクトル、 これは2つ目のサンプルのzベクトル… という感じでm個目まで続き、 それらが列として並べられたものが 大文字のZと呼ばれる行列です。 大文字のAも、大文字のXと同様に 教師サンプルが列になって 左から右に並んでいるものです。 この工程の最後には、 Y hat=g(Z[4])が求められ、 これはA[4]にも等しいです。 これが、全ての教師サンプルの予測値を 水平方向に並べたものです。 では表記法を整理しましょう。 上に書かれている表記法ですが、 小文字のzとaを 大文字に変えることができます。 つまりここが大文字のZになりますね、 ここまでで、ベクトル化を使って
教師サンプル全体に対して 一気に順伝播を行う方法を見てきました。 A[0]はXですね。 このベクトル化した実装を見ると、 ここにforループをつけたら よさそうです。 for l=1...4、つまり lが1から大文字のLになるまでのループです。 そして、一層目の活性化を計算し、 二層目、三層目、四層目…と続きます。 なので、ここにforループを置けば うまくいくでしょう。 もちろん、
ニューラルネットワークを実装するとき なるべくforループを避けたいのは
わかっています。 しかし、ここはforループを使う以外に 実装する方法がないと 思われる場所なので、 順伝播を実装するときには forループを書いて全然構いません。 こうすることで、 一層目、二層目、三層目…と
計算することができます。 forループを使う以外に lを１から大文字のL、 つまりニューラルネットワークの
層の数だけ回せる方法を 知っている人はいないし、 私もわかりません。 なので、ここでforループを使うのは 全く問題ないのです。 ここまでで、 ディープニューラルネットワークの表記法と 順伝播の方法が終わりました。 これらのことにあなたが少し 親しみを感じられたなら、 隠れ層が1つだけのニューラルネットワークで やったことを取ってきて、 それに似たものを 繰り返しただけだからです。 ディープニューラルネットワークを 実装する中で、 バグのない実装をする方法の一つは、 行列のサイズについて 秩序正しく慎重に 考えることです。 私が自分のコードを書くときには、 ちょっと紙を取ってきて 今扱っている行列のサイズを 注意深く考えます。 次の動画で、 その方法を学びましょう。