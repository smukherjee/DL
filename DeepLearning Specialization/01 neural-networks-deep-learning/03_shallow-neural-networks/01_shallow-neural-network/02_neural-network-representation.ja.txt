ニューラルネットワークの絵を
何回か描きました このビデオでは
これらが一体何を意味するのかを話します 言い換えれば 描いてきたニューラルネットワークが
なにを表現しているかです まずは隠れ層が１つの
ニューラルネットワークに 焦点を当てて
始めたいと思います ニューラルネットワークの絵です それぞれの部分に
名前を付けます x1、x2、x3 が
縦に重ねられています これは入力層と
言われています 当然ではありますが
ニューラルネットワークへの入力が格納されています この丸の層があります これは隠れ層と
呼ばれています なぜ「隠れ」なのか
すぐに説明しますが この１つのノードで形成されている最後の層は 出力層と呼ばれています 予測値のyハットを生成します 教師あり学習で学習させるニューラルネットワークでは トレーニングセットには
入力値のx と目的出力値のy が含まれています 隠れ層が意味しているのは
トレーニングセットの中では この中央部のノードの真の値は
観測されていません つまりトレーニングセットからは
どんな値であるべきかが分かりません 入力は分かります 出力も分かりますが トレーニングセットの中では
隠れ層のものは分かりません トレーニングセットの中では
見えないので隠れ層という呼び方を 説明しているようなものです 表記の方法を紹介しましょう ベクトルｘで入力特徴量を
表わしていましたが 他の方法では 入力特徴量の値は
a[0] と表記します ａはまた活性化を表わし これはニューラルネットワークの それぞれの層が次の層に値を渡すのを
表わします 入力層はｘの値を
隠れ層に渡します このため入力層の活性化を
a(0)と呼びます 次の層の隠れ層では
これを受けて活性化の値を生成します これをａ[1]と書きます この最初のユニット、ノードは ａ[1]1の値を生成します ２つ目のノードは ａ[1]2の値を生成と
いうように続きます ａ[1]が意味するのは ４次元のベクトルで Python では４ｘ１の列ベクトルで
このようなものです ４次元なのは
４つの隠れノード、ユニットが 隠れ層にあるからです そして最終的に
出力層が値を生成します ａ[2]で実数です yハットはａ[2]の値を取ります 次にロジステック回帰で
ｙハット＝ａと書く理由です ロジステック回帰では１つしか出力層がないので []を付けて表記しないのです ニューラルネットワークでは
どの層から来たかを 明示的に表すために
[]を使っていきます 表記の決まりで
おかしいところは この見ているネットワークは
２層ニューラルネットワークと呼ばれます この理由はニューラルネットワークの層を数えるときに 入力層を数に入れないからです つまり隠れ層が１層目で
出力層は２層目です この表記方法の決まりでは
入力層を０層目と呼びます つまりこのニューラルネットワークは
厳密には３層でもあるかもしれません 入力層、隠れ層、出力層と
あるからです 研究論文や他のものを読んだ時もそうですが
このコースでは慣例の使われ方では この例のようなニューラルネットワークは
２層のニューラルネットワークと呼びます 公式には入力層は
数にいれないからです 最後に 後で出てくることに 隠れ層と出力層には
関連付けられるパラメーターがあります 隠れ層にはｗ、ｂというパラメーターが
関連付けられています これには[1]と書いてこのパラメーターが 隠れ層の１層目と関連があることを示します 後にこの例の場合 ｗは４ｘ３の行列で ｂは４ｘ１のベクトルに
なっているのを見ていきます 座標の最初の４は
隠れ層に４つのノードが 存在する事実から来ていて ３は３つの入力特徴量がある事実から来ています 後ほど この行列の配列について話します その時には より理解できているかもしれません しかし 出力層もまた関連付けられたパラメーターがあります パラメーターｗ[2]、ｂ[2]があります これらの配列は１ｘ４と１ｘ１となります １ｘ４なのは
隠れ層に４つの隠れユニットがあり 出力層が１ユニットしかないためです 繰り返しますが これらの行列やベクトルの配列を
今後のビデオで見ていきます さてここでは２層のニューラルネットワークが
どんなものか見てきました １つの隠れ層のニューラルネットワークでした 見ての通り、ここある。 このニューラルネットワークが
一体何を計算しているのか見ていきましょう ニューラルネットワークがｘを入力され ｙハットを計算するところまで
どのようにたどり着くかを見ましょう