В останньому відео ти бачив/ла 
як обчислити передбачення нейронної мережі (НМ), маючи один тренувальний зразок. В цьому відео ти побачиш 
як провести векторизацію для багатьох тренувальних зразків. Результати будуть дуже схожі до тих, що ти бачив/ла в ЛР, де складанням різних тренувальних зразків в стовпці матриці ми зможемо застосувати рівняння з попереднього відео і, трішки змінивши рівняння, 
дозволити НМ обчислити результати для всіх зразків за, практично, той же час. Давай детальніше розглянемо як це зробити. Ось 4 рівняння, які ми отримали в попередньому відео. 
Вони обчислюють z[1], a[1], z[2] і a[2]. Вони показують як, маючи вектор вхідних ознак x, можна отримати a[2], що =ŷ для окремого тренувального зразка. Тепер, коли ми маємо m тренувальних зразків, 
цей процес потрібно повторити для першого тренувального зразка - x з верхнім індексом 1 в круглих дужках - щоб обчислити ŷ⁽¹⁾ - передбачення для 1-ого тренувального зразка. Потім x⁽²⁾, щоб згенерувати передбачення ŷ⁽²⁾. І т.д. до x⁽ᵐ⁾, щоб згенерувати передбачення ŷ⁽ᵐ⁾. Щоб позначити всі ці функції активації, я буду використовувати позначення a[2]⁽¹⁾. Це буде a[2]⁽²⁾. і a[2]⁽ᵐ⁾. Тож це позначення - a[2]ŷ⁽ⁱ⁾. "i" в круглих дужках позначає i-ий тренувальний зразок, а 2 в квадратних дужках позначає 2-ий шар. Ось що означають індекси в круглих і квадратних дужках. Якщо подумати, то при невекторизованій реалізації, щоб обчислити передбачення для всіх тренувальних зразків потрібно застосувати цикл for по i від 1 до m. Тоді реалізувати оці 4 рівняння, правильно? Потрібно обчислити z[1]⁽ⁱ⁾=w[1]x⁽ⁱ⁾+b[1], a[1]⁽ⁱ⁾=σ(z[1]⁽ⁱ⁾), z[2]⁽ⁱ⁾=w[2]a[1]⁽ⁱ⁾+b[2] і z[2]⁽ⁱ⁾=w[2]a[1]⁽ⁱ⁾+b[2] і a[2]⁽ⁱ⁾=σ(z[2]⁽ⁱ⁾). Тож, по суті, до цих рівнянь вгорі ми додаємо верхній індекс i в круглих дужках. 
До всіх змінних, що залежать від тренувального зразка. Тобто, додаємо верхній індекс i в круглих дужках до x, z та a, якщо хочемо обчислити вихідні дані для m тренувальних зразків. Але ми б хотіли векторизувати ці обчислення і позбутися циклу for. І, до речі, якщо здається, що я використовую багато базової лінійної алгебри, то, виявляється, 
що це дуже важливо в еру глибокого навчання, це дає змогу правильно реалізувати такі обчислення. І ми ретельно підбирали позначення для цього курсу, щоб зробити кроки векторизації якомога простішими. Тож, я надіюсь, що ця база допоможе тобі швидше і правильно реалізувати ці алгоритми. Отож. Давай скопіюємо це блок на наступний слайд і подивимось як їх векторизувати. Тож ось що ми маємо з попереднього слайду в циклі for для всіх наших m тренувальних зразків. Тож згадаймо, що ми визначили матрицю X як складені ось так в стовпці наші тренувальні зразки. Тож беремо тренувальні зразки і складаємо їх в стовпці. Тож це стає n або nₓ*m розміру матрицю. Одразу скажу що потрібно зробити щоб отримати векторизовану реалізацію цього циклу for. Виявляється, що потрібно обчислити Z[1]=W[1]X+b[1], A[1]=σ(Z[1]), Z[2]=W[2]A[1]+b[2] і Z[2]=W[2]A[1]+b[2] і потім A[2]=σ(Z[2]). Тож це дуже схоже. Ми замінили маленькі x векторів на великі X матриць, склавши маленькі x в стовпці. Якщо зробити те саме для z. Наприклад, якщо взяти z[1]⁽¹⁾, z[1]⁽²⁾ і т.д. (а вони є векторами-стовпцями) до z[1]⁽ᵐ⁾, так? Тож, якщо оцю величину взяти і скласти в стовпці, то ми отримаємо матрицю Z[1]. І, аналогічно, для оцієї величини. Візьмемо a[1]⁽¹⁾, a[1]⁽²⁾ і т.д. до a[1]⁽ᵐ⁾ і складемо їх в стовпці. І тоді (так само як ми перейшли від маленьких x до великих і від маленьких z до великих) перейдемо від маленьких a, векторів, до великої A[1] ось тут і, аналогічно, до Z[2] і A[2]. Вони так само отримуються оцими векторами складеними по горизонталі і оцими векторами складеними по горизонталі, щоб отримати велику Z[2] і велику A[2]. Одна з властивостей таких позначень, яка, можливо, допоможе краще це уявити, - це те, що ці матриці Z та A горизонтально індексують тренувальні зразки. Ось чому горизонтальний [стовпцевий] індекс 
відповідає тренувальним зразкам, коли ми переміщуємось вліво-вправо, 
то ми переміщуємось по тренувальних комірках. А вертикально, вертикальний [стовпцевий] індекс відповідає вузлам НМ. Тож, наприклад, оцей вузол, оця величина вгорі зліва відповідає активаторам першого прихованого вузла першого тренувального зразка. Величина на одну позицію нижче 
відповідає активатору в 2-ому прихованому вузлі 1-ого тренувального зразка. Далі 3-ій прихований вузол 1-ого тренувального зразка і т.д. Тож, переміщаючись вниз, 
індекс відображає позицію прихованого вузла. В той же час, переміщуючись горизонтально, 
від 1-ого прихованого вузла 1-ого тренувального зразка, до 1-ого прихованого вузла, але вже 2-ого тренувального зразка, 3-ого тренувального зразка і т.д. до оцього вузла, що відповідає активатору 1-ого прихованого вузла останнього тренувального зразка -
 n-ого тренувального зразка. Зрозуміло? Тож по матриці A 
горизонтально ми рухаємось по тренувальних зразках, а вертикально - індекси матриці відповідають прихованим вузлам. Подібним чином побудовані і матриці Z та X, в яких горизонтально індекси відповідають тренувальним зразкам, а вертикально - вхідним ознакам, які, насправді, є вузлами вхідного шару НМ. Тож, маючи ці рівняння, ти тепер знаєш як реалізувати НМ з векторизацією, векторизацією для багатьох зразків. В наступному відео я постараюсь обґрунтувати чому це правильна реалізація цього типу векторизації. Виявляється, що обґрунтування подібне до того, 
яке ми вже бачили для ЛР. Давай перейдемо до наступного відео.