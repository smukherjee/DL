Daha önce vektörleştirmenin (vectorization) kodunuzu belirgin bir şekilde nasıl hızlandırdığından bahsetmiştik. Bu videoda lojistik regresyon işleminde vektörleştirme uygulamasını nasıl yapacağınızdan bahsedeceğiz. Böylelikle, tüm bir eğitim seti (training set) için bu işlemi yani, gradyan düşümü uygulamasının tüm eğitim seti için tek bir adımı için bile tek bir ayrı for döngüsü kullanmadan uygulayabileceğiz. Bu teknik konusunda, ve sinir ağlarında, üzerinde daha sonra da konuşacağımız, tek bir ayrı for döngüsü bile kullanmama konusunda oldukça heyecanlıyım. Haydi başlayalım! Öncelikle, lojistik regresyonun ileri doğru yayılım (forward propogation) adımlarını inceleyelim. Eğer eğitim setiniz m tane elemandan oluşuyorsa, birinci eleman üzerinden bir tahmin yapmak için, bu şekilde, bildiğiniz bu formüllerle z'yi hesaplamanız gerekir. Daha sonra aktivasyonları, ve bu ilk eleman için y şapkayı, daha sonra ikinci eğitim seti elemanı üzerinden tahmin yapmak için bunu, ve üçüncü eğitim seti elemanı üzerinden tahmin yapmak için de bunu hesaplamalı ve bu şekilde devam etmelisiniz. Bunu bu şekilde m kez yapmanız gerekecektir, eğer eğitim setinizde m tane elemanınız varsa. Göreceğiz ki, ileri doğru yayılım adımını tamamlamak için, yani eğitim setindeki m tane eğitim verisi için tahmin hesaplamalarını yapmak üzere, bunu yapmanın ayrı ayrı for döngüsü kullanmaksızın bir yolu var. Nasıl yapabilirsiniz görelim: Öncelikle, hatırlayın ki, X matrisini, eğitim girdilerimizi temsil etmek üzere, yanyana bu şekilde birleşen farklı sütunlar halinde tanımlamıştık. Bu bir matris, n_x * m boyutlu bir matris. Bunu Python numpy.shape fonksiyonu formunda bu şekilde yazıyorum. Bu, sadece, X'in n_x * m boyutlu bir matris olsuğunun bir ifadesi. Şimdi, ilk göstermek istediğim, z1, z2, z3 ve diğerlerinin hepsi tek adımda, yani tek satırlık bir kodla nasıl hesaplanabileceği. Evet, 1 * m boyutunda, aslında bir satır vektörü olan, bir matris oluşturacağım, z1, z2 ve zm'e kadar hepsini aynı anda hesaplayacağım, Öyle görünüyor ki, bu, w'nun transpozu çarpı X matrisi, artı, bu (b,b,...,b) vektörü, şeklinde ifade edilebilir. Burada bu, (b,b,...,b), 1 * m boyutlu bir vektör, başka bir deyişle, m boyutlu satır vektörü şeklindeki 1 * m 'lik bir matris. Matrisi çarpımı konusuna ne kadar aşina olduğunuza bağlı olarak, görebilirsiniz ki, w'nun transpozu X1, X2 ve Xm'e kadar, ve burada w'nun transpozu bir satır vektörü olabilir, Evet yani buradaki w transpoz, bu şekilde bir satır vektörü olacak. Ve bu ilk terim, w transpoz X1, w transpoz X2, nokta nokta nokta, w transpoz xm şeklinde çıkacak. Buna ikinci terim olan, (b, b,b...,b)'yi eklediğimizde, her elemana b eklemiş gibi olacaksınız. Böylelikle başka bir 1*m'lik vektör elde etmiş olursunuz. Ve evet, bu ilk eleman, bu ikinci eleman, ve bu şekilde, bu da m. eleman. Ve yukardaki tanımlara bakarsanız, aslında bu ilk eleman tam olarak z1'e ikinci eleman tam olarak z2'ye,.. eşit olacak şekilde devam eder. Dolayısıyla, aynı X matrisini daha önce elde ettiğimiz yolla, (eğitim seti elemanlarımızı alıp, yatay olarak, yan yana koyarak, oluşturmuştuk) bu büyük Z'yi de, küçük z'leri alıp yatay olarak yan yana koyduğumuz hali olarak tanımlayacağım. Dolayısıyla, farklı eğitim seti elemanlarına tekabül eden küçük x'leri yan yana koyduğunuzda, yatay olarak, büyük X matrisini elde ettiğiniz gibi; aynı şekilde, bu küçük z değişkenlerini de yatay olarak yan yana koyduğunuzda, bu büyük Z değişkenini elde etmiş olursunuz. Bunu uygulamak için kullanılacak olan numpy komutu, büyük Z eşittir np.dot (w.T, X) yani w transpoz, X ve artı b. Python'da bu işlemle ilgili bir incelik mevcut; Burada b bir reel sayı, ya da isterseniz buna 1 * 1'li matris de diyebilirsiniz, normal bir reel sayı. Fakat, bu vektörü bu reel sayıya eklediğinizde, Pyton otomatik olarak bu b reel sayısını 1 * m'lik satır vektörü haline genişletiyor. Her ne kadar bu işlem biraz garip gözükse de, bu, Python'da "broadcasting" olarak tanımlanıyor, ve bunu şu an için düşünmenize gerek yok, ilerde daha sonra buna biraz daha deyineceğiz. Önemli olan nokta şu ki, sadece bir satırlık kodla, bu kod satırıyla, büyük Z'yi hesaplayabilirsiniz. Büyük Z, tüm küçük z'leri içeren 1 * m'lik bir matris olacak, küçük z1'den küçük zm'ye kadar. Evet bu Z idi. Peki ya a değerleri? Sıradaki yapmak istedğimiz şey, a1, a2 ve am'ye kadar, hepsini tek bir seferde hesaplamak, ve tıpkı küçük x'leri yanyana getirmenin büyük X'i, küçük z'leri yatay olarak yan yana getirmenin büyük Z'yi oluşturmuş olması gibi, küçük a'ları yan yana getirmek de yeni bir değişken olan büyük A'yı tanımlayacak. Programlama ödevinde vektör değerli bir sigmoid fonksiyonunu uygulamayı göreceğiz. sigmoid fonksiyonu büyük Z'yi bir değişken olarak girdi kabul edecek şekilde alıp büyük A'yı oluşturacak. Dolayısıyla bunun ayrıntılarını programlama ödevinde göreceksiniz. Genel olarak toparlamak gerekirse, Bu sunumda gördüğümüz şey; bir döngü kurma ihtiyacı hissetmeden m tane eğitim seti elemanı üzerinde küçük z ve küçük a'yı hesaplamak, hepsi için tek seferde, ve tek satırlık bir kodla, bütün bu z'leri aynı anda hesaplamak ve daha sonra bu tek satır kod ile küçük sigmanın buna doğru bir şekilde uygulanmasıyla tüm bu küçük a'ları aynı anda hesaplamak. Evet, işte bu işlem vektörleştirme dediğimiz m tane eğitim verisi için ileri doğru yayılımı tek seferde gerçekleştirme işlemi... Özetle, mükemmel bir şekilde tüm aktivasyonları, tüm küçük a'ları aynı anda hesaplamak için vektörleştirmeyi nasıl kullanacağımızı görmüş oldunuz. Devamında, göreceğiz ki, vektörleştirmeyi ayrıca geri yayılım hesaplamasında da gradyanları hesaplamak için kullanacağız. Şimdi bunu bir sonraki videoda nasıl yapacağımızı görelim...