1
00:00:00,060 --> 00:00:03,750
В попередньому відео ми бачили як

2
00:00:01,890 --> 00:00:05,819
обчислювати похідні і застосовувати

3
00:00:03,750 --> 00:00:07,500
Градієнтний спуск для

4
00:00:05,819 --> 00:00:09,929
одного тренувального зразка для логістичної

5
00:00:07,500 --> 00:00:12,450
регресії (ЛР). Тепер ми хочемо це зробити для m

6
00:00:09,929 --> 00:00:14,429
тренувальних зразків. Спочатку, давай

7
00:00:12,450 --> 00:00:17,460
згадаємо визначення

8
00:00:14,429 --> 00:00:19,380
функції Витрат J. Функція Витрат від w і b,

9
00:00:17,460 --> 00:00:22,699
яка нам потрібна, = середньому,

10
00:00:19,380 --> 00:00:25,350
тобто 1/m, від суми Втрат при i від 1 до m,

11
00:00:22,699 --> 00:00:29,519
коли алгоритм

12
00:00:25,350 --> 00:00:33,510
видає a⁽ⁱ⁾ для зразка y, де

13
00:00:29,519 --> 00:00:36,120
a⁽ⁱ⁾ - передбачення для i-ого

14
00:00:33,510 --> 00:00:40,620
тренувального зразка, що = σ від z⁽ⁱ⁾,

15
00:00:36,120 --> 00:00:46,800
що = σ від wᵀ [транспоноване] помноженого на x⁽ⁱ⁾)

16
00:00:40,620 --> 00:00:48,510
плюс b. Тож, на попередньому слайді

17
00:00:46,800 --> 00:00:51,600
ми показали, як для кожного окремого

18
00:00:48,510 --> 00:00:55,620
тренувального зразка обчислити

19
00:00:51,600 --> 00:01:00,180
похідні. Ми мали лише один

20
00:00:55,620 --> 00:01:03,809
тренувальний зразок. Тож dw₁, dw₂ і

21
00:01:00,180 --> 00:01:06,689
db тепер будуть з верхнім індексом ⁽ⁱ⁾ і

22
00:01:03,809 --> 00:01:08,369
позначатимуть відповідні значення, які ми отримаємо,

23
00:01:06,689 --> 00:01:10,799
якщо будемо робити те, що ми робили

24
00:01:08,369 --> 00:01:15,030
на попередньому слайді для одного

25
00:01:10,799 --> 00:01:17,850
тренувального зразка (x⁽ⁱ⁾, y⁽ⁱ⁾). Вибачте,

26
00:01:15,030 --> 00:01:20,759
тут теж повинно бути ⁽ⁱ⁾.

27
00:01:17,850 --> 00:01:22,530
Як бачимо, загальна функція Витрат є сумою

28
00:01:20,759 --> 00:01:26,220
розділеною на m, а тому дійсно є середньою

29
00:01:22,530 --> 00:01:29,369
окремою втратою.

30
00:01:26,220 --> 00:01:32,810
Виявляється, що похідна

31
00:01:29,369 --> 00:01:38,600
загальної функції Витрат, скажімо, по w₁

32
00:01:32,810 --> 00:01:42,600
також буде середнім від

33
00:01:38,600 --> 00:01:46,170
похідних по w₁

34
00:01:42,600 --> 00:01:48,240
від окремих втрат. Проте, раніше ми

35
00:01:46,170 --> 00:01:54,119
вже бачили як обчислюється цей вираз,

36
00:01:48,240 --> 00:01:55,890
тобто dw₁⁽ⁱ⁾. Ми вже

37
00:01:54,119 --> 00:01:57,659
знаємо з попереднього слайду як

38
00:01:55,890 --> 00:02:00,450
його обчислювати для окремого тренувального зразка.

39
00:01:57,659 --> 00:02:03,119
Тож що нам потрібно зробити, це обчислити

40
00:02:00,450 --> 00:02:04,680
ці похідні так само як

41
00:02:03,119 --> 00:02:07,350
на попередньому тренувальному зразку і

42
00:02:04,680 --> 00:02:10,379
знайти середнє. І це дасть нам

43
00:02:07,350 --> 00:02:10,830
загальний градієнт, який ми можемо використати для

44
00:02:10,379 --> 00:02:12,870
реалізації

45
00:02:10,830 --> 00:02:15,390
Градієнтного спуску. Отже, я розумію, що тут

46
00:02:12,870 --> 00:02:17,730
багато інформації, проте, давай

47
00:02:15,390 --> 00:02:19,920
спробуємо використати це в конкретному

48
00:02:17,730 --> 00:02:21,690
алгоритмі. Тобто розглянемо, що потрібно зробити,

49
00:02:19,920 --> 00:02:24,960
щоб змусити працювати зв'язку

50
00:02:21,690 --> 00:02:28,350
ЛР і Градієнтного спуску. Тож ось

51
00:02:24,960 --> 00:02:37,770
що можна зробити. Давай встановимо J в 0,

52
00:02:28,350 --> 00:02:40,140
dw₁ в 0, dw₂ в 0, db в 0.

53
00:02:37,770 --> 00:02:43,190
І давай використаємо

54
00:02:40,140 --> 00:02:45,690
цикл for для тренувального набору і

55
00:02:43,190 --> 00:02:47,670
обчислимо похідні для кожного

56
00:02:45,690 --> 00:02:49,020
тренувального зразка і просумуємо їх.

57
00:02:47,670 --> 00:02:51,480
Тож ось що ми робимо. Для i від 1

58
00:02:49,020 --> 00:02:54,360
до m (m - це кількість

59
00:02:51,480 --> 00:02:57,090
тренувальних зразків) обчислюємо z⁽ⁱ⁾=

60
00:02:54,360 --> 00:03:00,360
=wᵀx⁽ⁱ⁾+b,

61
00:02:57,090 --> 00:03:04,020
передбачення a⁽ⁱ⁾=σ(z⁽ⁱ⁾)

62
00:03:00,360 --> 00:03:09,120
і давай просумуємо J:

63
00:03:04,020 --> 00:03:12,360
J+=-(y⁽ⁱ⁾log(a⁽ⁱ⁾)

64
00:03:09,120 --> 00:03:14,010
+(1-y⁽ⁱ⁾)(1-log(a⁽ⁱ⁾))). Поставимо

65
00:03:12,360 --> 00:03:15,959
знак "-" перед усім виразом.

66
00:03:14,010 --> 00:03:20,580
І, як ми бачили раніше, dz⁽ⁱ⁾

67
00:03:15,959 --> 00:03:28,500
=a⁽ⁱ⁾-y⁽ⁱ⁾ і

68
00:03:20,580 --> 00:03:33,180
dw₁+=x₁⁽ⁱ⁾dz⁽ⁱ⁾,

69
00:03:28,500 --> 00:03:35,280
dw₂+=x₂⁽ⁱ⁾dz⁽ⁱ⁾. Я записав це

70
00:03:33,180 --> 00:03:37,680
таким чином, тому що ми маємо лише

71
00:03:35,280 --> 00:03:41,070
2 ознаки, тобто m=2.

72
00:03:37,680 --> 00:03:45,480
В іншому випадку це потрібно буде зробити для dw₁, dw₂, dw₃

73
00:03:41,070 --> 00:03:47,430
і т.д. Далі db+=dz⁽ⁱ⁾.

74
00:03:45,480 --> 00:03:49,350
І це кінець циклу for.

75
00:03:47,430 --> 00:03:51,900
І, нарешті, зробивши це для m

76
00:03:49,350 --> 00:03:54,959
тренувальних зразків, нам потрібно буде

77
00:03:51,900 --> 00:03:56,880
поділити це на m, щоб обчислити

78
00:03:54,959 --> 00:04:01,920
середнє. Тож

79
00:03:56,880 --> 00:04:04,260
dw₁/=m, dw₂/=m,

80
00:04:01,920 --> 00:04:07,019
db/=m (щоб обчислити

81
00:04:04,260 --> 00:04:09,060
середні). Отже, у всіх цих

82
00:04:07,019 --> 00:04:11,160
щойно проведених обчисленнях ми знайшли

83
00:04:09,060 --> 00:04:14,250
[часткові] похідні функції Витрат J

84
00:04:11,160 --> 00:04:17,010
по кожному параметру w₁, w₂ і b.

85
00:04:14,250 --> 00:04:22,079
Кілька коментарів про те, що ми робимо.

86
00:04:17,010 --> 00:04:25,020
Ми використовуємо dw₁, dw₂ і db

87
00:04:22,079 --> 00:04:28,169
як накопичувачі. Тобто після

88
00:04:25,020 --> 00:04:31,500
цих обчислень ми знаємо, що dw₁=

89
00:04:28,169 --> 00:04:33,509
[частковій] похідній всієї

90
00:04:31,500 --> 00:04:36,780
функції Витрат по w₁ і,

91
00:04:33,509 --> 00:04:39,720
аналогічно, по dw₂ і db. Відміть, що

92
00:04:36,780 --> 00:04:41,520
dw₁ і dw₂ не мають верхнього індексу ⁽ⁱ⁾,

93
00:04:39,720 --> 00:04:43,379
тому що ми використовуємо їх в коді

94
00:04:41,520 --> 00:04:45,690
як накопичувачі, щоб просумувати для всього

95
00:04:43,379 --> 00:04:48,960
тренувального набору. На противагу dz⁽ⁱ⁾

96
00:04:45,690 --> 00:04:51,539
ось тут, який відноситься до

97
00:04:48,960 --> 00:04:53,490
окремого тренувального зразка.

98
00:04:51,539 --> 00:04:55,740
Тому воно має верхній індекс ⁽ⁱ⁾, бо відноситься

99
00:04:53,490 --> 00:04:58,379
до одного тренувального зразка, який

100
00:04:55,740 --> 00:05:00,960
обчислюється. Отже, щоб закінчити

101
00:04:58,379 --> 00:05:03,449
всі ці обчислення і реалізувати один крок

102
00:05:00,960 --> 00:05:06,360
Градієнтного спуску ми виконуємо коригування:

103
00:05:03,449 --> 00:05:10,710
w₁ як w₁ мінус навчальний темп

104
00:05:06,360 --> 00:05:13,740
помножений на dw₁, w₂ як w₂ мінус

105
00:05:10,710 --> 00:05:17,190
навчальний темп помножений на dw₂ і b

106
00:05:13,740 --> 00:05:21,000
як b мінус навчальний темп помножений на db.

107
00:05:17,190 --> 00:05:23,879
Де dw₁, dw₂ і db ми

108
00:05:21,000 --> 00:05:27,000
обчислюємо і, нарешті, J

109
00:05:23,879 --> 00:05:28,590
отримує правильне значення для

110
00:05:27,000 --> 00:05:31,050
функції Витрат. Тож, все на цьому слайді

111
00:05:28,590 --> 00:05:33,060
реалізується всього за один крок

112
00:05:31,050 --> 00:05:35,699
Градієнтного спуску і ми повинні

113
00:05:33,060 --> 00:05:37,680
повторювати все, що є на цьому слайді, багато

114
00:05:35,699 --> 00:05:40,469
разів, щоб здійснити багато кроків

115
00:05:37,680 --> 00:05:41,819
Градієнтним спуском. Якщо це

116
00:05:40,469 --> 00:05:43,830
здається дуже складним, то,

117
00:05:41,819 --> 00:05:45,960
знову ж, не дуже хвилюйся

118
00:05:43,830 --> 00:05:48,599
поки. Я надіюсь, воно стане зрозумілішим

119
00:05:45,960 --> 00:05:50,520
коли почнеш реалізовувати це в

120
00:05:48,599 --> 00:05:54,120
програмувальному завданні. Виявляється,

121
00:05:50,520 --> 00:05:57,300
що ми маємо 2 слабких місця

122
00:05:54,120 --> 00:05:59,729
в обчисленнях, які ми тут описали.

123
00:05:57,300 --> 00:06:01,440
Вони наступні. Щоб реалізувати

124
00:05:59,729 --> 00:06:03,960
ЛР цим способом, ми повинні

125
00:06:01,440 --> 00:06:05,490
написати 2 цикли for. Перший -

126
00:06:03,960 --> 00:06:07,770
для всіх (m) тренувальних

127
00:06:05,490 --> 00:06:10,919
зразків, а другий -

128
00:06:07,770 --> 00:06:13,139
для всіх ось цих характеристик.

129
00:06:10,919 --> 00:06:15,930
В цьому прикладі ми маємо лише 2

130
00:06:13,139 --> 00:06:17,879
ознаки - m=2. (і nₓ=2)

131
00:06:15,930 --> 00:06:21,000
Проте, коли ознак буде більше,

132
00:06:17,879 --> 00:06:23,099
потрібно буде записати dw₁, dw₂ і,

133
00:06:21,000 --> 00:06:25,979
аналогічно, для dw₃

134
00:06:23,099 --> 00:06:29,009
і так далі до dwₘ. Тож виявиться, що

135
00:06:25,979 --> 00:06:31,279
тут потрібно мати цикл for для

136
00:06:29,009 --> 00:06:33,199
цих ознак, для всіх m ознак.

137
00:06:31,279 --> 00:06:36,049
Коли реалізуєш алгоритми глибокого навчання (ГН),

138
00:06:33,199 --> 00:06:38,419
то починаєш розуміти, що явні

139
00:06:36,049 --> 00:06:41,839
цикли for в коді значно

140
00:06:38,419 --> 00:06:44,149
знижують швидкість роботи алгоритму. А, так як

141
00:06:41,839 --> 00:06:46,669
в епоху ГН ми використовуємо

142
00:06:44,149 --> 00:06:48,649
все більші і більші набори даних, то

143
00:06:46,669 --> 00:06:50,779
можливість реалізувати алгоритми

144
00:06:48,649 --> 00:06:52,969
без використання явних циклів for стає

145
00:06:50,779 --> 00:06:55,129
дуже важливою і допоможе

146
00:06:52,969 --> 00:06:56,719
перейти до набагато більших наборів даних.

147
00:06:55,129 --> 00:06:58,129
Виявляється, що є набір

148
00:06:56,719 --> 00:07:01,159
методик, які називаються методиками

149
00:06:58,129 --> 00:07:03,559
векторизації і які дозволяють позбавитись

150
00:07:01,159 --> 00:07:06,169
явних циклів for в коді.

151
00:07:03,559 --> 00:07:08,199
Я думаю, що в пре-ГН епоху,

152
00:07:06,169 --> 00:07:11,239
тобто перед злетом ГН,

153
00:07:08,199 --> 00:07:13,159
векторизація було добре мати:

154
00:07:11,239 --> 00:07:15,589
можна було інколи прискорити код,

155
00:07:13,159 --> 00:07:17,749
а інколи цього можна було не робити. Проте, в

156
00:07:15,589 --> 00:07:20,029
епоху ГН векторизація,

157
00:07:17,749 --> 00:07:22,699
яка прибирає цикли for (як ось цей

158
00:07:20,029 --> 00:07:25,039
чи ось цей), стала дуже важливою.

159
00:07:22,699 --> 00:07:26,989
Тому що ми все частіше і частіше тренуємо

160
00:07:25,039 --> 00:07:29,239
дуже великі набори даних, тож, справді,

161
00:07:26,989 --> 00:07:31,209
потребуємо, щоб код був дуже ефективним.

162
00:07:29,239 --> 00:07:34,219
В кількох наступних відео ми поговоримо

163
00:07:31,209 --> 00:07:37,339
про векторизацію і як реалізувати все ось це

164
00:07:34,219 --> 00:07:40,879
без використання навіть одного циклу for.

165
00:07:37,339 --> 00:07:43,069
Тож тепер, я надіюсь ти маєш розуміння

166
00:07:40,879 --> 00:07:44,299
як реалізувати ЛР

167
00:07:43,069 --> 00:07:46,339
або Градієнтний спуск для ЛР.

168
00:07:44,299 --> 00:07:47,959
Це стане ще зрозуміліше

169
00:07:46,339 --> 00:07:50,299
коли ти виконаєш програмувальні вправи.

170
00:07:47,959 --> 00:07:51,829
Проте, перед тим як робити програмувальні

171
00:07:50,299 --> 00:07:54,079
вправи, давай спочатку поговоримо про

172
00:07:51,829 --> 00:07:56,419
векторизацію, щоб ти міг/могла виконати

173
00:07:54,079 --> 00:07:58,369
всі ці розрахунки в одній

174
00:07:56,419 --> 00:08:01,479
ітерації Градієнтного спуску без

175
00:07:58,369 --> 00:08:01,479
використання циклів for.