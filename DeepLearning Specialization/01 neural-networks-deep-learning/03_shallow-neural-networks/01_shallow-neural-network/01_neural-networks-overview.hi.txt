पुनः स्वागत है. इस सप्ताह में आप सीखेंगे इम्प्लमेंट करना एक न्यूरल नेटवर्क. पहले जाने के तकनीकी विवरण में, मैं चाहता था इस वीडियो में देना आपको एक त्वरित ओवरव्यू जो आप देखेंगे इस सप्ताह के वीडियोज़ में. तो यदि आपको नहीं समझ आता है सारा विवरण इस वीडियो में, चिंता न करें इस की. हम जाएँगे तकनीकी विवरण में अगले कुछ वीडियोज़ में. लेकिन अभी के लिए चलिए देते हैं एक त्वरित ओवरव्यू कि कैसे आप इम्प्लमेंट करते हैं आपका नेटवर्क. पिछले हफ़्ते हमने बात की लजिस्टिक रेग्रेशन के बारे में और हमने देखा कि कैसे यह मॉडल मेल खाता है निम्नलिखित कॉम्प्यूटेशन ग्राफ़ से जहाँ आपने नहीं रखे फ़ीचर्ज़ X और पैरामिटर W और b जिससे आप कम्प्यूट कर पाते हैं Z जो तब इस्तेमाल करते है कम्प्यूट करने के लिए a 
और हम इस्तेमाल कर रहे थे अदल-बदल कर के a 
और इस आउट्पुट Y हैट का और फिर आप कम्प्यूट 
कर सकते हैं लॉस फ़ंक्शन l. एक न्यूरल नेटवर्क दिखता है ऐसे और जैसा कि मैंने अब तक पहले ही इंगित किया है आप बना सकते हैं एक न्यूरल नेटवर्क स्टैक करके एक साथ बहुत से छोटे सिग्मोईड यूनिट्स. जबकि पहले यह नोड समान था दो स्टेप्स कैल्क्युलेशन्स के. पहला है कम्प्यूट करना Z वैल्यू, दूसरा है यह कम्प्यूट करता है a वैल्यू. इस न्यूरल नेटवर्क में यह स्टैक नोड्स का समान है Z की तरह की कैल्क्युलेशन्स के तथा एक a तरह की कैल्क्युलेशन्स के उस जैसे और फिर वह नोड समान होगा एक अन्य Z और एक अन्य a की तरह की कैल्क्युलेशन के. तो नोटेशन जो हमें बाद में उपयोग करनी चाहिए दिखेगी इस तरह. पहले हम इनपुट करते हैं फ़ीचर्ज़ X साथ में कुछ पैरामीटरर्स के W और B और इससे आप कम्प्यूट कर पाएँगे z1 तो नयी नोटेशन जो इस्तेमाल करनी चाहिए है कि हम लेंगे एक सुपरस्क्रिप्ट वर्ग कोष्ठक 1 रेफ़र करने के लिए मात्राएँ जो जुड़ी है नोड्स के इस स्टैक के साथ जिसे कहते हैं 
एक लेयर और फिर बाद हम लेंगे एक सुपरस्क्रिप्ट वर्ग कोष्ठक 2 रेफ़र करने के लिए मात्राओं को जो जुड़ी हैं उस नोड से. उसे कहते हैं एक अन्य लेयर नेटवर्क की और सुपरस्क्रिप्ट वर्ग कोष्ठक जैसे हमारे पास हैं यहाँ वे नहीं हैं समान सुपरस्क्रिप्ट गोल कोष्ठक के जो हमने इस्तेमाल किए हैं रेफ़र करने को प्रत्येक ट्रेनिंग इग्ज़ाम्पल. तो जहाँ X सूपरस्क्रिप्ट गोल कोष्ठक I रेफ़र करता है Ith ट्रेनिंग इग्ज़ाम्पल को, सूपरस्क्रिप्ट वर्ग कोष्ठक 1 और 2 रेफ़र करते हैं इन विभिन्न लेयर्स लेयर 1 और लेयर 2 को इस नेटवर्क में लेकिन वे कर रहे हैं z1 कम्प्यूट करने के बाद समान लॉजिस्टिक रेग्रेशन के. वहाँ होगी एक कॉम्प्यूटेशन कम्प्यूट करने के लिए a1 और वह सिर्फ़ कुछ सिग्मोईड z1 का और फिर आप कम्प्यूट करते हैं z2 इस्तेमाल कर के एक अन्य 
लिनीअर इक्वेज़न का और फिर कम्प्यूट करते हैं a2 और a2 है अंतिम आउट्पुट न्यूरल नेटवर्क की और की जाएगी इस्तेमाल अदल-बदल करके Y से. तो मैं जानता हूँ वह था बहुत सा विवरण लेकिन मुख्य बात समझने की है कि जहाँ लॉजिस्टिक रेग्रेशन के लिए हमारे पास था यह Z उसके बाद एक कैल्क्युलेशन और यह न्यूरल नेटवर्क यहाँ. हम सिर्फ करते हैं इसे बहुत बार. एक Z उसके बाद a कैल्क्युलेशन और एक Z उसके बाद a कैल्क्युलेशन और फिर अंत में आप कम्प्यूट करते हैं लॉस अंत में और आपको याद होगा कि सिर्फ़ रेग्रेशन के लिए हमारे पास था कुछ बैकवर्ड कैल्क्युलेशन करने के लिए कम्प्यूट डेरिवेटिव्स. इसलिए कम्प्यूटिंग da dz और इसी प्रकार आगे एक न्यूरल नेटवर्क में हमें मिलेगी एक बैक्वर्ड कैल्क्युलेशन जो दिखती है ऐसे और हम आगे जाते हैं आपको मिलता है da2 dz2 जो आपको कम्प्यूट करने देता है dw2 db2 और इसी प्रकार आगे. इस क्रम में दाएँ से बाएँ बैकवर्ड कैल्क्युलेशन जो दर्शाएँ हैं लाल रंग के तीरों से. तो धन्यवाद आपका. त्वरित ओवरव्यू कि क्या है एक न्यूरल नेटवर्क सिर्फ़ है लेना एक लॉजिस्टिक रेग्रेशन और करना इसे दो बार. मैं जानता हूँ कि बहुत सी नई नोटेशन थी बहुत सी नई जानकारी थी. चिंता न करें यदि 
आपको नहीं समझ आया सभी कुछ. हम जाएँगे विस्तार में धीरे-धीरे अगले कुछ वीडियोज़ में. तो चलिए चलते हैं 
अगले वीडियो में. हम शुरू करते हैं बात करना न्यूरल नेटवर्क रेप्रेज़ेंटेशन की.