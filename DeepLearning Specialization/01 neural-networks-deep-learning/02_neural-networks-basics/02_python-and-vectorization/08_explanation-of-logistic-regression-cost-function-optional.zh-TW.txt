在前面的影片中, 我寫下了
一個表單給 羅吉斯迴歸分析的成本函數 在這個額外的影片, 我要
給您一個快速的道理對於 為什麼我們要使用這個成本函數對於
羅吉斯迴歸分析 快速回顧一下，在羅吉斯迴歸分析中 我們的預測 y-hat 
是S型函數 of w 轉置 x + b 而S型函數是這個熟悉的函數 我們說我們想解釋
y-hat為 p(y = 1 | x) 我們要我們的演算法
輸出 y-hat 是 給予一組輸入特徵 x, y = 1 的機會 另一種方式說法是
如果 y 等於 1 那 y 給予 
x 下等於 y-hat 的機會 反之如果 y 等於 0 y 為 0 的機會是 1 - y-hat
對吧 如果 y-hat 是 y = 1 的機會 那  1 - y-hat 是 y = 0 的機會 讓我拿最後這兩個方程式
複製它們到下一張投影片 我要做的是
拿這兩個方程式 基本上定義 p(y|x) 
在兩種情況下 y = 0 或 y = 1 然後拿這兩個方程式
總結成為一個方程式 要指出的是 y 一定是 0 
或 1 因為是二元分類 y = 0 或 1 是唯二
的情況 有人將這兩個方程式
總結成為以下 讓我寫下來它的樣子
然後我們來解釋為什麼像這樣 所以 y-hat 的 y 次方乘(1-y-hat) 的 (1-y) 次方 實際上這一行
總結了上面兩個方程式 讓我解釋一下為什麼 第一種情況
假設 y=1, 對吧 如果 y=1 那這個
項目會變成 y-hat 因為這是 y-hat 的 y 次方 這個項目是 1 - y-hat 的
 1 - 1 次方也就是 0 次方 但任何數的 0 次方
等於 1, 所以這會不見 所以這個公式
就是 p(y|x) = y-hat 當 y = 1 這正是我們要的 那第二個情況呢 
如果 y = 0? 如果 y = 0, 那這個上面的公式
會是 p(y|x) = y-hat 的 0 次方 但任何數的 0 次方
等於 1 所以這等於 1 乘
 1- y-hat 的 1-y 次方 而 1 -  y 是 1 - 0, 所以這是 1 所以這等於 1 
乘 (1 - y-hat) 等於 1 - y-hat 這裡我們當 y = 0 時
p(y|x) = 1 - y-hat 這正是我們上面要的 我們剛看過
這個公式 是正確的 p(y|x) 的定義 最後因為對數
函數是嚴格單調 增加函數
您的 log of p(y|x) 最大化會給您 最大值的p(y|x)
如果您計算 log of p(y|x) 等於 log y-hat 
的 y 次方 1 - y-hat 的 1 - y 次方 簡化以後是 y log y-hat 加 1 - y 乘 log 1 - y-hat, 對吧? 所以
這實際上是負的損失 函數我們之前定義的 這裡的負號是因為
通常如果您訓練一個學習 演算法, 您想要
讓機率變大 而在羅吉斯迴歸分析
我們這樣表示 我們要盡量將損失函數最小化 所以最小化損失函數相當於
最大化這個對數函數的機率 這是損失函數
在單一訓練例子的樣子 成本函數呢? 整體的成本函數在
整個訓練集 m 個例子呢? 讓我們看看 所有的
訓練集的標籤的機率 用非正式的方式這樣寫 如果您假設訓練例子們
是獨立的或者說 IID 相同獨立分佈 那所有例子的機率是
機率的乘積 是乘積從 i = 1 到
m p(y(i) | x(i)) 如果您要進行
最大似然估計 您想要極大化這個
找到參數來極大化 您的觀察跟訓練集的機率 但極大化這個跟
極大化這個 log 是一樣的 我們放 log 在兩邊 所以 log of 標籤
在訓練集的機率等於 log of 這個乘積, 變成和的 log 就是和從 i=1 到
m log(p(y(i) | x(i)) 我們從前面
的投影片知道 這是負的 L of y-hat(i), y(i) 在統計學中, 有一個法則
稱為最大似然法則 估計，也就是說選擇
參數來極大化這個 換句話說
極大化這個 負的和從i=1到
m L(y-hat(i)<i>,y(i)<i>) 只是移動負號
到和的外面 這個說明了我們的 羅吉斯迴歸分析的成本是
 J(w,b) 因為我們想要最小化
這個成本, 而非最大化之 我們去掉這個負號 最後方便起見
我們確保這個數字是比較好的比例 我們加上 1 除以 m 
額外的縮放比例 總結一下, 最小化
這個成本函數 J(w,b) 我們 使用最大似然估計
在羅吉斯迴歸分析模型 在我們的
訓練集的假設是 IID 或者 相同地獨立分佈 謝謝您觀看這段影片
即使這是可選的 我希望您一些感覺為什麼
我們使用成本函數對於 羅吉斯迴歸分析的假設函數 有了這個, 我希望您進入
練習, 程式練習跟 這個禮拜的測驗 祝您好運在測驗及
以下的練習