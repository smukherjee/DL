1
00:00:00,920 --> 00:00:02,860
こんにちは、そしておかえりなさい。

2
00:00:02,860 --> 00:00:08,860
今週は、ニューラルネットワークプログラミングの
基礎を学びます。

3
00:00:08,860 --> 00:00:11,990
ニューラルネットワークを実装するときには、

4
00:00:11,990 --> 00:00:16,260
いくつかの実装技術が大切になってきます。

5
00:00:16,260 --> 00:00:21,150
例えば、m個の教師サンプルがあれば、

6
00:00:21,150 --> 00:00:25,110
そのm個のサンプルをforループで

7
00:00:25,110 --> 00:00:28,240
処理するかもしれません。

8
00:00:28,240 --> 00:00:31,260
しかし、ニューラルネットワークの実装では、

9
00:00:31,260 --> 00:00:34,540
サンプル全体を明示的なforループなしで

10
00:00:34,540 --> 00:00:39,040
処理したいのです。

11
00:00:39,040 --> 00:00:42,940
その方法を、今週学習しましょう。

12
00:00:42,940 --> 00:00:47,700
もう一つの例です。
ネットワーク内での計算を組み立てる際、

13
00:00:47,700 --> 00:00:51,670
フォワードプロパゲーションと呼ばれるものに続いて、

14
00:00:51,670 --> 00:00:56,100
バックプロパゲーションと呼ばれるステップを
作ることになります。

15
00:00:56,100 --> 00:01:00,010
今週の教材では、なぜニューラルネットワークの学習は

16
00:01:00,010 --> 00:01:04,830
フォワードプロパゲーションと
バックプロパゲーションによって

17
00:01:04,830 --> 00:01:08,010
構成されるのかについても学びます。

18
00:01:09,100 --> 00:01:12,620
今週の教材では、理解しやすくするために
これらのことを

19
00:01:12,620 --> 00:01:16,170
ロジスティック回帰を使ってお伝えします。

20
00:01:16,170 --> 00:01:19,970
もしロジスティック回帰をすでに見たことがあっても、

21
00:01:19,970 --> 00:01:23,845
今週の教材から
新しく興味深い考えを得ることができるでしょう。

22
00:01:23,845 --> 00:01:25,815
では始めましょう。

23
00:01:25,815 --> 00:01:30,605
ロジスティック回帰は、
二項分類のためのアルゴリズムです。

24
00:01:30,605 --> 00:01:33,145
まず問題を設定しましょう。

25
00:01:33,145 --> 00:01:36,925
これが、二項分類の例です。

26
00:01:36,925 --> 00:01:41,545
このような画像の入力があり、

27
00:01:41,545 --> 00:01:47,260
この画像を猫と認識すれば1というラベルを出力し、

28
00:01:47,260 --> 00:01:52,140
そうでなければ0を出力したいとします。

29
00:01:52,140 --> 00:01:57,740
そして、出力されるラベルをyと示します。

30
00:01:57,740 --> 00:02:01,550
コンピュータで画像が
どのように表されるかを見てみましょう。

31
00:02:01,550 --> 00:02:05,680
画像を保持するためには、
コンピュータは三つの別々の行列を保持します。

32
00:02:05,680 --> 00:02:09,890
それらは画像の赤、緑、青のカラーチャンネルに
対応します。

33
00:02:10,990 --> 00:02:15,900
入力画像が64ピクセル×64ピクセルなら、

34
00:02:15,900 --> 00:02:21,700
64×64の行列を三つ持つことになります。

35
00:02:21,700 --> 00:02:27,230
それぞれが画像のピクセルの
赤、緑、青の強さに対応します。

36
00:02:27,230 --> 00:02:31,290
この小さいスライドを作るために
なるべく小さい行列を描いたので、

37
00:02:31,290 --> 00:02:35,320
64×64ではなく
5×4になってしまいました。

38
00:02:35,320 --> 00:02:41,640
ピクセルの色の強さの値を
特徴ベクトルにするために、

39
00:02:41,640 --> 00:02:48,000
これらを全て一列に展開し、
特徴ベクトルxに入れましょう。

40
00:02:48,000 --> 00:02:53,782
三つのピクセルデータを特徴ベクトルに展開するために、

41
00:02:53,782 --> 00:02:59,580
画像に対応する特徴ベクトルを次のように定義します。

42
00:02:59,580 --> 00:03:03,960
全てのピクセルデータ、255,231などを取って、

43
00:03:03,960 --> 00:03:10,827
全ての赤ピクセルを並べるまで続けます。

44
00:03:10,827 --> 00:03:15,737
そして255,134、255,134と続きます。

45
00:03:15,737 --> 00:03:20,952
最終的には、赤、緑、青のピクセルデータ

46
00:03:20,952 --> 00:03:25,570
全てが並べられた長い特徴ベクトルができあがります。

47
00:03:25,570 --> 00:03:31,043
画像が64×64なら、

48
00:03:31,043 --> 00:03:36,401
ベクトルxの次元数は64×64×3になります。

49
00:03:36,401 --> 00:03:41,320
なぜなら、これは三つの行列の合計だからです。

50
00:03:41,320 --> 00:03:44,097
この場合、12,288となります。

51
00:03:44,097 --> 00:03:47,330
これらの数字をかけた結果がそうなるのです。

52
00:03:47,330 --> 00:03:51,870
なので、nx=12288というのを

53
00:03:51,870 --> 00:03:55,080
入力される特徴xの次元数として用います。

54
00:03:55,080 --> 00:03:59,280
簡潔にするため、小文字のnを使って

55
00:03:59,280 --> 00:04:02,720
入力される特徴ベクトルの次元数を表すこともあります。

56
00:04:02,720 --> 00:04:07,510
二項分類では、私たちの目的は

57
00:04:07,510 --> 00:04:10,760
特徴ベクトルxで表される画像を入力できる
分類器を学習することです。

58
00:04:10,760 --> 00:04:15,460
そして、それに対応するラベルyが
1か0かを予測します。

59
00:04:15,460 --> 00:04:19,000
つまり、画像が猫なのかそうでないかを
予測するということです。

60
00:04:19,000 --> 00:04:21,560
これからのコースを通じて使っていく

61
00:04:21,560 --> 00:04:23,820
表記法を見ていきましょう。

62
00:04:23,820 --> 00:04:29,453
一つの教師サンプルはx,yというペアで表され、

63
00:04:29,453 --> 00:04:34,446
xはnx次元の特徴ベクトル、

64
00:04:34,446 --> 00:04:39,320
yは0か1かのラベルです。

65
00:04:39,320 --> 00:04:44,550
教師セットは小文字のm個の教師サンプルで
できています。

66
00:04:44,550 --> 00:04:50,320
なので、教師セットの一番目のサンプルは
(x1,y1)と表され、

67
00:04:50,320 --> 00:04:55,370
(x2,y2)が二つ目で、

68
00:04:55,370 --> 00:05:01,980
最後のサンプルである(xm,ym)まで続きます。

69
00:05:01,980 --> 00:05:05,650
これらすべてが教師セットとなるのです。

70
00:05:05,650 --> 00:05:10,170
小文字のmを教師サンプルの数を表すために使います。

71
00:05:10,170 --> 00:05:14,418
そして、これが教師サンプルの数であることを
強調するため、

72
00:05:14,418 --> 00:05:16,437
たまにmをm trainと書くことがあります。

73
00:05:16,437 --> 00:05:18,692
テストセットについて書くときは、

74
00:05:18,692 --> 00:05:24,430
テストサンプルの数を示すために
m testという表記を使うことがあります。

75
00:05:24,430 --> 00:05:27,430
これがテストサンプルの数です。

76
00:05:27,430 --> 00:05:33,440
最後に、
全ての教師サンプルをコンパクトに示すために、

77
00:05:33,440 --> 00:05:36,840
大文字のXという行列を定義します。

78
00:05:36,840 --> 00:05:41,592
これは教師セットの入力x1,x2…を取ってきて、

79
00:05:41,592 --> 00:05:44,568
それらを列として並べてできるものです。

80
00:05:44,568 --> 00:05:49,958
x1を取ってきてこの行列の一つ目の列とし、

81
00:05:49,958 --> 00:05:54,798
x2を二つ目の列とし、xmまで続けます。

82
00:05:54,798 --> 00:05:58,000
これが、大文字のXという行列です。

83
00:05:58,000 --> 00:06:03,005
なので、この行列Xは教師サンプルの数であるm列あり、

84
00:06:03,005 --> 00:06:08,665
行の数、つまり高さはnxです。

85
00:06:08,665 --> 00:06:14,400
他のコースでは、一つの教師サンプルを一つの行として

86
00:06:14,400 --> 00:06:19,390
定義した行列Xを見たことがあるかもしれません。

87
00:06:19,390 --> 00:06:23,940
x1の転置からxmの転置までが並べられたものです。

88
00:06:23,940 --> 00:06:27,704
しかし、左に書いたような慣習の方が、

89
00:06:27,704 --> 00:06:32,218
ニューラルネットワークの実装がかなり楽になるのです。

90
00:06:32,218 --> 00:06:37,171
おさらいすると、Xはnx × m次元の行列で、

91
00:06:37,171 --> 00:06:40,404
Pythonで実装するときには

92
00:06:40,404 --> 00:06:45,362
X.shapeというのが
行列の形を確認するコマンドなのですが、

93
00:06:45,362 --> 00:06:50,325
これを実行するとnx,mと表示されます。

94
00:06:50,325 --> 00:06:53,255
nx × m次元の行列だという意味です。

95
00:06:53,255 --> 00:06:58,785
これが、教師サンプルxをひとまとめにする方法です。

96
00:06:58,785 --> 00:07:01,315
では出力ラベルyはどうでしょうか。

97
00:07:01,315 --> 00:07:04,815
ニューラルネットワークの実装を
より簡単にするためには、

98
00:07:04,815 --> 00:07:10,030
yも列として並べた方が都合がいいです。

99
00:07:10,030 --> 00:07:14,650
なので、大文字のYをy1,y2からymまで

100
00:07:14,650 --> 00:07:18,580
並べたものとして定義します。

101
00:07:18,580 --> 00:07:24,980
なので、Yは1×m次元の行列です。

102
00:07:24,980 --> 00:07:30,530
繰り返しますが、Pythonのコマンドを使うと、
Yの形は1,mであることがわかります。

103
00:07:30,530 --> 00:07:34,810
つまり、1×m次元の行列だということです。

104
00:07:34,810 --> 00:07:39,660
後のコースで
ニューラルネットワークを実装する際には、

105
00:07:39,660 --> 00:07:43,630
異なる教師サンプルデータに関連付けられたデータを
取るというのは便利な慣習となるでしょう。

106
00:07:43,630 --> 00:07:48,580
データというのは、xやy、
そして後に登場する他の数量のことです。

107
00:07:48,580 --> 00:07:49,900
ここでxとyについてやったように

108
00:07:49,900 --> 00:07:52,990
異なる教師サンプルに関連付けられたデータを取って

109
00:07:52,990 --> 00:07:57,430
別々の列に保持するということです。

110
00:07:58,450 --> 00:08:01,380
これらがロジスティック回帰と

111
00:08:01,380 --> 00:08:04,060
後に出てくるニューラルネットワークで使う
表記法です。

112
00:08:04,060 --> 00:08:07,430
ある表記が何を表しているのか忘れてしまったら、

113
00:08:07,430 --> 00:08:08,300
例えばmやnってなんだっけ？となったら、

114
00:08:08,300 --> 00:08:12,630
表記ガイドをコースのサイトに投稿してあるので、

115
00:08:12,630 --> 00:08:17,430
ある表記がどういう意味だったかを
すばやく調べることができます。

116
00:08:17,430 --> 00:08:20,890
さあ、次の動画に進んで、

117
00:08:20,890 --> 00:08:23,190
これらの表記法を使って
ロジスティック回帰を始めましょう。