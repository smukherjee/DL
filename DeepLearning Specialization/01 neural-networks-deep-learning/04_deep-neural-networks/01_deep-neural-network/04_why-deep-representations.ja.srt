1
00:00:00,000 --> 00:00:03,339
深いニューラルネットワークが
多くの問題に対して本当に良い成績をあげていると

2
00:00:03,339 --> 00:00:07,073
みなさんも聞いていますよね
ただ大きいニューラルネットワークにする必要があるのではなく

3
00:00:07,073 --> 00:00:10,718
特に隠れ層を多く持った深いネットワークに
する必要があると聞いたでしょう

4
00:00:10,718 --> 00:00:12,208
これはなぜでしょうか

5
00:00:12,208 --> 00:00:15,833
何個か例を見て なぜ深層ネットワークが
成績が良いかの洞察を

6
00:00:15,833 --> 00:00:17,720
得てみましょう

7
00:00:17,720 --> 00:00:22,181
まず 深層ネットワーク計算とは
なんでしょうか

8
00:00:22,181 --> 00:00:25,393
もし顔認識や顔位置認識のシステムを
作ろうとした場合には

9
00:00:25,393 --> 00:00:29,631
こんな感じのことを
深層ニューラルネットワークだと行っているでしょう

10
00:00:29,631 --> 00:00:35,059
顔の写真を入力すると
ニューラルネットワークの最初の層は

11
00:00:35,059 --> 00:00:40,000
特徴検出器や境界検出器と
考えてもらってもよいです

12
00:00:40,000 --> 00:00:45,519
この例では この画像に対して
20個相当の隠れ層ユニットがあるニューラルネットワークが

13
00:00:45,519 --> 00:00:48,017
計算するだろうことを描画します

14
00:00:48,017 --> 00:00:52,357
20個の隠れユニットは
この小さい四角で表現します

15
00:00:52,357 --> 00:00:57,325
例えば この左上の表現する隠れ層ユニットが
行おうとしているのは

16
00:00:57,325 --> 00:01:01,978
画像上のどこにこの向きの境界があるのかを
見つけようとしています

17
00:01:01,978 --> 00:01:05,914
この右下の隠れユニットは

18
00:01:05,914 --> 00:01:09,955
たぶん この水平の境界が
画像のどこにあるのかを探しています

19
00:01:09,955 --> 00:01:13,184
のちのコースで
畳み込みネットワークを話すときには

20
00:01:13,184 --> 00:01:16,129
この表現が
より理解できるでしょう

21
00:01:16,129 --> 00:01:19,562
最初のニューラルネットワークの層が行うのは
このイメージの中を見て

22
00:01:19,562 --> 00:01:22,690
どこに境界があるのかを
検出しようとするものと考えてよいです

23
00:01:22,690 --> 00:01:27,356
ピクセルをグループ化することで
この画像の境界が

24
00:01:27,356 --> 00:01:28,730
形成できたので

25
00:01:28,730 --> 00:01:34,670
検出した境界を使ってグループ化して
顔の部品を形成できます

26
00:01:34,670 --> 00:01:40,289
例えば 目を見つけようとする
神経ニューロンがあったり

27
00:01:40,289 --> 00:01:44,480
この鼻の一部を探そうとする別のニューロンがあるでしょう

28
00:01:44,480 --> 00:01:47,463
つまり 多くの境界を
組み合わせることで

29
00:01:47,463 --> 00:01:50,970
様々な顔の部品を
検出し始めることができます

30
00:01:50,970 --> 00:01:56,035
最後に 目や鼻や耳や顎などの
様々な顔の部品を組み合わせることで

31
00:01:56,035 --> 00:02:01,006
様々な種類の顔を
認識したり検出することが

32
00:02:01,006 --> 00:02:03,564
できます

33
00:02:03,564 --> 00:02:07,755
直感できるかと思いますが
始めのころのニューラルネットワークの層は

34
00:02:07,755 --> 00:02:10,190
境界のような簡単な関数を
検出すると考えられます

35
00:02:10,190 --> 00:02:14,573
次に 後々の層では
組み立てていると考えられ

36
00:02:14,573 --> 00:02:17,625
より複雑な関数を
学ぶようになれます

37
00:02:17,625 --> 00:02:23,640
畳み込みネットワークに関して話すと
これらの表現がより理解できるでしょう

38
00:02:23,640 --> 00:02:26,203
この表現の技術的な点を話すと

39
00:02:26,203 --> 00:02:29,802
境界検出器は
画像の比較的狭い範囲を見ていて

40
00:02:29,802 --> 00:02:31,703
多分画像のこの部分くらいですが

41
00:02:31,703 --> 00:02:36,616
顔の部分の検出器は
多分もっと広い領域を見ることができます

42
00:02:36,616 --> 00:02:41,308
この例から理解してほしい主な洞察は
境界のような簡単なものをまず見つけ

43
00:02:41,308 --> 00:02:43,675
それを積み上げていくということです

44
00:02:43,675 --> 00:02:47,216
それを組み合わせて
目や鼻などのより複雑なものを検出し

45
00:02:47,216 --> 00:02:50,530
検出したものを組み合わせて
より複雑なものを組み立てます

46
00:02:50,530 --> 00:02:55,665
単純なものから複雑なものへと行く
階層的表現や組成的表現などの

47
00:02:55,665 --> 00:02:58,508
この種類は

48
00:02:58,508 --> 00:03:04,114
画像や顔認識以外にもまた
適用できます

49
00:03:04,114 --> 00:03:08,593
例えば 音声認識システムを作ろうとする時にです

50
00:03:08,593 --> 00:03:10,908
音声を表現するのは難しいですが

51
00:03:10,908 --> 00:03:15,684
音のファイルを入力すると
最初のニューラルネットワークの層は

52
00:03:15,684 --> 00:03:20,863
音量が上下するような音の波の形などの特徴を学習しようと

53
00:03:20,863 --> 00:03:21,703
するでしょう

54
00:03:21,703 --> 00:03:26,869
ホワイトノイズや
舌を滑らせる音だったり

55
00:03:26,869 --> 00:03:27,903
周波数だったりするでしょう

56
00:03:27,903 --> 00:03:31,124
これらの細かいレベルの形の音の特徴になるでしょう

57
00:03:31,124 --> 00:03:34,233
細かいレベルの音の形を
組み合わせることで

58
00:03:34,233 --> 00:03:37,937
音の基本的ユニットを
検出することを学習します

59
00:03:37,937 --> 00:03:40,297
言語学での
音素です

60
00:03:40,297 --> 00:03:45,098
例えば 猫の単語cat では
cやaが音素です

61
00:03:45,098 --> 00:03:46,787
t も音素です

62
00:03:46,787 --> 00:03:49,987
音の基本的なユニットを見つけたら

63
00:03:49,987 --> 00:03:54,688
次にこれを組み合わせて
音の中の単語を学習します

64
00:03:54,688 --> 00:03:58,270
これをさらに組み合わせて

65
00:03:58,270 --> 00:04:02,912
句全体や文全体を
認識します

66
00:04:02,912 --> 00:04:07,572
複数の隠れ層を持つ
深層ニューラルネットワークであれば

67
00:04:07,572 --> 00:04:10,477
始めのころの層は
細かいレベルの簡単な特徴を学び

68
00:04:10,477 --> 00:04:15,339
のちに 深めの層では
検出した簡単なものを組み立てて

69
00:04:15,339 --> 00:04:19,392
特定の単語やさらに句や文や全体などの
より複雑なものを

70
00:04:19,392 --> 00:04:21,040
検出して

71
00:04:21,040 --> 00:04:24,745
音声認識を実施します

72
00:04:24,745 --> 00:04:30,168
ここに見られるのは
初期の層が計算しているのは

73
00:04:30,168 --> 00:04:35,673
境界がどこにあるかなどの
入力の比較的簡単な関数です

74
00:04:35,673 --> 00:04:41,046
ネットワークの深いところに行くと
驚くほど複雑な関数となっています

75
00:04:41,046 --> 00:04:44,876
顔を検出したり
単語や句や文を検出します

76
00:04:44,876 --> 00:04:48,767
深層ニューラルネットワークと
人間の脳が類似していると

77
00:04:48,767 --> 00:04:52,656
言いたい人もいます
私たちや神経科学者が信じているような

78
00:04:52,656 --> 00:04:57,162
人の脳も目に入ってくる境界のように
比較的単純なものを最初に検出し

79
00:04:57,162 --> 00:05:00,370
その後で顔のようなより複雑なものを
検出するように積み上げていくという

80
00:05:00,370 --> 00:05:02,440
考えです

81
00:05:02,440 --> 00:05:05,038
この深層学習と
人間の脳が

82
00:05:05,038 --> 00:05:08,276
類似していると考えるのは
時に少し危険だと思っています

83
00:05:08,276 --> 00:05:13,301
もちろんどう人間の脳が動作するのかについて
事実な部分もあります

84
00:05:13,301 --> 00:05:18,102
恐らくは 人間の脳は境界のように単純な形を検出し

85
00:05:18,102 --> 00:05:22,598
それを積み上げてより複雑な物を
構成するという事実です

86
00:05:22,598 --> 00:05:27,430
これが深層学習のゆるい形態での刺激として
役立ってきました

87
00:05:27,430 --> 00:05:29,850
人間の脳や生物学的な脳のことは

88
00:05:29,850 --> 00:05:33,065
今週の後から出てくるビデオで
もっと深く見ていきます

89
00:05:35,534 --> 00:05:40,407
もう１点の深層ネットワークが
良い成績を上げる理由の洞察は

90
00:05:40,407 --> 00:05:42,756
次の通りです

91
00:05:42,756 --> 00:05:47,868
この結果は回路理論から来ています

92
00:05:47,868 --> 00:05:53,760
AND、OR、NOTゲートなどの様々なロジックゲートによって
どんな計算が可能かに関するものです

93
00:05:53,760 --> 00:05:58,860
砕けた言い方をすれば
比較的狭いが深いニューラルネットワークで計算する関数があります

94
00:05:58,860 --> 00:06:03,595
狭いで意味するのは
隠れユニットの数が比較的少ないということです

95
00:06:03,595 --> 00:06:07,553
この計算を浅いネットワークで
同じような関数を作る場合

96
00:06:07,553 --> 00:06:09,178
十分な隠れ層がない場合

97
00:06:09,178 --> 00:06:13,296
指数関数的なより多くの隠れユニットが
計算に必要ということです

98
00:06:13,296 --> 00:06:18,109
砕けた方法で例示するために
１つ例をあげます

99
00:06:18,109 --> 00:06:21,423
全ての入力特徴に関して
XORまたはパリティを

100
00:06:21,423 --> 00:06:23,349
計算しようしています

101
00:06:23,349 --> 00:06:28,430
X1 XOR X2 XOR X3 に続き

102
00:06:28,430 --> 00:06:33,064
XOR Xn まで計算するとします
nx の特徴がある場合です

103
00:06:33,064 --> 00:06:39,924
XOR のツリーを作る場合
まずX1 とX2 のXORを計算し

104
00:06:39,924 --> 00:06:44,586
X3 とX4 のXOR を計算します

105
00:06:44,586 --> 00:06:49,392
厳密にいえば AND ゲートと
NOT ゲートだけを使う場合には

106
00:06:49,392 --> 00:06:54,196
XOR を計算するには１層ではなく
複数の層が必要になりますが

107
00:06:54,196 --> 00:06:58,791
比較的小さい回路で
XOR を計算できます

108
00:06:58,791 --> 00:07:03,987
続けてブールのXOR のツリーを作って行き

109
00:07:03,987 --> 00:07:12,090
最終的に出力する回路が出てくるまで続けます
出力はy と呼びましょう

110
00:07:12,090 --> 00:07:15,236
出力yハット=yを出力します

111
00:07:15,236 --> 00:07:18,398
全てのビットのXOR 
またはパリティの値です

112
00:07:18,398 --> 00:07:24,790
XOR を計算する深さは
log n の階数になります

113
00:07:24,790 --> 00:07:27,410
この形のXOR ツリーです

114
00:07:27,410 --> 00:07:30,836
このネットワークではノードの数
または回路の部品の数

115
00:07:30,836 --> 00:07:33,929
もしくはゲートの数は
それほど多くはありません

116
00:07:33,929 --> 00:07:38,452
XOR を計算するには
ゲートは多く必要ありません

117
00:07:38,452 --> 00:07:43,458
もし複数の隠れ層を持ったニューラルネットワークが使えない場合には

118
00:07:43,458 --> 00:07:48,203
この場合のlog n の階数の層でしたが
１つの隠れ層で

119
00:07:48,203 --> 00:07:53,382
この関数を計算するのを
強いられたらどうでしょうか

120
00:07:53,382 --> 00:07:57,912
この場合 全ての入力が
隠れ層に行った後で

121
00:07:57,912 --> 00:08:02,116
これがYを出力します

122
00:08:02,116 --> 00:08:07,120
そしてXOR 関数を計算するには

123
00:08:07,120 --> 00:08:12,124
この隠れ層が
指数関数的に大きい必要があります

124
00:08:12,124 --> 00:08:18,397
というのは網羅的に
２のN乗の可能な組み合わせを列挙する必要があるからです

125
00:08:18,397 --> 00:08:23,139
つまり入力ビットの可能な組み合わせで

126
00:08:23,139 --> 00:08:27,898
XOR で１か０のどちらかの結果ができます

127
00:08:27,898 --> 00:08:32,213
このため隠れ層には
ビット数が指数関数的に大きいものが

128
00:08:32,213 --> 00:08:33,554
必要になります

129
00:08:33,554 --> 00:08:38,229
厳密には ２のn-1乗の
隠れユニットでできますが

130
00:08:38,229 --> 00:08:43,948
２のｎ階数の指数関数的に
大きい数のビット数になります

131
00:08:43,948 --> 00:08:49,149
深いネットワークで計算する方が
浅いネットワークで計算するよりも

132
00:08:49,149 --> 00:08:55,275
簡単に計算できる数学的関数があることの
意味が分かるようになったでしょうか

133
00:08:55,275 --> 00:09:01,028
回路理論の結果は
洞察を得るにはそれほど役に立たないと

134
00:09:01,028 --> 00:09:05,985
気付いていましたが
深層表現の価値を説明するときに

135
00:09:05,985 --> 00:09:11,223
よく使われる結果の１つです

136
00:09:11,223 --> 00:09:13,600
これらの理由に加え

137
00:09:13,600 --> 00:09:16,897
深層ニューラルネットワークを
好む理由の１つは私たちにもあります

138
00:09:16,897 --> 00:09:22,204
深層学習という用語が流行した他の理由は
ただブランディングされたからでもあります

139
00:09:22,204 --> 00:09:26,776
これらは昔は多層の隠れ層のニューラルネットワークと
呼ばれていましたが

140
00:09:26,776 --> 00:09:31,198
深層学習という語句はただただ凄いブランドです
もの凄く深いという響きです

141
00:09:31,198 --> 00:09:36,284
この語句が流行って
多層の隠れ層のニューラルネットワークが

142
00:09:36,284 --> 00:09:39,622
新しくブランド化されたのと同時に

143
00:09:39,622 --> 00:09:42,970
よく使われる想像を
掴むのに一役かったと思います

144
00:09:42,970 --> 00:09:47,479
PR のブランディングと関係なく
深層ネットワークは実に成績が良いです

145
00:09:47,479 --> 00:09:51,342
あまりに多くの隠れ層を使うことを
主張するほどにやり過ぎてしまう人も時々いますが

146
00:09:51,342 --> 00:09:55,500
わたしは新しい問題に取り組むときには
まずはロジステック回帰から

147
00:09:55,500 --> 00:09:58,803
始めることも多いです
そして１つか２つの隠れ層を試し

148
00:09:58,803 --> 00:10:01,722
これをハイパーパラメーターに使います

149
00:10:01,722 --> 00:10:05,731
これを調整を行うパラメーターか
ハイパーパラメーターに使って

150
00:10:05,731 --> 00:10:07,935
ニューラルネットワークの
適切な深さを探すのに使います

151
00:10:07,935 --> 00:10:12,800
過去数年にわたって
特定の利用方法については

152
00:10:12,800 --> 00:10:17,590
非常に深いニューラルネットワークが
時に膨大な数の層にも上りますが

153
00:10:17,590 --> 00:10:22,264
これが問題に対する最良のモデルとなることを
発見する流れがあります

154
00:10:22,264 --> 00:10:27,605
今回は深層学習が良い成績を
上げているように思える理由の洞察でした

155
00:10:27,605 --> 00:10:31,411
次は 順誤差伝播法だけでなく逆誤差伝播法を
実装する方法の仕組みを

156
00:10:31,411 --> 00:10:33,769
見ていきましょう