पिछले वीडियो में, आपने देखा कैसे कम्प्यूट करना है 
प्रिडिक्शन एक न्यूरल नेटवर्क पर, दिया होने पर एक ट्रेनिंग इग्ज़ाम्पल. इस वीडियो में, आप देखेंगे कैसे वेक्टराइज़ करना है 
बहुत से ट्रेनिंग इग्ज़ाम्पल्ज़ पर. और परिणाम काफ़ी समान होगा जो 
आपने देखा लॉजिस्टिक रेग्रेशन के लिए. जहाँ स्टैक करके विभिन्न ट्रेनिंग इग्ज़ाम्पल्ज़ को भिन्न कॉलम्स में मेट्रिक्स के, आप ले पाएँगे इक्वेज़न्स जो 
आपके पास थी पिछले वीडियो में. और बहुत थोड़े बदलाव से, बदलने से ताकि 
न्यूरल नेटवर्क कम्प्यूट करे आउट्पुट सभी ट्रेनिंग इग्ज़ाम्पल्ज़ पर सब एक साथ. चलो देखते हैं विस्तार से कि कैसे वह करना है. हमारे पास चार इक्वेज़न्स थी पिछले वीडियो से कि 
कैसे कम्प्यूट करना है z1, a1, z2 और a2. और वे बताते हैं आपको कैसे, 
दिया होने पर एक इनपुट फ़ीचर वेक्टर x, आप इस्तेमाल कर सकते हैं उनका बनाने के लिए
 a2 = y हैट एक अकेले ट्रेनिंग इग्ज़ाम्पल के लिए. अब यदि आपके पास हैं m ट्रेनिंग इग्ज़ाम्पल्ज़, 
आपको इस प्रक्रिया को दोहराने की ज़रूरत है मान लो पहले ट्रेनिंग इग्ज़ाम्पल के लिए. x सुपरस्क्रिप्ट (1) कम्प्यूट करने के लिए y हैट 1 जो है प्रिडिक्शन आपके पहले ट्रेनिंग इग्ज़ाम्पल पर. तब x (2) का उपयोग करें प्रिडिक्ट करने के लिए y hat (2). और इसी प्रकार आगे x(m) तक बनाने के लिए y हैट (m). तो लिखने के लिए ये सभी ऐक्टिवेशन फ़ंक्शन्स नोटेशन में, मैं लिखूँगा इसे a[2](1) की तरह. और इसे a[2](2) की तरह. और इसे a[2](m), तो यह नोटेशन a[2](i). यह गोल कोष्ठक i संदर्भित करता हैं ट्रेनिंग इग्ज़ाम्पल i को, और वर्ग कोष्ठक 2 संदर्भित करता हैं लेयर 2 को, ठीक है. तो ऐसे वर्ग कोष्ठक और गोल कोष्ठक सूचकांक काम करते हैं. तो वे कहते हैं कि यदि आपके पास है 
एक अनवेक्टराइज्ड इम्प्लमेंटेशन और चाहते हैं कम्प्यूट करना प्रिडिक्शन आपके 
सारे ट्रेनिंग इग्ज़ाम्पल्ज़ पर, आपको करना पड़ेगा वह i के लिए 1 से m तक. तब मूलत: इम्प्लमेंट ये चार इक्वेज़न्स, ठीक है? आपको बनाना पड़ेगा एक z[1](i) = W(1) x(i) + b[1], a[1](i) = सिग्मोईड ऑफ़ z[1](1). z[2](i) = w[2]a[1](i) + b[2] और z[2](i) बराबर है w[2]a[1](i) प्लस b[2] और a[2](i) = सिग्मोईड ऑफ़ of z[2](i). तो यह है मूलत: ये चार इक्वेज़न्स ऊपर जोड़ कर सूपरस्क्रिप्ट गोल कोष्ठक i सभी वेरिएबल्स को जो निर्भर करते हैं ट्रेनिंग इग्ज़ाम्पल पर. तो जोड़ने से यह सूपरस्क्रिप्ट गोल कोष्ठक i x को है z और a, यदि आप कम्प्यूट करना चाहते हैं सभी 
आउट्पुट्स आपके m ट्रेनिंग इग्ज़ाम्पल्ज़ पर, हम क्या करना चाहेंगे कि वेक्टराइज़ करे ये सारी कॉम्प्यूटेशन, 
ताकि हटा सके यह फ़ोर लूप. और वैसे, यदि ऐसा लग रहा है कि मैं 
बहुत विस्तार में जा रहा हूँ लिनीअर ऐल्जेब्रा में, ऐसा होता है कि इसे इम्प्लमेंट कर पाना सही ढंग से महत्वपूर्ण है डीप लर्निंग के युग में. और हमने वास्तव में चुनी है नोटेशन बहुत ध्यान से 
इस कोर्स के लिए और बनाया है यह वेक्टराइज़ेशन स्टेप सरल जितना हो सके. तो मैं उम्मीद करता हूँ कि जानना ये बारीकियाँ 
वास्तव में मदद करेंगी आपको अधिक जल्दी पाने के लिए सही इम्प्लमेंटेशन 
इन अल्गोरिद्म को चलाने के लिए. ठीक है, तो चलो मैं कॉपी करता हूँ यह पूरा ब्लॉक कोड का 
अगली स्लाइड में और तब हम देखते हैं कैसे इसे वेक्टराइज़ करते हैं. तो यहाँ है जो हमारे पास था पिछली स्लाइड से जिसमें फ़ॉर लूप चल रहा है हमारे m ट्रेनिंग इग्ज़ाम्पल्ज़ पर. तो याद करो कि हमने परिभाषित की थी मेट्रिक्स x जो बराबर है हमारे ट्रेनिंग इग्ज़ाम्पल्ज़ के जिन्हें स्टैक किया है 
इन कॉलम्स में इस प्रकार. तो लेते हैं ट्रेनिंग इग्ज़ाम्पल्ज़ और स्टैक करते हैं उन्हें इन कॉलम्स में. तो यह बन जाती है एक n, या शायद n बाई m डिमेन्शन का मेट्रिक्स. मैं आपको सिर्फ़ दूँगा पंच लाइन और बताऊँगा कि 
क्या आपको इम्प्लमेंट करने की आवश्यकता है पाने के लिए एक वेक्टराइज्ड इम्प्लमेंटेशन इस फ़ॉर लूप की. ऐसा होता है कि आपको क्या करना है कि कम्प्यूट करना है Z[1] = W[1] X + b[1], A[1]= सिग्मोईड ऑफ़ z[1]. फिर Z[2] = w[2] A[1] + b[2] और फिर A[2]= सिग्मोईड ऑफ़ z[2]. तो यदि आप चाहते हैं समानता वह है कि हम गए वेक्टर x से वेक्टर X पर स्टैक करते हुए x को विभिन्न कॉलम्स में. यदि आप करते हैं वही चीज़ z के लिए, तो उदाहरण के लिए, यदि आप लेते हैं z[1](1), z[1](2), और इसी प्रकार आगे, और ये सारे हैं कॉलम वेक्टर्स, आगे z[1](m) तक, ठीक है. तो वह है पहली चीज़ कि सभी वे m, 
और स्टैक करते हैं उन्हें कॉलम्स में. वह तब देता है आपको मेट्रिक्स z[1]. और इसी प्रकार आप देखते हैं मान लो इस चीज़ को और आप लेते हैं a[1](1), a[1](2), और इसी प्रकार और a[1](m) तक, और स्टैक करते हैं उन्हें कॉलम्स में. तब यह, जैसे हम गए x से केपिटल X तक, और z से केपिटल Z तक, यह जाता है a से, जो हैं वेक्टर्स इस A[1] के, वह है वहाँ और इसी प्रकार Z[2] और A[2] के लिए. तो वे भी मिलते हैं लेने से ये वेक्टर्स और स्टैक करने से उन्हें क्षैतिज रूप से. और लेने से ये वेक्टर्स और स्टैक करने से उन्हें क्षैतिज रूप से, पाने के लिए Z[2] और A[2]. एक गुण इस नोटेशन के जो शायद मदद करे आपको सोचने में इस बारे में है कि यह मेट्रिक्स मान लो Z और A, क्षैतिज रूप से हम इंडेक्स करेंगे पूरे ट्रेनिंग इग्ज़ाम्पल्ज़ पर. तो इसलिए क्षैतिज इंडेक्स दर्शाता है विभिन्न ट्रेनिंग इग्ज़ाम्पल्ज़, जब आप जाते हैं बाएँ से दाएँ आप जा रहें हैं ट्रेनिंग सेट से. और अनुलम्ब रूप से, यह अनुलम्ब इंडेक्स दर्शाता है विभिन्न नोड्ज़ न्यूरल नेटवर्क में. तो उदाहरण के लिए, यह नोड, यह वैल्यू सबसे ऊपर, सबसे ऊपर बाएँ कोने में मेट्रिक्स के दर्शाता है ऐक्टिवेशन को पहली हिडन यूनिट के पहले ट्रेनिंग इग्ज़ाम्पल पर. एक वैल्यू नीचे दर्शाता है ऐक्टिवेशन दूसरी हिडन यूनिट का पहले ट्रेनिंग इग्ज़ाम्पल पर, फिर तीसरी हिडन यूनिट पहले ट्रेनिंग इग्ज़ाम्पल पर 
और इसी प्रकार आगे. तो आप जा सकते हैं नीचे यह है आपका 
इंडेक्स हिडन यूनिट नम्बर्स का. जबकि यदि आप जाते हैं क्षैतिज रूप से, 
आप जा रहे हैं पहली हिडन यूनिट से और पहले ट्रेनिंग इग्ज़ाम्पल से सब पहली हिडन यूनिट पर और और दूसरे ट्रेनिंग इग्ज़ाम्पल पर, 
तीसरे ट्रेनिंग इग्ज़ाम्पल पर. और इसी प्रकार आगे जब तक यह
 नोड यहाँ दर्शाता है ऐक्टिवेशन पहली हिडन यूनिट का अंतिम ट्रेनिंग इग्ज़ाम्पल पर और 
mth ट्रेनिंग इग्ज़ाम्पल पर. ठीक है तो क्षैतिज रूप से मेट्रिक्स A जाता है 
विभिन्न ट्रेनिंग इग्ज़ाम्पल्ज़ पर. और अनुलम्ब रूप से विभिन्न इंडेक्स मेट्रिक्स A में दर्शाते हैं विभिन्न हिडन यूनिट्स. और एक यही समान चीज़ सत्य है 
मेट्रिक्स Z और X के लिए भी जहाँ क्षैतिज इंडेक्स दर्शाता है 
विभिन्न ट्रेनिंग इग्ज़ाम्पल्ज़, और अनुलम्ब रूप से, यह दर्शाता है 
विभिन्न इनपुट फ़ीचर्ज़ जो है बिल्कुल भिन्न इनपुट लेयर के 
नोड्ज़ से न्यूरल नेटवर्क में. तो इन इक्वेज़न्स को, आप जानते हैं 
कैसे इम्प्लमेंट करना है न्यूरल नेटवर्क में वेक्टराइज़ करके, 
मतलब वेक्टराइज़ करना बहुत से इग्ज़ाम्पल्ज़ पर. अगले वीडियो में मैं दिखाना चाहता हूँ 
आपको थोड़ा अधिक औचित्य कि क्यों यह है एक सही इम्प्लमेंटेशन इस तरह के 
वेक्टराइज़ेशन की. ऐसा है कि औचित्य होगा समान जो 
आपने पहले देखा है [सुनाई नहीं दिया]. तो चलिए चलते हैं अगले वीडियो में.