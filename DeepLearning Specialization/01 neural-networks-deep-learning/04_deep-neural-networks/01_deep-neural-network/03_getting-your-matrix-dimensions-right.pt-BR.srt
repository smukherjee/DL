1
00:00:00,028 --> 00:00:04,605
Quando eu implemento uma rede neural,
uma das ferramentas de depuração que utilizo

2
00:00:04,605 --> 00:00:08,118
para checar a validade do meu código,
é pegar um pedaço de papel

3
00:00:08,118 --> 00:00:11,727
e apenas verificar as dimensões e matrizes com
as quais trabalho.

4
00:00:11,727 --> 00:00:15,895
Então, vou mostrar como fazer isso,
pois espero que isso também facilite

5
00:00:15,895 --> 00:00:18,275
a implementação da sua rede neural profunda.

6
00:00:18,275 --> 00:00:23,174
L = 5, não contando a camada de entrada,

7
00:00:23,174 --> 00:00:27,390
existem cinco camadas aqui. Assim,
são quatro camadas ocultas e uma camada de saída.

8
00:00:27,390 --> 00:00:34,878
Assim, se você implementar propagação para frente,

9
00:00:34,878 --> 00:00:41,408
a primeira etapa será  z'¹' = w'¹' . x + b'¹'.

10
00:00:41,408 --> 00:00:48,144
Então, vamos ignorar o termo viés b,
por enquanto, e focar os parâmetros w.

11
00:00:48,144 --> 00:00:54,501
Agora, essa primeira camada oculta tem três
unidades ocultas. Então essa é a camada 0

12
00:00:54,501 --> 00:00:59,517
camada 1, camada 2, camada 3,
camada 4 e camada 5.

13
00:00:59,517 --> 00:01:05,741
Assim, usando a mesma notação utilizada no
vídeo anterior, temos que n'¹',

14
00:01:05,741 --> 00:01:11,265
que é o número de unidades ocultas
na camada 1, é igual a 3.

15
00:01:11,265 --> 00:01:16,202
E aqui nós temos que  n'²' = 5,

16
00:01:16,202 --> 00:01:23,018
n'³' = 4 ,   n'⁴' = 2   e   n'⁵' = 1.

17
00:01:23,018 --> 00:01:27,715
Até aqui, nós vimos redes neurais
 com uma única unidade de saída. Porém, mais adiante

18
00:01:27,715 --> 00:01:32,497
falaremos sobre redes neurais
com múltiplas unidades de saída.

19
00:01:32,497 --> 00:01:36,989
E, finalmente, para a camada de entrada,

20
00:01:36,989 --> 00:01:40,443
nós também temos   n'⁰' = nₓ = 2.

21
00:01:40,443 --> 00:01:45,860
Agora, vamos pensar sobre as dimensões de z, w e x.

22
00:01:45,860 --> 00:01:49,120
z é o vetor de ativações para

23
00:01:49,120 --> 00:01:54,244
essa primeira camada oculta. Assim,
z será 3 por 1,

24
00:01:54,244 --> 00:01:58,675
será um vetor com dimensão 3.

25
00:01:58,675 --> 00:02:03,093
E eu posso escrever isso como um vetor
 dimensional   (n'¹',1)

26
00:02:03,093 --> 00:02:08,546
ou uma matriz dimensional (n'¹', 1)
ou (3, 1), nesse caso.

27
00:02:08,546 --> 00:02:12,319
Agora, como fica a entrada de características x?
Para x, nós temos 2 entradas de características.

28
00:02:12,319 --> 00:02:18,622
Assim x, nesse exemplo, é 2 por 1 e,
generalizando, (n'⁰',1).

29
00:02:18,622 --> 00:02:24,082
                 Agora, o que precisamos é que a matriz w'¹'
seja alguma coisa que

30
00:02:24,082 --> 00:02:30,181
                             ao multiplicar o vetor (n'⁰', 1) com ela,
                             nós tenhamos um vetor (n'¹',1), certo?

31
00:02:30,181 --> 00:02:34,747
Assim, temos um vetor de dimensão 3, igual

32
00:02:34,747 --> 00:02:38,600
a alguma coisa vezes um vetor de dimensão 2.

33
00:02:38,600 --> 00:02:42,993
E assim,
pelas regras de multiplicação de matrizes,

34
00:02:42,993 --> 00:02:46,041
isso tem que ser uma matriz 3 por 2.

35
00:02:46,041 --> 00:02:51,138
Porque, uma matriz 3 por 2
vezes uma matriz 2 por 1,

36
00:02:51,138 --> 00:02:56,249
ou vezes um vetor 2 por 1,
 resulta em um vetor 3 por 1.

37
00:02:56,249 --> 00:03:02,771
E, generalizando, isso será uma matriz
de dimensão (n'¹', n'⁰').

38
00:03:02,771 --> 00:03:07,167
Então, o que percebemos aqui é que

39
00:03:07,167 --> 00:03:12,665
as dimensões de w'¹' tem que ser  (n'¹',n'⁰').

40
00:03:12,665 --> 00:03:20,191
E, generalizando, as dimensões de w[L]
serão n[L] por n[L-1].

41
00:03:20,191 --> 00:03:26,021
Assim, por exemplo, as dimensões de  w'²',

42
00:03:26,021 --> 00:03:31,508
nesse caso, seriam de (5, 3),

43
00:03:31,508 --> 00:03:35,119
ou  (n'²',n'¹').

44
00:03:35,119 --> 00:03:40,036
Porque temos que calcular

45
00:03:40,036 --> 00:03:45,132
z'²' = w'²' . a'¹' + b'¹'  e, novamente,

46
00:03:45,132 --> 00:03:50,059
por enquanto vamos ignorar o viés.

47
00:03:50,059 --> 00:03:54,584
a'¹' será (3, 1)

48
00:03:54,584 --> 00:03:59,432
e aqui temos z'²' = (5,1) e, assim,

49
00:03:59,432 --> 00:04:03,169
w'²' tem que ser (5, 3).

50
00:04:03,169 --> 00:04:10,273
Do mesmo modo, w'³' será a
dimensão da próxima camada

51
00:04:10,273 --> 00:04:15,501
virgula, a dimensão da camada anterior.

52
00:04:15,501 --> 00:04:19,266
Assim, isso fica (4, 5)

53
00:04:22,055 --> 00:04:27,489
w'⁴' = (2, 4), e                              

54
00:04:27,489 --> 00:04:34,405
w'⁵' = (1, 2), certo?                                          

55
00:04:34,405 --> 00:04:38,730
Assim, a fórmula geral pra checar é que quando

56
00:04:38,730 --> 00:04:43,416
você estiver implementando
uma matriz de camada L,

57
00:04:43,416 --> 00:04:48,475
a dimensão da matriz será n[L] por n[L-1].

58
00:04:48,475 --> 00:04:55,362
Agora, vamos pensar na dimensão
desse vetor b.

59
00:04:55,362 --> 00:05:01,017
Isto dá um vetor 3 por 1
e você tem que adicioná-lo a outro

60
00:05:01,017 --> 00:05:06,008
vetor 3 por 1, para ter um
vetor 3 por 1 como saída.

61
00:05:06,008 --> 00:05:11,287
Ou, neste exemplo, precisamos adicionar isto
que será 5 por 1.

62
00:05:11,287 --> 00:05:14,823
Então aqui teremos outro vetor 5 por 1.

63
00:05:14,823 --> 00:05:19,122
Para que a soma dessas duas coisas
que eu tenho

64
00:05:19,122 --> 00:05:22,767
nessas caixas, resulte em
um vetor 5 por 1.

65
00:05:22,767 --> 00:05:30,090
                                    Assim, generalizando temos que,
                   no exemplo a esquerda

66
00:05:30,090 --> 00:05:35,470
                                b'¹' = (n'¹', 1), ou (3, 1)

67
00:05:35,470 --> 00:05:41,156
e no segundo exemplo,                                           
b'²' = (n'²', 1)                               

68
00:05:41,156 --> 00:05:45,891
E assim, de modo mais genérico,

69
00:05:45,891 --> 00:05:50,637
b[L] deve ser dimensional de n[L] por 1.

70
00:05:50,637 --> 00:05:56,402
Veja que essas duas equações
podem ajudar a checar se as dimensões

71
00:05:56,402 --> 00:06:02,091
das matrizes w e também dos vetores b
estão corretas.

72
00:06:02,091 --> 00:06:06,206
E, claro, se você estiver implementando
retro-propagação,

73
00:06:06,206 --> 00:06:10,657
então a dimensão de dw
deve ter a mesma dimensão de w.

74
00:06:10,657 --> 00:06:16,373
Então dw deve ter a mesma dimensão de w
dw[L] = (n[L], n[L-1])

75
00:06:16,373 --> 00:06:22,276
e db deve ter a mesma dimensão de b
db[L] = (n[L],1)

76
00:06:22,276 --> 00:06:28,399
Agora, o outro conjunto-chave de valores
cujas dimensões precisam ser checadas

77
00:06:28,399 --> 00:06:33,658
são esses z, x e também o a[L],
do qual não falamos muito aqui.

78
00:06:33,658 --> 00:06:39,856
Então z[L] é igual a g[L](a[L]),
aplicando "Produto de Hadamard":
z[L] = g[L] (a[L])

79
00:06:39,856 --> 00:06:46,914
Então, z e a devem ter a mesma dimensão
nesses tipos de rede.

80
00:06:46,914 --> 00:06:51,582
Agora, vamos ver o que acontece quando
uma implementação vetorizada que aparece

81
00:06:51,582 --> 00:06:53,258
em vários exemplos ao mesmo tempo.

82
00:06:53,258 --> 00:06:56,092
Mesmo para implementação vetorizada,

83
00:06:56,092 --> 00:07:00,687
claro que as dimensões de w , b ,
dw, e db serão as mesmas.

84
00:07:00,687 --> 00:07:04,929
Mas as dimensões de z,
a e também de x,

85
00:07:04,929 --> 00:07:09,771
mudarão um pouco na
implementação vetorizada.

86
00:07:09,771 --> 00:07:13,420
Assim, anteriormente,

87
00:07:13,420 --> 00:07:18,372
nós tínhamos z'¹' = w'¹' . x + b'¹' ,

88
00:07:18,372 --> 00:07:23,845
onde:  z'¹' tem dimensão (n'¹', 1)

89
00:07:23,845 --> 00:07:28,276
w'¹' tem dimensão (n'¹', n'⁰')

90
00:07:28,276 --> 00:07:35,846
x tem dimensão (n'⁰', 1)

91
00:07:35,846 --> 00:07:40,979
e  b tem dimensão (n'¹', 1)
Agora,

92
00:07:40,979 --> 00:07:46,398
numa implementação vetorizada,
nós temos

93
00:07:46,398 --> 00:07:53,536
Z'¹' = W'¹' . X + b'¹'

94
00:07:53,536 --> 00:07:58,023
Onde agora, Z'¹' é obtido
 pegando z'¹' para

95
00:07:58,023 --> 00:08:03,575
um exemplo individual.
Assim,  z'¹''¹' ,  z'¹''²'

96
00:08:03,575 --> 00:08:10,207
até z[1][m], e empilhando tudo, como segue,
isso dá Z'¹'.

97
00:08:10,207 --> 00:08:15,042
Assim, a dimensão de Z'¹' é tal que,
ao invés de (n'¹', 1),

98
00:08:15,042 --> 00:08:20,285
resultará em (n'¹', m), onde
m é o tamanho do conjunto de testes.

99
00:08:20,285 --> 00:08:26,140
As dimensões de W'¹' permanecem as mesmas.
Nesse caso (n'¹', n'⁰')

100
00:08:26,140 --> 00:08:29,201
e x, ao invés de (n'⁰', 1), é agora

101
00:08:29,201 --> 00:08:33,431
todos os exemplos de treino
empilhados horizontalmente.

102
00:08:33,431 --> 00:08:38,565
Isso fica (n'⁰',m)
e veja que quando pegamos

103
00:08:38,565 --> 00:08:43,833
a matriz (n'¹', n'⁰') e
a multiplicamos pela matriz (n'⁰',m),

104
00:08:43,833 --> 00:08:50,160
juntas resultam numa
matrix (n'¹',m), como esperado.

105
00:08:50,160 --> 00:08:55,030
Agora, o detalhe final é que
b'¹' tem dimensões (n'¹', 1)

106
00:08:55,030 --> 00:09:01,147
mas quando pegamos isso aqui e adicionamos ao b
ocorre o "broadcasting" do Python,

107
00:09:01,147 --> 00:09:08,218
isso é duplicado e vira uma matrix (n'¹',m)
e depois elemento por elemento é adicionado.

108
00:09:08,218 --> 00:09:14,977
Assim, na tela anterior nós falamos
sobre as dimensões de w, b, dw e db.

109
00:09:14,977 --> 00:09:21,143
E aqui nós temos que
enquanto as minúsculas z'ˡ' e

110
00:09:21,143 --> 00:09:26,922
a'ˡ' são de dimensões (n'ˡ',1),

111
00:09:26,922 --> 00:09:34,650
por outro lado, temos agora as maiúsculas
Z'ˡ' e Az'ˡ' que são de dimensões (n'ˡ',m).

112
00:09:34,650 --> 00:09:40,410
E um caso especial disso é quando
L é igual a 0 e,

113
00:09:40,410 --> 00:09:45,188
nesse caso, A'⁰', que é igual ao

114
00:09:45,188 --> 00:09:49,543
conjunto de dados de treino para
as características de entrada X,

115
00:09:49,543 --> 00:09:54,616
será igual a (n'⁰', m),
como esperado.

116
00:09:54,616 --> 00:10:01,259
E quando estiver implementando isso
em retro-propagação,

117
00:10:01,259 --> 00:10:06,749
você verá, mais tarde, que terminará
calculando dZ'ˡ' e também dA'ˡ'.

118
00:10:06,749 --> 00:10:11,327
E claro que ambos têm

119
00:10:11,327 --> 00:10:15,736
a mesma dimensão de Z'ˡ' e A'ˡ',
 (n'ˡ',m).

120
00:10:15,736 --> 00:10:19,467
Assim, espero que esse pequeno exercício
que fizemos ajude a explicar as dimensões

121
00:10:19,467 --> 00:10:21,685
das várias matrizes,
com as quais estamos trabalhando.

122
00:10:21,685 --> 00:10:25,947
Quando for implementar retro-propagação
numa rede neural profunda, a medida que

123
00:10:25,947 --> 00:10:30,350
criar o código, certifique-se de que todas as
dimensões das matrizes sejam consistentes.

124
00:10:30,350 --> 00:10:31,825
Isso ajudará muito e,

125
00:10:31,825 --> 00:10:35,908
de algum modo, eliminará
causas de possíveis erros.

126
00:10:35,908 --> 00:10:40,325
Assim, espero que esse exercício de descobrir
as dimensões de várias matrizes,

127
00:10:40,325 --> 00:10:41,979
seja útil quando estiver trabalhando com isso.

128
00:10:41,979 --> 00:10:44,788
Ao implementar uma rede neural profunda,

129
00:10:44,788 --> 00:10:48,241
se você atentar para as dimensões das várias
matrizes e vetores, com as quais estiver trabalhando,

130
00:10:48,241 --> 00:10:52,162
certamente isso ajudará a eliminar
algumas causas de erros.

131
00:10:52,162 --> 00:10:54,467
Isso também me ajuda a escrever um código correto.

132
00:10:54,467 --> 00:10:58,882
Assim, até então vimos algumas
metodologias de como fazer

133
00:10:58,882 --> 00:11:01,227
propagação para frente em uma rede neural.

134
00:11:01,227 --> 00:11:04,163
Mas porque redes neurais profundas são
tão efetivas e

135
00:11:04,163 --> 00:11:07,243
porque funcionam melhor que
representações mais superficiais?

136
00:11:07,243 --> 00:11:09,939
Para discutir isso, vamos gastar
alguns minutos com o próximo vídeo. 
[Tradução: Carlos R. Lacerda | Revisão: Carlos Lage]