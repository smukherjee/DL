1
00:00:00,329 --> 00:00:05,700
如果说深度学习和神经网络

2
00:00:03,060 --> 00:00:07,470
背后的基本思想

3
00:00:05,700 --> 00:00:09,870
已经存在数十年了

4
00:00:07,470 --> 00:00:12,090
为什么它们最近才风靡起来

5
00:00:09,870 --> 00:00:14,130
在本次视频中，让我来回顾一下

6
00:00:12,090 --> 00:00:16,170
深度学习崛起背后的一些主要驱动力量

7
00:00:14,130 --> 00:00:18,090
因为我认为这能够帮助你更好地在你的工作单位内

8
00:00:16,170 --> 00:00:20,850
发现最好的机会

9
00:00:18,090 --> 00:00:22,439
来运用这些工具

10
00:00:20,850 --> 00:00:24,240
这些年来，很多人问我

11
00:00:22,439 --> 00:00:26,820
为什么深度学习

12
00:00:24,240 --> 00:00:28,949
突然间这么好使了

13
00:00:26,820 --> 00:00:31,109
当我向他们回答这个问题时

14
00:00:28,949 --> 00:00:33,210
这个通常是我给他们画的图

15
00:00:31,109 --> 00:00:36,180
让我们先来画个坐标图

16
00:00:33,210 --> 00:00:39,270
横坐标表示，在某个任务上我们拥有的数据量

17
00:00:36,180 --> 00:00:42,570
纵坐标表示

18
00:00:39,270 --> 00:00:44,430
学习算法的性能

19
00:00:42,570 --> 00:00:48,180
比如垃圾邮件分类器的准确率

20
00:00:44,430 --> 00:00:51,960
或者广告点击预测的准确率

21
00:00:48,180 --> 00:00:53,969
或者对于自动驾驶来说

22
00:00:51,960 --> 00:00:56,399
探测其他车子位置的

23
00:00:53,969 --> 00:00:58,440
神经网络的准确率

24
00:00:56,399 --> 00:01:00,270
如果你将传统的学习算法

25
00:00:58,440 --> 00:01:02,460
比如支持向量机或者逻辑回归

26
00:01:00,270 --> 00:01:04,710
这些算法的性能来作为数据量的函数

27
00:01:02,460 --> 00:01:07,619
来作图

28
00:01:04,710 --> 00:01:09,720
你或许会得到如图的曲线

29
00:01:07,619 --> 00:01:11,670
这个曲线里面，刚开始性能逐渐上升

30
00:01:09,720 --> 00:01:14,280
当你加入越多的数据

31
00:01:11,670 --> 00:01:16,200
但是过了一段时间

32
00:01:14,280 --> 00:01:18,630
性能逐渐地趋于平坦

33
00:01:16,200 --> 00:01:21,180
这本该是水平的线

34
00:01:18,630 --> 00:01:25,320
我这画的不太好

35
00:01:21,180 --> 00:01:28,140
他们不知道该拿大规模的数据怎么办

36
00:01:25,320 --> 00:01:30,689
但是过去10年

37
00:01:28,140 --> 00:01:32,850
我们社会发生的事情是

38
00:01:30,689 --> 00:01:34,820
对于非常多的问题

39
00:01:32,850 --> 00:01:38,610
我们从只有相对来说少量的数据

40
00:01:34,820 --> 00:01:40,979
到相对大量的数据

41
00:01:38,610 --> 00:01:43,979
这些数据大多都是归功于社会的信息化

42
00:01:40,979 --> 00:01:46,979
非常多的人类行为

43
00:01:43,979 --> 00:01:48,720
已经被电子化和信息化

44
00:01:46,979 --> 00:01:51,180
我们在电脑上、网站上、手机APP

45
00:01:48,720 --> 00:01:54,320
花了大量的时间

46
00:01:51,180 --> 00:01:57,960
这些在电子设备上的行为创造了数据

47
00:01:54,320 --> 00:02:00,360
因为手机内置的相机

48
00:01:57,960 --> 00:02:02,369
还有加速计

49
00:02:00,360 --> 00:02:05,909
还有各种物理网中的感应器

50
00:02:02,369 --> 00:02:07,890
这些东西的普及

51
00:02:05,909 --> 00:02:11,129
我们已经收集了越来越多的数据

52
00:02:07,890 --> 00:02:12,870
所以在过去20年

53
00:02:11,129 --> 00:02:13,560
对于很多应用

54
00:02:12,870 --> 00:02:16,319
我们已经积累了非常多的数据

55
00:02:13,560 --> 00:02:17,550
超出了传统的学习算法

56
00:02:16,319 --> 00:02:20,520
所能有效利用的

57
00:02:17,550 --> 00:02:22,560
而如果使用神经网络

58
00:02:20,520 --> 00:02:26,310
你如果训练一个

59
00:02:22,560 --> 00:02:28,470
小型的神经网络

60
00:02:26,310 --> 00:02:31,349
那么它的性能也许看起来像这个样子

61
00:02:28,470 --> 00:02:34,590
你如果训练一个稍微大点的神经网络

62
00:02:31,349 --> 00:02:36,330
让我们叫它中型的神经网络

63
00:02:34,590 --> 00:02:39,900
它的性能会稍微好些

64
00:02:36,330 --> 00:02:42,180
你如果训练一个非常大的神经网络

65
00:02:39,900 --> 00:02:44,580
它的性能会越来越好

66
00:02:42,180 --> 00:02:46,890
所以观察到几个现象

67
00:02:44,580 --> 00:02:49,410
第一，如果你想要达到如此高的性能

68
00:02:46,890 --> 00:02:52,620
你需要2件事

69
00:02:49,410 --> 00:02:54,420
首先，你需要训练

70
00:02:52,620 --> 00:02:57,360
足够大的网络

71
00:02:54,420 --> 00:02:59,670
来利用大量的数据

72
00:02:57,360 --> 00:03:02,010
其次，你需要

73
00:02:59,670 --> 00:03:05,430
在x轴的这里，你需要非常多的数据

74
00:03:02,010 --> 00:03:07,799
所以我们通常说

75
00:03:05,430 --> 00:03:10,860
规模，驱动深度学习进展

76
00:03:07,799 --> 00:03:12,900
对于规模，我的意思是

77
00:03:10,860 --> 00:03:15,150
不仅仅是神经网络的规模

78
00:03:12,900 --> 00:03:17,069
网络有着很多隐含单元、很多参数、很多连接

79
00:03:15,150 --> 00:03:21,480
还有数据的规模

80
00:03:17,069 --> 00:03:23,910
事实上今天

81
00:03:21,480 --> 00:03:25,440
想要达到高性能，

82
00:03:23,910 --> 00:03:27,390
最可靠的方法之一

83
00:03:25,440 --> 00:03:29,940
是要么训练一个大网络

84
00:03:27,390 --> 00:03:31,829
要么放更多的数据

85
00:03:29,940 --> 00:03:33,359
然后这个在一定程度后也会达到瓶颈

86
00:03:31,829 --> 00:03:35,640
因为最终你会用光所有数据

87
00:03:33,359 --> 00:03:37,769
或者最终你的网络实在太大了

88
00:03:35,640 --> 00:03:40,200
训练得花很长时间

89
00:03:37,769 --> 00:03:42,690
但是，光是改善规模就足以

90
00:03:40,200 --> 00:03:45,810
帮助我们在深度学习里面前进一大步

91
00:03:42,690 --> 00:03:48,060
为了让这个示意图

92
00:03:45,810 --> 00:03:49,920
更加的准确，让我再加点东西

93
00:03:48,060 --> 00:03:53,040
我在X轴上写了数据量

94
00:03:49,920 --> 00:03:57,900
更准确地来说，这是被标记过的数据量

95
00:03:53,040 --> 00:04:00,180
被标记过的意思是

96
00:03:57,900 --> 00:04:03,630
对于训练集，

97
00:04:00,180 --> 00:04:05,910
我们同时有输入X和标记Y

98
00:04:03,630 --> 00:04:07,709
我来稍微介绍一下这门课

99
00:04:05,910 --> 00:04:10,769
我们将用到的符号

100
00:04:07,709 --> 00:04:12,540
我们将使用m来表示训练集的大小

101
00:04:10,769 --> 00:04:13,739
我们将使用m来表示训练集的大小，

102
00:04:12,540 --> 00:04:15,690
也就是训练样本的数量。

103
00:04:13,739 --> 00:04:18,989
这个小m就是水平坐标。

104
00:04:15,690 --> 00:04:20,310
这个图里还有一些细节

105
00:04:18,989 --> 00:04:23,340
要说明

106
00:04:20,310 --> 00:04:26,970
在训练集的这个区间里，

107
00:04:23,340 --> 00:04:29,700
不同算法的顺序（排名）

108
00:04:26,970 --> 00:04:31,590
是不规则的。

109
00:04:29,700 --> 00:04:34,500
如果你没有很大的数据集，

110
00:04:31,590 --> 00:04:36,510
那么你自己提取出来的的特征很大程度上

111
00:04:34,500 --> 00:04:39,090
决定了算法的结果（性能，表现）。

112
00:04:36,510 --> 00:04:41,910
所以，很有可能发生这样的事，

113
00:04:39,090 --> 00:04:44,070
有人用手工设计好的特征去训练

114
00:04:41,910 --> 00:04:46,320
SVM （支持向量机），

115
00:04:44,070 --> 00:04:48,300
而有人在这个很小的训练集上

116
00:04:46,320 --> 00:04:50,730
训练一个很大网络，

117
00:04:48,300 --> 00:04:53,130
SVM 可能会得到更好的结果。

118
00:04:50,730 --> 00:04:55,020
所以，在这个图像左边的这个区域里，

119
00:04:53,130 --> 00:04:57,090
所以，在这个图像左边的这个区域里，

120
00:04:55,020 --> 00:04:59,550
不同算法的性能的排名

121
00:04:57,090 --> 00:05:01,919
是不固定的。

122
00:04:59,550 --> 00:05:03,389
这个性能更多的是由你提取特征的能力

123
00:05:01,919 --> 00:05:05,970
和算法的细节而决定的。

124
00:05:03,389 --> 00:05:08,850
只有在这个很大的数据集的区域里，

125
00:05:05,970 --> 00:05:12,000
只有在这个很大的数据集的区域里，

126
00:05:08,850 --> 00:05:14,669
只有在这个 m 很大的数据集的区域里，

127
00:05:12,000 --> 00:05:17,639
我们会经常看到很大的神经网络

128
00:05:14,669 --> 00:05:19,560
超过了其他方法。

129
00:05:17,639 --> 00:05:21,600
如果你的朋友问你

130
00:05:19,560 --> 00:05:23,700
神经网络为什么现在才流行起来，

131
00:05:21,600 --> 00:05:26,729
我建议你可以把这个图

132
00:05:23,700 --> 00:05:28,890
给他们画出来。

133
00:05:26,729 --> 00:05:29,310
在最近这次深度学习崛起的早期，

134
00:05:28,890 --> 00:05:32,070
两个不同的关于小脑学习的例子

135
00:05:29,310 --> 00:05:34,919
主要是靠大量的数据

136
00:05:32,070 --> 00:05:36,330
和高效的计算能力，

137
00:05:34,919 --> 00:05:39,479
也就是我们训练网络的能力

138
00:05:36,330 --> 00:05:41,850
无论是在 CPU 还是在 GPU 上。

139
00:05:39,479 --> 00:05:43,590
这个能力（的提升）让我们前进了很多。

140
00:05:41,850 --> 00:05:45,800
但是，我们在过去的几年里

141
00:05:43,590 --> 00:05:48,360
也看到了越来越多的

142
00:05:45,800 --> 00:05:50,539
算法层面上的创新。

143
00:05:48,360 --> 00:05:53,700
所以我也不想忽略这一点。

144
00:05:50,539 --> 00:05:56,940
有趣的是，

145
00:05:53,700 --> 00:06:01,139
很多在算法层面上的创新，

146
00:05:56,940 --> 00:06:03,510
使得神经网络能运行得更快。

147
00:06:01,139 --> 00:06:05,310
举一个例子，

148
00:06:03,510 --> 00:06:08,729
神经网络里一个重大的突破

149
00:06:05,310 --> 00:06:12,330
就是从 sigmoid 函数到 ReLU 函数的迁移。

150
00:06:08,729 --> 00:06:14,760
（左边的是 signmoid， 右边是ReLU）

151
00:06:12,330 --> 00:06:18,479
我们在之前的视频里已经讲过。

152
00:06:14,760 --> 00:06:20,190
如果你对我说的细节

153
00:06:18,479 --> 00:06:22,260
并不是很了解，

154
00:06:20,190 --> 00:06:24,389
不要担心。

155
00:06:22,260 --> 00:06:26,010
（要知道的是）sigmoid 函数在机器学习里有一个问题，

156
00:06:24,389 --> 00:06:27,870
有一个问题，

157
00:06:26,010 --> 00:06:29,520
那就是在

158
00:06:27,870 --> 00:06:30,280
这些梯度几乎为 0

159
00:06:29,520 --> 00:06:32,920
区域里，

160
00:06:30,280 --> 00:06:35,350
那么学习的进度会变得非常慢。

161
00:06:32,920 --> 00:06:37,060
那么学习的进度会变得非常慢。

162
00:06:35,350 --> 00:06:39,639
因为当你用梯度下降法的时候，

163
00:06:37,060 --> 00:06:41,470
梯度（几乎）为 0，

164
00:06:39,639 --> 00:06:44,740
那么参数会变得非常慢。

165
00:06:41,470 --> 00:06:46,450
而当我们把我们叫做

166
00:06:44,740 --> 00:06:48,600
激活函数的这个东西

167
00:06:46,450 --> 00:06:52,060
换成一个叫做ReLU的函数

168
00:06:48,600 --> 00:06:54,970
全名是修正线性单元（rectified linear unit）

169
00:06:52,060 --> 00:06:57,070
全名是修正线性单元（rectified linear unit）

170
00:06:54,970 --> 00:07:00,220
对于所有的正输入，

171
00:06:57,070 --> 00:07:03,100
梯度都是1，

172
00:07:00,220 --> 00:07:04,750
那么梯度就不会慢慢变成 0。

173
00:07:03,100 --> 00:07:07,300
那么梯度就不会慢慢变成 0。

174
00:07:04,750 --> 00:07:09,520
梯度在左边的时候为 0。

175
00:07:07,300 --> 00:07:12,580
结果证明，

176
00:07:09,520 --> 00:07:14,410
简单地把 sigmoid 函数换成 ReLU 函数

177
00:07:12,580 --> 00:07:16,960
使得梯度下降算法的速度

178
00:07:14,410 --> 00:07:19,169
提高很多。

179
00:07:16,960 --> 00:07:22,030
这个看似比 Bayesian 

180
00:07:19,169 --> 00:07:23,860
要简单许多的算法

181
00:07:22,030 --> 00:07:27,520
确实大大提高了运算速度。

182
00:07:23,860 --> 00:07:29,080
还有很多这样的例子，

183
00:07:27,520 --> 00:07:31,240
我们改进了算法，

184
00:07:29,080 --> 00:07:33,340
使得我们的代码

185
00:07:31,240 --> 00:07:35,140
可以更快的运行。

186
00:07:33,340 --> 00:07:37,479
从而我们可以训练更大的神经网络。

187
00:07:35,140 --> 00:07:39,520
从而我们可以训练更大的神经网络。

188
00:07:37,479 --> 00:07:42,250
当我们有一个很大的网络和很多的数据的时候，

189
00:07:39,520 --> 00:07:45,810
我们需要在合理的时间里去训练。

190
00:07:42,250 --> 00:07:48,610
快速计算很重要的另一个原因是

191
00:07:45,810 --> 00:07:51,070
快速计算很重要的另一个原因是

192
00:07:48,610 --> 00:07:53,710
你训练一个神经网络的过程

193
00:07:51,070 --> 00:07:56,350
是一个循环

194
00:07:53,710 --> 00:07:58,020
你有一个网络架构的想法，

195
00:07:56,350 --> 00:08:01,060
你去用代码实现，

196
00:07:58,020 --> 00:08:02,830
实现你的想法

197
00:08:01,060 --> 00:08:05,050
之后让你

198
00:08:02,830 --> 00:08:07,510
实验结果告诉你你的网络的表现怎么样

199
00:08:05,050 --> 00:08:10,030
这样你可以回过头去修改

200
00:08:07,510 --> 00:08:12,930
网络里面的细节

201
00:08:10,030 --> 00:08:15,880
然后重复这个循环。

202
00:08:12,930 --> 00:08:18,550
当你的神经网络需要花费很长时间训练的时候，<br />它就会花费很长时间在这个循环里。

203
00:08:15,880 --> 00:08:21,400
当你的神经网络需要花费很长时间训练的时候，<br />它就会花费很长时间在这个循环里

204
00:08:18,550 --> 00:08:24,039
当你的神经网络需要花费很长时间训练的时候，<br />它就会花费很长时间在这个循环里。

205
00:08:21,400 --> 00:08:26,740
在这中间，你的效率会受到很大的影响。

206
00:08:24,039 --> 00:08:29,560
比如你可以用10分钟或者最多一天尝试你的一个想法，看看它是否可行，

207
00:08:26,740 --> 00:08:34,169
比如你可以用10分钟或者最多一天尝试你的一个想法，看看它是否可行，

208
00:08:29,560 --> 00:08:36,370
比如你可以用10分钟或者最多一天尝试你的一个想法，看看它是否可行，

209
00:08:34,169 --> 00:08:39,490
也许，你需要1个月去训练你的网络。

210
00:08:36,370 --> 00:08:40,590
也许，你需要1个月去训练你的网络。

211
00:08:39,490 --> 00:08:42,570
有的时候确实会发生这样的情况。

212
00:08:40,590 --> 00:08:44,670
当你可以在10分钟或者1天之内得到结果的时候，你可以尝试很多想法。

213
00:08:42,570 --> 00:08:47,250
当你可以在10分钟或者1天之内得到结果的时候，你可以尝试很多想法。

214
00:08:44,670 --> 00:08:49,170
这会让你更有可能去发现你的网络是否针对你的应用是可行的。

215
00:08:47,250 --> 00:08:50,610
这会让你更有可能去发现你的网络是否针对你的应用是可行的。

216
00:08:49,170 --> 00:08:53,720
这会让你更有可能去发现你的网络是否针对你的应用是可行的。

217
00:08:50,610 --> 00:08:57,900
快速计算确实会提高你拿到实践结果的速度。

218
00:08:53,720 --> 00:08:59,730
快速计算确实会提高你拿到实践结果的速度。

219
00:08:57,900 --> 00:09:02,610
快速计算确实会提高你拿到实践结果的速度。

220
00:08:59,730 --> 00:09:05,400
无论是对于追求实践的人还是对于科研人员来说，

221
00:09:02,610 --> 00:09:07,550
无论是对于追求实践的人还是对于科研人员来说，

222
00:09:05,400 --> 00:09:10,650
这都对他们研究深度学习加快很多，

223
00:09:07,550 --> 00:09:13,320
这都对他们研究深度学习加快很多，

224
00:09:10,650 --> 00:09:16,589
对他们想法上的创新加快很多。

225
00:09:13,320 --> 00:09:18,570
这些都是对于深度学习这个领域很大的帮助。

226
00:09:16,589 --> 00:09:21,029
这些都是对于深度学习这个领域很大的帮助。

227
00:09:18,570 --> 00:09:23,370
这个领域的前沿有着不可思议的新算法和不断地发展。

228
00:09:21,029 --> 00:09:25,620
这个领域的前沿有着不可思议的新算法和不断地发展。

229
00:09:23,370 --> 00:09:28,920
这个领域的前沿有着不可思议的新算法和不断地发展。

230
00:09:25,620 --> 00:09:30,990
这些都是使得深度学习不断发展的动力。

231
00:09:28,920 --> 00:09:33,570
这些都是使得深度学习不断发展的动力。

232
00:09:30,990 --> 00:09:36,000
好消息是这些动力如今仍然在推动者深度学习更加进步。

233
00:09:33,570 --> 00:09:38,490
好消息是这些动力如今仍然在推动者深度学习更加进步。

234
00:09:36,000 --> 00:09:41,130
好消息是这些动力如今仍然在推动者深度学习更加进步。

235
00:09:38,490 --> 00:09:43,800
比如说数据，这个领域仍然生产越来越多的数据。

236
00:09:41,130 --> 00:09:45,660
比如说计算（能力），

237
00:09:43,800 --> 00:09:48,300
随着针对深度学习的硬件的发展，比如GPU、高速网络、或其他的硬件，

238
00:09:45,660 --> 00:09:50,940
随着针对深度学习的硬件的发展，比如GPU、高速网络、或其他的硬件，

239
00:09:48,300 --> 00:09:53,250
我很有信心，我们建造大型的神经网络的能力会越来越强大。

240
00:09:50,940 --> 00:09:55,140
我很有信心，我们建造大型的神经网络的能力会越来越强大。

241
00:09:53,250 --> 00:09:57,320
我很有信心，我们建造大型的神经网络的能力会越来越强大。

242
00:09:55,140 --> 00:10:00,360
我很有信心，我们建造大型的神经网络的能力会越来越强大。

243
00:09:57,320 --> 00:10:02,880
至于算法，

244
00:10:00,360 --> 00:10:05,070
整个深度学习领域一直都在不断地在创新。

245
00:10:02,880 --> 00:10:07,680
整个深度学习领域一直都在不断地在创新。

246
00:10:05,070 --> 00:10:09,839
正因如此，我认为我们可以很乐观（至少我很乐观），

247
00:10:07,680 --> 00:10:11,370
正因如此，我认为我们可以很乐观（至少我很乐观），

248
00:10:09,839 --> 00:10:13,650
深度学习会在未来的几年变得越来越好。

249
00:10:11,370 --> 00:10:14,120
深度学习会在未来的几年变得越来越好。

250
00:10:13,650 --> 00:10:17,100
那么，让我们继续下一期教程。

251
00:10:14,120 --> 00:10:18,540
那么，让我们继续下一期教程。

252
00:10:17,100 --> 00:10:20,280
在下一部分，

253
00:10:18,540 --> 00:10:22,610
我们将会讨论一些你能从这个课程中学到的知识。

254
00:10:20,280 --> 00:10:22,610
我们将会讨论一些你能从这个课程中学到的知识。