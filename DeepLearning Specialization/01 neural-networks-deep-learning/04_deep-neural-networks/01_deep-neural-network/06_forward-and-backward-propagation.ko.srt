1
00:00:00,000 --> 00:00:01,350
이전 강의에서 여러분에게

2
00:00:01,350 --> 00:00:05,065
심층신경망을 도입하는 것의 기본 구성요소를 보았는데요,

3
00:00:05,065 --> 00:00:07,696
각각 층에 대한 전 방향전파 스텝과

4
00:00:07,696 --> 00:00:09,975
그에 상응하는 후 방향전파 스텝을 살펴봤습니다.

5
00:00:09,975 --> 00:00:12,370
이제 이런 단계를 어떻게 도입하는지 보겠습니다.

6
00:00:12,370 --> 00:00:14,230
어떻게 도입하는지 보겠습니다.

7
00:00:14,230 --> 00:00:20,045
어떻게 도입하는지 보겠습니다.

8
00:00:20,045 --> 00:00:21,795
결과값 al 이고, cache는 ZL입니다.

9
00:00:21,795 --> 00:00:24,615
이행 시점에서 보면,

10
00:00:24,615 --> 00:00:28,145
WL과 BL도 cache로 사용될 수 있습니다.

11
00:00:28,145 --> 00:00:31,585
함수가 조금더 쉽게 되고록 연습문제에서 말이죠.

12
00:00:31,585 --> 00:00:35,438
여기 공식은 이미 익숙해 보이시겠지만,

13
00:00:35,438 --> 00:00:46,813
forward function을 도입하기 
위한 방법은 이거는 w[l] * a[l-1] + b[l]

14
00:00:46,813 --> 00:00:53,620
그리고 a[l]은 z에 적용된 activation function입니다.

15
00:00:53,620 --> 00:00:57,250
이 도입에 대한 부분을 벡터화시키고 싶으시면

16
00:00:57,250 --> 00:01:06,218
이거 곱하기 a[l-1] + b 인데요

17
00:01:06,218 --> 00:01:09,930
b를 더하는 것은 하이퍼 브로드캐스팅 인데요

18
00:01:09,930 --> 00:01:15,296
a[l] = g applied element-wise to z 입니다.

19
00:01:15,296 --> 00:01:20,128
기억하시겠지만, forward step의 다이어그램에서

20
00:01:20,128 --> 00:01:22,565
이러한 박스로 된 체인이 앞으로 진행되었습니다.

21
00:01:22,565 --> 00:01:26,775
이것을 a[0] 값을 삽입하면서 초기화를 시키죠.

22
00:01:26,775 --> 00:01:29,305
a[0]는 x값과 동일합니다.

23
00:01:29,305 --> 00:01:31,440
초기화를 시켰습니다.

24
00:01:31,440 --> 00:01:33,380
입력값의 첫번째는 무엇인가를 확인합니다.

25
00:01:33,380 --> 00:01:40,207
a[0]는 하나의 트레이닝 샘플에 대한 입력 특성일 수 있고,

26
00:01:40,207 --> 00:01:42,255
한번에 한 예시만 쓰는 경우 말이죠.

27
00:01:42,255 --> 00:01:45,800
아니면 대문자 A[0] 일 수도 있습니다.

28
00:01:45,800 --> 00:01:48,215
전체 트레이닝 세트를 진행할 때 말이죠.

29
00:01:48,215 --> 00:01:51,990
이것이 forward function에 대한 첫번때 입력값인데요,

30
00:01:51,990 --> 00:01:53,970
이것을 반복해서 전 방향전파를 계산할 수 있게 해줍니다.

31
00:01:53,970 --> 00:01:57,085
왼쪽에서 오른쪽으로 진행해서 말이죠.

32
00:01:57,085 --> 00:02:00,110
다음으로는 후 방향전파 step에 대해 이야기해보겠습니다.

33
00:02:00,110 --> 00:02:03,585
여기서는 입력값은 da[l]을 갖고

34
00:02:03,585 --> 00:02:08,040
결과값은da[l-1], dw[l] 그리고 db[l]입니다.

35
00:02:08,040 --> 00:02:16,240
이제 이 값들을 계산하기 위한 단계를 적어보겠습니다:

36
00:02:16,240 --> 00:02:23,866
dz[l]은 da[l] 곱하기 element wise product with g[l] z[l]입니다.

37
00:02:23,866 --> 00:02:27,405
그리고 derivatives를 계산합니다.

38
00:02:27,405 --> 00:02:34,420
dW[l] = dz[l] * a[l - 1].

39
00:02:34,420 --> 00:02:37,445
이 부분은 제가 cache에 멸백히 두지 않았는데요,

40
00:02:37,445 --> 00:02:39,114
이것도 필요합니다.

41
00:02:39,114 --> 00:02:47,560
그리고 db[l] 은 dz[l] 입니다.

42
00:02:47,560 --> 00:02:59,089
마지막으로 da[l-1]은 w[l] transpose 곱하기 dz[l]입니다.

43
00:02:59,089 --> 00:03:02,310
이것이 어떻게 산출됐는지 정확히 다루지는 앖겠습니다.

44
00:03:02,310 --> 00:03:06,605
하지만 여기 이 da 의 값을 가지고 여기에다가 대입시키면

45
00:03:06,605 --> 00:03:10,260
이전에 여기 있었던 똑같은 공식이 되는데요.

46
00:03:10,260 --> 00:03:16,617
dz[l] 을 이전 dz[l] 함수로 말이죠.

47
00:03:16,617 --> 00:03:18,135
만약 이 값을 여기에다가

48
00:03:18,135 --> 00:03:33,817
대입시키면 dz[l] = w[l+1]_transpose dz[l+1] * g[l]` z[l].

49
00:03:33,817 --> 00:03:36,165
굉장히 많은 양의 산수인데요.

50
00:03:36,165 --> 00:03:38,360
이 값은 여러분이 직접 다시 한번 확인할 수 있습니다.

51
00:03:38,360 --> 00:03:40,820
이 공식은 제가 저번주에 

52
00:03:40,820 --> 00:03:43,055
back propagation을 위해 사용했던 공식입니다.

53
00:03:43,055 --> 00:03:45,930
한개의 hidden layer인 신경망에 대해 다룰 때 말이죠.

54
00:03:45,930 --> 00:03:48,602
기억하시겠지만, 여기 곱하기는 
element-wise product인데요,

55
00:03:48,602 --> 00:03:54,950
그러므로 backward function을 도입하기 위해 
필요한 것은 여기 위의 4개 공식이 전부입니다.

56
00:03:54,950 --> 00:03:58,735
마지막으로, 벡터화된 버전을 적을텐데요,

57
00:03:58,735 --> 00:04:04,135
첫번째 줄은 dz[l] = dA[l]

58
00:04:04,135 --> 00:04:11,045
element-wise product with g[l]` of z[l] 입니다.

59
00:04:11,045 --> 00:04:13,060
놀랄 부분은 없겠죠.

60
00:04:13,060 --> 00:04:23,715
dW[l] 은 1/m, dz[l] * a[l-1]_transpose 입니다.

61
00:04:23,715 --> 00:04:30,844
그리고 db[l] 은 1/m np.sum dz[l] 이고,

62
00:04:30,844 --> 00:04:37,970
axis = 1, keepdims = true 입니다.

63
00:04:37,970 --> 00:04:44,095
저희는 np.sum의 대한 사용을 지난주에 
이야기했었는데요, db를 계산하기 위한 방법 말이죠.

64
00:04:44,095 --> 00:04:56,155
그리고 마지막으로 그리고 마지막으로 
dA (l-1)은 W[l]_transpose * dz[l] 입니다.

65
00:04:56,155 --> 00:05:02,435
이것은 이 값을 입력하게 해주고, 여기 da에 말이죠.

66
00:05:02,435 --> 00:05:07,950
그리고 여기 결과값 dW[l], db[l] 은

67
00:05:07,950 --> 00:05:10,100
필요한 derivatives입니다.

68
00:05:10,100 --> 00:05:16,022
그리고 dA[l-1] 도 말이죠,

69
00:05:16,022 --> 00:05:18,905
이렇게 해서 backward function을 도입합니다.

70
00:05:18,905 --> 00:05:20,560
요약해보자면,

71
00:05:20,560 --> 00:05:23,585
입력값 x를 갖고,

72
00:05:23,585 --> 00:05:25,060
여기 이렇게 첫번째 층이 있을 수 있는데요,

73
00:05:25,060 --> 00:05:28,445
ReLu activation 함수로 되어 있을 수 있습니다.

74
00:05:28,445 --> 00:05:30,570
다음으로 2번째 층으로 넘어가서

75
00:05:30,570 --> 00:05:33,340
또 하나의 ReLu activation 함수로 되어 있을 수 있습니다.

76
00:05:33,340 --> 00:05:35,175
세번째로 넘어가면

77
00:05:35,175 --> 00:05:39,595
시그모이드 activation 함수로 되어 있을 수 있겠죠. 
이진분류법을 사용하게 되겠죠.

78
00:05:39,595 --> 00:05:41,875
이것이 ŷ의 결과값으로 표출하구요.

79
00:05:41,875 --> 00:05:43,890
그 다음으로, ŷ을 이용해서

80
00:05:43,890 --> 00:05:46,265
loss를 산출할 수 있습니다.

81
00:05:46,265 --> 00:05:49,685
그리고 이제는 이런 값들을 사용해서 
backward iteration을 시작할 수 있게 됩니다.

82
00:05:49,685 --> 00:05:51,775
먼저 화살표를 그리겠습니다.

83
00:05:51,775 --> 00:05:54,320
펜을 자주 바꾸고 싶지 않습니다.

84
00:05:54,320 --> 00:06:03,430
그러면 이제 후 방향전파가 derivatives를 계산할텐데요,

85
00:06:03,430 --> 00:06:16,063
dW[3], db[3], dW[2], db[2], dW[1], db[1],

86
00:06:16,063 --> 00:06:18,865
그리고 진행 중에는 역시

87
00:06:18,865 --> 00:06:24,820
cache를 산출하는데 이것은 
z[1], z[2], z[3], 이동시킬 것입니다.

88
00:06:24,820 --> 00:06:32,250
여기서는 da[2] da[1] 이동시킬 것입니다.

89
00:06:32,250 --> 00:06:34,730
이것은 da[0]을 산출할 수 있는데요.

90
00:06:34,730 --> 00:06:35,880
이것은 사용하지 않을 것이기 때문에

91
00:06:35,880 --> 00:06:37,935
지워도 되겠습니다.

92
00:06:37,935 --> 00:06:40,785
자 그럼 이렇게 전 방향전파와 후 방향전파를

93
00:06:40,785 --> 00:06:44,040
3층으로 이루어진 신경망을 도입할 때 사용하면 되겠는데요.

94
00:06:44,040 --> 00:06:46,140
제가 1가지 부분에 대해 이야기 하지 않았는데요,

95
00:06:46,140 --> 00:06:48,735
forward recursion 에서는

96
00:06:48,735 --> 00:06:52,420
입력데이터 x로 초기화할 것입니다.

97
00:06:52,420 --> 00:06:54,090
그러면 backward recursion일 때는 어떨까요?

98
00:06:54,090 --> 00:06:59,235
da[l], 여러분이 로지스틱 회귀분석을 사용할 때,

99
00:06:59,235 --> 00:07:01,065
da[l], 여러분이 로지스틱 회귀분석을 사용할 때,

100
00:07:01,065 --> 00:07:02,977
이진분류법을 이용할 때,

101
00:07:02,977 --> 00:07:09,685
da[l]은 y/a + 1-y/1-a 입니다.

102
00:07:09,685 --> 00:07:12,575
결과값에 대한 loss 함수의 derivative는

103
00:07:12,575 --> 00:07:14,180
ŷ 이 이 값과

104
00:07:14,180 --> 00:07:17,475
일치한다고 보여질 수 있고

105
00:07:17,475 --> 00:07:19,105
여러분이 미적분학이 익숙하시면,

106
00:07:19,105 --> 00:07:21,328
l loss 함수를 

107
00:07:21,328 --> 00:07:24,150
ŷ에 대해 아니면 a에 대한 derivative를 취하면

108
00:07:24,150 --> 00:07:26,505
여기 이 공식처럼 나오는 것을 알 수 있습니다.

109
00:07:26,505 --> 00:07:31,350
이 공식을 최종 층인 대문자 l 인 
경우에 da 에 적용시키면 됩니다.

110
00:07:31,350 --> 00:07:35,715
여러분이 만약에 벡터화된 도입을 구하는 경우,

111
00:07:35,715 --> 00:07:38,351
backward recursion을 초기화 합니다.

112
00:07:38,351 --> 00:07:43,706
여기 이 값으로 말고, 여기와 같이 dA (l)층으로 말입니다.

113
00:07:43,706 --> 00:07:48,465
이것은 똑같을 것입니다. 다른 예시 나누기 처럼,

114
00:07:48,465 --> 00:07:54,011
트레이닝 예시인 a + 1-y,

115
00:07:54,011 --> 00:07:55,285
처음 트레이닝 예시에 대해서 말이죠.

116
00:07:55,285 --> 00:07:58,153
이 값 나누기 1-a 의 처음 트레이닝 예시에 대해서

117
00:07:58,153 --> 00:08:05,185
여기는 1-a[m] 되겠죠.

118
00:08:05,185 --> 00:08:09,423
이렇게해서 벡터화된 버전을 도입하는 것입니다.

119
00:08:09,423 --> 00:08:13,055
이런 방법을 통해서 후 방향전파의 
벡터화된 버전을 초기화 시킬 수 있습니다.

120
00:08:13,055 --> 00:08:16,100
여러분은 이제 전 방향전파와 후 
방향전파의 기본구성요소를 보았는데요,

121
00:08:16,100 --> 00:08:20,030
여러분은 이제 전 방향전파와 후 
방향전파의 기본구성요소를 보았는데요,

122
00:08:20,030 --> 00:08:22,340
여러분이 이제 이 공식을 도입하면

123
00:08:22,340 --> 00:08:24,530
필요한 derivative를 얻기 위한

124
00:08:24,530 --> 00:08:27,640
board prop 과 후 방향전파 의 
올바른 도입을 할 수 있습니다.

125
00:08:27,640 --> 00:08:29,660
이 과정에서 여러분은 공식이 너무 많아서

126
00:08:29,660 --> 00:08:32,080
약간 헷갈리시고, 어떤 원리로 
작동하는지 감이 잘 안 올 수 있는데요,

127
00:08:32,080 --> 00:08:34,645
이렇게 느껴지시면 이렇게 느껴지시면,

128
00:08:34,645 --> 00:08:37,205
여러분이 이번주 프로그래밍 과제 부분에 도달하시면

129
00:08:37,205 --> 00:08:40,175
여기서 배운 내용을 직접 도입할 수 있게 되면서

130
00:08:40,175 --> 00:08:42,020
저금 더 구체적으로 알게 될 것입니다.

131
00:08:42,020 --> 00:08:43,805
저도 공식이 많다는 것은 아는데요.

132
00:08:43,805 --> 00:08:46,265
또 일부는 이해가 안가셨을 수도 있습니다.

133
00:08:46,265 --> 00:08:49,055
미적분학과 선형수학을 바탕으로

134
00:08:49,055 --> 00:08:50,905
확인을 할 수 는 있습니다. 
쉽지는 않지만 언제든지

135
00:08:50,905 --> 00:08:52,430
해보셔도 좋습니다.

136
00:08:52,430 --> 00:08:56,390
사실 이부분이 사실 이부분이 머신러닝에서 
끌고오는 derivation중 어려운 부분이기도 합니다.

137
00:08:56,390 --> 00:08:57,950
여기서 사용한 제가 적었던 공식들은

138
00:08:57,950 --> 00:09:02,685
사실 대부분 derivative을 산출하기 위한
 공식이였습니다. 특히, backdrop에서 말이죠.

139
00:09:02,685 --> 00:09:04,750
다시 한번 말씀드리지만, 
이것이 만약 추상적이고

140
00:09:04,750 --> 00:09:06,400
약간 미스테리하면.

141
00:09:06,400 --> 00:09:09,108
제가 조언드리고 싶은 것은 연습문제를 해보면,

142
00:09:09,108 --> 00:09:11,465
해당 내용이 조금 더 
구체적으로 이해가 될 수 있습니다.

143
00:09:11,465 --> 00:09:14,120
하지만 저도 말할 수 있는 부분이 오늘 날,

144
00:09:14,120 --> 00:09:16,805
제가 러닝 알고리즘을 도입할 때, 
저도 제 러닝 알고리즘이

145
00:09:16,805 --> 00:09:18,000
작동하면 놀랍니다.

146
00:09:18,000 --> 00:09:21,170
그 이유는 머신 러닝의 복잡성은 
사실 상당부분 데이터에서 옵니다.

147
00:09:21,170 --> 00:09:25,670
코드 라인 말고 말이죠. 
가끔식은 몇 줄의 코드를 도입해서

148
00:09:25,670 --> 00:09:27,095
정확히 무엇을 했는지 코딩을 어떻게 했는지

149
00:09:27,095 --> 00:09:28,685
잘 모르는 경우에도,

150
00:09:28,685 --> 00:09:30,110
바로 잘 작동하는 경우가 있습니다.

151
00:09:30,110 --> 00:09:31,625
이런 경우는 코딩의 마법 효과가 아니라,

152
00:09:31,625 --> 00:09:35,296
이런 경우는 코딩의 마법 효과가 아니라,

153
00:09:35,296 --> 00:09:37,090
이런 코드가 흔히 너무 길지는 않고,

154
00:09:37,090 --> 00:09:38,705
물론 굉장히 간단한 것은 아니지만

155
00:09:38,705 --> 00:09:40,730
10,000중 또는

156
00:09:40,730 --> 00:09:42,115
100,000줄의 코드로 이루어 진 것이 아닙니다.

157
00:09:42,115 --> 00:09:44,750
그렇지만 데이터를 굉장히 많이 삽입하기 때문에,

158
00:09:44,750 --> 00:09:46,850
머신러닝을 오래 접했음에도 불구하고

159
00:09:46,850 --> 00:09:49,400
가끔씩은 저도 놀랩니다.

160
00:09:49,400 --> 00:09:53,274
저의 러닝 알고리즘이 잘 작동할 때 말이죠. 
그 이유는 러닝 알고리즘의 복작성은

161
00:09:53,274 --> 00:09:55,970
상당부분 데이터에서 시작되기 때문입니다.

162
00:09:55,970 --> 00:10:01,020
아주 길게 1000줄 또는 수천줄의 
코드를 사용해서 온다기 보다 말이죠.

163
00:10:01,020 --> 00:10:05,935
자 이렇게해서 심층신경망을 
어떻게 도입하는지 배웠는데요,

164
00:10:05,935 --> 00:10:10,325
다시 말씀드리지만, 연습문제를 진행하면 
조금 더 구체적으로 이해하실 수 있을 것입니다.

165
00:10:10,325 --> 00:10:14,220
넘어가기 앞서, 다음 비디오에서는

166
00:10:14,220 --> 00:10:17,480
하이퍼 파라미터와 파라미터에 
대해 이야기를 나누고 싶습니다.

167
00:10:17,480 --> 00:10:19,681
여러분이 깊은 네트워크를 트레이닝 하는 경우

168
00:10:19,681 --> 00:10:22,225
하이퍼 파라미터를 조직화 시키는 능력이

169
00:10:22,225 --> 00:10:25,400
네트워크를 개발하는데 효윻적으로 
작업할 수 있도록 도롸줄 것입니다.

170
00:10:25,400 --> 00:10:29,000
다름 비디오에서는 이것이 정확히 
무슨 뜻인지 이야기해보겠습니다.