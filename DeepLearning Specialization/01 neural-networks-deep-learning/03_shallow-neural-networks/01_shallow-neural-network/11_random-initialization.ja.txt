ニューラルネットワークを訓練するとき 重みをランダムに初期化することは
大切です ロジスティック回帰では
重みを０に初期化して大丈夫でしたが ニューラルネットワークで
重みをすべて０にして 最急降下法を適用しても
うまくいきません そのわけを見ていきましょう ２つの特徴量の入力があります なのでn[0]=2で
二つの隠れ層があるのでn[1]=2ですね すると 隠れ層のための行列W[1]は 2×2になります 試しに全て０に初期化してみましょう
[0 0 0 0]という2×2の行列です そしてb[1]も[0 0]とします バイアス項bを０に初期化するのは
実は問題ありません しかしWを全て０にするのは問題です なぜ問題なのかというと 全てのサンプルに対して a1[1]とa2[1]が同じですよね？ この活性化とこの活性化が
同じだということです なぜなら 隠れ層の二つのユニットは
全く同じ関数を計算しているからです それから逆伝播を行うと dz1[1]とdz2[1]も同じで 対称になってしまいますね 隠れ層のユニットが二つとも
同じように初期化されるのです ここでは 出力されるときの重みも 同じであると想定しています W[2]も[0 0]だということです このようにニューラルネットワークを
初期化すると 隠れ層のこのユニットとこのユニットが
全く同じになります 完璧に対称であるとも言えます 全く同じ関数を計算している
という意味です 帰納法から どの訓練の繰り返しの後でも 二つのユニットは全く同じ関数を
計算しているといえます すると dWはこのように 全ての行が同じ値である
行列になります 重みを更新すると W[1]はW[1]-α×dWに
更新されます すると 全ての繰り返しの後で 一行目が二行目と同じであることが
わかるでしょう 帰納法から次のようなことがわかります Wの値を全て０に初期化すると 隠れ層の二つのユニットが
同じ関数の計算から始まるので そして二つともが出力ユニットに
同じ影響を与えるので 一回目の繰り返しの後でも 二つの隠れユニットは対称なままです なので帰納法を用いると
２回目 ３回目の繰り返しの後も いくらニューラルネットワークを
訓練しても 二つの隠れユニットは 全く同じ関数を
計算し続けていることがわかります この場合 隠れユニットを
一つ以上持つ意味がなくなってしまいます 全部が同じことを計算している
だけだからです もちろんもっと大きな
ニューラルネットワークで ３つの特徴量があって
たくさんの隠れユニットがあっても このニューラルネットワークと
同じことが言えるでしょう 全ての重みを０に初期化すると 全ての隠れユニットが
対称になってしまうのです そしてどれだけ最急降下法を実行しても 全く同じ関数を計算し続けるでしょう 異なる隠れユニットには
異なる関数を計算してほしいので これではうまくいきません 解決法は パラメータをランダムに
初期化することです 実際には次のようなことをします W1=np.random.randnと
設定します これでガウス分布の乱数が生成されます
(2,2)のサイズです そして大抵は とても小さい数字
0.01などを掛けます つまりとても小さい乱数に
初期化することになります bは対称になっても
問題ではない つまり 対称性の破れの問題と呼ばれるものを
持たないとわかっています そのため bはただの０に初期化して
大丈夫です Wがランダムに初期化されている限り それぞれの隠れユニットが
別の計算をしている状態で始められるからです すると もう対称性の破れの問題に
悩まされることはありません 同様にW[2]もランダムに初期化しましょう b[2]は０で初期化して構いません さて この定数はどこから来てなぜ0.01なのか
疑問に思っているかもしれません なぜ100や1000ではないのでしょう？ それは 普通は重みを
とても小さいランダムな値に 初期化するのが好まれるからです もしtanhやシグモイド関数や
他のシグモイドを 適用するとき
たとえそれが出力層だけであっても もし重みが大きすぎたら 活性化の値を計算したとき z[1]=W1x+bで a[1]がz[1]に活性化関数を
適用したものですね なので もしwがとても大きければ zのうちいくつかは
とても大きいか小さい値になりますよね するとtanhやシグモイド関数の
平らな部分にきてしまうので 勾配が小さくなるため 最急降下法は遅くなってしまいます つまり 学習が遅くなるのです おさらいすると wが大きすぎれば 訓練の初めの方であっても
zはとても大きくなってしまい tanhやシグモイド活性化関数が
飽和したような状態になるため 学習が遅くなってしまいます シグモイドやtanh活性化関数を
ニューラルネットワークで 全く使わないなら
それほど問題ではありませんが 二項分類をするなら
出力ユニットはシグモイド関数なので パラメータの初期状態は
大きくない方が良いです こういうわけで0.01や
他の小さい数字を 掛けることが
合理的だといえるのです そしてW[2]についても同じですね これがrandom.randnとなって この例では1×2の行列ですよね
それ掛ける0.01です ここの「s」を忘れていました 最後に 0.01より良い定数も
あり得ます 隠れ層が一つのみの
ニューラルネットワークを訓練しているとき 隠れ層の多くない 比較的浅い
ニューラルネットワークなので 0.01に設定しても大丈夫でしょう しかしすごく深いニューラルネットワークを
訓練する場合 0.01ではない定数を
選びたくなることもあるかもしれません 次の週には なぜ0.01ではない
定数を選びたくなることが あるのかをお話ししたいと思います どちらにせよ これはある程度
小さい値となります 今週の動画は以上です 隠れ層が一つの
ニューラルネットワークの作り方や パラメータの初期化の仕方
順伝播を使った予測の方法 そして誤差逆伝播法を使った
微分の求め方や 最急降下法の実装の仕方が
わかりましたね では きっと今週のクイズと プログラミング課題ができると思います 頑張ってくださいね プログラミング課題を楽しんでもらえれば
嬉しいです それでは四週目で会えるのを
心待ちにしています