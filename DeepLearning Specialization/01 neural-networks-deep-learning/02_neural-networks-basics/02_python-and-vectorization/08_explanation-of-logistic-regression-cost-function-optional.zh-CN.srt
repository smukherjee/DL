1
00:00:00,390 --> 00:00:03,860
在较早的视频中
我已经写下了

2
00:00:03,860 --> 00:00:05,230
逻辑回归的成本函数的公式

3
00:00:05,230 --> 00:00:09,370
在这个选看的视频里
我想向大家简短解释一下

4
00:00:09,370 --> 00:00:13,490
为什么我们在逻辑回归里使用这个代价函数

5
00:00:13,490 --> 00:00:17,709
我们快速地复习一下，逻辑回归中

6
00:00:17,709 --> 00:00:23,704
我们已经预测了ŷ=sigmoid(W^T X + b)

7
00:00:23,704 --> 00:00:27,711
其中的sigmoid函数就是这个

8
00:00:27,711 --> 00:00:34,353
同时，我们想将ŷ解释为p(y=1|x)，在x的条件下，y=1的概率

9
00:00:34,353 --> 00:00:39,776
因此我们希望算法输出ŷ

10
00:00:39,776 --> 00:00:45,030
表示当给定输入特征x的时候y=1的概率

11
00:00:45,030 --> 00:00:50,090
换句话说，如果y等于1

12
00:00:50,090 --> 00:00:56,020
那么p(y|x)就等于ŷ

13
00:00:56,020 --> 00:00:59,170
相反地，当y=0时

14
00:01:00,310 --> 00:01:05,840
p(y|x)就等于1-ŷ

15
00:01:05,840 --> 00:01:09,150
因此，如果ŷ表示当y=1的概率

16
00:01:09,150 --> 00:01:13,620
那么1-ŷ就表示y=0的概率

17
00:01:13,620 --> 00:01:18,057
现在把最后两个方程
复制到下一张幻灯片里

18
00:01:18,057 --> 00:01:22,684
我把两个方程写在这，

19
00:01:22,684 --> 00:01:28,010
它们给出了，在y=0和y=1两种情况下，
p(y|x)的定义

20
00:01:28,010 --> 00:01:33,110
接下来，把两个方程归纳为一个方程

21
00:01:33,110 --> 00:01:37,543
需要指出的是
y只能等于0或1，因为是二元分类函数

22
00:01:37,543 --> 00:01:41,110
因此这里只有y=0或1这两种可能性

23
00:01:41,110 --> 00:01:44,653
有人拿到这两个方程，然后归纳为下面这样

24
00:01:44,653 --> 00:01:48,774
我先写出这个方程
然后我们再解释为什么有人这样写

25
00:01:48,774 --> 00:01:54,040
方程是：p(y|x)=ŷ^y * (1-ŷ)^(1-y)

26
00:01:54,040 --> 00:01:58,920
所以上面的两个方程就只用一行就概括了

27
00:01:58,920 --> 00:02:00,500
让我解释一下原因

28
00:02:00,500 --> 00:02:04,643
在第一种情况下，假设y=1

29
00:02:04,643 --> 00:02:09,562
如果y=1，那么这一项就会变成了ŷ

30
00:02:09,562 --> 00:02:13,970
因为这是ŷ的一次幂

31
00:02:13,970 --> 00:02:21,120
这一项最后会变成(1-ŷ)的(1-1)次幂
也就是0次幂

32
00:02:21,120 --> 00:02:26,320
但是，任何数的零次幂等于1
所以这一项可以省略

33
00:02:26,320 --> 00:02:33,030
所以，当y=1时，这个方程就是p(y|x)=ŷ

34
00:02:33,030 --> 00:02:37,480
这完全就是我们想要的方程

35
00:02:37,480 --> 00:02:40,250
那么，第二种情况呢
如果y=0呢？

36
00:02:40,250 --> 00:02:47,057
如果y=0
那么上面这个方程就是p(y|x)=ŷ^0 ...

37
00:02:47,057 --> 00:02:51,920
但是，任何数的零次幂等于1

38
00:02:51,920 --> 00:02:58,267
所以这就等于1*(1-ŷ)^(1-y)

39
00:02:58,267 --> 00:03:02,770
所以1-y就是1-0，这项就是1

40
00:03:02,770 --> 00:03:07,610
这项等于1*(1-ŷ)=1-ŷ

41
00:03:10,700 --> 00:03:17,230
所以我们得到
当y=0时，P(y|x)=1-ŷ

42
00:03:17,230 --> 00:03:21,570
这完全就是我们上面想要的

43
00:03:21,570 --> 00:03:23,690
所以，刚才展示的这个公式的确是

44
00:03:25,330 --> 00:03:30,331
p(y|x)的正确定义

45
00:03:30,331 --> 00:03:36,513
最后，
因为对数函数是一个绝对的单调递增函数

46
00:03:36,513 --> 00:03:43,223
最大化log(p(y|x))会得出
和最大化p(y|x)相似的结果

47
00:03:43,223 --> 00:03:48,672
如果计算log(p(y|x))

48
00:03:48,672 --> 00:03:54,330
也就是log((ŷ^y)*(1-ŷ)^(1-y))

49
00:03:54,330 --> 00:03:59,818
简化为ylog(ŷ)+(1-y)log(1-ŷ)

50
00:03:59,818 --> 00:04:05,881
对吗

51
00:04:05,881 --> 00:04:09,832
其实，这就是我们之前找到的

52
00:04:09,832 --> 00:04:14,310
损失函数函数的相反数

53
00:04:14,310 --> 00:04:17,470
这里有一个负号
，因为通常在训练一个学习算法的时候

54
00:04:17,470 --> 00:04:20,460
我们想要让概率变大

55
00:04:20,460 --> 00:04:23,980
而在逻辑回归中，我们这样来表示

56
00:04:23,980 --> 00:04:25,820
我们想要最小化L(ŷ,y)这个损失函数

57
00:04:25,820 --> 00:04:30,640
因此，最小化损失函数相当于
最大化概率的对数

58
00:04:30,640 --> 00:04:33,925
这就是损失函数在单一样本上的例子

59
00:04:33,925 --> 00:04:35,435
那代价函数呢

60
00:04:35,435 --> 00:04:40,435
整体的代价函数，在所有样本的训练集里是怎样呢

61
00:04:40,435 --> 00:04:41,385
让我们搞明白

62
00:04:41,385 --> 00:04:45,710
把训练集里全部标签出现的概率

63
00:04:45,710 --> 00:04:47,750
用一种不是很正式地方法写下来

64
00:04:47,750 --> 00:04:51,945
如果假设取出的训练样本相互独立
或者说服从IID

65
00:04:51,945 --> 00:04:54,198
也就是独立同分布
(I.I.D: Independent and Identically Distributed)

66
00:04:54,198 --> 00:04:57,810
那么这些样本的概率就是各项概率的乘积

67
00:04:57,810 --> 00:05:03,143
在给定X(i)
从i=1到m时
p(y(i))的乘积

68
00:05:03,143 --> 00:05:07,970
如果想要采用最大似然估计

69
00:05:07,970 --> 00:05:12,476
那么要就找到一些参数，使其能够最大化

70
00:05:12,476 --> 00:05:15,948
观察点和训练集中的样本的概率

71
00:05:15,948 --> 00:05:20,200
但最大化这个式子本身
和最大化它的对数效果相同

72
00:05:20,200 --> 00:05:22,990
所以在方程两边取对数

73
00:05:22,990 --> 00:05:28,640
依据乘积的对数等于对数的和的性质

74
00:05:28,640 --> 00:05:30,990
训练集标签的概率的对数就可以表达为

75
00:05:30,990 --> 00:05:39,000
在给定X(i)时
从i=1到m的log(p(y(i)))的和

76
00:05:39,000 --> 00:05:43,582
在前一张幻灯片中，我们已经知道

77
00:05:43,582 --> 00:05:47,630
这就是-L(ŷ(i),y(i))

78
00:05:48,850 --> 00:05:55,220
在统计学上
有个原理叫做最大似然估计原理

79
00:05:55,220 --> 00:06:00,720
意思是选择能使之最大化的参数

80
00:06:00,720 --> 00:06:04,220
换句话说，选这个参数来使这个式子最大化

81
00:06:04,220 --> 00:06:09,510
这项-sum(L(ŷ^i, y^i)，从i=1到m
<i><i>

82
00:06:09,510 --> 00:06:11,840
可以直接将负号移到累加号的外边

83
00:06:11,840 --> 00:06:15,749
这样就解释了我们为逻辑回归设置的

84
00:06:15,749 --> 00:06:21,235
代价函数J(w,b)也就是对L(ŷ(i),y(i))的求和

85
00:06:21,235 --> 00:06:27,349
因为我们现在要最小化代价函数
而不是最大化似然值

86
00:06:27,349 --> 00:06:30,095
所以要去掉这个负号

87
00:06:30,095 --> 00:06:35,467
最后为了方便起见
我们使这些数值处于更好的尺度上

88
00:06:35,467 --> 00:06:39,310
就在前面添加一个缩放系数1/m

89
00:06:39,310 --> 00:06:43,960
综上，通过最小化代价函数J(w,b)

90
00:06:43,960 --> 00:06:48,430
实际上对逻辑回归模型进行了最大似然估计

91
00:06:48,430 --> 00:06:53,120
这是基于训练样本是i.i.d.

92
00:06:53,120 --> 00:06:55,530
或者说是独立同分布的假设之上的

93
00:06:55,530 --> 00:06:59,550
谢谢你看这个视频
即使这是选看的

94
00:06:59,550 --> 00:07:03,845
希望它帮助你理解
为什么我们将这个代价函数用于

95
00:07:03,845 --> 00:07:05,200
逻辑回归的实际学习算法

96
00:07:05,200 --> 00:07:09,287
我希望你继续完成练习，

97
00:07:09,287 --> 00:07:11,277
本周编程练习和问答题

98
00:07:11,277 --> 00:07:14,735
祝在接下来的测验和编程练习中好运
GTC字幕组 翻译