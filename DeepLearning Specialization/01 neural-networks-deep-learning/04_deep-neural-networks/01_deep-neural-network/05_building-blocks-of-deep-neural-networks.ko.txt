지난 몇 주 간 시청하신 강의와 이번주 초에 시청하신 강의에서 deep neural network를 구현할 때 필요한 핵심 요소인 순전파와 역전파의 기본 토대는 이미 배우셨습니다. 이제 이러한 컴포넌트들을 어떻게 하나로 합쳐 <br />deep network를 구성하는지 보시게 될껍니다 여기 몇 개의 레이어로 이루어진 네트워크가 있습니다 이제 레이어 하나를 선택하여 거기서 일어나는 연산에 집중해봅시다 레이어 L에는 WL과 BL같은 파라미터들이 있습니다 그리고 순전파가 일어나면 이전 레이어의 활성화 함수 출력인 AL-1를 입력으로 하여 AL을 출력하게 됩니다 지난 강의에서 저희는 ZL=WLxAL-1+BL를 계산한 후, AL=G(ZL)를 계산했습니다 바로 이런 방법으로 입력 AL-1으로부터 출력 AL이 나오게 됩니다 그리고 이 과정에서 계산된 ZL의 값도 캐싱해두면 나중에 재사용이 필요할 때 매우 유용합니다 그러니 이 캐시도 여기 포함하겠습니다 왜냐하면 이렇게 저장해둔 ZL 값을 나중에 역전파에서 유용하게 써먹을 수 있거든요 backward 단계에서는, 그러니까 역전파 단계에서는 다시 같은 레이어 L에 집중해서 dAL을 입력으로 받아 dAL-1을 출력하는 함수를 구현하게 됩니다 좀더 구체화해보면 입력은 dAL과 미리 계산해 저장해두었던 ZL의 값이고, 출력으로는 dAL-1뿐만 아니라 경사 하강법 구현을 위해 필요한 기울기도 올 수 있습니다 이것이 우리가 forward function라고 부르는 순전파 단계와 backward function이라고 부르는 역전파 단계를 구현하는 기본 구조입니다 요약하면 레이어 L에서는 AL-1을 입력으로 받아 AL을 출력하는 순전파 함수가 있는데, 이 때 계산을 위해 WL과 BL이 필요하고 출력은 캐싱되는 ZL값입니다. 그리고 역전파 함수에서는 dAL을 입력으로 받아 dAL-1을 출력합니다 이 함수는 현재 레이어의 활성화 함수에 대한 미분 dAL이 주어졌을 때, 이전 레이어의 미분은 어떻게 되는지, 즉, AL-1을 얼마나 변화시키길 원하는지 말해줍니다. 결국 이전 레이어의 활성화 함수에 대한 미분 dAL-1을 계산하는 것이죠 이 박스 안에서도 WL과 BL이 필요합니다 그리고 따라가다보면 결국 dZL을 계산하게 됩니다 그리고 역전파 함수를 나타내는 이 박스는 dWL과 dBL도 출력합니다 그런데 전 역전파를 나타내기 위해 때때로 빨간 화살표를 사용합니다 그러니 여러분도 원하시면 이 화살표들을 빨간색으로 칠하세요 어려분이 이 두 함수를 구현할 수 있다면, 신경망의 가본 계산은 다음과 같을껍니다 입력으로 feature A0를 넣으면, 첫번째 레이어의 활성화 함수 값이 계산됩니다 이걸 A1이라 부르죠 이 계산을 위해 W1과 B1이 필요했고, Z1을 캐싱해둘껍니다 그러면 이제 이것들을 두번째 레이어에 집어넣습니다 그리고 W2와 B2를 사용하여 이 레이어의 활성화 함수 값인 A2를 계산합니다 이런 식으로 계속하다보면 결국 AL을 출력하게 되는데 이건 Y_hat과 같습니다 그리고 이 과정에서 모든 Z 값들을 캐싱해두었습니다 이게 순전파 단계입니다 이제 역전파 단계에서는 역으로 거슬러올라가면서 기울기들을 계산하게 됩니다 dAL을 집어넣으면 이 상자는 dAL-1을 주고 이런 식으로 계속하다보면 종국에는 dA2와 dA1를 얻을 수 있습니다 사실 dA0도 얻을 수 있긴 한데, 이건 입력 feature에 대한 미분이므로, 지도 신경망의 가중치 훈련에는 별 쓸모가 없습니다 그러니 여기서 멈춰도 됩니다 이 과정에서 역전파는 dWL과 dBL도 출력하게 됩니다 파라미터 WL과 BL만 사용해서요 이런 식으로 dW3와 dB3 등을 출력합니다 결국 여러분이 필요한 모든 미분값을 얻게 되는 것이죠 이 구조에 내용을 좀 더 채우면 이 상자들도 WL과 BL을 사용하게 될껍니다. 그리고 나중에 보겠지만, 이 상자 안에서 dZL도 계산됩니다 신경망 학습의 한 이터레이션은 A0인 X로 시작하여 다음과 같이 순전파 과정을 거쳐서 Y_hat을 계산합니다. 그리고 이 값을 사용하여 역전파를 합니다 이제 필요한 모든 미분값들이 있으니 각 레이어에서 W는 기존값에서 학습률과 dW의 곱을 뺀 값이 될껍니다 마찬가지로 b도 역전파를 통해 필요한 모든 미분값을 얻어 업데이트됩니다 여기까지가 신경망 경사 하강법의 한 이터레이션입니다 진도를 더 나가기 전에 구현 세부사항을 하나 더 살펴보죠 개념적으로는 여기서 캐시를 역방향 함수에서 쓰기 위해 Z 값을 저장해두는 것으로 생각하면 유용할껍니다 하지만 이전 연습 문제에서 보신 것처럼 이걸 구현할 때 캐시는 파라미터 W1과 B1을 역방향 함수에 전달하는 편리한 방법이기도 합니다 이전 연습문제에서, 실제로 캐시에 Z를 저장하고, W와 B도 저장합니다 따라서 그냥 Z2, W2, B2를 저장하는거죠 구현 관점에서 보면, 이건 파라미터들을 복사하여 나중에 역전파 계산에서 사용할 수 있도록 하는 편리한 방법입니다 이건 그냥 프로그래밍 연습문제에서 보게 될 구현 세부사항입니다 지금까지 심층 신경망을 구현할 때 필요한 기본 구성 요소에 대해 살펴보았습니다 각 레이어에는 순전파 단계가 있고 대응하는 역전파 단계가 있으며, 한 쪽에서 다른 쪽으로 정보를 복사하는 캐시가 있습니다 다음 비디오에서는 이제 이 구성 요소들을 어떻게 실제로 구현하는지 살펴보겠습니다 다음 강의를 보시죠