1
00:00:00,060 --> 00:00:04,380
В останньому відео ми описали що таке

2
00:00:02,250 --> 00:00:06,150
глибока l-шарова нейронна мережа (НМ). А також

3
00:00:04,380 --> 00:00:08,550
поговорили про позначення, які використовуються

4
00:00:06,150 --> 00:00:10,650
для опису таких НМ. В цьому відео

5
00:00:08,550 --> 00:00:13,769
ти побачиш як застосовувати пряме поширення

6
00:00:10,650 --> 00:00:16,440
в глибоких НМ. Як завжди, давай спочатку

7
00:00:13,769 --> 00:00:18,660
розглянемо як виглядає пряме поширення

8
00:00:16,440 --> 00:00:21,330
для окремого тренувального зразка x,

9
00:00:18,660 --> 00:00:22,920
а пізніше поговоримо про

10
00:00:21,330 --> 00:00:24,810
векторизовану версію, де

11
00:00:22,920 --> 00:00:26,849
будемо застосовувати пряме поширення для

12
00:00:24,810 --> 00:00:29,660
всього тренувального набору зараз.

13
00:00:26,849 --> 00:00:32,579
Маючи один тренувальний зразок x,

14
00:00:29,660 --> 00:00:34,800
ось як можна обчислити активатори

15
00:00:32,579 --> 00:00:42,329
1-ого шару. Тож для 1-ого шару

16
00:00:34,800 --> 00:00:48,239
ми обчислюємо z[1]=w[1]x+b[1].

17
00:00:42,329 --> 00:00:51,120
Тож w[1] і b[1] - це параметри,

18
00:00:48,239 --> 00:00:53,879
які впливають на активатори 1-ого шару.

19
00:00:51,120 --> 00:00:56,899
Ось 1-ий шар НМ.

20
00:00:53,879 --> 00:00:59,280
А далі ми обчислюємо

21
00:00:56,899 --> 00:01:04,979
активатори для цього шару, що

22
00:00:59,280 --> 00:01:06,810
=g(z[1]). Якою буде функція g

23
00:01:04,979 --> 00:01:09,090
залежить від того, в якому шарі ми знаходимось.

24
00:01:06,810 --> 00:01:11,010
Тож індекс тут теж повинен позначати функцію

25
00:01:09,090 --> 00:01:12,689
активації 1-ого шару. Зробивши це,

26
00:01:11,010 --> 00:01:13,320
ми обчислимо активатори

27
00:01:12,689 --> 00:01:18,360
1-ого шару.

28
00:01:13,320 --> 00:01:24,470
Як щодо 2-ого шару? Оцього шару.

29
00:01:18,360 --> 00:01:32,189
Тут ми обчислимо z[2]=w[2]a[1]+b[2].

30
00:01:24,470 --> 00:01:34,950
Тож активатором

31
00:01:32,189 --> 00:01:39,180
2-ого шару буде

32
00:01:34,950 --> 00:01:44,270
вихід 1-ого шару, тобто оця величина плюс

33
00:01:39,180 --> 00:01:49,579
вектор зсуву для 2-ого шару. Далі a[2]

34
00:01:44,270 --> 00:01:55,770
=функції активації, застосованій до z[2].

35
00:01:49,579 --> 00:01:57,990
Добре. Тож це був 2-ий шар.

36
00:01:55,770 --> 00:02:00,299
І т.д. аж до вихідного шару,

37
00:01:57,990 --> 00:02:06,240
а це - 4-ий шар, де ми

38
00:02:00,299 --> 00:02:09,959
матимемо z[4]= параметри

39
00:02:06,240 --> 00:02:11,780
шару помножені на активатори

40
00:02:09,959 --> 00:02:14,569
попереднього шару

41
00:02:11,780 --> 00:02:23,930
плюс вектор зсуву і,

42
00:02:14,569 --> 00:02:26,720
аналогічно, a[4]=g[4](z[4]).

43
00:02:23,930 --> 00:02:29,900
Ось як ми обчислюємо наше передбачення ŷ.

44
00:02:26,720 --> 00:02:35,390
Варто відмітити,

45
00:02:29,900 --> 00:02:38,270
що x тут також =a[0], тому що

46
00:02:35,390 --> 00:02:41,209
вектор вхідних ознак x є також

47
00:02:38,270 --> 00:02:44,000
активаторами 0-ого шару. Тож ми можемо закреслити

48
00:02:41,209 --> 00:02:47,000
x і написати замість нього a[0].

49
00:02:44,000 --> 00:02:48,709
Тепер, як бачимо, всі ці

50
00:02:47,000 --> 00:02:53,980
рівняння дуже схожі. Правильно?

51
00:02:48,709 --> 00:03:02,750
Тож можемо написати загальне рівняння: z[l]=

52
00:02:53,980 --> 00:03:05,750
w[l]a[l-1]+b[l];

53
00:03:02,750 --> 00:03:10,630
і тепер активатори для цього шару

54
00:03:05,750 --> 00:03:16,850
будуть функцією активації від

55
00:03:10,630 --> 00:03:20,120
значень z. Тож це - загальні рівняння прямого

56
00:03:16,850 --> 00:03:23,540
поширення. Отже, ми зробили все це

57
00:03:20,120 --> 00:03:26,299
для окремого тренувального зразка. Як

58
00:03:23,540 --> 00:03:29,660
щодо векторизованої версії

59
00:03:26,299 --> 00:03:32,720
для всього тренувального набору одночасно?

60
00:03:29,660 --> 00:03:35,030
Рівняння виглядатимуть дуже схоже.

61
00:03:32,720 --> 00:03:40,060
Для 1-ого шару матимемо

62
00:03:35,030 --> 00:03:48,410
велика Z[1]=w[1] помножене на

63
00:03:40,060 --> 00:03:54,650
велике X плюс b[1]. І A[1]=g[1](Z[1]).

64
00:03:48,410 --> 00:03:57,920
Правильно? Май на увазі, що X=A[0].

65
00:03:54,650 --> 00:03:59,959
Тобто це просто

66
00:03:57,920 --> 00:04:01,850
тренувальні зразки складені в

67
00:03:59,959 --> 00:04:05,450
стовпці. І ми можемо просто взяти

68
00:04:01,850 --> 00:04:08,269
і закреслити X і написати замість нього A[0].

69
00:04:05,450 --> 00:04:08,720
І далі для наступного шару

70
00:04:08,269 --> 00:04:16,720
матимемо щось схоже.

71
00:04:08,720 --> 00:04:21,980
Z[2]=w[2]A[1]+B[2] і

72
00:04:16,720 --> 00:04:24,530
A[2]=g[2](Z[2]).

73
00:04:21,980 --> 00:04:28,370
Ми просто беремо оці вектори z та a

74
00:04:24,530 --> 00:04:29,810
і складаємо їх по горизонталі.

75
00:04:28,370 --> 00:04:34,310
Тож ось вектор z для 1-ого тренувального зразка,

76
00:04:29,810 --> 00:04:37,310
вектор z для 2-ого тренувального зразка

77
00:04:34,310 --> 00:04:39,830
і т.д. до m-ого тренувального зразка.

78
00:04:37,310 --> 00:04:43,700
Тож складаємо їх в стовпці

79
00:04:39,830 --> 00:04:47,390
і називаємо велике Z.

80
00:04:43,700 --> 00:04:50,000
Аналогічно для великої A, як і для великої X,

81
00:04:47,390 --> 00:04:52,040
всі тренувальні зразки є векторами-стовпцями,

82
00:04:50,000 --> 00:04:53,720
складеними зліва-направо. І потім,

83
00:04:52,040 --> 00:04:59,450
знову ж, ми обчислюватимемо

84
00:04:53,720 --> 00:05:03,200
ŷ, що =g[4](z[4]), що

85
00:04:59,450 --> 00:05:04,670
в свою чергу, =A[4] і це

86
00:05:03,200 --> 00:05:08,000
будуть передбачення для всіх тренувальних

87
00:05:04,670 --> 00:05:09,980
зразків складених по горизонталі.

88
00:05:08,000 --> 00:05:12,590
Щоб узагальнити наші позначення, я

89
00:05:09,980 --> 00:05:17,720
зміню оці позначення вгорі. Тепер

90
00:05:12,590 --> 00:05:19,820
ми можемо замінити малі z та a їхніми

91
00:05:17,720 --> 00:05:22,070
великими візаві. (Хоча Z вже

92
00:05:19,820 --> 00:05:23,810
виглядає як велика). І, таким чином,

93
00:05:22,070 --> 00:05:25,790
ми отримуємо векторизовану версію прямого

94
00:05:23,810 --> 00:05:29,060
поширення, що обробляє

95
00:05:25,790 --> 00:05:32,990
одразу весь тренувальний набір, бо ж A[0] - це X.

96
00:05:29,060 --> 00:05:35,240
Тепер. Якщо ми глянемо на цю

97
00:05:32,990 --> 00:05:37,670
реалізацію векторизації, то нам здасться,

98
00:05:35,240 --> 00:05:40,370
що тут мав би бути цикл for.

99
00:05:37,670 --> 00:05:44,360
Правильно? Тут мало б бути щось типу

100
00:05:40,370 --> 00:05:47,000
"for l=1..4". Для l від 1 до

101
00:05:44,360 --> 00:05:48,950
великої L. Тож ми б мали обчислити

102
00:05:47,000 --> 00:05:51,860
активатори для 1-ого шару, потім для 2-ого,

103
00:05:48,950 --> 00:05:54,370
потім для 3-ого і потім для 4-ого шару. Тож

104
00:05:51,860 --> 00:05:56,660
тут, схоже, має бути цикл for.

105
00:05:54,370 --> 00:05:58,550
Я знаю, що коли ми реалізуємо нашу НМ,

106
00:05:56,660 --> 00:06:00,770
ми, зазвичай, хочемо позбутись

107
00:05:58,550 --> 00:06:03,290
явних циклів for. Але це - єдине місце,

108
00:06:00,770 --> 00:06:05,060
де я не думаю, що є якийсь спосіб

109
00:06:03,290 --> 00:06:06,590
реалізувати це без явного

110
00:06:05,060 --> 00:06:09,080
циклу for. Тож, коли ми реалізуємо пряме

111
00:06:06,590 --> 00:06:10,700
поширення, це абсолютно нормально мати

112
00:06:09,080 --> 00:06:12,740
цикл for для обчислення активаторів

113
00:06:10,700 --> 00:06:15,050
1-ого шару, потім 2-ого, потім 3-ого

114
00:06:12,740 --> 00:06:17,210
і потім 4-ого шару. Ніхто не знає

115
00:06:15,050 --> 00:06:19,970
і я не думаю, що є хоч якийсь спосіб

116
00:06:17,210 --> 00:06:23,060
зробити це без циклу for, який йтиме

117
00:06:19,970 --> 00:06:24,620
від 1 до великої L, тобто від 1 до

118
00:06:23,060 --> 00:06:27,830
загальної кількості шарів в нашій НМ.

119
00:06:24,620 --> 00:06:30,980
Тож в цьому місці абсолютно нормально

120
00:06:27,830 --> 00:06:32,690
мати явний цикл for.

121
00:06:30,980 --> 00:06:35,300
Тож це все щодо позначень глибоких НМ

122
00:06:32,690 --> 00:06:37,760
і щодо того, як реалізувати пряме

123
00:06:35,300 --> 00:06:39,680
поширення в цих НМ. Якщо

124
00:06:37,760 --> 00:06:41,900
рівняння, які ми щойно бачили, здаються дещо

125
00:06:39,680 --> 00:06:44,000
знайомими, то це тому, що

126
00:06:41,900 --> 00:06:45,830
ми брали рівняння дуже

127
00:06:44,000 --> 00:06:47,750
схожі на ті, що ми бачили в НМ

128
00:06:45,830 --> 00:06:50,750
з одним прихованим

129
00:06:47,750 --> 00:06:53,420
шаром і просто повторювали їх більше разів.

130
00:06:50,750 --> 00:06:55,420
Тож, як виявляється, ми реалізували глибоку НМ.

131
00:06:53,420 --> 00:06:57,860
Одним зі способів

132
00:06:55,420 --> 00:06:59,450
збільшити шанси здійснити

133
00:06:57,860 --> 00:07:01,580
реалізацію без помилок - це обдумати (дуже

134
00:06:59,450 --> 00:07:03,500
системно і обережно)

135
00:07:01,580 --> 00:07:05,300
розміри матриць, з якими ми працюємо.

136
00:07:03,500 --> 00:07:07,280
Коли я намагаюсь виправити помилки в своєму коді,

137
00:07:05,300 --> 00:07:08,960
я часто беру аркуш паперу і просто

138
00:07:07,280 --> 00:07:11,480
ретельно записую

139
00:07:08,960 --> 00:07:13,940
розміри матриць, з якими працюю.

140
00:07:11,480 --> 00:07:16,570
Давай подивимось як ми можемо це робити

141
00:07:13,940 --> 00:07:16,570
в наступному відео.