У попередньому відео ми бачили як можна використати векторизацію, 
щоб обчислити передбачення - малі a - для всього тренувального набору одночасно. В цьому відео ми розглянемо як можна використати векторизацію, щоб обчислити градієнти для всіх m тренувальних зразків. І, знову ж, одночасно. І потім в кінці відео ми об'єднаємо ці обчислення і покажемо як можна записати дуже ефективну реалізацію логістичної регресії (ЛР). Тож, ти можливо пам'ятаєш, що для обчислення градієнтів ми обчислювали dz⁽¹⁾ для першого зразка, який =a⁽¹⁾-y⁽¹⁾, потім dz⁽²⁾= a⁽²⁾-y⁽²⁾ і т.д. І т.д. для всіх m тренувальних зразків. Тож, давай визначимо нову змінну dZ, яка буде = dz⁽¹⁾, dz⁽²⁾, dz⁽ᵐ⁾. Знову ж, всі змінні d малі z складаємо горизонтально. Тож це буде матриця розміру 1*m або вектор-рядок розмірністю m. Тепер згадаємо з попереднього слайду. Ми вже знайшли як обчислити велике A, а саме: від a⁽¹⁾ до a⁽ᵐ⁾. Ми визначили велике Y від y⁽¹⁾ до y⁽ᵐ⁾. Також складене горизонтально. Тож, базуючись на цих визначеннях, можливо, ти вже побачив/ла, що dZ може бути обчислене як A-Y. Тому що це буде a⁽¹⁾-y⁽¹⁾ (тобто перший елемент), a⁽²⁾-y⁽²⁾ (тобто другий елемент) і т.д. Тож цей перший елемент a⁽¹⁾-y⁽¹⁾ якраз буде визначенням dz⁽¹⁾, другий елемент - якраз визначення dz⁽²⁾ і т.д. Тож лише одним рядком коду можна обчислити все оце одночасно. В попередній реалізації ми позбулись одного циклу for, проте, все ще маємо оцей другий цикл for для тренувальних зразків. Тож ми ініціалізуємо dw в 0, у вектор нулів. Але все ще маємо цикл для тренувальних зразків, де в нас є dw+=x⁽¹⁾dz⁽¹⁾ для першого тренувального зразка, dw+=x⁽²⁾dz⁽²⁾ і т.д. Тож ми робимо це m разів, а потім ділимо dw на m. І, аналогічно, для b, так? db ініціалізувалось в 0, db+=dz⁽¹⁾, db+=dz⁽²⁾ і до dz⁽ᵐ⁾ і db ділиться на m. Ось це ми мали в попередній реалізації. Ми вже позбулись одного циклу for. Тож, принаймі зараз, dw є вектором і ми не будемо окремо коригувати dw⁽¹⁾, dw⁽²⁾ і т.д. Тож ми вже позбулись цього, проте, все ще маємо цикл for для m зразків тренувального набору. Тож давай векторизуємо ці операції. Ось що ми можемо зробити. Для реалізації векторизації db робиться додавання всіх оцих dz з подальшим діленням на m. Тож, db, по суті, - 1/m, сума, по i від 1 до m, dz⁽ⁱ⁾, де всі dz - це ось цей вектор-рядок. Тож в Python реалізація виглядатиме наступним чином: 1/m*np. sum(dz) Тож треба просто взяти цю змінну і викликати функцію np.sum() для неї і вона дасть нам db. А що з dw? Я просто запишу правильне рівняння, яке вказує що робити. dw буде 1/m помножити на матрицю X, помножити на dzᵀ. Спробуємо зрозуміти чому це так. це = 1/m, потім матриця X - стовпчики від x⁽¹⁾ до x⁽ᵐ⁾ - і dzᵀ буде ось таким від dz⁽¹⁾ до dz⁽ᵐ⁾. І, якщо вивести що буде добутком цієї матриці і цього вектора, то це буде 1/m помножити на [x⁽¹⁾dz⁽¹⁾ плюс крапка-крапка-крапка плюс x⁽ᵐ⁾dz⁽ᵐ⁾]. Тож цей вектор розміру n*1 і буде результатом обчислень dw, тому що dw брало оці x⁽ⁱ⁾dz⁽ⁱ⁾, додавало їх і саме це і робить оцей добуток матриці на вектор. І, знову ж, ми можемо обчислити dw одним рядком коду. Тож, ось це - векторизована реалізація обчислення похідних. Ми використовуємо цей рядок, щоб реалізувати db і оцей - щоб реалізувати dw. 
І, відміть, що без циклу for для тренувального набору ми тепер можемо обчислити коригування для наших параметрів. Тож тепер давай запишемо повністю як реалізувати ЛР. Ось наш первинна, дуже неефективна, невекторизована реалізація. Тож, перше, що ми зробили в попередньому відео, 
то це позбулись ось цього циклу for, так? Тож, замість того, щоб проходити циклом по dw₁, dw₂ і т.д., ми підставили тут вектор dw, що =dw+=x⁽ⁱ⁾, яке тепер є вектором, помножити на dz⁽ⁱ⁾. А тепер ми побачимо, що можемо позбутись не лише циклу for внизу, а й ось цього циклу for. Тож ось як це можна зробити. Тож, скориставшись попереднім слайдом, можемо сказати, що велике Z =wᵀX+b, а в коді буде Z=np. dot(w.T, X)+b. І a=σ(Z). Тож тепер ми обчислили оце все і оце все для всіх значень i. Далі. На попередньому слайді ми сказали, що обчислюватимемо dZ як велике A мінус велике Y. Тож тепер ми обчислюємо оце все для всіх значень i. І, нарешті, dw=1/m*X* *dzᵀ, а db=1/m* *np.sum(dz). Тож ми щойно зробили пряме і зворотне поширення, тобто обчислили передбачення і похідні для всіх m тренувальних зразків без використання циклу for. І, отже, коригування Градієнтного спуску буде w=w-α*dw, яке ми щойно вгорі обчислили, і b=b-α*db. Інколи тут ставлять двокрапку, щоб позначити, що це - присвоєння, але я не повністю дотримувався цих позначень. Проте, в будь-якому випадку, ми щойно реалізували одну ітерацію Градієнтного спуску для ЛР. Тепер. Я знаю, що сказав, що нам треба позбавлятись явних циклів for якщо це можливо, проте, якщо ми хочемо реалізувати багато ітерацій Градієнтного спуску, 
то нам, все-таки, потрібен цикл for для кількості ітерацій. Тож, якщо ми хочемо виконати 1000 ітерацій Градієнтного спуску, то нам потрібен цикл for для цієї кількості ітерацій. Це - такий собі крайній цикл for. Я не думаю, що є спосіб позбутися цього циклу for. Проте, я думаю, що буде надзвичайно цікаво, 
якщо ти зможеш реалізувати хоча б одну ітерацію Градієнтного спуску без використання циклу for. Тож, це все. Тепер ти маєш високовекторизовану і високоефективну реалізацію Градієнтного спуску для ЛР. Є ще одна деталь, про яку я хотів би поговорити в наступному відео. Вона була в цьому відео, я коротко про неї згадав, 
вона зветься посів. Посів являється методикою Python і NumPy. 
Він дозволяє зробити окремі частини коду набагато ефективнішими. Тож давай детальніше розглянемо посів в наступному відео.