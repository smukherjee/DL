欢迎大家来学习第四周的课程 在之前的课程中 我们学习了单隐藏层神经网络中的 前向传播和反向传播 逻辑回归 我们还学习了向量化 也了解了对权重参数进行随机初始化的重要性 如果你完成了前几周的作业 那你应该已经 实现并亲眼看到了某些理念实现后的效果 迄今为止 我们已经学过实现深度神经网络需要的大部分知识 本周的任务是 把这些理念组合起来 并利用它们实现你自己的深度神经网络 完成本周的习题所需要的时间会比较长 可能要多花一点心思 我会压缩 本周的视频课程 让你观看视频的时间稍微短一些 这样你就能有更多的时间去完成编程作业 我希望这个作业可以加深你对神经网络的理解<br />并能得到一个让你引以为豪的深度学习模型 到底什么是深度神经网络呢? 我们已经见过这个逻辑回归的结构图了 也已经学习过单隐藏层的神经网络结构 先来看一个例子 这里有一个双隐层的神经网络模型 还有一个含有5个隐藏层的神经网络模型 我们一般会说逻辑回归 是非常“浅”的模型 而右下角这个模型的层数更深 浅和深 只是程度的区别 这是单隐藏层的神经网络 也就是2层神经网络 要记住 当我们计算神经网络的层数时 不包括输入层 我们只计算隐藏层和输出层 所以这是2层神经网络 它仍然比较浅 但比逻辑回归要深一点 从技术上来说 逻辑回归是单层神经网络 但是过去几年 在人工智能领域 机器学习社区 认识到有一些函数 只有很深的神经网络才能学习<br />而比较浅的网络模型无法做到这点 虽然对于任何特定的问题来说 可能很难事先得知 你需要多深的网络 所以一般我们会先尝试逻辑回归 然后再尝试 一个 两个隐藏层 可以把隐藏层的数量作为 另一个超参数 你可以尝试很多不同的值 然后通过交叉验证或者 开发集进行评估 之后我们会深入讨论 现在我们来看看 描述深度网络使用的符号 这是一 二 三 四 四层神经网络 有三个隐藏层 隐藏层中 的单元数量是5 5 3 然后有一个输出单元 我们使用大写的L来表示 神经网络中的层数 这里 L = 4 即层数为4 我们用n上标小写l来表示 第l层上的单元数 让我们来标上序号 输入层是第0层 这是第1层 这是第2层 这是第3层 这是第4层 然后我们可以得出 n[1] 的标记是这样的 第一个隐藏层这儿 n[1]=5 因为在这儿有5个隐藏单元 对于这个 我们可以写成n[2] 代表第二个隐藏层中的单元数量 也等于5 n[3] = 3 n[4] = n[L] 因为大写的L等于4 输出层的单元数为1 我们再来看看输入层 输入层n[0] = nx = 3 这就是我们用来描述不同层中单元数 的符号 对于每一层l 我们还会使用 a[l]代表第l层中的激活函数 之后我们会看到 在前向传播的场景中 你最后要计算的 激活函数g(z[l])的结果a[l] 我们也可以用层数l来对激活函数进行标注 另外 我们用W[l]来表示 计算第l层的中间值z[l]时使用的权重 类似的 b[l]表示计算z[l]时用到的偏置参数 最后 我们来总结一下所使用的符号 我们把输入特征称为x 而x同时也是第零层的激活函数 所以a[0] = x 最后一层的激活函数 a[L] = y帽 所以a[L]等于神经网络的预测输出<br />也就是等于预测值y帽 现在我们已经知道了 神经网络看起来是什么样的 以及在构建深度神经网络时会使用的各种符号 我知道在本节课中我们已经 介绍了很多这样的符号 如果你不经意间忘记了某个符号的含义
不用担心 我们在课程网站上公布了符号列表 你可以在那儿找到这些符号的定义 后面我会跟大家讲讲 在这种类型的网络中 前向传播究竟是什么样的 我们下节课见
翻译 | 审阅：Cousera Global Translator Community