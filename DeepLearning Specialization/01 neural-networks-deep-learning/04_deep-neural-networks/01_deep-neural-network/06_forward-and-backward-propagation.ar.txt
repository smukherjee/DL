في الفيديو السابق رأيتم اللبنات الأساسية لتنفيذ الشبكة العصبية العميقة. ورأينا خطوة الانتشار الأمامي لكل طبقة وخطوة الانتشار الخلفي المقابلة لها. دعونا نرى كيف يمكن أن نقوم بتنفيذ
هذه الخطوات بشكل فعلي. سوف نبدأ بالانتشار الأمامي. تذكروا أن ما سيؤدي ذلك إلى إنتاجه هو
المدخلات [l-1] والمخرجات a[l] والذاكرة الوسيطة z[l]. كما قلنا للتو أنه من وجهة نظر التنفيذ ربما حيث تكون الذاكرة الوسيطة w[l] وb[l] كذلك فقط لتسهيل ظهور الدوال في التدريب الخاص بالمشكلة. وبالتالي، فإن الدوال المتعلقة بذلك يجب أن تكون معروفة لكم. طريقة تنفيذ دالة أمامية تكون يساوي
w[l] * a[l-1] + b[l] ثم a[l] يساوي دالة إلغاء
التنشيط المطبقة على "z". وإذا كنت ترغب في توجيه التنفيذ يكون فقط من خلال هذا عدة مرات a[l-1] + b مع إضافة "b"، التي تشير إلى الإفراط في البث وa[l] = g المطبقة بشكل يعتمد
على العناصر على "z". وكما تذكرون، في مخطط الخطوة الأمامية تذكرون أننا كان لدينا هذه السلسلة من المربعات
التقدمية نحو الأمام ويمكن تهيئة ذلك من خلال إدخال a[0] والتي تساوي "X". وبالتالي، تقوم بتهيئة ذلك. في الواقع، ما المدخلات الخاصة بالشيء الأول؟ في الواقع، تكون المدخلات a[0]
وهي سمات المدخلات لمثال تدريب واحد إذا كنت تقوم بعمل مثال واحد في كل مرة أو A[0]، لمجموعة التدريب برمتها إذا كنت تقوم بمعالجة مجموعة التدريب برمتها. إذًا، هذه هي مدخلات أول أربع دوال في السلسلة ومجرد تكرار ذلك يسمح لك بحساب الانتشار الأمامي من اليسار إلى اليمين. بعد ذلك، سوف نتحدث عن خطوة الانتشار الخلفي. هنا، هدفك إدخال da[l] وإخراج da[l-1] وdW[l] وdb. دعوني أشير إلى الخطوات التي تحتاج إليها لحساب
هذه الأشياء: dz[l] = da[l] منتج يعتمد على العنصر مع g[l]` z[l] ثم، لحساب المشتقات dW[l] = dz[l] * a[l - 1]. لم أضع ذلك بشكل صريح في
الذاكرة الوسيطة، إلا أنه يتضح أنك تحتاج إلى ذلك كذلك. وبعد ذلك، db[l] = dz[l]، وفي النهاية da[l-1] = w[l]_نقل في dz[l]، حسنًا؟ ولا أريد الخوض في المشتقات التفصيلية لذلك لكن، يتضح أنك إذا أخذت هذا
التعريف لـ da ووضعته هنا تحصل على نفس الصيغة التي
كانت لدينا الأسبوع الماضي حيال كيفية حساب dz[l] كدالة
لقيمة dz[l] السابقة في الواقع، حسنًا إذا وضعت ذلك هنا ينتهي الأمر بـ dz[l] = w[l+1]_نقل
dz[l+1] * g[l]` z[l]. أنا أعلم أن هذا يبدو مثل الجبر كثيرًا يمكنك التحقق بنفسك من أن هذه هي نفس المعادلة التي كتبناها للانتشار الخلفي في الأسبوع الماضي عندما كنا نقوم بعمل الشبكات العصبية
بطبقة مخفية واحدة فقط. وأود أن أذكركم أنه في هذه المرة
هذا المنتج يعتمد على العناصر وبالتالي كل ما تحتاج إليه هو هذه المعادلات الأربعة
لتنفيذ الدالة الخلفية. وفي النهاية، سوف أكتب نسخة موجهة. وبالتالي يكون أول سطر dz[l] = dA[l] منتج يعتمد على العناصر مع g[l]` of z[l]. وربما لا تكون هناك أي مفاجآت هنا. فـ dW[l] تصبح 1/m, dz[l] * a[l-1]_نقل، ثم db[l] تصبح 1/m np.sum dz[l] ثم، axis = 1 وkeepdims = true. لقد تحدثنا عن استخدام np.sum
في الأسبوع الماضي لحساب db. وفي النهاية dA[l-1] تكون W[l]_نقل * dz[l]. إذًا، يسمح لك ذلك بإدخال هذه الكمية، da، هنا وإخراج dW[l], db[l] المشتقات التي تحتاج إليها وكذلك dA[l-1]، حسنًا؟ كما يلي. إذًا تلك هي طريقة تنفيذ الدالة الخلفية. حسنًا، لكي نلخص ذلك قم بأخذ المدخلات "X" يمكن أن يكون لديك أول طبقة وربما لديك دالة تنشيط ReLU. ثم انتقل إلى الطبقة الثانية ربما تستخدم دالة تنشيط ReLU أخرى وتنتقل إلى الطبقة الثالثة وربما يكون لديك دالة تنشيط Sigmoid إذا كنت
تقوم بإجراء التصنيف الثنائي ويؤدي ذلك إلى إنتاج y_hat. بعد ذلك، باستخدام y_hat يمكنك حساب الخسائر ويسمح لك ذلك ببدء التكرار الخلفي. سوف أرسم الأسهم أولاً، حسنًا؟ حتى لا أحتاج إلى تغيير الأقلام كثيرًا. سيقوم الانتشار الخلفي بحساب المشتقات، وحساب dW[3] وdb[3] وdW[2]
وdb[2] وdW[1] وdb[1] وطوال الطريق، سوف تقوم بالحساب وعلى ما أعتقد، يمكن أن تقوم الذاكرة الوسيطة بنقل
z[1] وz[2] وz[3] وهنا، توقف da[2] وda[1] الخلفية مؤقتًا. يمكن أن يؤدي ذلك لحساب da[0] إلا أننا لا نريد استخدام ذلك. لذا، يمكنك تجاهل ذلك، حسنًا؟ وبالتالي، تلك هي طريقة حساب
الانتشار الأمامي والانتشار الخلفي لشبكة عصبية مكونة من ثلاث طبقات. الآن، هناك تفصيل أخير لم أتحدث عنه وهو أنه بالنسبة للتكرار الأمامي سوف نقوم بالبدء فيه باستخدام بيانات المدخلات "X". وماذا عن التكرار الخلفي؟ حسنًا، يتضح أن da[l] عندما تستخدم التراجع اللوجيستي أثناء إجراء التصنيف الثنائي يساوي y/a + 1-y/1-a. لذا، يتضح أن مشتقة دالة الخسارة فيما يتعلق بالمخرجات وفيما يتعلق بـ y_hat،
يمكن أن يظهر على ما هو عليه. إذا كنت على دراية بحساب التفاضل والتكامل إذا بحثت عن دالة الخسارة "l" وأخذت المُشتقات، فيما يتعلق بـ y_hat أو a يتضح أنك تحصل على هذه الصيغة. إذًا، هذه هي الصيغة التي يجب أن تستخدمها
لـ da للطبقة النهائية حرف "L" كابيتال. وبالقطع، إذا كنت ترغب في إجراء التنفيذ الموجه تقوم ببدء تكرار خلفي ليس من خلال ذلك، ولكن باستخدام da للطبقة l ويسري نفس الأمر على الأمثلة المختلفة عبر "a"، لمثال التدريب الأول، + 1-y لمثال التدريب الأول عبر 1-a، لمثال التدريب الأول ...حتى نهاية مثال التدريب، حيث 1-a[m]. إذًا، تلك هي طريقة تنفيذ النسخة الموجهة. تلك هي طريقة تهيئة النسخة
الموجهة من الانتشار الخلفي. إذًا، لقد رأيت الآن اللبنات الأساسية للانتشار الأمامي وكذلك الانتشار الخلفي. والآن، إذا قمنا بتنفيذ هذه المعادلات فسوف يتم تنفيذ الانتشار الأمامي والانتشار الخلفي بشكل صحيح للحصول
على المشتقات التي تريدها. وربما يراود ذهنك أنه، لأننا لدينا الكثير من المعادلات تشعر بالارتباك قليلاً، وتشعر أنك لست متأكدًا تمامًا
من أنك ترى كيف يعمل ذلك. إذا كنت تشعر بذلك، فإن نصيحتي لك أنك عندما تصل إلى مهمة البرمجة الخاصة بهذا الأسبوع سوف تتمكن من تنفيذ ذلك بنفسك وستكون الأشياء ملموسة بشكل أكبر. وأنا أدرك أن هناك الكثير من المعادلات وربما بعض المعادلات لا يمكن فهمها منطقيًا
بشكل كامل لكن، إذا كنت تتعامل مع حساب التفاضل والتكامل والجبر الخطي، وهو أمر ليس سهلاً يمكن أن تبدأ في التجربة إلا أن هذا من أصعب الاشتقاقات في التعلم الآلي. ويتضح أن المعادلات تتدفق أو مجرد معادلات التفاضل والتكامل لحساب المشتقات
خصوصًا في الانتشار الخلفي. لكن، مرة أخرى، إذا كان
ذلك يبدو أمرًا تجريديًا بشدة وغامضًا بعض الشيء بالنسبة لك فإني أقول لك إنه بعد
الانتهاء من التدريب الرئيسي فإنك ستشعر أنها ملموسة
بشكل أفضل بالنسبة لك. رغم أنني مجبر على أن أقول الآن أنه عند تنفيذ خوارزمية
تعلم، في بعض الأحيان فإنه حتى أنا أشعر بالاندهاش عندما يؤتي تنفيذ خوارزميات التعلم الخاصة بي ثماره
وهذا يرجع إلى أن الكثير من التعقيد في التعلم الآلي يأتي
من البيانات وليس من سطور التعليمات البرمجية. وبالتالي، في بعض الأحيان، تشعر بأنك تقوم بتنفيذ عدد قليل من سطور
التعليمات البرمجية. ولا تكون متأكدًا تمامًا من نتيجتها إلا أنها تعمل بشكل سحري لأن القدر الأكبر من السحر لا يكمن في
التعليمات البرمجية التي تكتبها والتي غالبًا لا تكون طويلة. إنها ليست بسيطة تمامًا لكنها لا تكون عشرات الآلاف ولا مئات الآلاف من سطور التعليمات البرمجية لكنك تقوم بإدخال قدر كبير من البيانات التي ما زالت رغم أنني عملت لفترة طويلة في مجال التعلم الآلي تدهشني إلى حد ما عندما تؤتي خوارزميات التعلم الخاصة بي ثمارها لأن قدر
كبير من تعقيد خوارزميات التعلم الخاصة بك يأتي من البيانات وليس بالضرورة من كتابة الآلاف والآلاف من سطور
التعليمات البرمجية. حسنًا. إذًا تلك هي طريقة تنفيذ الشبكات العصبية العميقة. ومرة أخرى، سوف يصبح ذلك ملموسًا بشكل أفضل
لك عندما تنتهي من التدريب الرئيسي لك. وقبل المتابعة، في الفيديو التالي أريد مناقشة المتغيرات المفرطة والمتغيرات. فقد اتضح أنك عندما تقوم بتدريب الشبكات العميقة فإن التمكن من ترتيب المتغيرات
المفرطة لديك بشكل جيد يساعدك على أن تكون أكثر فاعلية في تطوير
الشبكات الخاصة بك. في الفيديو التالي، سوف نتحدث
عن ماهية ذلك بدقة.