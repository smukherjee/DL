Pekala, zannediyorum ki bu, heyecan verici bir video olacak.Bu videoda, gradyan düşümü (gradient descent) algoritmasının tek saklı katmanlı sinirsel ağınıza (neural network) nasıl uygulanacağını göreceksiniz. Bu videoda, size sadece ihtiyacınız olan uygulanacak denklemleri vereceğim ki gradyan düşümünün geri yayılımı (back propagation) çalışabilsin. Ve sonra bir sonraki videoda, neden özellikle bu denklemlerin sinirsel ağınız için eğimi hesaplamada ihtiyacınız olan en uygun yada en doğru denklemler olduğuyla ilgili bilgi vereceğim. Evet. Şimdilik sadece bir katmanı olan basit sinirsel ağınızın W1, V1, W2 VE B2 değişkenleri olacak ve hatırlatma olarak, eğer ki nx yada bunun yerine n0 girdi değişkeniniz varsa ve bir saklı biriminiz ve n2 çıkış birimleri varsa 
mesela bizim örneğimizde şimdilik n2 ikiye eşit. Böylece W1 matriksi, (n1, n0) olacak. b1, n1 boyutunda bir vektör olacak.
 Bu yüzden onu (n1, 1) boyutunda bir matriks olarak yazabiliriz, aslında bir sütun vektörü. W2'nin boyutları (n2, n1) ve B2'nin boyutları (n2,1) olacak. Evet, şimdiye kadar yalnızca n2'nin 1'e eşit olduğu örnekler gördük ki burada yalnızca 1 saklı biriminiz vardı. Evet, aynı zamanda bir de sinirsel ağlar için maaliyet fonksiyonunuz (cost function) var. Şimdilik, sizin ikili sınıflandırma yaptığınızı farzedeceğim. Yani bu durumda maaliyet fonksiyonunuzun değişkenleri şöyle olacak: 
1 bölü m çarpı yitim fonksiyonunun (loss function) ortalaması. Ve buradaki L, 
sinirsel ağınızı ý 'yi doğru tahmin ederkenki yitimi (loss) oluyor ki bu aslında a2'dir eğer taban ý olarak sınıflandırılmışsa. Eğer ikili sınıflandırma yapıyorsanız, yitim fonksiyonu önceden kullandığınız lojistik regresyonunki (logistic regression) ile aynısı olabilir.
Evet, Algoritmalarınızın değişkenlerini eğitmek için gradyan düşümünü uygulamalısınız. Sinirsel ağınızı eğitirken değişkenlerin başlangıç değerlerini hepsini sıfır yapmaktansa rastegele atamak önemlidir. Daha sonra neden böyle olduğunu göreceğiz. Ancak başlangıç değerlerini herhangi bir değere atadıktan sonra, gradyan düşümünün her bir döngüsü tahminleri hesaplıyor olacak. Yani, aslında i 1'den m'e giderken ý(i)'ı hesaplıyorsunuz. Daha sonra türevi hesaplamanız gerekiyor bu yüzden dw(1)'ı hesaplamanız lazım ve bu da görüyoruzki yitim fonksiyonunun türevi w(1)' göre alınıyor. 
Başka bir değer daha hesaplamanız gerekiyor ki buna db(1) diyeceğiz ki o da yitim fonksiyonunun eğiminmin b(1) değerine göre alınmış türevidir vesayire. Aynı şekilde, diğer değişkenler de: w(2) ve b(2).
Ve sonunda, gradyan düşümünün güncellemesi w(1)'in w(1) eksi alfa öğrenme oranı (learning rate) çarpı dw(1) ile güncellenmesi, b1'in b1 eksi öğrenme oranı çarpı db(1) olarak güncellenmesidir.
Aynı w2 ve b2 için de olduğu gibi. Bazen iki nokta üstüste bazen de eşittir kullanıyorum, iki sembol de doğru sayılıyor. Evet, bu gradyan düşümünün bir döngüsü oluyor ve bunu
birkaç kez tekrarlıyorsunuz ta ki değişkenleriniz bir noktaya yöneliyormuş gibi görünene kadar. Geçmiş videolarda, nasıl tahminlerin hesaplandığından, nasıl çıktıların hesaplandığından bahsettik ve bunu nasıl vektörlerle yapabileceğimizi de gördük. Önemli olan nokta nasıl bu kısmi türevlerin hesaplandığını bilmek, dw(1)'in, db(1)'in ve aynı zamanda dw(2) ve db(2)'nin. Evet, yapmak istediğim şey size yalnızca türev hesaplamak için ihtiyacınız olacak denklemleri vermek. Ve bir sonraki videoya erteleyeceğim ki bu zorunlu olmayan bir video bu formülleri nasıl bulduğumuz hakkında Jeff'e geri döneceğiz. Evet, o zaman tekrar ileri yayılımın (forward propagation) denklemlerini özet geçelim. Elinizde z(1) eşittir w(1)x artı b(1) var ve a(1) eşittir 
bu katmandaki aktivasyon fonksiyonuna z(1)'in uygulanması, ve z(2) eşittir w(2) A(1) artı b(2), ve son olarak -tüm bunlar eğitme setinizde (training set) vektörize edilmiş şekilde var- a(2) eşittir g(2) fonksiyonunda z(2). Yine dediğim gibi, eğer ikili sınıflandırma yaptığınızı varsayarsak,
bu aktivasyon fonksiyonu aslında sigmoid fonksiyonu olmalıydı bunu şimdilik buraya bırakıyorum. Evet, bu "ileri yayılım" yada soldan sağa ilerleyen hesaplama sinirsel ağınız için. Şimdi, haydi sinirsel ağınız için türevleri hesaplayalım. Evet, bu geri yayılım adımı. Daha sonra
dz(2) eşittir A(2) eksi gerçek y değeri eşittir A(2) eksi gerçek y değeri ve hatırlatma olarak söylüyorum ki örnektekilerin hepsi vektörleştirilmiş.
Evet, Y m tane örneğinizin yatay olarak listelendiği
1'e m boyutunda bir matris. Böylece anlaşılıyorki dw(2) buna eşit. Aslına bakılırsa Bu ilk üç denklem Bu ilk üç denklem lojistik lojistik regresonda gradyan düşümüne çok benzer. çok benzer. x eşittir 1 virgül keepdims eşittir Doğru. Küçük bir ayrıntı, np.sum bir python numpy komutu. bir python numpy komutu. Bir matriste tek boyutta komut vermek için.
Mesela bu örnekte, yatay yönde toplama yapmak. yatay yönde toplama yapmak. ve keepdims'in yaptığı ise python'ın bazı komik bir dereceli diziler sonuç vermesine engel olmak 
ki burada boyutlar olmak ki burada boyutlar bildiğiniz gibi (n, ). Dizilişi doğru şekilde tutarak python'ın şekilde tutarak python'ın db(2)'yu (n,1) boyutunda bir vektör olarak çıkartması
garantileniyor. Teknik olarak bu sanırım (n(2), 1). Bu durumda (n(2), 1) bu durumda bu sadece 1'e 1'lik bir sayı ve belkide önemli değil.
Ama daha sonra Ama daha sonra ne zaman ne zaman önemli olacağını göreceğiz. 
Evet, şimdiye kadar yaptıklarımız lojistik regresyona çok benziyor. Ama şimdi, benziyor. Ama şimdi, geri yayılımı çalıştırmaya devam ettikçe, şunu
hesaplayacaksınız. dz(2) çarpı z(1)'e göre g(1)'in türevi dz(2) çarpı z(1)'e göre g(1)'in türevi Buradaki g(1)', saklı katmanınızda hangi aktivasyon fonsksiyonu hangi aktivasyon fonsksiyonu kullanılıyorsanız onun türevi. Ve çıkış katmanı için Ve çıkış katmanı için tahminimce
sigmoid fonksiyonu ile ikili sınıflandırma yapıyorsunuz. ikili sınıflandırma yapıyorsunuz. ki bu zaten dz(2) için yazılan formülle uygulandı ve bu sefer, bu her bir elemana göre çarpma işlemi.
Evet, burası çarpma işlemi.
Evet, burası n(1)'e m boyutunda bir matris ve burası bu elemana göre türev de n(1)'e m boyutunda bir matris olacak.
Bu sefer, burası, elemana göre iki burası, elemana göre iki matrisin çarpı. Son olarak, dw(1) buna eşit ve
db(1) şuna eşit ve eşit ve
db(1) şuna eşit ve np.sum(dz(1), axis=1, keepdims = True).
Evet, keepdims = True).
Evet, bundan önce, n2 bire eşit olduğunda keepdims parametresi daha az önemliydi. Yani aslında 1'e 1'lik şey bir gerçek sayı Burada, db(1) n(1)'e 1'lik bir dizi (vektör) yani, python np.sum fonksiyonunun bu boyutlarda bir çıktı vermesini istersiniz.
Bu boyutlarda ilerideki hesaplamalarınızı bozacak saçma bir dizin vermesini değil. Başka bir yol, keepdims parametresini kullanmadan doğrudan reshape fonksiyonunu çağırmak böylece np.sum'ın sonucunu bu boyutlara yeniden boyutlandırmak için. Evet bu, 4 denklemde ileri yayılım
(forward propagation) ve 6 denklemde geri yayılım (back propagation). 
Sadece denklemleri yazdığımın farkındayım ama bir sonraki zorunlu olmayan videoda, geri yayılımın 6 denkleminde nasıl türev alındığının iyice üzerinden geçeriz. 
İzleyip izlememekte serbest olduğunuzu olduğunuzu bilin ama iki türlü de eğer bu iki algoritmayı uygularsanız, ileri ve geri yayılımın
uygulamalarına sahip olmuş olacaksınız ve sinirsel ağınızdaki parametreleri öğrenmek için ihtiyacınız olan gradyan düşümününün türevlerini hesaplayabileceksiniz. Bu algoritmayı uygulamak ve kavramak yüksek matematiği derinden anlamaya gerek kalmadan mümkün. Birçok başarılı derin öğrenme uygulayacısı böyle yapıyor. Ama eğer isterseniz bir sonraki videoyu izleyebilirsiniz. Sadece bu denklemlerin türevleri hakkında daha fazla anlayışınız/bilginiz olabilsin diye. olabilsin diye.