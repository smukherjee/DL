1
00:00:00,000 --> 00:00:01,530
이전 강의에서 여러분에게

2
00:00:01,530 --> 00:00:06,885
이전 비디오에서는, x 매트릭스에서 트레이닝 샘플이 가로로 쌓아져서, 

3
00:00:06,885 --> 00:00:11,158
신경망으로 이어지는 벡터화된 도입값을 알아낼 수 있었는데요, 

4
00:00:11,158 --> 00:00:14,760
이제 조금 더 구체적으로 저희가 이용한 공식이

5
00:00:14,760 --> 00:00:19,775
어떻게 복수의 샘플에 대해 올바른 벡터화된 도입값을 주는지 그 타당성을 보겠습니다. 

6
00:00:19,775 --> 00:00:25,590
그러면 전 방향전파의 예제를 한번 살펴보겠습니다.

7
00:00:25,590 --> 00:00:27,645
첫번째 트레이닝 샘플에 대해서, 

8
00:00:27,645 --> 00:00:29,130
여러분이 이 값

9
00:00:29,130 --> 00:00:38,970
x1 더하기 b1 그리고 두번째 트레이닝 샘플에서, 

10
00:00:38,970 --> 00:00:49,310
이 값, x2 더하기 b2, 

11
00:00:49,310 --> 00:00:50,900
그리고 세번째 트레이닝 샘플은, 

12
00:00:50,900 --> 00:00:56,064
여기 3 더하기 b1이 됩니다.

13
00:00:56,064 --> 00:01:00,930
이 슬라이드 내용을 간략하기 위해 일단 b는 무시할 것입니다.

14
00:01:00,930 --> 00:01:08,395
공식을 조금 더 심플하게 하기 위해서, b를 0으로 만들 것입니다.

15
00:01:08,395 --> 00:01:11,140
저희가 주장하는 바는, 이런 작은 

16
00:01:11,140 --> 00:01:14,320
변화에도 성립한다는 것입니다. b가 0이 아닌 경우에도 말이죠. 

17
00:01:14,320 --> 00:01:17,610
이렇게 슬라이드 내용을 심플하게 만들겠습니다.

18
00:01:17,610 --> 00:01:21,110
그러면, w1은 어떤 매트릭스 이겠죠, 맞죠?

19
00:01:21,110 --> 00:01:25,625
몇개의 줄로 이루어진 매트릭스 입니다.

20
00:01:25,625 --> 00:01:28,296
그러면 여기 x1의 계산을 보면 

21
00:01:28,296 --> 00:01:30,070
얻게 되는 것은, 

22
00:01:30,070 --> 00:01:40,021
w1 곱하기 x1 세로 벡터를 얻게 됩니다. 이렇게 그리죠.

23
00:01:40,021 --> 00:01:47,420
비슷하게 여기 x2 벡터를 보면, 

24
00:01:47,420 --> 00:01:54,730
이 값을 갖고, 즉 w1곱하기

25
00:01:54,730 --> 00:02:00,460
x2는 또 다른 세로 벡터를 줍니다. 맞죠? 

26
00:02:00,460 --> 00:02:03,250
이것을 z12라고 할 수 있겠죠. 

27
00:02:03,250 --> 00:02:06,730
마지막으로 x3을 보면, 

28
00:02:06,730 --> 00:02:12,315
여기서는 w1 곱하기 x3이구요, 

29
00:02:12,315 --> 00:02:19,530
이러면 세번째 세로 벡터를 주는데요, 이것은 z13입니다.

30
00:02:19,530 --> 00:02:25,250
대문자 X인 트레이닝 세트를 고려하면, 

31
00:02:25,250 --> 00:02:31,475
이것은 저희의 모든 트레이닝 샘플을 쌓아서 만들어집니다.

32
00:02:31,475 --> 00:02:37,010
그러므로 대문자 X 매트릭스는 x1 벡터를 x2에 쌓아

33
00:02:37,010 --> 00:02:43,430
올리고, 그 다음에 x3에 쌓아올린 것입니다.

34
00:02:43,430 --> 00:02:46,250
이것은 3개의 트레이닝 샘플만 있을때의 경우겠죠. 

35
00:02:46,250 --> 00:02:50,371
만약 더 있다고 하면, 아시겠지만, 이렇게 계속 가로로 쌓아 올려질 것입니다.

36
00:02:50,371 --> 00:02:57,790
하지만 이제 x 매트릭스를 갖고 w를 곱하면, 

37
00:02:57,790 --> 00:03:00,190
매트릭스가 어떤 원리를 곱해지는지 생각해보면, 

38
00:03:00,190 --> 00:03:02,680
첫번째 세로줄은, 

39
00:03:02,680 --> 00:03:06,313
여기 보라색으로 그린 값과 같이 되구요. 

40
00:03:06,313 --> 00:03:10,930
두번째 세로 줄은 여기 그려진 4개의 값과 동일할 것입니다. 

41
00:03:10,930 --> 00:03:16,612
3번째 세로줄은 여기 그려진 

42
00:03:16,612 --> 00:03:19,480
오렌지색 값일 것입니다.

43
00:03:19,480 --> 00:03:27,740
이 값은 당연히 z11이 세로 벡터로 표현된 값과 

44
00:03:27,740 --> 00:03:37,185
그 다음으로 z12가 세로 벡터로 표현된 값과, 그 다음으로 z13가

45
00:03:37,185 --> 00:03:39,273
세로 벡터로 표현된 값이죠.

46
00:03:39,273 --> 00:03:41,100
이것은 마찬가지로 3개의 트레이닝 샘플일 경우입니다.

47
00:03:41,100 --> 00:03:44,255
만약에 더 많은 샘플이 있을 경우, 세로줄 벡터의 개수가 늘어나겠죠. 

48
00:03:44,255 --> 00:03:51,220
그러면 이것은 대문자 Z1 매트릭습니다.

49
00:03:51,220 --> 00:03:55,230
이 내용이 우리가 이전에 어떻게 w1 곱하기 

50
00:03:55,230 --> 00:04:02,830
xi를 통해 1개의 트레이닝 샘플의 경우, z1i 값을 

51
00:04:02,830 --> 00:04:08,310
구하게 되었는지 이해에 도움이 됐으면 좋겠습니다.

52
00:04:08,310 --> 00:04:12,565
복수의 트레이닝 샘플을 다른 줄에 쌓는 경우에는, 

53
00:04:12,565 --> 00:04:15,250
그에 상응하는 값이, 

54
00:04:15,250 --> 00:04:18,725
이렇게 줄별로 다른 z들의 값으로 나타납니다. 

55
00:04:18,725 --> 00:04:24,565
이 강의에서 직접 다루진 않겠지만 파이썬 broadcasting 통해서도 확인 가능합니다.

56
00:04:24,565 --> 00:04:26,245
여기 값들을 다시 더하면,

57
00:04:26,245 --> 00:04:30,534
b들의 값 말이죠, 그 값들은 맞습니다.

58
00:04:30,534 --> 00:04:34,540
파이썬 broadcasting 에서는 어떻게 되는 것이냐하면,

59
00:04:34,540 --> 00:04:41,790
bi의 값을 각각별로 더해줍니다. 여기 각각의 매트릭스 줄별로 말이죠.

60
00:04:41,790 --> 00:04:48,220
이번 슬라이드에서는 z1이 

61
00:04:48,220 --> 00:04:51,980
w1x 더하기 b1가 되고, 

62
00:04:51,980 --> 00:04:54,020
이것이 첫번째 단계의 벡터화임을 보여줬는데요, 

63
00:04:54,020 --> 00:04:57,493
이전 슬라이드의 4가지 단계 중에서 말이죠. 

64
00:04:57,493 --> 00:04:59,990
하지만 비슷한 분석을 통해서 

65
00:04:59,990 --> 00:05:02,660
다른 나머지 단계도 똑같은 

66
00:05:02,660 --> 00:05:08,105
로직을 통해 즉, 줄에서 표현되는 입력값을 쌓아서, 

67
00:05:08,105 --> 00:05:11,510
공식에서 그에 상응하는 결과값을 찾게됩니다. 

68
00:05:11,510 --> 00:05:14,970
마지막으로, 이번 비디오에서 다룬 내용을 전체적으로 복습해보겠습니다.

69
00:05:14,970 --> 00:05:16,520
이게 여러분의 신경망이라고 하면,

70
00:05:16,520 --> 00:05:21,693
저희는 전 방향전파 을 도입하기 위해서는 그렇게 해야 된다고 했는데요, 

71
00:05:21,693 --> 00:05:27,693
트레이닝 예시를별로 i에서 m까지 말이죠. 그리고, 

72
00:05:27,693 --> 00:05:34,100
트레이닝 샘플을 줄별로 쌓자고 했습니다. 여기 z1, a1, z2, a2에 해당하는 값에 대해서요.

73
00:05:34,100 --> 00:05:38,265
그리고 줄별로 쌓았습니다.

74
00:05:38,265 --> 00:05:43,820
이것은 a1에 대한 예제인데요, z1, a1, 그리고 a2에도

75
00:05:43,820 --> 00:05:46,975
마찬가지로 적용됩니다.

76
00:05:46,975 --> 00:05:51,090
이전 슬라이드에서 그 다음으로 보여준 것은 

77
00:05:51,090 --> 00:05:58,785
이 라인이 모든 m 샘플에 거쳐서 동시에 벡터화 시킬 수 있다는 것입니다.

78
00:05:58,785 --> 00:06:00,555
비슷한 로직으로,

79
00:06:00,555 --> 00:06:03,880
여기 이 4줄의 코딩에 대한 벡터화 값이

80
00:06:03,880 --> 00:06:08,811
모두 옳은 값이라는 것을 확인할 수 있습니다.

81
00:06:08,811 --> 00:06:10,675
다시 말씀드리자면, 

82
00:06:10,675 --> 00:06:18,960
x의 값은 a0입니다. 

83
00:06:18,960 --> 00:06:27,980
입력 특성 벡터 x가 a0이고, xi는 a0i입니다.

84
00:06:27,980 --> 00:06:30,870
여기 이 공식들이 비슷한 점이 있는데요, 

85
00:06:30,870 --> 00:06:34,110
첫번째 공식은 

86
00:06:34,110 --> 00:06:41,790
z1 = w1 a0 더하기 b1으로 쓰일 수 있습니다.

87
00:06:41,790 --> 00:06:45,680
그러면 보이시겠지만 여기 이 공식과

88
00:06:45,680 --> 00:06:51,805
여기 이 공식이 굉장히 비슷합니다. 여기 인덱스 지수 수치가 1의 차이를 보이는 것
외에는요.

89
00:06:51,805 --> 00:06:55,880
이것을 통해 저희는 신경망의 다른 층들이 사실은

90
00:06:55,880 --> 00:07:00,585
비슷한 활동을 한다는 것을 알 수 있습니다. 똑같은 산출을 반복적으로 하는 것이지요.

91
00:07:00,585 --> 00:07:04,220
여기는 two-layer 신경망이 있는데요, 

92
00:07:04,220 --> 00:07:08,475
다음주 비디오에서 이러한 심층신경망에 대해 배우겠습니다.

93
00:07:08,475 --> 00:07:11,670
이런 깊은 신경 네트워크도 사실 이와 같은

94
00:07:11,670 --> 00:07:16,215
2가지의 단계를 거쳐서 단순히 그 횟수를 늘리는 것입니다.

95
00:07:16,215 --> 00:07:21,255
자 그럼, 이번 강의에서 배웠듯이 여러 복수 트레이닝 샘플에서 신경망을 
벡터화시키는 방법은 이렇습니다.

96
00:07:21,255 --> 00:07:25,590
현재까지는, 신경망에서 시그모이드 함수를 이용하는 것을 배웠었는데요, 

97
00:07:25,590 --> 00:07:27,925
이것이 가장 좋은 선택이라고는 할 수 없겠습니다.

98
00:07:27,925 --> 00:07:29,675
다음 비디오에서는, 

99
00:07:29,675 --> 00:07:32,450
조금 다른 내용에 대해, 

100
00:07:32,450 --> 00:07:37,190
시그모이드 함수가 일부이기도 한, activation 함수에 대해 더욱 자세히 알아보겠습니다.