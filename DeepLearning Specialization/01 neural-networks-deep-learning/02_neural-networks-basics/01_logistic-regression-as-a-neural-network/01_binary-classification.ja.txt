こんにちは、そしておかえりなさい。 今週は、ニューラルネットワークプログラミングの
基礎を学びます。 ニューラルネットワークを実装するときには、 いくつかの実装技術が大切になってきます。 例えば、m個の教師サンプルがあれば、 そのm個のサンプルをforループで 処理するかもしれません。 しかし、ニューラルネットワークの実装では、 サンプル全体を明示的なforループなしで 処理したいのです。 その方法を、今週学習しましょう。 もう一つの例です。
ネットワーク内での計算を組み立てる際、 フォワードプロパゲーションと呼ばれるものに続いて、 バックプロパゲーションと呼ばれるステップを
作ることになります。 今週の教材では、なぜニューラルネットワークの学習は フォワードプロパゲーションと
バックプロパゲーションによって 構成されるのかについても学びます。 今週の教材では、理解しやすくするために
これらのことを ロジスティック回帰を使ってお伝えします。 もしロジスティック回帰をすでに見たことがあっても、 今週の教材から
新しく興味深い考えを得ることができるでしょう。 では始めましょう。 ロジスティック回帰は、
二項分類のためのアルゴリズムです。 まず問題を設定しましょう。 これが、二項分類の例です。 このような画像の入力があり、 この画像を猫と認識すれば1というラベルを出力し、 そうでなければ0を出力したいとします。 そして、出力されるラベルをyと示します。 コンピュータで画像が
どのように表されるかを見てみましょう。 画像を保持するためには、
コンピュータは三つの別々の行列を保持します。 それらは画像の赤、緑、青のカラーチャンネルに
対応します。 入力画像が64ピクセル×64ピクセルなら、 64×64の行列を三つ持つことになります。 それぞれが画像のピクセルの
赤、緑、青の強さに対応します。 この小さいスライドを作るために
なるべく小さい行列を描いたので、 64×64ではなく
5×4になってしまいました。 ピクセルの色の強さの値を
特徴ベクトルにするために、 これらを全て一列に展開し、
特徴ベクトルxに入れましょう。 三つのピクセルデータを特徴ベクトルに展開するために、 画像に対応する特徴ベクトルを次のように定義します。 全てのピクセルデータ、255,231などを取って、 全ての赤ピクセルを並べるまで続けます。 そして255,134、255,134と続きます。 最終的には、赤、緑、青のピクセルデータ 全てが並べられた長い特徴ベクトルができあがります。 画像が64×64なら、 ベクトルxの次元数は64×64×3になります。 なぜなら、これは三つの行列の合計だからです。 この場合、12,288となります。 これらの数字をかけた結果がそうなるのです。 なので、nx=12288というのを 入力される特徴xの次元数として用います。 簡潔にするため、小文字のnを使って 入力される特徴ベクトルの次元数を表すこともあります。 二項分類では、私たちの目的は 特徴ベクトルxで表される画像を入力できる
分類器を学習することです。 そして、それに対応するラベルyが
1か0かを予測します。 つまり、画像が猫なのかそうでないかを
予測するということです。 これからのコースを通じて使っていく 表記法を見ていきましょう。 一つの教師サンプルはx,yというペアで表され、 xはnx次元の特徴ベクトル、 yは0か1かのラベルです。 教師セットは小文字のm個の教師サンプルで
できています。 なので、教師セットの一番目のサンプルは
(x1,y1)と表され、 (x2,y2)が二つ目で、 最後のサンプルである(xm,ym)まで続きます。 これらすべてが教師セットとなるのです。 小文字のmを教師サンプルの数を表すために使います。 そして、これが教師サンプルの数であることを
強調するため、 たまにmをm trainと書くことがあります。 テストセットについて書くときは、 テストサンプルの数を示すために
m testという表記を使うことがあります。 これがテストサンプルの数です。 最後に、
全ての教師サンプルをコンパクトに示すために、 大文字のXという行列を定義します。 これは教師セットの入力x1,x2…を取ってきて、 それらを列として並べてできるものです。 x1を取ってきてこの行列の一つ目の列とし、 x2を二つ目の列とし、xmまで続けます。 これが、大文字のXという行列です。 なので、この行列Xは教師サンプルの数であるm列あり、 行の数、つまり高さはnxです。 他のコースでは、一つの教師サンプルを一つの行として 定義した行列Xを見たことがあるかもしれません。 x1の転置からxmの転置までが並べられたものです。 しかし、左に書いたような慣習の方が、 ニューラルネットワークの実装がかなり楽になるのです。 おさらいすると、Xはnx × m次元の行列で、 Pythonで実装するときには X.shapeというのが
行列の形を確認するコマンドなのですが、 これを実行するとnx,mと表示されます。 nx × m次元の行列だという意味です。 これが、教師サンプルxをひとまとめにする方法です。 では出力ラベルyはどうでしょうか。 ニューラルネットワークの実装を
より簡単にするためには、 yも列として並べた方が都合がいいです。 なので、大文字のYをy1,y2からymまで 並べたものとして定義します。 なので、Yは1×m次元の行列です。 繰り返しますが、Pythonのコマンドを使うと、
Yの形は1,mであることがわかります。 つまり、1×m次元の行列だということです。 後のコースで
ニューラルネットワークを実装する際には、 異なる教師サンプルデータに関連付けられたデータを
取るというのは便利な慣習となるでしょう。 データというのは、xやy、
そして後に登場する他の数量のことです。 ここでxとyについてやったように 異なる教師サンプルに関連付けられたデータを取って 別々の列に保持するということです。 これらがロジスティック回帰と 後に出てくるニューラルネットワークで使う
表記法です。 ある表記が何を表しているのか忘れてしまったら、 例えばmやnってなんだっけ？となったら、 表記ガイドをコースのサイトに投稿してあるので、 ある表記がどういう意味だったかを
すばやく調べることができます。 さあ、次の動画に進んで、 これらの表記法を使って
ロジスティック回帰を始めましょう。