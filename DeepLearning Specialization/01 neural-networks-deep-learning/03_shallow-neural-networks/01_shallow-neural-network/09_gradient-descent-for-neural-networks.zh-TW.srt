1
00:00:00,000 --> 00:00:02,520
好的，我認為這段影片會很令人激動

2
00:00:02,520 --> 00:00:04,635
在這個影片中，您將看到如何

3
00:00:04,635 --> 00:00:08,490
對於有一隱藏層的神經網路建置梯度下降法

4
00:00:08,490 --> 00:00:12,090
於此影片，我只給你需要的方程式

5
00:00:12,090 --> 00:00:16,245
來建置反向傳播，使得梯度下降法起效果

6
00:00:16,245 --> 00:00:18,555
並且，在下一段影片中

7
00:00:18,555 --> 00:00:20,940
我會給出直覺，關於為什麼

8
00:00:20,940 --> 00:00:24,150
這些特定的方程式是精確的、

9
00:00:24,150 --> 00:00:28,320
正確的方程當你需為你的神經網路計算梯度時

10
00:00:28,320 --> 00:00:29,910
所以，你的神經網路

11
00:00:29,910 --> 00:00:31,875
現在只有一個隱藏層的話

12
00:00:31,875 --> 00:00:34,964
會有參數W[1]

13
00:00:34,964 --> 00:00:39,285
B[1],W[2]與B[2]

14
00:00:39,285 --> 00:00:40,800
所以，作為提醒，

15
00:00:40,800 --> 00:00:48,150
如果您有N_x或換句話說N[0]個輸入特徵的話

16
00:00:48,150 --> 00:00:51,090
和N[1]個隱藏單元

17
00:00:51,090 --> 00:00:57,260
跟N[2]個輸出單元在我們的例子中

18
00:00:57,260 --> 00:00:59,690
到目前為止，我只有N[2]=1

19
00:00:59,690 --> 00:01:05,720
那麼矩陣W[1]=N[1] 乘 N[0]

20
00:01:05,720 --> 00:01:08,870
B[1]會時N[1]維度向量

21
00:01:08,870 --> 00:01:12,770
所以我們將他寫作N[1]乘以1維矩陣

22
00:01:12,770 --> 00:01:14,120
實際上是一個列向量

23
00:01:14,120 --> 00:01:18,395
W[2]的維度是(N[2],N[1])

24
00:01:18,395 --> 00:01:25,485
B[2]的維度是(N[2],1)

25
00:01:25,485 --> 00:01:28,925
好的，目前我們只看到N[2]=1的例子

26
00:01:28,925 --> 00:01:32,180
其中你只有一個隱藏單位

27
00:01:32,180 --> 00:01:39,405
所以，對於一個神經網路，你也有一個成本函數

28
00:01:39,405 --> 00:01:43,370
從現在開始，我將假設你在做二元分類

29
00:01:43,370 --> 00:01:45,110
所以，這樣子的話

30
00:01:45,110 --> 00:01:50,600
您的參數的成本將是

31
00:01:50,600 --> 00:01:56,520
1/M的成本函數求平均值

32
00:01:56,520 --> 00:02:02,580
所以，在這裡的L是成本，當你的神經網路預測 y-hat

33
00:02:02,580 --> 00:02:06,750
實際上這裡是a[2]當梯度標籤=y時

34
00:02:06,750 --> 00:02:08,560
如果您要做二元分類，

35
00:02:08,560 --> 00:02:13,310
損失函數會恰好等於之前用的羅吉斯迴歸分析的損失函數

36
00:02:13,310 --> 00:02:15,890
所以，為了訓練您的演算法的參數

37
00:02:15,890 --> 00:02:19,705
你需要執行梯度下降

38
00:02:19,705 --> 00:02:21,570
當訓練神經網路時

39
00:02:21,570 --> 00:02:26,435
隨機初始化這些權重而不是全部設為0
是很重要的

40
00:02:26,435 --> 00:02:28,340
我們之後就會看見為什麼會這樣

41
00:02:28,340 --> 00:02:31,110
但是經過初始參數為某些值的話

42
00:02:31,110 --> 00:02:34,555
每個迴圈或梯度下降計算出的預測

43
00:02:34,555 --> 00:02:38,270
所以，你基本上計算y-hat_i

44
00:02:38,270 --> 00:02:41,765
i=1,...,m，然後

45
00:02:41,765 --> 00:02:44,450
你需計算導數

46
00:02:44,450 --> 00:02:47,750
所以你需要計算dw1

47
00:02:47,750 --> 00:02:54,279
這是成本函數對於w1參數的導數

48
00:02:54,279 --> 00:02:56,499
你可以計算另外一個變數

49
00:02:56,499 --> 00:02:58,375
我把它稱作db[1]

50
00:02:58,375 --> 00:03:02,260
它等於你的成本函數的導數或者說斜率

51
00:03:02,260 --> 00:03:06,190
相對於變數b[1]...

52
00:03:06,190 --> 00:03:09,685
對於w[2]和b[2]的計算很相似

53
00:03:09,685 --> 00:03:17,775
那麼最後，梯度下降更新會更新w[1]為w[1]-alpha

54
00:03:17,775 --> 00:03:21,150
學習率*dw[1]

55
00:03:21,150 --> 00:03:26,310
b[1]更新為b[1]-學習率 (alpha)

56
00:03:26,310 --> 00:03:32,280
乘以db[1]，同樣對w[2]和b[2]計算相似的過程

57
00:03:32,280 --> 00:03:35,560
有時，我使用 :=，有時 = 

58
00:03:35,560 --> 00:03:37,630
兩者都可使用

59
00:03:37,630 --> 00:03:40,790
所以，這會是梯度下降的一次迭代

60
00:03:40,790 --> 00:03:42,580
並且你重複這個迭代幾次

61
00:03:42,580 --> 00:03:45,100
直到您的參數看到來收斂了

62
00:03:45,100 --> 00:03:46,300
所以，在上一段影片中

63
00:03:46,300 --> 00:03:49,055
我們討論如何計算預測

64
00:03:49,055 --> 00:03:50,400
如何計算輸出之類的

65
00:03:50,400 --> 00:03:52,960
並且我們看到在向量形式的表示法下<br />如何做這些事

66
00:03:52,960 --> 00:03:57,820
所以重點是如何計算這些偏導數

67
00:03:57,820 --> 00:04:03,010
dw[1]，db[1]和導數dw[2],db[2]

68
00:04:03,010 --> 00:04:06,730
所以，我想做的只是給您

69
00:04:06,730 --> 00:04:11,050
您需要的方程式來計算導數

70
00:04:11,050 --> 00:04:15,130
請不要忘記看下段影片，下段是選看影片

71
00:04:15,130 --> 00:04:19,030
更深入的了解我們如何
導出了這些公式。

72
00:04:19,030 --> 00:04:25,360
所以,讓我再總結一遍
傳播的公式。

73
00:04:25,360 --> 00:04:32,510
所以,你有Z[1]等於 W[1] 乘 X 加 b[1],

74
00:04:32,510 --> 00:04:42,560
然後 A[1] 等於啟動函數
應用在該層中逐元素 - Z[1],

75
00:04:42,560 --> 00:04:46,610
然後Z[2]等於W[2],

76
00:04:46,610 --> 00:04:52,595
A[1] 加 b[2],最後,

77
00:04:52,595 --> 00:04:55,295
所有的訓練集都一樣的向量化,對嗎?

78
00:04:55,295 --> 00:05:00,580
A[2] 等於 g[2] Z[2]

79
00:05:00,580 --> 00:05:03,605
再次,現在,如果我們假設我們
在做二元分類分類,

80
00:05:03,605 --> 00:05:07,120
這裡的啟動函數應該是S型函數(sigmoid)

81
00:05:07,120 --> 00:05:08,995
只是作為結束。

82
00:05:08,995 --> 00:05:11,900
所以,這是向前傳播或左向右

83
00:05:11,900 --> 00:05:14,690
給你的神經網路做的運算

84
00:05:14,690 --> 00:05:16,730
接下來,讓我們計算導數。

85
00:05:16,730 --> 00:05:21,725
所以,這是反向傳播步驟。

86
00:05:21,725 --> 00:05:30,900
然後,我計算 dZ[2] 等於 A[2]
減去基本真值 Y,

87
00:05:30,900 --> 00:05:33,130
只是要提醒一下,

88
00:05:33,130 --> 00:05:35,240
所有這些例子都已向量化。

89
00:05:35,240 --> 00:05:38,540
因此,矩陣 Y 是這個 1 by m

90
00:05:38,540 --> 00:05:44,600
的矩陣,所有您的例子水平的堆放

91
00:05:44,600 --> 00:05:50,599
原來實際上 dw[2] 等於這個,

92
00:05:50,599 --> 00:05:54,920
事實上,這前面三個方程

93
00:05:54,920 --> 00:06:00,630
非常類似于羅吉斯迴歸分析的梯度下降。

94
00:06:00,910 --> 00:06:03,170
axis 等於 1,

95
00:06:03,170 --> 00:06:08,635
逗號, keepdims等於True。

96
00:06:08,635 --> 00:06:13,600
只是一點點細節,這個np.sum是

97
00:06:13,600 --> 00:06:18,700
個 Python NumPy 的指令
用於求和於所有的一維矩陣。

98
00:06:18,700 --> 00:06:21,450
在這個例子是水平的總和

99
00:06:21,450 --> 00:06:25,645
而 keepdims 做的是避免Python

100
00:06:25,645 --> 00:06:30,750
產生奇怪的
等級 1 的數列,對不對?

101
00:06:30,750 --> 00:06:33,525
維度是 (n,)。

102
00:06:33,525 --> 00:06:36,045
所以, 通過保持 keepdims 等於 True。

103
00:06:36,045 --> 00:06:43,210
Python 輸出 db[2] 是一個 n 乘 1 向量

104
00:06:43,210 --> 00:06:47,145
事實上,從技術上講,我想這將是 n2 乘 1。

105
00:06:47,145 --> 00:06:49,680
在這種情況下,它只是一個數字,

106
00:06:49,680 --> 00:06:51,795
所以也許沒關係.

107
00:06:51,795 --> 00:06:55,350
但稍後,我們將看到它何時變成真正重要。

108
00:06:55,350 --> 00:06:59,825
所以,到目前為止,我們所做的是非常
類似于羅吉斯迴歸分析

109
00:06:59,825 --> 00:07:04,260
但現在,當你繼續反向傳播,

110
00:07:04,260 --> 00:07:05,790
您需要計算這個

111
00:07:05,790 --> 00:07:16,340
(dZ[1] = ...)dZ[2] 乘以g[1] prime of z[1]。

112
00:07:16,340 --> 00:07:19,190
所以,這個數量 g[1] prime是

113
00:07:19,190 --> 00:07:23,945
用於隱藏層的啟動函數的導數,

114
00:07:23,945 --> 00:07:25,750
對於輸出層,

115
00:07:25,750 --> 00:07:29,470
我假設你在進行二元分類
使用 S型函數 sigmoid 。

116
00:07:29,470 --> 00:07:32,630
這已經包括到dZ[2]的公式中

117
00:07:32,630 --> 00:07:37,735
提醒一下, 這個 乘 是逐元素乘積

118
00:07:37,735 --> 00:07:45,650
所以,這裡將是一個 (n1, m) 矩陣,在這裡,

119
00:07:45,650 --> 00:07:51,545
這個逐元素的導數也將是 (n1, m) 矩陣,

120
00:07:51,545 --> 00:07:55,910
所以這個乘那個是兩個矩陣逐元素相乘的積。

121
00:07:55,910 --> 00:08:00,950
最後, dW[1] 等於這個,

122
00:08:00,950 --> 00:08:07,010
和 dB[1] 等於這個,

123
00:08:07,010 --> 00:08:14,930
和dp.sum dZ[1], axis

124
00:08:14,930 --> 00:08:20,820
等於 1, keepdims等於True。

125
00:08:20,820 --> 00:08:26,455
所以,而以前如果 n[2] 等於 1,
 keepdims 可能就不那麼重要了。

126
00:08:26,455 --> 00:08:29,475
結果只是 (1,1) 的項目,只是一個實數。

127
00:08:29,475 --> 00:08:36,330
在這裡,db1 將是一個 (n1, 1) 的向量,

128
00:08:36,330 --> 00:08:39,180
所以你想要Python,你想要np.sum 功能正常。

129
00:08:39,180 --> 00:08:43,990
我會放個維度keppdims的東西,
而不產生一個奇怪的等級 1 的數列

130
00:08:43,990 --> 00:08:49,720
同樣的維度,可能最終
搞砸了一些資料的計算。

131
00:08:49,720 --> 00:08:52,914
另一種方法是,不用 keepsims 參數,

132
00:08:52,914 --> 00:09:01,470
但明確的用reshape 參數,將 np.sum 的輸出到此維度,

133
00:09:01,470 --> 00:09:04,665
您希望 db 的維度.

134
00:09:04,665 --> 00:09:09,655
所以,這是向前傳播
我想,有四個方程式,

135
00:09:09,655 --> 00:09:13,400
和反向傳播,
我想,有六個方程式。

136
00:09:13,400 --> 00:09:15,590
我知道我只是寫下了這些方程式,

137
00:09:15,590 --> 00:09:17,990
但在下一個選修的影片中,

138
00:09:17,990 --> 00:09:20,690
讓我們用一些直覺的方式去

139
00:09:20,690 --> 00:09:24,730
導出反向傳播演算法的六個方程式.

140
00:09:24,730 --> 00:09:26,455
隨您意看或不看.

141
00:09:26,455 --> 00:09:28,875
不管如何, 如果您建置這些演算法,

142
00:09:28,875 --> 00:09:33,255
您要知道如何正確的建置
正向傳播跟反向傳播

143
00:09:33,255 --> 00:09:38,120
您要能夠計算你需要的導數
來應用梯度下降,

144
00:09:38,120 --> 00:09:40,360
來學習神經網路的參數。

145
00:09:40,360 --> 00:09:43,190
您可以運用這個演算法,

146
00:09:43,190 --> 00:09:46,000
有成果,而沒有深入理解微積分。

147
00:09:46,000 --> 00:09:49,505
很多成功的深度學習從業者是這樣做。

148
00:09:49,505 --> 00:09:50,975
但是,如果你想,

149
00:09:50,975 --> 00:09:52,505
您也可以觀看下一段影片

150
00:09:52,505 --> 00:09:57,680
會得到多一點這些方程式推導的直覺。