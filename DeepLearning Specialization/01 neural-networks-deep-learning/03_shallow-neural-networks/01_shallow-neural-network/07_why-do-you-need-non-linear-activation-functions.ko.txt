여러분의 신경망이 비선형 activation 
함수가 필요한 것일까요? 알고 보니, 신경망이 흥미로운 함수들을 산출하기 위해서는 비선형 activation 함수를 골라야 합니다. 왜 그런지 한번 보겠습니다. 여기서 
보면 신경망의 전 방향전파 공식인데요, 여기 이 부분을 빼도록 하죠 g의 함수를 빼겠습니다. 
그리고 a1을 z1으로 지정하겠습니다. 또는 g(z)가 z와 동일하다고 표현해도 됩니다. 가끔씩은 이것이 선형 activation 
함수라고 표현하는데요, 더 나은 이름은 identity activation 함수일 것입니다. 
이유는 이것들은 단지 입력값이 들어간 그대로 결과값이 나오기 때문입니다. 더 나은 이름은 identity activation 함수일 것입니다. 
이유는 이것들은 단지 입력값이 들어간 그대로 결과값이 나오기 때문입니다. 여기 이것의 목적을 위해서, 만약 a2가 여기서 z2가되면 어떻게 될까요? 이렇게 하면 이 모델은 단지 y 와 ŷ을 선형함수로 계산합니다. 
입력값 x에 대해서 말이죠. 첫번째 2개의 공식을 가지고 오면, a1=z1=w1 곱하기 x 더하기 b1인데요, 그리고 a2=z2=w2 a1 더하기 b2인데요 여기서 a1의 
정의를 가지고 여기에 대입시키면, a2=w2 (w1 x + b1) a2=w2 (w1 x + b1) 이 부분은 a1이 구요 그리고 더하기 b2 인데요 그러므로 이것은 w2 w1 X 더하기 W2 b1 더하기 b2 인데요, 이것은 그러면 w 프라임 그리고 b 프라임이라 하겠습니다. 그러면 이 공식은 w 프라임 x 더하기 b 프라임이 됩니다. 여러분이 선형 activation 함수를 사용한다고 하거나, identity activation 함수를 쓴다고 하면, 그러면 신경망은 선형함수에서 
입력값에 대한 결과값을 주게 되는 것입니다. 나중에 많은 층으로 이루어진 
심츠신경망에 대해서 이야기 하겠습니다. 나중에 많은 층으로 이루어진 
심츠신경망에 대해서 이야기 하겠습니다. 나중에 많은 층으로 이루어진 
심츠신경망에 대해서 이야기 하겠습니다. 만약 여러분이 선현 activation 함수를 쓰는 경우, 또는 activation 함수가 없는 경우에는, 신경망이 몇개의 층으로 이루어 졌다고 하더라도, 이것이 하는 것은 단순히 
선형 activation 함수를 산출하는 것입니다. 그렇기 때문에 이왕이면 
숨겨진 레이어가 없는 것이 좋겠죠. 몇개의 케이스를 제가 말한 것중에, 여기 선형 activation 함수가 있는데, 그리고 여시 시그모이드 함수가 있는경우 이 모델은 더이상 표현적이지 않습니다. 숨겨진 레이어 가 없는 일반 
로지스틱 회귀 분석과 비교했을 때 말이죠, 그것을 구지 증명하지는 않겠습니다.
여러분께서 나중에 증명은 혼자 진행하셔도 됩니다. 중요한 부분은 선형에서의 숨겨진 레이어는 
거의 쓸모가 없다는 것입니다. 왜냐면 2개의 선형 함수의 구성요소는 
그 자체가 선형함수이기 때문입니다. 그러므로 비선형 특성을 기입하지 않는 이상은 더 흥미로운 함수를 산출하는 것이 아닙니다. 신경망이 더 깊어질 때 말이죠. 이런 경우, 선형 함수를 쓰는 
한가지의 경우가 있기는 한데요 g(z)=z 의 경우입니다. 이 같은 경우는 머신러닝에서 그럼 만약 Y가 실수인 경우 예를 들어, 여러분이 집값을 예측하려고 한다면 Y는 0 1 이 아니라 실수가 될 것입니다. 0달러에서 집 값이 얼마이던지 
그 값만큼의 범위를 가질 것입니다. 집은 사실 잠재적으로 
몇십억까지 가격이 올라갈 수 있겠죠. 그 값이 얼마던 Y값에 그 값만큼 
데이터세트로 반영될 것입니다. 그러나 만약 Y가 이러한 실수를 갖고, 이런 경우는 사실 선형함수를 가져도 괜찮을 수 있습니다. 그러면 결과값 ŷ은 또한 마이너스 무한대에서 플러스 무한대의 값을 범위로 할텐데요 이 경우, 숨겨진 유닛은 새로운 
activation 함수를 쓰면 안됩니다. 숨겨진 유닛은 ReLu나 tan h Leaky ReLu를 
쓸수도 있거나 다른 것들을 쓸수도 있겠죠. 그래서 선형함수를 쓰는 
경우는 보통 결과값 층에서 입니다. 하지만, 그 이외의 선형 activation 함수를 숨겨진 레이어에서 쓰는 것은 아주 compression 과 같은 아주 특별한 경우를 제외하고는 이 경우에 대해서는 직접 다루진 않겠지만 그 외에는 선형 activation 함수를 
쓰는 것은 매우 드문 일입니다. 그리고 당연히 오늘날 집값을 예측하는 것은 첫째주에서 봤듯이, 집값이 모두 양수의 값이기 때문에, 이 경우에도 ReLu 함수를 사용할 수 있을 것입니다. ŷ 결과값이 0 또는 더 큰 값이 되도록 말이죠. 이 강의가 여러분이 왜 비선형 함수를 신경망에서 반드시 가져야 하는지 설명해줬기 바랍니다. 다음으로는 기울기 강하에 대해 이야기할 것인데요, 또 기울기 강하와 관련하여 
토론을하기 위해서는 다음 비디오에서 여러분에게 함수의 slope를 구하는 방법이나 또는 개인 activation 함수에 대한 
derivative를 구하는 방법을 알려드리겠습니다. 그러면 다음 비디오로 넘어가겠습니다.