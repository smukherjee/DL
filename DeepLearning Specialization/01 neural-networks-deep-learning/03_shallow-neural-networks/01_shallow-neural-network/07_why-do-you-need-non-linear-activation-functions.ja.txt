なぜニューラルネットワークに
非線形活性化関数が必要なのでしょうか ニューラルネットワークに興味深い関数を
計算させようとすると 非線形の活性化関数が必要だと分かります これはニューラルネットワークの
フォーワードプロパゲーションの式です この式を削除してみましょう 関数gを削除して
a[1]=z[1]とします またはg(z)=zとします これは時として線形活性化関数と呼ばれます より良い名前としては 恒等活性化関数があります
入力をそのまま出力しているからです この目的のためには a[2]=z[2]
だとしたらどうでしょうか この場合には モデルはy hat を
入力特徴量x の線形関数として計算しているだけです この最初の２つの式を考えます a[1]=z[1]=w[1]x+b
だとして a[2]=z[2]=w[2]a[1]+b[2]だとします そしてa[1]の定義を
下の式に代入すると a[2]=w[2](w[1]x+b[1]) になりますよね この部分はa[1]です そして+b[2]です
単純化すると (w[2]w[1])x + (w[2]+b[1]+b[2]) (w[2]w[1])x + (w[2]+b[1]+b[2])
となります 結果 この部分をwプライム、bプライムと呼びましょう 結果wプライムxとbプライムになります ここで万が一線形活性化関数を使う場合や 恒等活性化関数と呼ぶものを使う場合には ニューラルネットワークは単に
入力を線形関数で出力するだけです ニューラルネットワークに数多くの隠れ層のある 深いネットワークについては 後に話しますが もし線形活性化関数を使うか 活性化関数を使わない場合には ニューラルネットワークに
どれだけ多くの層があっても 線形活性化関数の計算を
しているだけになります 隠れ層が全く無いものに
しても構いません 次のような場合もあります 線形活性化関数を ここに使い
シグモイド関数をここに使うと このモデルは標準的な 隠れ層なしのロジステック回帰と
同じものになります わざわざ証明しませんが
試してみても構いません ここでは 隠れ層に線形を使うのは
意味がありません ２つの線形関数を合わせても
それ自体が線形関数になるからです 非線形のものを入れない限り 特になにかを 関数が計算するわけではありません
ネットワークを深くした場合でもです 線形活性化関数を
使うかもしれない箇所が１つあります g(z)=z これは回帰問題に
機械学習を使う時です y が実数の時です 例えば住宅価格を予測する時です y は０、１ではなく
実数です 住宅価格は０ドルから
可能なだけ高くなります 数百万ドルの場合もありえますよね でもデータセット内で
どれだけ高くても y は実数を取ります この場合には線形活性化関数を
使っても良いかもしれません y hat も 実数でマイナス無限大、プラス無限大に
なりえます しかし隠れ層のユニットは
活性化関数を使ってはいけません ReLU、Tanh、Leaky ReLU や
他のものを使います 線形活性化関数を使う可能性のある個所は
通常は出力層です それ以外では 線形活性化関数を 隠れ層で使うのは
非常に特別な場合のみです 圧縮に関してはありますが 線形活性化関数を使うのは
非常に稀なことです １週目のビデオで見た
住宅価格の予測では 住宅価格はマイナス値になりませんが この場合でもReLU 活性化関数を使って y hat が
常に０以上になるようにします これで非線形活性化関数が ニューラルネットワークに必須なことが
分かってもらえたかと思います 次からは勾配降下法について
扱い始めます 次のビデオで勾配降下法を話すために 個々の活性化関数の 傾きや微分の推定方法を
示したいと思います 次のビデオに行きましょう