1
00:00:00,000 --> 00:00:03,475
在这节视频中 我们来复习一下逻辑回归

2
00:00:03,475 --> 00:00:07,080
使用这种学习算法会得到的输出标签 y

3
00:00:07,080 --> 00:00:10,690
y在监督学习问题中全是0 或者1

4
00:00:10,690 --> 00:00:13,600
因此这是一种针对二分类问题的算法

5
00:00:13,600 --> 00:00:18,350
给定的输入特征向量 x 和一幅图片对应

6
00:00:18,350 --> 00:00:23,150
我们希望识别这是否是一张猫的图片

7
00:00:23,150 --> 00:00:26,525
因此我们想要一种算法能够输出一个预测值

8
00:00:26,525 --> 00:00:28,254
我们称之为 y帽 (yhat)

9
00:00:28,254 --> 00:00:31,130
这代表对真实标签 Y 的估计

10
00:00:31,130 --> 00:00:35,896
形式上讲 yhat 是当给定输入特征x时

11
00:00:35,896 --> 00:00:40,630
预测标签 y 为1的概率

12
00:00:40,630 --> 00:00:43,880
换种说法就是当x是一张图片

13
00:00:43,880 --> 00:00:45,530
就像我们在上一个视频看到的

14
00:00:45,530 --> 00:00:47,300
你想要yhat告诉你这是一张猫图的概率

15
00:00:47,300 --> 00:00:49,820
你想要yhat告诉你这是一张猫图的概率

16
00:00:49,820 --> 00:00:53,420
因此就像我在之前视频中说的

17
00:00:53,420 --> 00:00:56,960
x是一个n_x维的向量

18
00:00:56,960 --> 00:01:02,000
约定逻辑回归的参数是 w

19
00:01:02,000 --> 00:01:07,745
w 也是一个n_x维的向量

20
00:01:07,745 --> 00:01:11,670
另外 参数b是一个实数

21
00:01:11,670 --> 00:01:16,055
因此给定了一个输入x 以及参数 w 和 b

22
00:01:16,055 --> 00:01:20,595
那么如何产生输出 yhat 呢？

23
00:01:20,595 --> 00:01:22,970
有一种方法可以试试 尽管它不怎么奏效

24
00:01:22,970 --> 00:01:27,590
就是让 yhat 等于 w.T*x+b

25
00:01:27,590 --> 00:01:33,045
这是输入x的一个线性函数输出

26
00:01:33,045 --> 00:01:37,145
事实上 如果使用线性回归 就是这样操作的

27
00:01:37,145 --> 00:01:41,345
但是这对于二分类并不是一个好的算法

28
00:01:41,345 --> 00:01:45,575
因为你希望 yhat 能够输出 y 为1的概率

29
00:01:45,575 --> 00:01:50,480
因此 yhat 的值应该在0和1之间

30
00:01:50,480 --> 00:01:54,697
而这种算法很难实现这个要求

31
00:01:54,697 --> 00:01:58,475
因为w.T*x+b可能会比1大很多或者是一个负数

32
00:01:58,475 --> 00:02:00,905
这对于概率就失去了意义

33
00:02:00,905 --> 00:02:03,620
因此 yhat 值要在0和1之间

34
00:02:03,620 --> 00:02:07,670
所以 让逻辑回归中的输出 yhat 

35
00:02:07,670 --> 00:02:12,050
等于对这个值应用sigmoid函数的结果

36
00:02:12,050 --> 00:02:14,850
sigmoid函数是这样的

37
00:02:14,850 --> 00:02:24,000
如果水平轴的标签为z 那么函数sigmoid(z)是这样的

38
00:02:24,000 --> 00:02:28,050
它从0平滑地升高到1

39
00:02:28,050 --> 00:02:30,120
让我给轴标上标签

40
00:02:30,120 --> 00:02:34,915
这是0 然后它会在0.5处和竖直轴交叉

41
00:02:34,915 --> 00:02:41,305
因此这就是sigmoid(z)函数 这里用z表示 (w.T*x+b)

42
00:02:41,305 --> 00:02:43,020
因此这就是sigmoid(z)函数 这里用z表示 (w.T*x+b)

43
00:02:43,020 --> 00:02:46,230
这是sigmoid函数的公式

44
00:02:46,230 --> 00:02:49,380
当z是一个实数 sigmoid(z)就等于

45
00:02:49,380 --> 00:02:52,510
1 / ( 1+e^(-z) )

46
00:02:52,510 --> 00:02:54,695
注意这几件事

47
00:02:54,695 --> 00:03:01,255
如果z非常大 e^(-z)就会接近0

48
00:03:01,255 --> 00:03:03,420
所以sigmoid(z)就大约等于

49
00:03:03,420 --> 00:03:07,255
1 / (1+ 一个接近0的数字)

50
00:03:07,255 --> 00:03:11,280
因为 e^(很大的数的相反数) 会非常接近0

51
00:03:11,280 --> 00:03:13,505
因此这项会接近1

52
00:03:13,505 --> 00:03:16,255
事实上 如果你看左边的图

53
00:03:16,255 --> 00:03:20,475
如果z非常大 那么sigmoid(z)非常接近1

54
00:03:20,475 --> 00:03:24,105
相反地 如果z非常小

55
00:03:24,105 --> 00:03:28,970
或者他是一个非常大的负值

56
00:03:29,180 --> 00:03:39,640
sigmoid(z)会变成(1+ e^(-z) )

57
00:03:39,640 --> 00:03:42,565
这项会变成很大的数

58
00:03:42,565 --> 00:03:47,944
而这一项 思考一下 1/(1+ 很大的数)

59
00:03:47,944 --> 00:03:54,473
而这一项 思考一下 1/(1+ 很大的数)

60
00:03:54,473 --> 00:03:56,570
sigmoid(z) 就会接近0

61
00:03:56,570 --> 00:04:00,325
因此 当z变成一个很大的负数

62
00:04:00,325 --> 00:04:03,505
sigmoid(z)的值就会非常接近0

63
00:04:03,505 --> 00:04:06,070
因此当你实现逻辑回归时

64
00:04:06,070 --> 00:04:10,350
你的目标是尽力学习到参数w和b

65
00:04:10,350 --> 00:04:15,220
因此yhat就能很好地估计y等于1的概率

66
00:04:15,220 --> 00:04:18,955
在下个知识点之前 还有一点需要强调

67
00:04:18,955 --> 00:04:20,830
当进行神经网络编程时

68
00:04:20,830 --> 00:04:26,855
我们通常会将参数w和参数b分开看待

69
00:04:26,855 --> 00:04:30,000
这里的b对应一个偏置量

70
00:04:30,000 --> 00:04:31,295
在其他的一些课程中

71
00:04:31,295 --> 00:04:35,110
可能看到其他符号 用不同的方式处理参数

72
00:04:35,110 --> 00:04:42,205
在一些课程中 定义了一个额外的特征 x0 并让x0等于1

73
00:04:42,205 --> 00:04:47,250
因此x的维度就变成了 n_x+1

74
00:04:47,250 --> 00:04:53,865
之后 定义yhat等于sigmoid(θ.T *x)

75
00:04:53,865 --> 00:04:56,685
在这种符号约定中

76
00:04:56,685 --> 00:05:00,510
有一个向量参数θ

77
00:05:00,510 --> 00:05:03,175
θ0 θ1 θ2

78
00:05:03,175 --> 00:05:09,520
一直到θ_nx

79
00:05:09,520 --> 00:05:11,723
θ0取代了参数b

80
00:05:11,723 --> 00:05:13,663
这是一个实数

81
00:05:13,663 --> 00:05:18,505
而从θ1到θ_nx是代替了参数w

82
00:05:18,505 --> 00:05:20,350
而事实上 当实现神经网络时

83
00:05:20,350 --> 00:05:26,145
将b和w当做相互独立的参数会更加简单

84
00:05:26,145 --> 00:05:27,430
因此 在这门课程中

85
00:05:27,430 --> 00:05:32,087
我们不会使用红笔写的这种符号表示

86
00:05:32,087 --> 00:05:36,330
如果你没有在其他课程中看到过这种符号表示 不用担心

87
00:05:36,330 --> 00:05:39,610
这部分只是为了向那些看过这种符号的人明确说明

88
00:05:39,610 --> 00:05:43,730
我们不会在这节课中使用这种符号

89
00:05:43,730 --> 00:05:45,235
但如果你之前没有看过这种符号表示

90
00:05:45,235 --> 00:05:48,430
这儿讲的就不重要了 你没必要担心它

91
00:05:48,430 --> 00:05:52,465
因此 你现在已经知道逻辑回归是什么样子了

92
00:05:52,465 --> 00:05:57,140
接下来为了改变参数w和b 需要定义一个代价函数

93
00:05:57,140 --> 00:05:58,830
我们将在下一节课来讲解⏎GTC字幕组翻译