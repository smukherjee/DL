No último vídeo, você viu como calcular 
a previsão em uma rede neural, dado um único exemplo de treinamento. Neste vídeo, você verá como usar vetorização 
em múltiplos exemplos de treinamento. E o resultado será muito parecido com 
que você viu em regressão logística, em que, ao empilhar diversos exemplos 
de treinamento em diferentes colunas da matriz, você seria capaz de pegar as 
equações que você viu no vídeo anterior, e com pouquíssimas modificações, 
mudá-las para que a rede neural calcule os resultados de todos os 
exemplos ao mesmo tempo. Então, vamos ver os 
detalhes de como fazer isso. Essas eram as quatro equações que nós 
tínhamos no vídeo anterior de como calcular z'¹', a'¹', z'²' e a'²'. E eles lhe dizem como, dado um 
conjunto de características vetor x, você pode usá-los para gerar a'²' = ŷ 
para um único exemplo de treinamento. Agora, se você tem "m" exemplos de treinamento, 
você precisa repetir este processo para o primeiro exemplo de treinamento x⁽¹⁾
 para calcular ŷ⁽¹⁾ que é a previsão no seu 
primeiro exemplo de treinamento. Então, x⁽²⁾, usamos isso 
para gerar a previsão ŷ⁽²⁾. E assim por diante até x⁽m⁾ 
para gerar a previsão ŷ⁽m⁾. E então, em todas essas 
notações de ativação, eu vou escrever a'²'⁽¹⁾. E essa é a'²'⁽²⁾, e a'²'⁽m⁾. Então, essa notação a'²'⁽ⁱ⁾, o parêneses ⁽ⁱ⁾ refere-se 
ao exemplo de treinamento i, e o colchete '²' refere-se 
a camada 2. Ok. Então, é assim que funcionam os 
índices com parênteses e colchetes. E então, para sugerir que, se você tem 
uma implementação não-vetorizada e quer calcular as previsões de 
todos os exemplos de treinamento, você precisa fazer um laço de repetição:
for i = 1 to m Então, basicamente implementar 
essas quatro equações, certo? Você precisa fazer
z'¹'⁽ⁱ⁾ z'¹'⁽ⁱ⁾ = W'¹' x⁽ⁱ⁾ + b'¹' a'¹'⁽ⁱ⁾ = σ(z'¹'⁽ⁱ⁾). z'²'⁽ⁱ⁾ = w'²' a'¹'⁽ⁱ⁾ + b'²' z'²'⁽ⁱ⁾ = w'²' a'¹'⁽ⁱ⁾ + b'²'
a'²'⁽ⁱ⁾ = σ(z'²'⁽ⁱ⁾) a[2](i) = σ z([2](i)). Então, são basicamente essas quatro equações 
no topo com o i dentro dos parênteses para todas as variáveis 
dependentes dos exemplos de treinamento. Então, adicione o parênteses 
"i" para "x", "z" e "a" se quiser calcular todos os resultados 
nos seus "m" exemplos de treinamento. O que gostaríamos de fazer é vetorizar todo esse 
cálculo, para eliminarmos este laço de repetição "for...". E por falar isso, caso pareça que 
eu esteja trazendo muita álgebra linear complicada, resulta 
que saber implementar isso corretamente é importante na 
era da aprendizagem profunda. E nós, na verdade, escolhemos as notações 
com muito cuidado para esse curso e fizemos essas etapas de 
vetorização o mais fácil possível. Então, espero que essa chatice ajude-o a chegar mais rapidamente à 
implementação correta desses algoritmos. Ok! Então deixe-me copiar este bloco 
inteiro de código para o próximo slide e então veremos como vetorizar isso. Então, aqui está o que obtivemos 
do slide anterior com o laço de repetição "for..."
nos nossos "m" exemplos. Então, recapitulando 
que definimos a matriz X = aos nossos exemplos de treinamento 
empilhadas nessas colunas, desta forma. Então, pegue os exemplos de 
treinamento e empilhe-os em colunas. Então, isso torna-se um "n" ou talvez uma matriz de 
dimensão "nₓ" por "m". Eu vou revelar o grande final e 
dizer o que você precisa fazer para ter uma implementação 
vetorizada deste laço "for...". Resulta que, o que você 
precisa fazer é calcular Z'¹' = W'¹' X + b'¹' A'¹' = σ(Z'¹'). Depois, Z'²' = W'²' A'¹' + b'²' Z'²' = W'²' A'¹' + b'²' então, A'²' = σ(Z'²'). Então, se quiser, a analogia é 
que passamos de vetores x para a matriz X (maiúscula) empilhando 
os vetores x (minúsculos) em diferentes colunas. Se você fizer a mesma 
coisa para zs, por exemplo, se você pegar z'¹'⁽¹⁾, 
z'¹'⁽²⁾, e assim por diante, e esses são todos vetores 
coluna, até z'¹'⁽m⁾, certo? Então, empilhe essa primeira 
quantidade m em colunas. Então, este empilhamento resulta na matriz Z'¹'. E, igualmente, você olha, 
digamos, esta quantidade e pega a'¹'⁽¹⁾, a'¹'⁽²⁾ 
e assim por diante e a'¹'⁽m⁾, e os empilha em colunas. Então, assim como 
fomos do x para o X, e do z para o Z. Este vai do a, que 
são vetores para esta matriz A'¹' está ali, e da mesma 
forma para Z'²' e A'²'. Bem, eles também são 
obtidos pegando esses vetores e os empilhando horizontalmente. E pegando esses vetores e 
os empilhando horizontalmente, de forma a obter Z'²' e A'²'. Uma das propriedades desta 
notação que pode ajudar você a pensar é que 
estas matrizes Z e A, horizontalmente, nós vamos indexá-las 
através dos exemplos de treinamento. Então, isso é porque o índice horizontal 
corresponde a diferentes exemplos de treinamento, quando você vai da esquerda para direita, 
você está varrendo as células de treinamento. E verticalmente, este índice vertical 
corresponde a diferentes nós na rede neural. Então, por exemplo, este nó, 
este valor no topo superior, no canto esquerdo superior 
da média corresponde à ativação da primeira unidade de posição 
do primeiro exemplo de treinamento. Um valor abaixo corresponde à 
ativação na segunda unidade oculta no primeiro exemplo de treinamento, então a terceira unidade na primeira 
amostra de treinamento e assim por diante. Então, conforme você faz a varredura, 
este é o seu índice do número de unidades ocultas. Em que, se você se mover horizontalmente, 
você irá partir da primeira unidade oculta. E o primeiro exemplo de treinamento 
para, agora, a primeira unidade oculta e a segunda amostra de treinamento, 
o terceiro exemplo de treinamento, e assim por diante até que este nó 
aqui corresponda à ativação da primeira unidade oculta no exemplo de treinamento 
final, o enésimo exemplo de treinamento. OK, então a matriz A horizontal vai 
sobre os diferentes exemplos de treinamento. E verticalmente, os diferentes 
índices na matriz A correspondem a diferentes unidades ocultas. E uma intuição semelhante aplica-se 
para a matriz Z, assim como para X onde a horizontal corresponde 
aos diferentes exemplos de treinamento, e verticalmente corresponde as 
diferentes caraterísticas de entrada que são os diferentes nós 
da camada de entrada da rede neural. Então, você já sabe como implementar 
estas equações em sua rede com vetorização através de múltiplos exemplos. No próximo vídeo quero mostrar a 
você um pouco mais sobre o porquê esta é uma implementação 
correta deste tipo de vetorização. Resulta que esta explicação será 
similar à que você já viu em regressão logística. Vamos para o próximo vídeo.
Tradução: Renato Barata Gomes | Revisão: Carlos Lage.