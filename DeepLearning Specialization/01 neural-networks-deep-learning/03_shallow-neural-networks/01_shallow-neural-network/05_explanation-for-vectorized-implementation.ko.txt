이전 강의에서 여러분에게 이전 비디오에서는, x 매트릭스에서 트레이닝 샘플이 가로로 쌓아져서, 신경망으로 이어지는 벡터화된 도입값을 알아낼 수 있었는데요, 이제 조금 더 구체적으로 저희가 이용한 공식이 어떻게 복수의 샘플에 대해 올바른 벡터화된 도입값을 주는지 그 타당성을 보겠습니다. 그러면 전 방향전파의 예제를 한번 살펴보겠습니다. 첫번째 트레이닝 샘플에 대해서, 여러분이 이 값 x1 더하기 b1 그리고 두번째 트레이닝 샘플에서, 이 값, x2 더하기 b2, 그리고 세번째 트레이닝 샘플은, 여기 3 더하기 b1이 됩니다. 이 슬라이드 내용을 간략하기 위해 일단 b는 무시할 것입니다. 공식을 조금 더 심플하게 하기 위해서, b를 0으로 만들 것입니다. 저희가 주장하는 바는, 이런 작은 변화에도 성립한다는 것입니다. b가 0이 아닌 경우에도 말이죠. 이렇게 슬라이드 내용을 심플하게 만들겠습니다. 그러면, w1은 어떤 매트릭스 이겠죠, 맞죠? 몇개의 줄로 이루어진 매트릭스 입니다. 그러면 여기 x1의 계산을 보면 얻게 되는 것은, w1 곱하기 x1 세로 벡터를 얻게 됩니다. 이렇게 그리죠. 비슷하게 여기 x2 벡터를 보면, 이 값을 갖고, 즉 w1곱하기 x2는 또 다른 세로 벡터를 줍니다. 맞죠? 이것을 z12라고 할 수 있겠죠. 마지막으로 x3을 보면, 여기서는 w1 곱하기 x3이구요, 이러면 세번째 세로 벡터를 주는데요, 이것은 z13입니다. 대문자 X인 트레이닝 세트를 고려하면, 이것은 저희의 모든 트레이닝 샘플을 쌓아서 만들어집니다. 그러므로 대문자 X 매트릭스는 x1 벡터를 x2에 쌓아 올리고, 그 다음에 x3에 쌓아올린 것입니다. 이것은 3개의 트레이닝 샘플만 있을때의 경우겠죠. 만약 더 있다고 하면, 아시겠지만, 이렇게 계속 가로로 쌓아 올려질 것입니다. 하지만 이제 x 매트릭스를 갖고 w를 곱하면, 매트릭스가 어떤 원리를 곱해지는지 생각해보면, 첫번째 세로줄은, 여기 보라색으로 그린 값과 같이 되구요. 두번째 세로 줄은 여기 그려진 4개의 값과 동일할 것입니다. 3번째 세로줄은 여기 그려진 오렌지색 값일 것입니다. 이 값은 당연히 z11이 세로 벡터로 표현된 값과 그 다음으로 z12가 세로 벡터로 표현된 값과, 그 다음으로 z13가 세로 벡터로 표현된 값이죠. 이것은 마찬가지로 3개의 트레이닝 샘플일 경우입니다. 만약에 더 많은 샘플이 있을 경우, 세로줄 벡터의 개수가 늘어나겠죠. 그러면 이것은 대문자 Z1 매트릭습니다. 이 내용이 우리가 이전에 어떻게 w1 곱하기 xi를 통해 1개의 트레이닝 샘플의 경우, z1i 값을 구하게 되었는지 이해에 도움이 됐으면 좋겠습니다. 복수의 트레이닝 샘플을 다른 줄에 쌓는 경우에는, 그에 상응하는 값이, 이렇게 줄별로 다른 z들의 값으로 나타납니다. 이 강의에서 직접 다루진 않겠지만 파이썬 broadcasting 통해서도 확인 가능합니다. 여기 값들을 다시 더하면, b들의 값 말이죠, 그 값들은 맞습니다. 파이썬 broadcasting 에서는 어떻게 되는 것이냐하면, bi의 값을 각각별로 더해줍니다. 여기 각각의 매트릭스 줄별로 말이죠. 이번 슬라이드에서는 z1이 w1x 더하기 b1가 되고, 이것이 첫번째 단계의 벡터화임을 보여줬는데요, 이전 슬라이드의 4가지 단계 중에서 말이죠. 하지만 비슷한 분석을 통해서 다른 나머지 단계도 똑같은 로직을 통해 즉, 줄에서 표현되는 입력값을 쌓아서, 공식에서 그에 상응하는 결과값을 찾게됩니다. 마지막으로, 이번 비디오에서 다룬 내용을 전체적으로 복습해보겠습니다. 이게 여러분의 신경망이라고 하면, 저희는 전 방향전파 을 도입하기 위해서는 그렇게 해야 된다고 했는데요, 트레이닝 예시를별로 i에서 m까지 말이죠. 그리고, 트레이닝 샘플을 줄별로 쌓자고 했습니다. 여기 z1, a1, z2, a2에 해당하는 값에 대해서요. 그리고 줄별로 쌓았습니다. 이것은 a1에 대한 예제인데요, z1, a1, 그리고 a2에도 마찬가지로 적용됩니다. 이전 슬라이드에서 그 다음으로 보여준 것은 이 라인이 모든 m 샘플에 거쳐서 동시에 벡터화 시킬 수 있다는 것입니다. 비슷한 로직으로, 여기 이 4줄의 코딩에 대한 벡터화 값이 모두 옳은 값이라는 것을 확인할 수 있습니다. 다시 말씀드리자면, x의 값은 a0입니다. 입력 특성 벡터 x가 a0이고, xi는 a0i입니다. 여기 이 공식들이 비슷한 점이 있는데요, 첫번째 공식은 z1 = w1 a0 더하기 b1으로 쓰일 수 있습니다. 그러면 보이시겠지만 여기 이 공식과 여기 이 공식이 굉장히 비슷합니다. 여기 인덱스 지수 수치가 1의 차이를 보이는 것
외에는요. 이것을 통해 저희는 신경망의 다른 층들이 사실은 비슷한 활동을 한다는 것을 알 수 있습니다. 똑같은 산출을 반복적으로 하는 것이지요. 여기는 two-layer 신경망이 있는데요, 다음주 비디오에서 이러한 심층신경망에 대해 배우겠습니다. 이런 깊은 신경 네트워크도 사실 이와 같은 2가지의 단계를 거쳐서 단순히 그 횟수를 늘리는 것입니다. 자 그럼, 이번 강의에서 배웠듯이 여러 복수 트레이닝 샘플에서 신경망을 
벡터화시키는 방법은 이렇습니다. 현재까지는, 신경망에서 시그모이드 함수를 이용하는 것을 배웠었는데요, 이것이 가장 좋은 선택이라고는 할 수 없겠습니다. 다음 비디오에서는, 조금 다른 내용에 대해, 시그모이드 함수가 일부이기도 한, activation 함수에 대해 더욱 자세히 알아보겠습니다.