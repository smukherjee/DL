1
00:00:00,000 --> 00:00:03,090
深層学習の裏にある基本的な考え方が

2
00:00:03,090 --> 00:00:06,162
何十年も前からあったのなら、

3
00:00:06,162 --> 00:00:08,040
なぜ今になって
これほど人気が出たのでしょうか。

4
00:00:08,040 --> 00:00:10,290
この動画では、

5
00:00:10,290 --> 00:00:14,040
何が深層学習の繁栄に貢献したのかについて
いくつかお話しします。

6
00:00:14,040 --> 00:00:19,990
これは、あなたが実際にこれらのツールを使う機会を
見つける助けになるはずです。

7
00:00:19,990 --> 00:00:22,540
この二、三年、私は多くの人から
このような質問を受けました。

8
00:00:22,540 --> 00:00:25,735
「Andrew、何で深層学習が
急にうまく行きだしたの？」

9
00:00:25,735 --> 00:00:27,340
こう聞かれたとき、

10
00:00:27,340 --> 00:00:30,090
私はたいてい次のような図を描きます。

11
00:00:30,090 --> 00:00:33,300
水平方向の軸が私たちが持っているデータの量を示す

12
00:00:33,300 --> 00:00:37,025
というグラフを描いてみます。

13
00:00:37,025 --> 00:00:43,090
そして垂直方向の軸は、全ての学習アルゴリズムの
パフォーマンスを表します。

14
00:00:43,090 --> 00:00:46,225
これは、スパム分類の正確さだったり、

15
00:00:46,225 --> 00:00:48,700
広告クリック予測機能の性能だったり、

16
00:00:48,700 --> 00:00:51,820
自動運転車が他の車の場所を推測するための

17
00:00:51,820 --> 00:00:55,860
ニューラルネットワークの
正確さだったりというものです。

18
00:00:55,860 --> 00:00:59,570
昔からあった学習アルゴリズム、
例えばロジスティック回帰などの

19
00:00:59,570 --> 00:01:01,540
パフォーマンスを

20
00:01:01,540 --> 00:01:05,895
持っているデータの量の関数としてグラフにすると、

21
00:01:05,895 --> 00:01:09,155
このような曲線になるでしょう。

22
00:01:09,155 --> 00:01:12,510
データを増やすと
しばらくパフォーマンスは上がりますが、

23
00:01:12,510 --> 00:01:16,774
それ以降はあまり変わりません。

24
00:01:16,774 --> 00:01:20,070
うまく描けませんでしたが、
水平な線だと思ってください。

25
00:01:20,070 --> 00:01:26,130
そんなにデータがたくさんあっても
どうしたらいいのかわからないという感じです。

26
00:01:26,130 --> 00:01:28,700
この社会では、

27
00:01:28,700 --> 00:01:31,640
いくつもの問題に対して、

28
00:01:31,640 --> 00:01:35,295
20年ほど前は比較的少ないデータ量しか
なかったのですが、

29
00:01:35,295 --> 00:01:39,370
今ではかなり多くのデータを
持つことができるようになりました。

30
00:01:39,370 --> 00:01:42,890
そのほとんどは、デジタル化のおかげです。

31
00:01:42,890 --> 00:01:47,450
今日、人間の活動の多くがデジタルの領域にあります。

32
00:01:47,450 --> 00:01:50,270
コンピュータやウェブサイト、モバイルのアプリで
多くの時間を費やしますし、

33
00:01:50,270 --> 00:01:56,120
そのようなデジタルデバイスでの活動は
データを作り出します。

34
00:01:56,120 --> 00:02:01,445
携帯に備え付けられた
安いカメラや加速度センサーなど、

35
00:02:01,445 --> 00:02:05,349
「モノのインターネット」における
あらゆる種類のセンサーのおかげで、

36
00:02:05,349 --> 00:02:09,140
更にもっと多くのデータを
集めることができています。

37
00:02:09,140 --> 00:02:12,080
ここ20年間にわたって、

38
00:02:12,080 --> 00:02:15,380
従来の学習アルゴリズムが
効率的に活用できる範囲を超えた

39
00:02:15,380 --> 00:02:19,330
膨大な量のデータを蓄積できるようになったのです。

40
00:02:19,330 --> 00:02:20,960
ニューラルネットワークについて見てみます。

41
00:02:20,960 --> 00:02:24,550
小さいニューラルネットワークを訓練すれば

42
00:02:24,550 --> 00:02:27,294
パフォーマンスはこのようになります。

43
00:02:27,294 --> 00:02:30,779
もう少し大きなニューラルネットワークを
訓練すれば、

44
00:02:30,779 --> 00:02:33,649
これを中サイズの
ニューラルネットワークと呼びますね、

45
00:02:33,649 --> 00:02:36,995
そのパフォーマンスは少し良くなります。

46
00:02:36,995 --> 00:02:39,560
そしてとても大きなニューラルネットワークを
訓練すれば、

47
00:02:39,560 --> 00:02:43,230
そのパフォーマンスは大抵どんどん良くなります。

48
00:02:43,230 --> 00:02:44,844
ここから、いくつかのことがわかります。

49
00:02:44,844 --> 00:02:50,140
まず、高レベルのパフォーマンスを得たければ、
二つのことが必要だということです。

50
00:02:50,140 --> 00:02:53,120
一つは、膨大な量のデータを活用できるような

51
00:02:53,120 --> 00:02:58,355
十分な大きさのニューラルネットワークを
訓練できなければなりません。

52
00:02:58,355 --> 00:03:01,220
二つ目に、
x軸でこの位置にいなければなりません。

53
00:03:01,220 --> 00:03:03,520
つまり、多くの量のデータが必要なのです。

54
00:03:03,520 --> 00:03:08,150
「スケール」が深層学習の進歩を導いた
とよく言います。

55
00:03:08,150 --> 00:03:10,855
「スケール」というのは、

56
00:03:10,855 --> 00:03:14,595
ニューラルネットワークの大きさ、
つまり隠れ層が多くて

57
00:03:14,595 --> 00:03:15,815
多くのパラメータがあり、

58
00:03:15,815 --> 00:03:20,640
多くの繋がりがあるニューラルネットワークに加えて、
データの量のことも指しています。

59
00:03:20,640 --> 00:03:24,620
実際、今日ニューラルネットワークにおいて
良いパフォーマンスを得るための最も確実な方法は、

60
00:03:24,620 --> 00:03:29,576
より大きなネットワークを訓練するか
データを増やすかであることが多いです。

61
00:03:29,576 --> 00:03:33,230
それもある程度で終わってしまいます、
なぜならデータが尽きるか

62
00:03:33,230 --> 00:03:37,110
ニューラルネットワークが大きいため
訓練時間が長くなりすぎるので。

63
00:03:37,110 --> 00:03:42,540
しかし、ただスケールを改善するということが
深層学習においては非常に重要なのです。

64
00:03:42,540 --> 00:03:46,425
この図をもう少し技術的に正確にするために、

65
00:03:46,425 --> 00:03:48,405
少し付け足させてください。

66
00:03:48,405 --> 00:03:51,065
x軸にデータの量を書きました。

67
00:03:51,065 --> 00:03:57,530
専門的に言うと
これはラベル付けされたデータ、

68
00:03:57,530 --> 00:03:59,211
つまり入力であるxとラベルyの両方を持つ

69
00:03:59,211 --> 00:04:02,600
教師サンプルのデータを意味しています。

70
00:04:02,600 --> 00:04:06,500
このコースで後ほど使うことになる
いくつかの表記法を紹介します。

71
00:04:06,500 --> 00:04:11,921
教師サンプルの数を小文字のmで表します。

72
00:04:11,921 --> 00:04:17,180
すると水平方向の軸がmとなります。

73
00:04:17,180 --> 00:04:19,825
この図についてもう少し詳しく言うと、

74
00:04:19,825 --> 00:04:23,375
ここの教師サンプルが少ない範囲では、

75
00:04:23,375 --> 00:04:28,720
アルゴリズムの順番は
はっきりとは決まっていません。

76
00:04:28,720 --> 00:04:32,120
なので、もし教師サンプルが少なければ

77
00:04:32,120 --> 00:04:36,500
パフォーマンスは、あなたが特徴量を
どれだけうまく操作できるかにかかっています。

78
00:04:36,500 --> 00:04:42,290
もしSVMを使っていて
特徴量をよりうまく操作している人と

79
00:04:42,290 --> 00:04:44,810
大きなニューラルネットワークを
使っている人がいれば、

80
00:04:44,810 --> 00:04:48,640
この教師サンプルが少ない範囲の中では

81
00:04:48,640 --> 00:04:50,270
SVMの方がうまくいくということも
ありえます。

82
00:04:50,270 --> 00:04:53,570
なので、図の左側のこの領域では、

83
00:04:53,570 --> 00:04:58,490
アルゴリズム同士の相対的な位置は
それほどしっかり決まっていませんし、

84
00:04:58,490 --> 00:05:00,500
パフォーマンスはあなたの特徴量を操作する技術と

85
00:05:00,500 --> 00:05:03,950
アルゴリズムの細かい部分を操作する力に
かかっています。

86
00:05:03,950 --> 00:05:07,130
この教師サンプルが大きい領域、

87
00:05:07,130 --> 00:05:08,190
つまりmが大きくなる右側の領域において

88
00:05:08,190 --> 00:05:11,824
はじめて大きなニューラルネットワークが

89
00:05:11,824 --> 00:05:16,830
一貫して強くなるのです。

90
00:05:16,830 --> 00:05:18,800
もしあなたが友だちに

91
00:05:18,800 --> 00:05:20,744
「どうしてニューラルネットワークは
こんなに成功しているの？」と聞かれたら

92
00:05:20,744 --> 00:05:24,320
ぜひこの図を友だちにも
描いてあげてくださいね。

93
00:05:24,320 --> 00:05:27,170
深層学習の発展の初期には、

94
00:05:27,170 --> 00:05:28,865
大切なのは

95
00:05:28,865 --> 00:05:32,885
データのスケールと計算のスケールでした。

96
00:05:32,885 --> 00:05:36,890
CPUやGPUで大きなニューラルネットワークを
訓練できるというだけで

97
00:05:36,890 --> 00:05:40,995
大きな発展を遂げることができたのです。

98
00:05:40,995 --> 00:05:43,995
しかし、特にここ数年で、

99
00:05:43,995 --> 00:05:47,595
アルゴリズムの凄まじい革新も起こっています。

100
00:05:47,595 --> 00:05:50,320
そのことも軽視することはできません。

101
00:05:50,320 --> 00:05:54,860
興味深いことに、
アルゴリズムの革新の多くは

102
00:05:54,860 --> 00:06:00,440
ニューラルネットワークをもっと速く
動作させようとするものです。

103
00:06:00,440 --> 00:06:02,375
具体的な例としては、

104
00:06:02,375 --> 00:06:05,840
ニューラルネットワークにおける
大きな進歩の一つは

105
00:06:05,840 --> 00:06:12,140
こんな形のシグモイド関数から
ReLU関数に変えたことです。

106
00:06:12,140 --> 00:06:17,480
ReLU関数については前の動画で触れましたね、
こういう形のものです。

107
00:06:17,480 --> 00:06:21,450
今から言うことについては、細かい意味が
わからなくても気にしないでください。

108
00:06:21,450 --> 00:06:25,280
シグモイド関数を機械学習で使うことによって
起こる問題の一つは、

109
00:06:25,280 --> 00:06:29,390
このような傾きがほぼ0の領域が

110
00:06:29,390 --> 00:06:31,430
あるということです。

111
00:06:31,430 --> 00:06:33,290
そのせいで、学習はとても遅くなってしまいます。

112
00:06:33,290 --> 00:06:36,049
なぜなら、
最急降下法を実装したときに勾配が0に近いと、

113
00:06:36,049 --> 00:06:41,285
パラメータはゆっくりとしか変わらないので
学習も遅くなるからです。

114
00:06:41,285 --> 00:06:46,430
それに対して、ニューラルネットワークの
活性化関数と呼ばれるものを

115
00:06:46,430 --> 00:06:51,550
Rectified Linear Unit、
略してReLU関数に変えることで、

116
00:06:51,550 --> 00:06:57,060
入力が正の場合は勾配は常に1になります。

117
00:06:57,060 --> 00:07:02,570
なので、勾配が徐々に0になっていく
というようなことがなくなります。

118
00:07:02,570 --> 00:07:05,697
ここの左側での線の傾きは0ですが、

119
00:07:05,697 --> 00:07:10,410
ただシグモイド関数からReLU関数に変えるだけで

120
00:07:10,410 --> 00:07:15,890
最急降下法と呼ばれるアルゴリズムは
とても速く動くようになるのです。

121
00:07:15,890 --> 00:07:20,495
これはアルゴリズムの革新の中でも
比較的シンプルな例です。

122
00:07:20,495 --> 00:07:22,130
しかしこの革新は最終的に、

123
00:07:22,130 --> 00:07:26,660
計算を大きく手助けしました。

124
00:07:26,660 --> 00:07:29,990
このように、速くコードを走らせるために

125
00:07:29,990 --> 00:07:33,420
アルゴリズムを変更した例は多くあります。

126
00:07:33,420 --> 00:07:34,700
そのおかげで、私たちは

127
00:07:34,700 --> 00:07:36,929
大きなニューラルネットワークや
多くのデータがあったとしても

128
00:07:36,929 --> 00:07:41,680
ニューラルネットワークを手頃な時間で
訓練できるようになったのです。

129
00:07:41,680 --> 00:07:46,490
速い計算が大切なもう一つの理由は、

130
00:07:46,490 --> 00:07:52,220
ニューラルネットワークを訓練する過程が
反復であるということです。

131
00:07:52,220 --> 00:07:53,520
まずニューラルネットワークの
構造のアイデアがあり、

132
00:07:53,520 --> 00:07:57,770
そのアイデアをコードにして実装します。

133
00:07:57,770 --> 00:08:00,670
そしてそれを実験的に実行し、

134
00:08:00,670 --> 00:08:03,875
それによってニューラルネットワークが
どれほどうまくいっているかがわかり、

135
00:08:03,875 --> 00:08:08,740
それを見てまたニューラルネットワークに
細かい変更を加えるために戻ります。

136
00:08:08,740 --> 00:08:12,675
そうやってこのサイクルを何度も繰り返します。

137
00:08:12,675 --> 00:08:16,415
ニューラルネットワークの訓練に
長い時間がかかれば、

138
00:08:16,415 --> 00:08:20,000
このサイクルを回すのにも
長い時間がかかります。

139
00:08:20,000 --> 00:08:21,830
アイデアが浮かんでからそれを試すのに

140
00:08:21,830 --> 00:08:26,030
10分かかるか一日かかるか、

141
00:08:26,030 --> 00:08:32,870
もしくはひと月かかるかでは、

142
00:08:32,870 --> 00:08:35,435
生産性に大きな違いが出てきます。

143
00:08:35,435 --> 00:08:40,040
実際、ひと月かかることもたまにあります。

144
00:08:40,040 --> 00:08:44,150
10分か一日で結果がわかれば、
より多くのアイデアを試すことができて、

145
00:08:44,150 --> 00:08:47,180
そのアプリケーションにおいて
うまくいくニューラルネットワークを

146
00:08:47,180 --> 00:08:50,890
発見できる可能性が高くなります。

147
00:08:50,890 --> 00:08:55,580
このように、速い計算は

148
00:08:55,580 --> 00:09:01,210
実験の結果を得るスピードを速くするのを
大いに助けてきました。

149
00:09:01,210 --> 00:09:05,795
これは、ニューラルネットワークを実践する人が、

150
00:09:05,795 --> 00:09:08,390
そして深層学習の研究者が

151
00:09:08,390 --> 00:09:14,025
速く反復し、速くアイデアを改善するのに
役立ってきました。

152
00:09:14,025 --> 00:09:15,600
そしてもちろん、これら全てが

153
00:09:15,600 --> 00:09:19,760
驚くべきことに
毎月のように新しいアルゴリズムを開発している

154
00:09:19,760 --> 00:09:24,365
深層学習の研究コミュニティ全体の力となりました。

155
00:09:24,365 --> 00:09:27,290
これらのこと全てが、

156
00:09:27,290 --> 00:09:30,680
驚くべきほど新しいアルゴリズムを発明し、

157
00:09:30,680 --> 00:09:33,618
日進月歩で発展している

158
00:09:33,618 --> 00:09:37,300
深層学習の研究コミュニティ全体にとって
大きな恩恵となっています。

159
00:09:37,300 --> 00:09:41,755
ここまでのことが、
深層学習を興隆させてきた要因です。

160
00:09:41,755 --> 00:09:44,270
素晴らしいことに、これらの要因は

161
00:09:44,270 --> 00:09:47,725
深層学習を更に改善するために
活発にはたらいています。

162
00:09:47,725 --> 00:09:52,115
データにしても、この社会は多くのデジタルデータを
生み出し続けています。

163
00:09:52,115 --> 00:09:54,230
計算にしても、GPUやより速いネットワーク、

164
00:09:54,230 --> 00:09:57,514
いろいろな種類のハードウェアなど、

165
00:09:57,514 --> 00:09:59,060
特化したハードウェアが盛んになっています。

166
00:09:59,060 --> 00:10:02,360
計算という点においては、

167
00:10:02,360 --> 00:10:03,870
私たちが巨大なニューラルネットワークを
構築する能力は

168
00:10:03,870 --> 00:10:07,130
上がり続けていると自信を持って言えます。

169
00:10:07,130 --> 00:10:11,260
アルゴリズムにしても、

170
00:10:11,260 --> 00:10:16,530
深層学習の研究コミュニティ全体の前線が
驚異的に進歩しているので、

171
00:10:16,530 --> 00:10:19,315
楽観的になっていいと思うのです。

172
00:10:19,315 --> 00:10:21,650
私自身も、深層学習はこれから何年も

173
00:10:21,650 --> 00:10:24,700
改善を続けるだろうと
楽観的に考えています。

174
00:10:24,700 --> 00:10:27,575
では、このセクションの最後の動画に行って

175
00:10:27,575 --> 00:10:31,000
このコースで何を学んだかについて
もう少しお話ししましょう。