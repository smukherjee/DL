1
00:00:00,000 --> 00:00:03,195
다시 환영합니다. vectorization은 기본적으로 

2
00:00:03,195 --> 00:00:07,315
코딩에서 명백한 for loop들을 제거하는 기술인데요

3
00:00:07,315 --> 00:00:11,835
딥러닝 세대에서, 특히 딥러닝 실행부분에서는 

4
00:00:11,835 --> 00:00:15,210
여러분은 비교적 큰 데이터세트에서 트레이닝 하는 것을 

5
00:00:15,210 --> 00:00:18,475
보실 텐데요, 그 이유는 그 분야에서 딥러닝 알고리즘이 특출 나기 때문입니다.

6
00:00:18,475 --> 00:00:22,790
그렇기 때문에 코딩을 빨리 진행하는 것이 매우 중요합니다.

7
00:00:22,790 --> 00:00:24,525
딥러닝이 빅 데이터세트에서 실행되기 때문에, 

8
00:00:24,525 --> 00:00:27,000
여러분의 코드가 실행하는데 오래 걸릴 수도 있는데요, 

9
00:00:27,000 --> 00:00:30,255
그렇게 되면 결과값을 기다리는데 오래 걸릴 수도 있게 됩니다.

10
00:00:30,255 --> 00:00:32,035
그러면 딥러닝 세대에서는, 

11
00:00:32,035 --> 00:00:37,490
제 생각에는 vectorization을 진행 할 수 있는 것이 핵심 기술인 것 같습니다. 예제를 보겠습니다.

12
00:00:37,490 --> 00:00:40,010
우선 예제를 봅시다.

13
00:00:40,010 --> 00:00:42,225
vectorization은 무엇일까요?

14
00:00:42,225 --> 00:00:48,780
로지스틱 회귀분석에서는 z = w transpose x 더하기 b를 계산해야 됩니다.

15
00:00:48,780 --> 00:00:55,405
w는 세로줄 벡터이고 x도 이런 벡터였죠. 

16
00:00:55,405 --> 00:00:58,000
여러 가지 특성이 있으면 벡터도 큰 것으로 이루어질 수 있겠죠

17
00:00:58,000 --> 00:01:07,080
그러면 w 와 x는 모두 여기 R 의 n이거나 R의 nx dimension의 벡터겠죠.

18
00:01:07,080 --> 00:01:10,170
그러면 w transpose x를 계산하자면, 

19
00:01:10,170 --> 00:01:15,660
여러분이 만약 non-vectorized 도입을 했다면, 

20
00:01:15,660 --> 00:01:18,725
z 가 0인 것으로 했을 것입니다.

21
00:01:18,725 --> 00:01:24,860
그리고 i는 n-x의 범위를 갖도록 하구요

22
00:01:24,860 --> 00:01:27,330
즉, i 가 1에서 nx까지 인 것이죠, 

23
00:01:27,330 --> 00:01:34,040
z 플러스는 wi 곱하기 xi입니다.

24
00:01:34,040 --> 00:01:37,100
그리고 z 플러스 b를 끝에 할 수도 있겠죠.

25
00:01:37,100 --> 00:01:39,855
이것이 non-vectorized 도입 방식인데요, 

26
00:01:39,855 --> 00:01:43,090
이렇게 하면 매우 느릴 것입니다.

27
00:01:43,090 --> 00:01:48,560
반대로, vectorized 된 도입은, w transpose x를 직접 계산합니다.

28
00:01:48,560 --> 00:01:52,085
파이썬이나 넘피에서 말이죠, 

29
00:01:52,085 --> 00:02:01,428
쓰는 명령은 z = np 점 (w, x)인데요 

30
00:02:01,428 --> 00:02:06,270
이 값은 w tanspose x입니다.

31
00:02:06,270 --> 00:02:09,075
그리고 직접 b를 여기다 더할 수도 있습니다.

32
00:02:09,075 --> 00:02:12,400
이것이 더 빠른 것을 느끼실 텐데요, 

33
00:02:12,400 --> 00:02:17,075
데모를 통해 한번 묘사해보겠습니다. 

34
00:02:17,075 --> 00:02:21,960
이것은 Jupiter notebook인데요, 여기다가 파이썬 코드를 적어보겠습니다.

35
00:02:21,960 --> 00:02:28,041
처음으로는, 넘피 library를 불러오겠습니다.

36
00:02:28,041 --> 00:02:30,000
as np로 말이죠, 예를 들어, 

37
00:02:30,000 --> 00:02:36,570
A를 작성해서, array로 말이죠, 

38
00:02:36,570 --> 00:02:39,560
그리고 print a를 타입 해보겠습니다.

39
00:02:39,560 --> 00:02:41,160
그럼 여기 코드를 적었으니, 

40
00:02:41,160 --> 00:02:43,170
이제 shit + enter를 누르면, 

41
00:02:43,170 --> 00:02:44,847
코드를 실행합니다.

42
00:02:44,847 --> 00:02:47,970
이제 array A를 생성했고 print out 합니다.

43
00:02:47,970 --> 00:02:50,580
이제 vectorization 데모도 한번 해보겠습니다.

44
00:02:50,580 --> 00:02:51,990
이제 time library를 불러올 텐데요, 

45
00:02:51,990 --> 00:02:53,580
이것을 이용하기 때문이죠, 

46
00:02:53,580 --> 00:02:56,565
다른 운영작업이 얼마나 걸리는지 시간을 재기 위해서 말이죠. 

47
00:02:56,565 --> 00:02:59,139
array A를 생성할 것입니다. 

48
00:02:59,139 --> 00:03:02,905
이것은 random 점 rand 이고...

49
00:03:02,905 --> 00:03:10,065
이제 백만 다이멘션을 만들어보겠습니다.

50
00:03:10,065 --> 00:03:13,300
b = np.random.rand.입니다. 

51
00:03:13,300 --> 00:03:16,120
그리고 또 하나의 백만 개 array이고요, 

52
00:03:16,120 --> 00:03:20,810
이제 tic=time.time인데요, 이것은 현재시간을 측정합니다.

53
00:03:20,810 --> 00:03:26,395
c = np.dot (a, b)이고요, 

54
00:03:26,395 --> 00:03:28,649
toc은 time.time입니다.

55
00:03:28,649 --> 00:03:31,950
이제 프린트 해볼까요? 

56
00:03:31,950 --> 00:03:34,857
이것은 vectorized 버전인데요, 

57
00:03:34,857 --> 00:03:37,685
vectorized version을 타이핑하겠습니다.

58
00:03:37,685 --> 00:03:41,985
그러면 이제 이전 것을

59
00:03:41,985 --> 00:03:45,060
print out 할 텐데요, 

60
00:03:45,060 --> 00:03:48,330
이것은 toc 빼기 tic 곱하기 100인데요

61
00:03:48,330 --> 00:03:52,075
이렇게 해서 밀리세컨즈로 표현하겠습니다.

62
00:03:52,075 --> 00:03:54,075
ms가 밀리세컨즈입니다.

63
00:03:54,075 --> 00:03:56,435
이제 shift+enter를 칠 것입니다.

64
00:03:56,435 --> 00:04:01,890
그러면 이 코드를 만드는 세 3 milliseconds가 걸렸거나 이번 경우 1.5인데요, 

65
00:04:01,890 --> 00:04:06,170
한번에 1.5 또는 3.5 밀리세컨즈 소요됩니다.

66
00:04:06,170 --> 00:04:08,370
제가 실행하면서 조금씩 다르긴 한데요, 

67
00:04:08,370 --> 00:04:12,085
평균적으로는 1.5 밀리세컨즈가 소요되는 것 같습니다.

68
00:04:12,085 --> 00:04:15,665
이것을 실행하면 2 밀리세컨즈 소요될 수 있죠. 자 그럼, 

69
00:04:15,665 --> 00:04:16,967
좋아요.

70
00:04:16,967 --> 00:04:19,005
이런 코드 블럭을 계속 더해보겠습니다.

71
00:04:19,005 --> 00:04:22,270
이제는 non-vectorize 버전을 도입해보겠습니다.

72
00:04:22,270 --> 00:04:24,151
c=0이고요, 

73
00:04:24,151 --> 00:04:27,750
그리고 tic = time.time

74
00:04:27,750 --> 00:04:29,335
이제 공식을 도입해보겠습니다.

75
00:04:29,335 --> 00:04:35,348
I의 범위가 백만이고

76
00:04:35,348 --> 00:04:38,676
0의 개수를 잘 썼길 바랍니다.

77
00:04:38,676 --> 00:04:43,936
C += (a, i) x (b, i)이고요, 

78
00:04:43,936 --> 00:04:50,775
그리고 toc는 time.time입니다.

79
00:04:50,775 --> 00:04:57,725
마지막으로, print "for loop"을 칩니다. 

80
00:04:57,725 --> 00:05:15,225
소요되는 시간은 여기 1000 곱하기 tox - tic + "ms" 입니다.

81
00:05:15,225 --> 00:05:17,505
밀리세컨즈로 표기하기 위해 말이죠. 

82
00:05:17,505 --> 00:05:19,735
한가지만 더 하겠습니다.

83
00:05:19,735 --> 00:05:22,802
여기 C의 값을 print out 하겠습니다.

84
00:05:22,802 --> 00:05:27,960
저희가 계산한 값을 print out해서 2가지의 경우 모두 동일한 값임을 확인하겠습니다.

85
00:05:27,960 --> 00:05:35,770
저는 이제 shift + enter를 눌러서 실행하고 확인해보겠습니다.

86
00:05:35,770 --> 00:05:38,305
두 가지 경우 모두, vectorized 버전과 

87
00:05:38,305 --> 00:05:41,125
non-vectorized version모두 동일한 값을 산출했습니다.

88
00:05:41,125 --> 00:05:45,355
여러분도 알다시피 2.5에서 6.99 등등 말이죠 

89
00:05:45,355 --> 00:05:48,670
vectorized 버전은 1.5 밀리세컨즈가 소요됐고

90
00:05:48,670 --> 00:05:57,555
explicit for loop non-vectorized 버전은 400, 거의 500밀리세컨즈 소요됐습니다.

91
00:05:57,555 --> 00:06:01,285
이렇게 non-vectorize 버전이 거의

92
00:06:01,285 --> 00:06:05,660
300배 가까이 더 오래 걸렸습니다.

93
00:06:05,660 --> 00:06:11,230
이번 예제를 통해서 여러분이 코드만 vectorize 할 줄 알더라도

94
00:06:11,230 --> 00:06:15,120
300배는 더 빨리 실행된다는 것을 보았습니다.

95
00:06:15,120 --> 00:06:16,540
다시 한번 실행해보겠습니다.

96
00:06:16,540 --> 00:06:18,930
다시 한번이요. 

97
00:06:18,930 --> 00:06:22,235
예, 맞죠, vectorized 버전은 1.5 밀리세컨즈, 그리고 for loop

98
00:06:22,235 --> 00:06:25,960
의 경우 481 밀리세컨즈 입니다.

99
00:06:25,960 --> 00:06:29,535
거의 300배는 느리죠. for loop의 경우가요. 

100
00:06:29,535 --> 00:06:30,980
300배 느린 점의 차이는 

101
00:06:30,980 --> 00:06:33,880
여러분의 코드가 1분이 걸리거나 

102
00:06:33,880 --> 00:06:37,615
5시간이 걸리는 차이이기도 합니다.

103
00:06:37,615 --> 00:06:41,410
그리고 여러분이 딥러닝 알고리즘을 도입하는 경우에

104
00:06:41,410 --> 00:06:43,300
결과값을 훨씬 더 빨리 알아낼 수 있습니다.

105
00:06:43,300 --> 00:06:46,590
code를 vectorize하는 경우 훨씬 더 빠를 것입니다.

106
00:06:46,590 --> 00:06:49,300
‘Scalable Deep Learning’ 도입이 GPU 또는 Graphic Processing Unit에서

107
00:06:49,300 --> 00:06:54,260
이루어진다는 것을 들으셨을 것입니다.

108
00:06:54,260 --> 00:06:59,515
하지만 제가 the Jupiter notebook에서 방금 한 데모들은 모두 CPU에서 한 것입니다.

109
00:06:59,515 --> 00:07:04,530
알고 보면 CPU와 GPU 모두 parallelization instruction이 있습니다.

110
00:07:04,530 --> 00:07:07,530
가끔씩 이런 것을SIMD instructions이라 합니다.

111
00:07:07,530 --> 00:07:11,190
이것은 single instruction multiple data의 약자입니다.

112
00:07:11,190 --> 00:07:13,045
기본적으로 무슨 뜻이냐면, 

113
00:07:13,045 --> 00:07:16,835
이런 빌트인 기능을 이용하면, 

114
00:07:16,835 --> 00:07:23,495
np. function 이나 다른 for loop의 도입이 필요 없는 기능을요, 

115
00:07:23,495 --> 00:07:28,150
그런 경우, 파이썬 Pi 가 

116
00:07:28,150 --> 00:07:33,640
parallelism을 활용할 수 있게 계산을 빨리 처리하도록 해줍니다.

117
00:07:33,640 --> 00:07:38,610
이러한 산출에 관련한 내용은 CPU와 GPU에서 모두 적용되고요. 

118
00:07:38,610 --> 00:07:41,070
하지만 GPU가 특별히

119
00:07:41,070 --> 00:07:44,980
SIMD calculations 에 뛰어나기 때문에 그렇습니다. 하지만 CPU도 나쁘지 않습니다.

120
00:07:44,980 --> 00:07:47,510
GPU보단 조금 못하더라도 말이죠. 

121
00:07:47,510 --> 00:07:51,660
여러분은 어떻게 vectorizarion이 여러분의 코드의 속도를 현저하게 높일 수 있는지 보실 수 있습니다

122
00:07:51,660 --> 00:07:54,685
기억하실 경험 의거 규칙은 가능하면, 

123
00:07:54,685 --> 00:07:57,425
explicit for loop을 사용하는 것을 피하라는 것입니다.

124
00:07:57,425 --> 00:07:59,980
다음 비디오로 넘어가서

125
00:07:59,980 --> 00:08:04,000
더 많은 vectorization 예제를 살펴보고 로지스틱 회귀분석을 vectorization하는 방법을 배우겠습니다.