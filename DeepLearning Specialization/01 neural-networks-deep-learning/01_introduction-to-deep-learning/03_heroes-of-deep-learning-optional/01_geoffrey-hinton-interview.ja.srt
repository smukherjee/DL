1
00:00:00,620 --> 00:00:03,610
deeplearning.ai のコースの一部として

2
00:00:03,610 --> 00:00:07,590
深層学習の技術的な考えを
教えるだけではなく

3
00:00:07,590 --> 00:00:11,658
深層学習の英雄たちを
紹介したいと思っています

4
00:00:11,658 --> 00:00:13,160
このスペシャリゼーションや

5
00:00:13,160 --> 00:00:17,700
このコースで学ぶ考えの多くを
発明した人たちです

6
00:00:17,700 --> 00:00:21,420
これらのビデオでは
深層学習のリーダーに

7
00:00:21,420 --> 00:00:24,990
キャリアの助言をもらえるようにしたいです
深層学習に参入する方法や

8
00:00:24,990 --> 00:00:27,805
深層学習の研究を行ったり
仕事が見つけられるようにする方法を聞きます

9
00:00:27,805 --> 00:00:30,156
このインタビューシリーズの
初回として

10
00:00:30,156 --> 00:00:34,228
ジェフリー・ヒントンさんをお迎えし
紹介するのを光栄に思います

11
00:00:38,427 --> 00:00:44,150
ようこそジェフ
deeplearning.ai のインタビューを受けてくれてありがとう

12
00:00:44,150 --> 00:00:46,550
お招きありがとうございます

13
00:00:46,550 --> 00:00:50,088
現時点ではこの地球上で誰よりも
深層学習を支える多くのアイデアを

14
00:00:50,088 --> 00:00:52,835
発明した人はいないように思います

15
00:00:52,835 --> 00:00:57,650
多くの人が深層学習のゴッドファーザーと
呼んでると思っていました

16
00:00:57,650 --> 00:01:01,529
しかし数分前にお話して初めて
私が最初にそう呼んだと

17
00:01:01,529 --> 00:01:05,600
分かりました
そう呼んでみてよかったです

18
00:01:06,780 --> 00:01:11,320
多くの人が
あなたを伝説として見ています

19
00:01:11,320 --> 00:01:15,030
わたしが聞きたいのは
伝説の裏のあなた自身のお話です

20
00:01:15,030 --> 00:01:19,980
ずっと昔を振り返ってみて
どんな経緯でAIや機械学習やニューラルネットワークに

21
00:01:19,980 --> 00:01:21,520
関わることになったのですか

22
00:01:22,730 --> 00:01:26,960
高校生のときに
同級生がいて

23
00:01:26,960 --> 00:01:31,220
何についても私より優れていて
優秀な数学者でした

24
00:01:31,220 --> 00:01:37,010
ある日彼が学校にきて
「脳がホログラムを使っているって知ってる」と聞いてきました

25
00:01:38,190 --> 00:01:44,161
たしか1966年頃だと思うのですが
私は「ホログラムって何？」という感じでした

26
00:01:44,161 --> 00:01:47,390
ホログラムがあると
脳を半分切り離しても全体像がまだ残ると

27
00:01:47,390 --> 00:01:49,730
説明してくれました

28
00:01:49,730 --> 00:01:53,466
脳にある記憶は
脳全体に分散されていると言いました

29
00:01:53,466 --> 00:01:56,022
彼はラシュリーの実験を
読んだのだと思います

30
00:01:56,022 --> 00:01:57,939
実験では
ラットの脳の一部を切り離し

31
00:01:57,939 --> 00:02:01,740
ある部分が特定の記憶を保管している場所を
特定するのが非常に困難と発見した実験です

32
00:02:04,411 --> 00:02:08,920
このことをきっかけに
脳がどう記録を保管するのかに興味を持ちました

33
00:02:10,180 --> 00:02:12,220
そして大学に進んだ時に

34
00:02:12,220 --> 00:02:15,130
生理学と物理学を
学び始めました

35
00:02:16,400 --> 00:02:17,731
ケンブリッジ大学では

36
00:02:17,731 --> 00:02:20,260
この両方を勉強していたのは
学部では私だけでした

37
00:02:21,888 --> 00:02:25,270
その後これは諦めて

38
00:02:25,270 --> 00:02:29,170
哲学を学びました
より洞察が得られると思ったためです

39
00:02:29,170 --> 00:02:32,780
ですが
実は

40
00:02:32,780 --> 00:02:37,130
哲学では事実と違うことがあっても
分からないように思えました

41
00:02:37,130 --> 00:02:39,420
そして心理学に転向し

42
00:02:41,988 --> 00:02:45,920
心理学では
非常に単純化された理論が多く

43
00:02:45,920 --> 00:02:49,620
脳の動作を説明するには
絶望的に不適切と言ってもいいものでした

44
00:02:49,620 --> 00:02:52,737
その後少しのんびりして
大工になったあとに

45
00:02:52,737 --> 00:02:57,169
AIで試してみることにして
エジンバラでランガー・ヒギンスと

46
00:02:57,169 --> 00:02:59,580
AIを研究しました

47
00:02:59,580 --> 00:03:02,662
彼はとても良いニューラルネットワークの
研究をしていましたが

48
00:03:02,662 --> 00:03:07,830
ちょうど諦めていたところでした
私はウィノグラードの論文に衝撃を受けていました

49
00:03:07,830 --> 00:03:11,460
私が合流したときに
彼はわたしが古いことをしていると考えていましたが

50
00:03:11,460 --> 00:03:14,210
私はシンボリックAIを
始めなければと考えました

51
00:03:14,210 --> 00:03:18,210
かなり戦いましたが
信じたものをただただ続けました

52
00:03:18,210 --> 00:03:21,138
その後どうなりました

53
00:03:21,138 --> 00:03:28,033
最終的にAIで博士号をとり
イギリスでは仕事がありませんでしたが

54
00:03:28,033 --> 00:03:30,979
カリフォルニアのスローン・フェローシップの
素晴らしい広告を見て

55
00:03:30,979 --> 00:03:36,070
なんとかこれに
入ることができました

56
00:03:36,070 --> 00:03:40,625
カリフォルニアに行きましたが
何もかもが異なってました

57
00:03:40,625 --> 00:03:46,685
イギリスではニューラルネットワークは
馬鹿げたことだと見なされていましたが

58
00:03:46,685 --> 00:03:50,272
カリフォルニアでは
ドナルド・ノーマンや

59
00:03:50,272 --> 00:03:56,640
デビッド・ラメルハートは
ニューラルネットワークの考えにとてもオープンでした

60
00:03:56,640 --> 00:04:00,720
脳がどう機能しているのか
これが心理学と関連がある可能性を考えることを

61
00:04:00,720 --> 00:04:03,290
非常に有益なことと捉えられる所に来たのは

62
00:04:03,290 --> 00:04:05,650
初めてでした

63
00:04:05,650 --> 00:04:06,936
とても楽しかったです

64
00:04:06,936 --> 00:04:09,792
デビッド・ラメルハートと協力するのは
特に素晴らしかったです

65
00:04:09,792 --> 00:04:12,968
なるほど
1982年にカリフォルニア大学サンディエゴ校に

66
00:04:12,968 --> 00:04:16,177
ラメルハートさんといて

67
00:04:16,177 --> 00:04:20,182
誤差逆伝播法の学会論文を
書き上げたのですね

68
00:04:20,182 --> 00:04:23,292
実はもっと複雑でした

69
00:04:23,292 --> 00:04:24,796
なにがありましたか

70
00:04:24,796 --> 00:04:28,214
1982年初めのころ

71
00:04:28,214 --> 00:04:32,900
デビッド・ラメルハートと
ロナルド・ウィリアムスと私で

72
00:04:32,900 --> 00:04:37,967
誤差逆伝播法のアルゴリズムを
開発しました

73
00:04:37,967 --> 00:04:42,291
主としてデビッド・ラメルハートの考えでした

74
00:04:42,291 --> 00:04:46,390
後になって
他にも多くの人が発明していたことを発見しました

75
00:04:46,390 --> 00:04:52,798
デビット・パーカーがそうです
行ったのは後でしたが私たちの発表よりも先でした

76
00:04:52,798 --> 00:04:56,425
ポール・ワーボスは
数年前にすでに発表していましたが

77
00:04:56,425 --> 00:04:58,860
注目されませんでした

78
00:04:58,860 --> 00:05:01,923
他にもよく似たアルゴリズムを
開発した人たちがいましたが

79
00:05:01,923 --> 00:05:04,340
誤差逆伝播法が何を意味するのか
明らかではありませんでした

80
00:05:04,340 --> 00:05:08,055
連鎖律を使って
微分を出すのは新しいアイデアではありませんでした

81
00:05:08,055 --> 00:05:12,484
なるほど
なぜあなたの論文だけが

82
00:05:12,484 --> 00:05:15,940
コミュニティが理解するのに
そんなに貢献したのでしょうか

83
00:05:15,940 --> 00:05:20,540
相手がだれにしてもこのアルゴリズムが受け入れられたのは
まるで論文が伝染するかのよう

84
00:05:20,540 --> 00:05:22,934
です

85
00:05:22,934 --> 00:05:26,675
1986年にネイチャーに
発表することができました

86
00:05:26,675 --> 00:05:30,580
論文が受け入れられるように
政治的な仕事を随分としました

87
00:05:30,580 --> 00:05:34,622
審査員の１人が有名な心理学者の
スチュアート・サザーランドだろうと

88
00:05:34,622 --> 00:05:36,992
探りあてたので

89
00:05:36,992 --> 00:05:38,815
話しに行き
長い時間を使って

90
00:05:38,815 --> 00:05:41,480
彼に実際にどうなっているかを
説明しました

91
00:05:41,480 --> 00:05:44,140
彼は非常に感銘を受けていました

92
00:05:44,140 --> 00:05:48,970
誤差逆伝播法が
言葉の表現を学ぶことができる事実にです

93
00:05:48,970 --> 00:05:52,490
ベクトルであるこれらの表現を見ることができて

94
00:05:52,490 --> 00:05:55,950
個々の特徴量の意味を理解できます

95
00:05:55,950 --> 00:06:01,600
実際には家系図の中の３連の語を
学習させました

96
00:06:01,600 --> 00:06:06,420
例えばマリーには
ビクトリアという母がいるなどです

97
00:06:06,420 --> 00:06:11,550
最初の２語を与えて
最後の１語を予測するものでした

98
00:06:11,550 --> 00:06:12,970
学習させると

99
00:06:12,970 --> 00:06:17,780
個々の言葉を表現によって
特徴量を見ることができました

100
00:06:17,780 --> 00:06:19,950
人の国籍があったり

101
00:06:19,950 --> 00:06:25,180
何世代目か どの家系の子孫なのかが
特徴量で分かりました

102
00:06:25,180 --> 00:06:27,680
そのことによって
彼はこの事実に本当に感銘を受けました

103
00:06:27,680 --> 00:06:29,666
これが論文が
受け入れられた理由だと思います

104
00:06:29,666 --> 00:06:33,905
非常に早期の分散表現ですね
既に学習アルゴリズムから

105
00:06:33,905 --> 00:06:38,390
表われる学習した意味の特徴量を
見れたのですね

106
00:06:38,390 --> 00:06:44,090
心理学者の視点からすると
知識がどのようなものかという

107
00:06:44,090 --> 00:06:49,740
全く異なる意見の考えが
統合されたことを興味深いものでした

108
00:06:49,740 --> 00:06:53,460
つまり古い心理学者は
概念は大量の特徴量の塊という見解で

109
00:06:53,460 --> 00:06:56,810
この証拠は多くあります

110
00:06:56,810 --> 00:07:02,180
当時のAIの考えでは
かなり構造的な考えで

111
00:07:02,180 --> 00:07:06,190
概念というものは
お互いの概念がどう関連しているかというものでした

112
00:07:06,190 --> 00:07:09,820
概念をとらえるには
意味ネットワークやグラフ構造のようにする必要が

113
00:07:09,820 --> 00:07:11,640
ありました

114
00:07:11,640 --> 00:07:15,875
誤差逆伝播法の例が示したのは

115
00:07:15,875 --> 00:07:21,070
今回は家系図だったグラフ構造に
入るような情報は

116
00:07:22,080 --> 00:07:26,920
この情報を特徴量に変換でき
この特徴量は

117
00:07:26,920 --> 00:07:33,470
汎化のように一貫した情報を
取り出すのに使える形にできるということです

118
00:07:33,470 --> 00:07:38,438
重要なことは
家系図のグラフ表現と家系図を木構造表現との間での行き来で

119
00:07:38,438 --> 00:07:43,000
人々を表現したものが
巨大な特徴ベクトルによって

120
00:07:43,000 --> 00:07:46,715
表わされるということでした

121
00:07:46,715 --> 00:07:50,873
グラフのような表現から
特徴ベクトルに

122
00:07:50,873 --> 00:07:51,469
変換したければ、

123
00:07:51,469 --> 00:07:54,995
できて
特徴ベクトルからはグラフのような表現が得られるという事実です

124
00:07:54,995 --> 00:07:57,730
それは1986年ですか

125
00:07:57,730 --> 00:08:02,430
90年代の初期に
ベンジオが実データを

126
00:08:02,430 --> 00:08:07,420
英語のテキストを使って
同じ手法を適用しました

127
00:08:07,420 --> 00:08:13,980
英語のテキストから分散表現を作り
これは人々に大きな感銘を与えました

128
00:08:13,980 --> 00:08:18,682
GPUやスーパーコンピューターなどの高速計算機が
深層学習をどのように進展させているかが

129
00:08:18,682 --> 00:08:21,750
近年よく語られていますが

130
00:08:21,750 --> 00:08:26,376
1986年から90年代初期に既に
あなたとベンジオの間で

131
00:08:26,376 --> 00:08:29,570
この流れが始まっていたのは
知りませんでした

132
00:08:30,600 --> 00:08:32,630
非常に大きな進展です

133
00:08:32,630 --> 00:08:41,440
1986年には10分の１のメガフロップより遅いLispの機械を
使っていました

134
00:08:41,440 --> 00:08:47,720
1993年の頃には
10メガフロップの機械がありました

135
00:08:47,720 --> 00:08:49,600
なるほど
100倍です

136
00:08:49,600 --> 00:08:51,770
これが使いやすくなってきた時で

137
00:08:51,770 --> 00:08:53,580
コンピューターが
速くなっていきました

138
00:08:53,580 --> 00:08:56,960
過去数十年に渡って

139
00:08:56,960 --> 00:08:59,970
ニューラルネットワークと深層学習の
多くの手法を発明されていますが

140
00:08:59,970 --> 00:09:02,670
発明されたものすべてに
私は興味を持っています

141
00:09:02,670 --> 00:09:05,050
今でも一番興味があるのはどれですか

142
00:09:06,940 --> 00:09:09,590
一番美しいのはテリー・セジュノスキーとの

143
00:09:09,590 --> 00:09:12,620
ボルツマンマシンの研究だと思います

144
00:09:12,620 --> 00:09:14,500
つまり本当にとても単純なアルゴリズムで

145
00:09:14,500 --> 00:09:18,830
数個のノードしか見ることができないような
巨大な密結合ネットに

146
00:09:18,830 --> 00:09:23,550
適用できる学習アルゴリズムを
発見しました

147
00:09:23,550 --> 00:09:27,730
これは隠れ表現を学びますし
非常に単純なアルゴリズムです

148
00:09:27,730 --> 00:09:31,130
脳の中にも見つけられそうなものに
思えました

149
00:09:31,130 --> 00:09:34,210
各シナプスが知る必要があるのは
これが繋がっている２つのニューロンの挙動を

150
00:09:34,210 --> 00:09:35,940
知っておくだけでいいのです

151
00:09:37,010 --> 00:09:41,230
伝播される情報は
同じものでした

152
00:09:41,230 --> 00:09:45,160
２つの段階がありました
覚醒と睡眠と呼んでいましたが

153
00:09:45,160 --> 00:09:46,820
この異なる段階でも

154
00:09:46,820 --> 00:09:48,760
同じ方法で情報を
伝播していました

155
00:09:48,760 --> 00:09:52,360
誤差逆伝播法では
前方パスと後方パスがあります

156
00:09:52,360 --> 00:09:54,820
パス次第で
異なる動作をします

157
00:09:54,820 --> 00:09:56,379
異なる信号を送ります

158
00:09:58,100 --> 00:10:01,190
これが最も美しいものだと思います

159
00:10:01,190 --> 00:10:03,730
長年にわたって
これはただ好奇心としか言えないものでした

160
00:10:03,730 --> 00:10:05,090
遅すぎるだろうからです

161
00:10:06,210 --> 00:10:10,420
その後 美しさを少し取り払って
諦めがついてきて いくらかは単純化したネットワークにし

162
00:10:10,420 --> 00:10:13,730
１回分の実行にしました

163
00:10:13,730 --> 00:10:16,570
制限ボルツマン・マシンを生んで

164
00:10:16,570 --> 00:10:19,430
実際に実践では
効果的に動作しました

165
00:10:19,430 --> 00:10:21,586
例えばNetflix の競技では

166
00:10:21,586 --> 00:10:26,170
制限ボルツマン・マシンは
勝利した組が組み入れていた方法の１つでした

167
00:10:26,170 --> 00:10:30,210
2007年頃に始まった
ニューラルネットワークと深層学習が

168
00:10:30,210 --> 00:10:34,790
最近 再び浮かび上がってきたのは
制限ボルツマン・マシンでした

169
00:10:34,790 --> 00:10:37,710
制限なしボルツマン・マシンもそうでした
あなたとあなたのラボが作ったのでした

170
00:10:38,940 --> 00:10:42,130
そうです これもとても満足している研究の１つです

171
00:10:42,130 --> 00:10:46,290
１つだけ隠れ特徴の層がある制限付きボルツマン・マシンに
学習させることができ

172
00:10:46,290 --> 00:10:51,120
１つの特徴の層を
学習させることができる考えです

173
00:10:51,120 --> 00:10:54,850
この特徴をデータとして扱えて
同じことを再度できます

174
00:10:54,850 --> 00:10:57,953
新しい特徴をまた新しい層で学習でき
もう１度同じことができます

175
00:10:57,953 --> 00:10:59,570
何回でもできます

176
00:10:59,570 --> 00:11:03,060
とても良いもので
実際に使えるものでした

177
00:11:03,060 --> 00:11:08,709
UYテイ

178
00:11:08,709 --> 00:11:11,110
変な種類のモデルでした

179
00:11:11,110 --> 00:11:15,946
こんなモデルです
最上部に制限ボルツマン・マシンがあり

180
00:11:15,946 --> 00:11:20,626
下には何年も前に作られていたシグモイドのBelief ネットが

181
00:11:20,626 --> 00:11:23,060
あります

182
00:11:23,060 --> 00:11:24,620
これは直接モデルで

183
00:11:24,620 --> 00:11:28,651
これで制限付きボルツマン・マシンに
学習させることで

184
00:11:28,651 --> 00:11:32,760
シグモイドのBelief ネットで効率的に
予測を行う方法を作ることができました

185
00:11:33,830 --> 00:11:36,870
当時は

186
00:11:36,870 --> 00:11:41,270
密結合ネットを使おうと人たちが
ニューラルネットを使っている人はいましたが

187
00:11:41,270 --> 00:11:45,500
確率的予測を行う
良い方法がありませんでした

188
00:11:45,500 --> 00:11:50,050
私の子供たちと異なるように
グラフィカルモデルを行っていた人たちは

189
00:11:50,050 --> 00:11:55,603
予測を正しく行えた人もいましたが
疎結合ネットでしかできませんでした

190
00:11:55,603 --> 00:12:01,140
私たちはDeep Belief Net を学習させる方法
示すことができ

191
00:12:01,140 --> 00:12:06,280
近似の形で行う予測でした
とても速いものでした

192
00:12:06,280 --> 00:12:10,578
１つの前方パスのみを行い
美しい結果を生みました

193
00:12:10,578 --> 00:12:14,890
また追加で特徴の層を学習することを
保証することができました

194
00:12:16,010 --> 00:12:19,980
新しい層を学習するたびに
新しいBoundができ

195
00:12:19,980 --> 00:12:22,700
新しいBoundは
つねに古いBoundよりも優れていました

196
00:12:22,700 --> 00:12:25,810
層を足すことにできるVariational bound ですね

197
00:12:25,810 --> 00:12:26,970
ビデオを覚えています

198
00:12:26,970 --> 00:12:29,680
これが２つ目のとても嬉しかったことです

199
00:12:29,680 --> 00:12:35,600
３つ目のものは
neilとの

200
00:12:35,600 --> 00:12:40,750
変分法のものです
すでに統計学の学者が似た研究をしていたことが分かりましたが

201
00:12:40,750 --> 00:12:43,100
私たちは知りませんでした

202
00:12:44,610 --> 00:12:47,260
完全Eの計算を

203
00:12:47,260 --> 00:12:50,250
行う必要がなく近似的なEを行うことで
ENを算出する随分と良い方法を

204
00:12:50,250 --> 00:12:52,800
示すことができました

205
00:12:52,800 --> 00:12:55,320
ENは統計学では
巨大なアルゴリズムですが

206
00:12:55,320 --> 00:12:58,380
私たちは大きな一般化を
示しました

207
00:12:58,380 --> 00:13:02,490
特に1993年には
ヴァン・キャンプと

208
00:13:02,490 --> 00:13:07,040
最初の変分ベイズ法の論文を
一緒に書きました

209
00:13:07,040 --> 00:13:12,090
ベイズの学習の方法の１つを行う方法を示し

210
00:13:12,090 --> 00:13:17,950
ガウスの真の事後を近似することで
より扱いやすいものでした

211
00:13:17,950 --> 00:13:20,320
これをニューラルネットで出来ました

212
00:13:20,320 --> 00:13:22,600
これにはとても嬉しかったです

213
00:13:22,600 --> 00:13:23,680
なるほど

214
00:13:23,680 --> 00:13:26,670
私はそれらの論文を覚えています

215
00:13:26,670 --> 00:13:32,630
Neil,Hinton,yen/ian paperの論文を読むのに
長い時間を使いました

216
00:13:32,630 --> 00:13:36,070
こんにち使われて

217
00:13:36,070 --> 00:13:41,110
毎日使うようなアルゴリズムには
ドロップアウトやReLUがありますが

218
00:13:41,110 --> 00:13:46,570
これもあなたのグループが
発表しているものですよね

219
00:13:46,570 --> 00:13:47,390
どちらとも言えます

220
00:13:47,390 --> 00:13:51,470
Rectified Linear Units は
他の人が考えたものです

221
00:13:51,470 --> 00:13:56,860
私たちは制限付きボルツマン・マシンで
ReLU が

222
00:13:56,860 --> 00:14:02,880
ロジステックユニットでの層を重ねたもの全体と
ほぼ同じという研究をしました

223
00:14:02,880 --> 00:14:05,190
これはReLU が出回ったのに
貢献したものの１つです

224
00:14:05,190 --> 00:14:07,440
とても興味があります

225
00:14:07,440 --> 00:14:12,570
ReLU の論文は
多くの数学が含まれています

226
00:14:12,570 --> 00:14:15,530
この関数が非常に複雑な式によって
近似できることを示しています

227
00:14:15,530 --> 00:14:19,140
ご本人で数学的部分も行って
論文が学会で受け入れられるようにしたのですか

228
00:14:19,140 --> 00:14:24,840
max(0,x)の開発に影響を及ぼした
すべての数学の部分です

229
00:14:26,450 --> 00:14:30,440
この件は考えを開発するのに
数学が実は重要だった事例の

230
00:14:30,440 --> 00:14:32,350
１つでした

231
00:14:32,350 --> 00:14:35,262
つまり ロジステックユニットを知ってましたし
明らかにRectified Linear Units も

232
00:14:35,262 --> 00:14:36,821
知っていました

233
00:14:36,821 --> 00:14:39,250
ボルツマン・マシンを
研究していたので

234
00:14:39,250 --> 00:14:42,720
基本的な作りは
ロジステックユニットで行われていました

235
00:14:42,720 --> 00:14:45,120
そして命題は

236
00:14:45,120 --> 00:14:49,070
Rectified Linear Units で
学習アルゴリズムがきちんと動作するかでした

237
00:14:49,070 --> 00:14:54,400
Rectified Linear Units で表現することでも
ロジステックユニットで

238
00:14:54,400 --> 00:15:00,350
行ったのとほとんど同じでした
同じになる方法を数学で表わしていました

239
00:15:00,350 --> 00:15:01,508
なるほど

240
00:15:01,508 --> 00:15:05,890
それによって多くの人がReLU を使うことを鼓舞されたのですね
ロジステックユニットを理解しなくても同じように動作しますし

241
00:15:05,890 --> 00:15:08,000
そうです

242
00:15:08,000 --> 00:15:12,130
同じ目的で行うものを
必ずしも理解する必要がありません

243
00:15:13,150 --> 00:15:16,850
そうです
Google に行って１つ気付いたことがありました

244
00:15:16,850 --> 00:15:22,796
2014年にGoogle でReLU についてと
単位行列で初期化することについて

245
00:15:22,796 --> 00:15:26,660
話しました

246
00:15:26,660 --> 00:15:30,300
ReLU でいいことは
隠れ層をコピーし続けると

247
00:15:30,300 --> 00:15:32,667
単位行列で初期化していくと

248
00:15:32,667 --> 00:15:35,050
次の層にパターンを
コピーするだけになります

249
00:15:36,140 --> 00:15:40,120
そして300の隠れ層のネットワークを
学習させることができるのを示していました

250
00:15:40,120 --> 00:15:44,760
単位行列で初期化すると
本当に効果的に学習させることができました

251
00:15:44,760 --> 00:15:48,065
これをそれ以上は追いませんでした
そうしなかったことを本当に後悔しています

252
00:15:48,065 --> 00:15:52,507
リーさんと論文を発表しました
再帰ネットワークを

253
00:15:52,507 --> 00:15:55,565
そのように初期化できることを
示しました

254
00:15:55,565 --> 00:16:00,370
さらにこれを追求すべきでした
後にResidual ネットワークができたからです

255
00:16:00,370 --> 00:16:03,572
そういうことでした

256
00:16:03,572 --> 00:16:06,660
何年にも渡って
あなたが脳を語って

257
00:16:06,660 --> 00:16:09,447
脳と逆誤差伝播法の関係を
語るのを聞いてきました

258
00:16:09,447 --> 00:16:13,720
この関係については
今はどんな考えがありますか

259
00:16:13,720 --> 00:16:16,910
実はまさに今
論文を書いています

260
00:16:18,250 --> 00:16:21,160
主な考えはこれです

261
00:16:21,160 --> 00:16:25,570
もし逆誤差伝播法が
学習に良いアルゴリズムと分かったならば

262
00:16:26,620 --> 00:16:31,610
もちろん進化についても

263
00:16:32,730 --> 00:16:37,270
つまり今は眼球や歯を
作れる細胞があります

264
00:16:37,270 --> 00:16:42,440
細胞が変化できるなら
細胞に逆誤差伝播法を組み込むことができるはずです

265
00:16:42,440 --> 00:16:45,860
これに選択的な圧力がかかるはずです

266
00:16:45,860 --> 00:16:50,490
これが信じられるものではないという神経科学者の考えは
馬鹿げています

267
00:16:50,490 --> 00:16:52,890
分かりくい実現方法が
あるのかもしれません

268
00:16:52,890 --> 00:16:56,000
逆誤差伝播法と全く同じでないにしても
それにかなり近いものが

269
00:16:56,000 --> 00:16:58,620
脳にはあると考えています

270
00:16:58,620 --> 00:17:02,566
何年にもわたって
どうすればこれがあり得るのかという考えが浮かんでいます

271
00:17:02,566 --> 00:17:06,994
1987年に
ジェームズ・マクレランドと

272
00:17:06,994 --> 00:17:11,202
再循環アルゴリズムを作りました

273
00:17:11,202 --> 00:17:16,090
これの考えというのは
ループするものに情報を

274
00:17:17,470 --> 00:17:18,686
送り

275
00:17:18,686 --> 00:17:22,206
情報がこのループを回りながらも
構成は変わりません

276
00:17:22,206 --> 00:17:26,490
これを最も単純な形で表わすとしたら
入力ユニット２つと隠れユニット２つがあり

277
00:17:26,490 --> 00:17:31,046
情報を入力１から隠れユニット１に送り
入力２に戻し

278
00:17:31,046 --> 00:17:34,388
隠れ２に戻し入力に戻しというのを
繰り返します

279
00:17:34,388 --> 00:17:38,001
オートエンコーダーを学習させたいのですが

280
00:17:38,001 --> 00:17:42,300
逆誤差伝播法なしで
学習させるというものです

281
00:17:42,300 --> 00:17:47,250
学習させるのですが
活動での差異を取り除くものです

282
00:17:47,250 --> 00:17:51,922
シナプスの学習方法のルールは

283
00:17:51,922 --> 00:17:57,930
シナプス前入力への割合と
シナプス後入力の変更率の割合で

284
00:17:57,930 --> 00:18:01,780
重みを変更します

285
00:18:01,780 --> 00:18:04,060
再循環ではシナプス後入力を

286
00:18:04,060 --> 00:18:08,330
古いものを良いものとして
新しいものを悪いものにします

287
00:18:08,330 --> 00:18:09,620
この方向で変更していきます

288
00:18:11,010 --> 00:18:14,472
このアルゴリズムを発明しました
スパイクタイミング依存可塑性を

289
00:18:14,472 --> 00:18:16,521
神経科学者が発明する前にです

290
00:18:16,521 --> 00:18:20,700
スパイクタイミング依存可塑性は
同じアルゴリズムですが方向が逆です

291
00:18:20,700 --> 00:18:26,220
この学習ルールでは新しいものが良く
古いものは悪いとしています

292
00:18:26,220 --> 00:18:30,010
これでは重み変更は
シナプス前活動かける

293
00:18:30,010 --> 00:18:35,690
(新しいシナプス前活動から古い活動を引いたもの)を行います

294
00:18:37,060 --> 00:18:42,020
後に2007年に分かったのですが

295
00:18:42,020 --> 00:18:47,830
制限付きボルツマン・マシンを重ねて学習させて

296
00:18:47,830 --> 00:18:52,620
学習済みになると
逆誤差伝播法を実装するのと

297
00:18:52,620 --> 00:18:56,450
全く同じ状態になりました
再構築しようとするだけでできました

298
00:18:56,450 --> 00:19:01,124
再構築誤差を観察すると
再構築誤差は

299
00:19:01,124 --> 00:19:05,728
識別性能の違いを教えてくれます

300
00:19:05,728 --> 00:19:12,079
2007年の深層学習ワークショップでは
これについて話したのですが

301
00:19:12,079 --> 00:19:16,454
ほぼ完璧なまでに
無視されました

302
00:19:16,454 --> 00:19:19,799
後にヨシュア・ベンジオは
この考えを使い

303
00:19:19,799 --> 00:19:24,340
私が私自身で研究しているよりも

304
00:19:24,340 --> 00:19:26,490
実はより多くの研究を行いました

305
00:19:26,490 --> 00:19:33,280
このオートエンコーダーを積み重ねることで

306
00:19:33,280 --> 00:19:38,440
逆方向に活動を送り誤差を出し
再構築誤差を突き止める考えは

307
00:19:38,440 --> 00:19:42,520
本当に興味深い考えで
脳が行っている可能性は十分にあります

308
00:19:42,520 --> 00:19:47,520
他の話題としては
あなたが色々と考え研究していると聞いていることで

309
00:19:47,520 --> 00:19:51,930
深層学習で再帰結合型(multiple time scales)をどう扱うかです

310
00:19:51,930 --> 00:19:54,468
お考えを共有してもらえますか

311
00:19:54,468 --> 00:19:58,910
はい 実は大学院生の１年目のことでした

312
00:19:58,910 --> 00:20:04,040
私がFast weight(速い重み)と呼ぶものを
使うことについて最初の講演をしました

313
00:20:04,040 --> 00:20:07,560
重みが高速で適応するが
高速で忘れます

314
00:20:07,560 --> 00:20:08,832
このため 短期記憶を保持します

315
00:20:08,832 --> 00:20:13,496
1973年に非常に簡単なシステムで示しました

316
00:20:13,496 --> 00:20:16,590
重みを真に再帰処理できる仕組みでした

317
00:20:16,590 --> 00:20:23,010
真の再帰処理というのは
特徴を表現するのに使われているニューロンが

318
00:20:23,010 --> 00:20:28,470
再帰コアで特徴を表現するために
再利用されるという意味です

319
00:20:30,210 --> 00:20:31,750
知識を表現するための重みは

320
00:20:31,750 --> 00:20:34,388
再帰コアで再利用されます

321
00:20:34,388 --> 00:20:39,170
このことは次の疑問を導きます
再帰コアを引き抜いた時は

322
00:20:39,170 --> 00:20:41,600
今まさに行っていたことを
どうやって覚えているのかという疑問です

323
00:20:41,600 --> 00:20:42,970
記憶はどこか？

324
00:20:42,970 --> 00:20:45,015
なぜなら再帰コアに
ニューロンを使っています

325
00:20:46,080 --> 00:20:49,240
答えは この記憶を速い重みに入れれば

326
00:20:49,240 --> 00:20:53,940
速い重みから
ニューロンの活動状態を復元できます

327
00:20:53,940 --> 00:20:56,151
つい最近では
ジミー・バーと一緒に研究し

328
00:20:56,151 --> 00:21:00,141
このように再帰で速い重みを使った方法について
NIPS に論文を発表しました

329
00:21:00,141 --> 00:21:00,898
なるほど

330
00:21:00,898 --> 00:21:04,145
これにはかなり長い時間差がありました

331
00:21:04,145 --> 00:21:08,746
最初のモデルは1973年で
出版されませんでしたが

332
00:21:08,746 --> 00:21:14,966
ジミー・バーのモデルは
2015年か2016年と思います

333
00:21:14,966 --> 00:21:16,469
およそ40年後です

334
00:21:16,469 --> 00:21:22,840
もう１つの聞きたい考えは
随分前に そう ５年くらい前に

335
00:21:22,840 --> 00:21:29,350
「カプセル」についてです
今はどんな状態でしょうか

336
00:21:29,350 --> 00:21:34,150
分かりました これについては
昔の私と同じ状況に戻りました

337
00:21:34,150 --> 00:21:39,320
本当に信じている考えがあるのですが
誰も信じていない状況です

338
00:21:39,320 --> 00:21:42,120
これの論文を提出しても
全て却下されましたが

339
00:21:42,120 --> 00:21:45,938
本当にこの考えを信じていて
押し続けるつもりです

340
00:21:45,938 --> 00:21:53,880
これには要となる何点かの概念があります

341
00:21:53,880 --> 00:22:00,000
１つ目は多次元をどう表現するかで

342
00:22:00,000 --> 00:22:05,070
少しベクトル操作をするだけで表現できます

343
00:22:05,070 --> 00:22:07,630
どれか１つが分かればいいのです

344
00:22:07,630 --> 00:22:12,150
この考えでは画像のそれぞれの領域で
何か１つの種類の特徴が

345
00:22:12,150 --> 00:22:14,000
あることを想定して

346
00:22:15,200 --> 00:22:18,020
大量のニューロンを利用して

347
00:22:18,020 --> 00:22:23,190
その活動によって
特徴の異なる側面を表現させます

348
00:22:24,230 --> 00:22:27,270
その領域内で
x、y座標の値であったり

349
00:22:27,270 --> 00:22:28,780
向きは？

350
00:22:28,780 --> 00:22:29,930
移動速度は？

351
00:22:29,930 --> 00:22:30,630
色は？

352
00:22:30,630 --> 00:22:31,270
明るさは？

353
00:22:31,270 --> 00:22:32,590
などのものです

354
00:22:32,590 --> 00:22:36,350
大量のニューロンを使って
同じものを様々な次元で表現することが

355
00:22:36,350 --> 00:22:37,710
できます

356
00:22:37,710 --> 00:22:39,410
ただし１つだけなのが条件です

357
00:22:40,490 --> 00:22:46,110
これは従来のニューラルネットを使った表現とは

358
00:22:46,110 --> 00:22:48,155
非常に異なる表現方法です

359
00:22:48,155 --> 00:22:49,820
通常はニューラルネットでは
非常に大きな層を使うだけで

360
00:22:49,820 --> 00:22:52,080
全てのユニットは利用され
色々に動作します

361
00:22:52,080 --> 00:22:55,770
しかしユニットを
異なる座標を表現する小さなグループに

362
00:22:55,770 --> 00:22:57,310
分けることは考えていません

363
00:22:58,660 --> 00:23:02,080
追加される構造であるべきと考えています

364
00:23:02,080 --> 00:23:05,020
これと共にある他の概念は

365
00:23:05,020 --> 00:23:07,410
ということは
分散された表現が

366
00:23:07,410 --> 00:23:09,280
別々のサブセットに

367
00:23:09,280 --> 00:23:11,270
分割した表現が

368
00:23:11,270 --> 00:23:13,900
表現するのですね

369
00:23:13,900 --> 00:23:15,600
それぞれのサブセットを「カプセル」と呼んでいます

370
00:23:15,600 --> 00:23:16,180
なるほど

371
00:23:16,180 --> 00:23:21,078
この概念「カプセル」によって
特徴のインスタンスを表現することができますが

372
00:23:21,078 --> 00:23:21,794
１つだけです

373
00:23:21,794 --> 00:23:27,130
これがその特徴の異なる属性を
表現します

374
00:23:27,130 --> 00:23:29,880
１つの属性のスカラーしか無ような

375
00:23:29,880 --> 00:23:34,530
従来のニューロンとニューラルネットと違って
多くの属性を持つ特徴です

376
00:23:34,530 --> 00:23:36,240
なるほど

377
00:23:36,240 --> 00:23:41,423
これで出来るようになるのは
通常のニューラルネットでは非常に成績の悪いことでも

378
00:23:41,423 --> 00:23:48,980
このことを私が合意のルーチンと呼んでいる動作で
出来るようになります

379
00:23:48,980 --> 00:23:52,960
例えばセグメンテーションを
行いたいとしましょう

380
00:23:52,960 --> 00:23:56,660
口かもしれないものと
鼻かもしれないものがあるとしましょう

381
00:23:57,910 --> 00:24:02,179
これを組み立てて１つのものを
作るべきかどうか知りたいです

382
00:24:02,179 --> 00:24:03,879
この概念では

383
00:24:03,879 --> 00:24:06,040
口のパラメーターを持つ口のカプセルと

384
00:24:06,040 --> 00:24:10,582
鼻のパラメーターを持つ鼻のカプセルがあります

385
00:24:10,582 --> 00:24:13,797
次に１つに組み立てるかどうかを決めるために

386
00:24:13,797 --> 00:24:18,670
カプセルのそれぞれが
顔にはどんなパラメーターとなるべきかに投票させます

387
00:24:19,930 --> 00:24:23,718
もし口と鼻が正しい空間的関係にあれば
合意するはずです

388
00:24:23,718 --> 00:24:24,725
このように

389
00:24:24,725 --> 00:24:28,888
１つのレベルで２つのカプセルが
同じパラメーターの組み合わせに合意するような時には

390
00:24:28,888 --> 00:24:32,106
次のレベルに進みます
高次元空間で合意することは非常に起こりにくいので

391
00:24:32,106 --> 00:24:35,350
カプセルは十中八九正しいと
想定することができます

392
00:24:36,950 --> 00:24:42,109
これは通常ニューラルネットで行うフィルタリングとは

393
00:24:42,109 --> 00:24:46,130
非常に異なる方法です

394
00:24:46,130 --> 00:24:50,708
この合意によるルーチンは
ニューラルネットが限定的なデータから

395
00:24:50,708 --> 00:24:56,700
汎化するのに極めて重要になると考えています

396
00:24:56,700 --> 00:24:59,797
この方法は視点が変わることに対応したり
セグメンテーションを行うために

397
00:24:59,797 --> 00:25:01,500
非常によいものだろうと考えています

398
00:25:01,500 --> 00:25:04,794
ニューラルネットで今行っているよりも
統計的に圧倒的に効率的にできることを

399
00:25:04,794 --> 00:25:06,147
望んでいます

400
00:25:06,147 --> 00:25:08,575
今だと視点の変更に対応するには

401
00:25:08,575 --> 00:25:12,000
視点変更に合わせた変更を与えて
全てで学習させています

402
00:25:12,000 --> 00:25:16,460
なるほど
教師あり学習の誤差伝播法のみによって学習させるのではなく

403
00:25:16,460 --> 00:25:19,120
異なる方法で学習させるのですね

404
00:25:20,220 --> 00:25:24,120
でも 依然 教師あり学習で
行おうかと考えていますが

405
00:25:24,120 --> 00:25:27,720
前方パスの仕組みは
非常に異なります

406
00:25:27,720 --> 00:25:32,010
純粋な前方パスではなく
少しだけ繰り返し処理を行います

407
00:25:32,010 --> 00:25:36,550
口を見つけたと思う箇所と
鼻を見つけたと思う箇所について

408
00:25:36,550 --> 00:25:39,127
一緒に組み合わせると顔になるかを
決定するために

409
00:25:39,127 --> 00:25:42,530
繰り返し処理を少し行います

410
00:25:42,530 --> 00:25:46,352
繰り返しでは
逆誤差伝播法を使うこともできるでしょう

411
00:25:46,352 --> 00:25:50,286
識別力のあるものを
試して実行することもできるでしょう

412
00:25:50,286 --> 00:25:54,417
トロントの私のグループで
この研究を行っています

413
00:25:54,417 --> 00:26:00,260
ブレインチームの一部のGoogle のチームも
仲間にいます

414
00:26:00,260 --> 00:26:02,127
これが今楽しんでいることです

415
00:26:02,127 --> 00:26:02,891
なるほど  いいですね

416
00:26:02,891 --> 00:26:05,366
論文が発表されるのを
楽しみにしています

417
00:26:05,366 --> 00:26:10,750
もし掲載されればね(笑)

418
00:26:10,750 --> 00:26:13,040
あなたは数十年に渡って
深層学習の研究をしています

419
00:26:13,040 --> 00:26:15,330
これらの間に AI の理解や考え方は

420
00:26:15,330 --> 00:26:18,760
どう変わって来たかに
非常に興味があります

421
00:26:20,380 --> 00:26:27,678
私の知的な歴史の多くは
誤差逆伝播法やそれをどう使うかや

422
00:26:27,678 --> 00:26:33,531
その力を活かす方法に
関するもののように思えます

423
00:26:33,531 --> 00:26:36,966
最初から話すと
まず80年代半ばに

424
00:26:36,966 --> 00:26:40,203
それを評価学習に使っていて
きちんと動作していました

425
00:26:40,203 --> 00:26:42,405
そして90年代の初めころまでに

426
00:26:42,405 --> 00:26:46,749
人間の学習はそのほとんどが
教師なし学習だと確信しました

427
00:26:46,749 --> 00:26:50,138
そのことから教師なし学習に
より強く興味を持つようになりました

428
00:26:50,138 --> 00:26:54,300
これはウェグスティン(Wegstein)アルゴリズムに
取り組んでいたころです

429
00:26:54,300 --> 00:26:58,306
このころのあなたのコメントは
私の考え方にも大きく影響を与えてました

430
00:26:58,306 --> 00:27:03,010
あなたの影響で 私が率いていたGoogle ブレインでは

431
00:27:03,010 --> 00:27:07,900
最初のプロジェクトは
教師なし学習でした

432
00:27:07,900 --> 00:27:09,740
そうですか
間違って導いたかもしれないですね

433
00:27:09,740 --> 00:27:11,470
なぜなら長い目で見れば

434
00:27:11,470 --> 00:27:13,840
教師なし学習は
絶対に重要になると思いますが

435
00:27:15,160 --> 00:27:19,376
現実を直視する必要も出てきます

436
00:27:19,376 --> 00:27:24,107
過去10年くらいで上手くいったのは
教師あり学習で

437
00:27:24,107 --> 00:27:27,179
評価学習でした
この場合にはラベルがあり

438
00:27:27,179 --> 00:27:31,810
連続するもので次に何が来るのかを予測しようとするものでした
これもラベルでした

439
00:27:31,810 --> 00:27:33,769
これは非常にうまく動作しました

440
00:27:37,528 --> 00:27:42,266
私は依然として 教師なし学習が
重要になると信じていますし

441
00:27:42,266 --> 00:27:47,145
もし適切に動作させることができれば
今よりも本当にかなり精度が上がるでしょうが

442
00:27:47,145 --> 00:27:48,200
まだです

443
00:27:49,990 --> 00:27:53,225
そうですね 私も含めて
深層学習で経験を積んできた人は

444
00:27:53,225 --> 00:27:56,074
教師なし学習にかなり興奮しています

445
00:27:56,074 --> 00:28:01,513
どうやればいいのか
まだ誰もほどんど分からないだけです

446
00:28:01,513 --> 00:28:04,983
あなたは分かるかも
私は分かりません

447
00:28:04,983 --> 00:28:08,160
変分オートエンコーダーは
reparameterization trick を使っていて

448
00:28:08,160 --> 00:28:10,120
とても良い考えに思えます

449
00:28:10,120 --> 00:28:15,260
敵対的生成ネットワーク(GAN)も
とても良い考えに思えます

450
00:28:15,260 --> 00:28:18,645
敵対的生成ネットワークは

451
00:28:18,645 --> 00:28:23,430
深層学習の考え方では新しいものの中では
とても根本的に異なる考えの１つです

452
00:28:23,430 --> 00:28:26,363
「カプセル」を同じように成功させられればと
考えていますが

453
00:28:26,363 --> 00:28:31,740
今は敵対的生成ネットワークは
大きな飛躍的進歩だと思います

454
00:28:31,740 --> 00:28:34,439
スパース性と「slow features」については
どうでしょうか

455
00:28:34,439 --> 00:28:38,806
教師なし学習のモデルを作る原則の２つです

456
00:28:41,556 --> 00:28:47,788
スパース性については
君ほど有名ではないですよ

457
00:28:47,788 --> 00:28:52,672
でも「slow features」は
「slow」を追うのは間違いだと

458
00:28:52,672 --> 00:28:53,660
思います

459
00:28:53,660 --> 00:28:57,880
基本的な考えは正しいですが
変化のない特徴を狙ってはいけません

460
00:28:57,880 --> 00:29:00,660
予測できるように変化する特徴を
狙うべきです

461
00:29:01,680 --> 00:29:07,060
これはモデル化する場合の
基本的な原則のようなものですが

462
00:29:08,620 --> 00:29:13,391
この場合にはデータを使って
非線形の変換をこのデータに適用します

463
00:29:13,391 --> 00:29:17,612
これを状態ベクトルとなるまで適用します

464
00:29:17,612 --> 00:29:22,672
状態ベクトルは実は線形なのです

465
00:29:22,672 --> 00:29:26,103
ただカルマン・フィルターを使いながらも
まるで線形かのように扱ってはいけません

466
00:29:26,103 --> 00:29:29,625
観測データから変数への変形が見つかります

467
00:29:29,625 --> 00:29:32,616
変数への行列積のような線形の演算が

468
00:29:32,616 --> 00:29:37,480
役割を演じます

469
00:29:37,480 --> 00:29:39,700
例えば視点を変更したいとします

470
00:29:39,700 --> 00:29:42,890
他の視点からの画像を作成したいとすると

471
00:29:42,890 --> 00:29:46,900
ピクセルから座標へと行うべきです

472
00:29:47,950 --> 00:29:50,686
座標表現にたどり着いたら

473
00:29:50,686 --> 00:29:54,120
これは「カプセル」でできればと
思っていることですが

474
00:29:54,120 --> 00:29:57,350
行列積を行い
視点を変えます

475
00:29:57,350 --> 00:29:59,210
そしてピクセルへと割り戻します

476
00:29:59,210 --> 00:29:59,893
なるほど
だから

477
00:29:59,893 --> 00:30:02,170
これは本当に一般的な原則です

478
00:30:02,170 --> 00:30:04,773
だから顔の合成の研究を行っているのですね

479
00:30:04,773 --> 00:30:09,355
顔を取り入れて
非常に低次元のベクトルに圧縮し

480
00:30:09,355 --> 00:30:12,450
ベクトルを入れて
顔を取得する方法ですね

481
00:30:12,450 --> 00:30:15,950
研究している学生はいますが
私自身はそれほど研究してません

482
00:30:17,100 --> 00:30:19,180
この質問はよく受けるとは思いますが

483
00:30:19,180 --> 00:30:23,920
もし深層学習に参入したい人は
何をすべきですか

484
00:30:23,920 --> 00:30:25,040
どんな助言がありますか

485
00:30:25,040 --> 00:30:28,938
１対１の状況では
多くの助言をしているでしょうが

486
00:30:28,938 --> 00:30:31,550
世界全体の視聴者に対して

487
00:30:31,550 --> 00:30:35,999
参入しようとしている人には
どんな助言をしますか

488
00:30:35,999 --> 00:30:42,171
いいでしょう 私の助言は文献は読んでも
読みすぎないようにということです

489
00:30:42,171 --> 00:30:48,030
これは私の指導者からもらったもので
ほとんどの人は言いそうにないことです

490
00:30:48,030 --> 00:30:52,474
ほとんどの人は
何年かは文献を読んだ後で

491
00:30:52,474 --> 00:30:55,421
自分の考えを
研究し始めるべきと言います

492
00:30:55,421 --> 00:31:00,295
研究者にはこれが当てはまる人もいるでしょうが
創造的な研究者なら

493
00:31:00,295 --> 00:31:03,803
少し文献を読んで

494
00:31:03,803 --> 00:31:07,792
他の人たちは間違っていると
あなたが思うことを探します

495
00:31:07,792 --> 00:31:10,340
人と正反対ということですが

496
00:31:10,340 --> 00:31:13,568
見ていて
正しくないと感じることを探します

497
00:31:13,568 --> 00:31:15,660
そして正しい方法を探すのです

498
00:31:16,890 --> 00:31:22,476
人がそれは良くないと言った時でも
ただ続けてください

499
00:31:22,476 --> 00:31:26,339
続けるようにするための良い原則を
知っています

500
00:31:26,339 --> 00:31:29,996
それは直感が正しかろうが
正しくなかろうが

501
00:31:29,996 --> 00:31:32,030
直感が正しければ
それに従うべきですし

502
00:31:32,030 --> 00:31:34,060
最終的には
成功するでしょう

503
00:31:34,060 --> 00:31:36,478
直感が間違っていたら
何をしても無駄でしょう

504
00:31:36,478 --> 00:31:40,329
なるほど(笑)

505
00:31:40,329 --> 00:31:43,420
刺激的な助言です
した方がましということですね

506
00:31:43,420 --> 00:31:45,410
直感を信じたほうがましです

507
00:31:45,410 --> 00:31:47,847
信じない理由がありません

508
00:31:47,847 --> 00:31:49,420
なるほど

509
00:31:49,420 --> 00:31:55,193
私は通常 論文を読むだけではなく
再現するように助言します

510
00:31:55,193 --> 00:31:58,161
これは自然と
読める量を制限しています

511
00:31:58,161 --> 00:32:00,800
というのも結果を再現するのは
とても時間のかかることだからです

512
00:32:01,910 --> 00:32:05,312
正しいです
論文を再現しようとすると

513
00:32:05,312 --> 00:32:08,100
動作させるための細かな技を
色々と発見できます

514
00:32:08,100 --> 00:32:11,938
他の助言としては
プログラムは続けましょうということです

515
00:32:11,938 --> 00:32:15,577
優秀でない学生に宿題をさせると

516
00:32:15,577 --> 00:32:18,550
戻ってきて
動作しなかったと言うでしょう

517
00:32:18,550 --> 00:32:22,030
動作しなかった原因は
重要なことだと気付かない細かな

518
00:32:22,030 --> 00:32:25,100
決定かもしれません

519
00:32:25,100 --> 00:32:28,850
UY テイのような優秀な学生に与えれば

520
00:32:28,850 --> 00:32:31,120
何を宿題に出しても
戻ってきて動作させれたと言うでしょう

521
00:32:32,670 --> 00:32:36,420
こうしたのを覚えています
私は「UYちょっとまって

522
00:32:36,420 --> 00:32:37,330
この間話した後に

523
00:32:37,330 --> 00:32:40,380
これこれの理由で
動作するはずがないと気づいた」と伝えると

524
00:32:40,380 --> 00:32:43,586
UYは「すぐに気づいたので
異なることを意図していたと想像しました」と言いました

525
00:32:43,586 --> 00:32:47,627
(笑)
それは面白いですね

526
00:32:47,627 --> 00:32:51,575
どうでしょう
他には

527
00:32:51,575 --> 00:32:57,782
AI や深層学習に
参入したい人に助言はありますか

528
00:32:57,782 --> 00:33:02,000
基本的には十分な量を読んで
直感を発展させ始めて

529
00:33:02,000 --> 00:33:05,811
直感を信じて
突き進んでください

530
00:33:05,811 --> 00:33:10,783
みんなが無意味だと言っても
気にしすぎないでください

531
00:33:10,783 --> 00:33:14,352
無意味だと言ってる人たちが
正しいかどうかが

532
00:33:14,352 --> 00:33:19,950
分かる方法はないですから
突き進めて見つけるしかないということですよね

533
00:33:19,950 --> 00:33:24,350
もうひとつあります
あなたが本当に良い考えだと思うことを

534
00:33:24,350 --> 00:33:27,201
他のひとが完全に無意味だと言われる事柄なら

535
00:33:27,201 --> 00:33:29,761
何か価値のあることなのだと
分かります

536
00:33:29,761 --> 00:33:33,960
それの１つの例はCamp(不明)と私が
変分法を思いついた時のことです

537
00:33:35,420 --> 00:33:40,690
私の学生だったピーター・ブラウンに
これを説明するメールを送りました

538
00:33:40,690 --> 00:33:42,560
彼はENをよく分かっていました

539
00:33:43,570 --> 00:33:46,967
彼が研究を一緒にしている人たちに
見せたところ

540
00:33:46,967 --> 00:33:51,253
相手はデラ・ピエトラ・ブラザーズで確か双子です

541
00:33:51,253 --> 00:33:55,914
ピーターは彼らが何と言ったか教えてくれて

542
00:33:55,914 --> 00:34:00,277
彼らは「このひとは酔ってるか
ただ愚かだ」と言ったそうで

543
00:34:00,277 --> 00:34:04,260
彼らは本当に本当に
無意味だと思ったのです

544
00:34:04,260 --> 00:34:06,460
部分的には私の説明の仕方にも
よったのかもしれません

545
00:34:06,460 --> 00:34:08,043
というのも直感的な言葉で
説明していました

546
00:34:09,150 --> 00:34:13,100
自分が良い考えだと思っていることを

547
00:34:13,100 --> 00:34:16,810
他のひとが完全にゴミだと言った時は
本当に良い考えの兆しでもあります

548
00:34:18,026 --> 00:34:21,555
なるほど
研究の分野の話ですが

549
00:34:21,555 --> 00:34:26,183
新卒の人たちは
何をするべきでしょうか

550
00:34:26,183 --> 00:34:30,707
「カプセル」、教師なし学習
ほかには何があるでしょうか

551
00:34:30,707 --> 00:34:34,078
新卒の方々への良い助言としては

552
00:34:34,078 --> 00:34:38,344
自分と同じような信念を持っている助言者を
探してみてください

553
00:34:38,344 --> 00:34:42,637
というのは 助言者が思い入れのあることを
研究していれば

554
00:34:42,637 --> 00:34:47,170
助言者は良い助言や時間を
多くくれるでしょう

555
00:34:47,170 --> 00:34:50,590
助言者が興味のないことを研究していると

556
00:34:50,590 --> 00:34:55,262
得られるのは助言はあるでしょうが
それほど役に立つものではないでしょう

557
00:34:55,262 --> 00:34:58,386
なるほど
助言について最後に１つ

558
00:34:58,386 --> 00:35:02,440
学習者に対しては
博士課程に行く人に対して

559
00:35:02,440 --> 00:35:09,687
トップ企業やトップ研究機関に行く人もいます
この人たちについてはどう感じてますか

560
00:35:09,687 --> 00:35:13,890
これは複雑なことです
現在起こっていることは

561
00:35:13,890 --> 00:35:18,727
大学で深層学習の教育が必要な人の全員を
教えるほどの数の人は

562
00:35:18,727 --> 00:35:21,125
教育者側にはいないのです

563
00:35:21,125 --> 00:35:25,011
今は学部が持つ教員の人員が
十分ではないのですが

564
00:35:25,011 --> 00:35:27,780
これは一時的なことだと思います

565
00:35:27,780 --> 00:35:32,410
これまで起こっていたのは
多くの学部では起こっている革命を

566
00:35:32,410 --> 00:35:34,890
理解するのに
非常に時間がかかったのです

567
00:35:34,890 --> 00:35:38,720
部分的には同意しますが
第２の産業革命とは言えないものですが

568
00:35:38,720 --> 00:35:41,000
それに近い規模のものではあります

569
00:35:41,000 --> 00:35:43,691
根本的な大変化が起こっています

570
00:35:43,691 --> 00:35:47,980
われわれのコンピューターとの関係が
変わってしまったからです

571
00:35:47,980 --> 00:35:53,920
プログラムするのではなく
見せて 勝手に分かるようになりました

572
00:35:53,920 --> 00:35:56,570
これは全く異なるコンピューターの使い方です

573
00:35:56,570 --> 00:36:01,210
コンピューター科学の学部は
コンピューターをプログラムするという考えに基づいて作られています

574
00:36:01,210 --> 00:36:03,480
このコンピューターに見せることが

575
00:36:05,000 --> 00:36:09,330
コンピューターにプログラムすることと同じくらい重要になるとは
分かっていません

576
00:36:09,330 --> 00:36:13,940
コンピューターに見せることで作業をさせることを
学部の教員の半分は行っているべきだということが分かって

577
00:36:13,940 --> 00:36:16,510
いません

578
00:36:16,510 --> 00:36:22,183
私のいる学部でも
見せることを出来る人を多く抱えるべきことを

579
00:36:22,183 --> 00:36:24,790
認めたがりません

580
00:36:24,790 --> 00:36:28,730
これが出来る人は数人か もう少しいるかもしれませんが
多くはいません

581
00:36:31,260 --> 00:36:32,452
この状況では

582
00:36:32,452 --> 00:36:36,510
大きな企業でも多くの訓練を行う必要があるのを
覚えておいてもらう必要があります

583
00:36:36,510 --> 00:36:40,335
Google はブレインのレジデンスの人たちを
訓練しています

584
00:36:40,335 --> 00:36:43,792
大学は最終的には
追いつくのではと想定しています

585
00:36:43,792 --> 00:36:48,360
なるほど 実は
多くの学生がこれに気付いたかもしれません

586
00:36:48,360 --> 00:36:53,131
トップクラスの博士課程の多くが
半分以上の志願者は

587
00:36:53,131 --> 00:36:57,079
プログラムではなく
見せることをしたいと望んでいます

588
00:36:57,079 --> 00:37:00,720
そして称賛されるべき点を
称賛するために言いますが

589
00:37:00,720 --> 00:37:04,930
deeplearning.ai が
深層学習のスペシャリゼーションを作っている一方で

590
00:37:04,930 --> 00:37:09,239
私の知る限り深層学習のMOOC は
実はあなたがCoursera で

591
00:37:09,239 --> 00:37:11,752
さかのぼること2012年に
行われました

592
00:37:12,828 --> 00:37:14,430
奇妙なことに

593
00:37:14,430 --> 00:37:18,900
この年にあなたは
RMSProp のアルゴリズムを発表しました

594
00:37:20,240 --> 00:37:25,910
そうです 知ってるでしょうけど
MOOC に誘われたからで

595
00:37:25,910 --> 00:37:30,239
とても懐疑的でしたが
ずっと押し続けましたね

596
00:37:30,239 --> 00:37:34,340
大変な作業が伴いましたが
行って非常によかったです

597
00:37:34,340 --> 00:37:37,409
そうですね 行ってくれてありがとうございます
作業がどれだけのものか

598
00:37:37,409 --> 00:37:38,351
苦情を言われたのを覚えています

599
00:37:38,351 --> 00:37:42,413
あなたは夜も残ってましたが
多くの学習者が

600
00:37:42,413 --> 00:37:47,330
最初のMOOC の恩恵を受けました
私はとても感謝しています

601
00:37:47,330 --> 00:37:49,260
ならよかったです

602
00:37:49,260 --> 00:37:53,290
AI のパラダイムシフトが起こる中で
何年にもわたってあなたがAIのパラダイムの議論に

603
00:37:53,290 --> 00:37:57,030
巻き込まれるのを見てきました

604
00:37:57,030 --> 00:37:59,984
これに関するあなたの考えを
共有してもらえますか

605
00:37:59,984 --> 00:38:05,157
よろこんで
さかのぼって50年代の初期のころには

606
00:38:05,157 --> 00:38:10,335
フォン・ノイマンやチューリンなどの人々は
シンボリックAI を信じていませんでした

607
00:38:10,335 --> 00:38:14,220
脳にはるかに強く
惹きつけられていました

608
00:38:14,220 --> 00:38:20,127
残念ながら彼らは若すぎて亡くなり
意見は人に届きませんでした

609
00:38:20,127 --> 00:38:21,806
AI の初期には

610
00:38:21,806 --> 00:38:26,259
知性に必要な表現は
なんらかの種類のシンボリックな表現が必要だと

611
00:38:26,259 --> 00:38:30,500
人々は完全に納得していました

612
00:38:30,500 --> 00:38:35,509
単調でない事柄を実行できるような場合には
整理されたロジックが必要だと考えられていました

613
00:38:35,509 --> 00:38:41,143
ロジックではないのですがロジックのようなもので
知性の本質は推論と考えられていました

614
00:38:41,143 --> 00:38:45,662
今では起こっているのは
完全に異なる見解があるということです

615
00:38:45,662 --> 00:38:50,984
考えというものは
神経活動の巨大なベクトルだということです

616
00:38:50,984 --> 00:38:55,200
考えがシンボリック表現ということとは
対照的になるものです

617
00:38:55,200 --> 00:38:59,087
考えがシンボリック表現と考えた人々は
大きな間違いをしてしまったと

618
00:38:59,087 --> 00:39:00,140
思います

619
00:39:01,210 --> 00:39:07,030
入力するのは文字列で
出力するのは文字列です

620
00:39:08,140 --> 00:39:12,580
このため 文字列は
明らかに事柄を表現する方法です

621
00:39:12,580 --> 00:39:15,710
その中にあるのは
文字列か文字列のような何かがあると

622
00:39:15,710 --> 00:39:18,360
考えたのです

623
00:39:18,360 --> 00:39:21,310
その中にあるのは
文字列とは異なるものだと考えます

624
00:39:21,310 --> 00:39:26,060
思考がなんらかの言語の中にあるという考えは
空間世界での配置の理解が

625
00:39:26,060 --> 00:39:30,980
ピクセルの中にあるという考えと同様に
馬鹿げています

626
00:39:30,980 --> 00:39:34,280
ピクセルが入ってくるのです

627
00:39:34,280 --> 00:39:37,930
仮にドットマトリックスプリンターが
人間に接続できたら

628
00:39:37,930 --> 00:39:41,929
ピクセルが出力されてくるでしょうが
その中にあるのはピクセルではありません

629
00:39:43,210 --> 00:39:46,620
私は考えというのは超巨大なベクトルだと
巨大なベクトルは因果を考える力があると

630
00:39:46,620 --> 00:39:48,460
考えます

631
00:39:48,460 --> 00:39:50,490
巨大なベクトルだからです

632
00:39:50,490 --> 00:39:56,100
思考がシンボリック表現という標準的なAI への考え方
全くありえないものです

633
00:39:56,100 --> 00:39:56,700
なるほど

634
00:39:57,740 --> 00:40:01,560
AI はこの新しい見解に
確かに到達しつつありますと思えています

635
00:40:01,560 --> 00:40:02,660
一部です

636
00:40:02,660 --> 00:40:08,230
AI 領域では多くの人が
思考はシンボリック表現の必要があると今でも考えていると思います

637
00:40:08,230 --> 00:40:09,780
このインタビューを受けていただいて
ありがとうございます

638
00:40:09,780 --> 00:40:12,970
深層学習が何十年もかけて進化したことや
今でも未来に向けてあなたが進めていることを

639
00:40:12,970 --> 00:40:17,680
聞くけたのは魅力的なことでした
ジェフさん ありがとうございました

640
00:40:17,680 --> 00:40:19,038
いえ この機会を与えてくれてありがと

641
00:40:19,038 --> 00:40:20,147
ありがとうございます。