1
00:00:00,060 --> 00:00:04,380
前回の動画では、l個の層を持つ

2
00:00:02,250 --> 00:00:06,150
ディープニューラルネットワークとは何か、

3
00:00:04,380 --> 00:00:08,550
そしてそのようなネットワークを
表現するために使う

4
00:00:06,150 --> 00:00:10,650
表記法をお話ししました。

5
00:00:08,550 --> 00:00:13,769
この動画では、
ディープネットワークにおいて

6
00:00:10,650 --> 00:00:16,440
どのように順伝播を行うのか見ていきます。

7
00:00:13,769 --> 00:00:18,660
まずいつものように、
一つの教師データXについて

8
00:00:16,440 --> 00:00:21,330
順伝播がどのように行われるか学び、

9
00:00:18,660 --> 00:00:22,920
その後、全ての教師サンプルに対して

10
00:00:21,330 --> 00:00:24,810
一気に順伝播を行うために

11
00:00:22,920 --> 00:00:26,849
ベクトル化する方法について

12
00:00:24,810 --> 00:00:29,660
お話しします。

13
00:00:26,849 --> 00:00:32,579
一つの教師サンプルxが与えられたとき、

14
00:00:29,660 --> 00:00:34,800
一つ目の層の活性化を計算する方法です。

15
00:00:32,579 --> 00:00:42,329
一つ目の層では、

16
00:00:34,800 --> 00:00:48,239
z[1]=W[1]x+b[1]です。

17
00:00:42,329 --> 00:00:51,120
W[1]とb[1]は、

18
00:00:48,239 --> 00:00:53,879
一層目の活性化に影響を与えるパラメータです。

19
00:00:51,120 --> 00:00:56,899
これがニューラルネットワークの一層目です。

20
00:00:53,879 --> 00:00:59,280
次に、この層の活性化を計算します。

21
00:00:56,899 --> 00:01:04,979
g(z[1])ですね。

22
00:00:59,280 --> 00:01:06,810
活性化関数gは

23
00:01:04,979 --> 00:01:09,090
層によって違うので、

24
00:01:06,810 --> 00:01:11,010
[1]というインデックスをつけて

25
00:01:09,090 --> 00:01:12,689
一層目の活性化関数であることを示します。

26
00:01:11,010 --> 00:01:13,320
さあ、一層目の活性化を

27
00:01:12,689 --> 00:01:18,360
計算し終わりました。

28
00:01:13,320 --> 00:01:24,470
二層目、この層はどうでしょう。

29
00:01:18,360 --> 00:01:32,189
z[2]=W[2]a[1]+b[2]なので、

30
00:01:24,470 --> 00:01:34,950
二層目の活性化は

31
00:01:32,189 --> 00:01:39,180
重みの行列×一層目からの出力

32
00:01:34,950 --> 00:01:44,270
この値ですね、

33
00:01:39,180 --> 00:01:49,579
そこに二層目のバイアスベクトルを
足したものになります。

34
00:01:44,270 --> 00:01:55,770
そしてa[2]は、
z[2]に活性化関数を適用したものです。

35
00:01:49,579 --> 00:01:57,990
二層目は以上です。

36
00:01:55,770 --> 00:02:00,299
ここからも同様に進めていって、

37
00:01:57,990 --> 00:02:06,240
出力レイヤー、つまり四層目まで行きます。

38
00:02:00,299 --> 00:02:09,959
z[4]は、

39
00:02:06,240 --> 00:02:11,780
その層のためのパラメータ

40
00:02:09,959 --> 00:02:14,569
×前の層からの活性化

41
00:02:11,780 --> 00:02:23,930
+バイアスベクトル

42
00:02:14,569 --> 00:02:26,720
そして今回も、a[4]=g[4](z[4])です。

43
00:02:23,930 --> 00:02:29,900
これが、予測される出力、
つまりy hatを計算する方法です。

44
00:02:26,720 --> 00:02:35,390
一つ言っておきたいのは、

45
00:02:29,900 --> 00:02:38,270
ここのxはa[0]と等しいということです。

46
00:02:35,390 --> 00:02:41,209
なぜなら、入力ベクトルxは

47
00:02:38,270 --> 00:02:44,000
0層目の活性化だからです。

48
00:02:41,209 --> 00:02:47,000
なので、xを消してa[0]と書いてみます。

49
00:02:44,000 --> 00:02:48,709
すると、これら全ての等式が

50
00:02:47,000 --> 00:02:53,980
同じような形になります。

51
00:02:48,709 --> 00:03:02,750
一般的なルールは、

52
00:02:53,980 --> 00:03:05,750
z[l]=W[l]a[l-1]+b[l]

53
00:03:02,750 --> 00:03:10,630
そしてこの層の活性化は

54
00:03:05,750 --> 00:03:16,850
zの値に活性化関数が適用されたものです。

55
00:03:10,630 --> 00:03:20,120
これが、一般的な順伝播の計算です。

56
00:03:16,850 --> 00:03:23,540
これで、

57
00:03:20,120 --> 00:03:26,299
単一の教師サンプルの話は終わりました。

58
00:03:23,540 --> 00:03:29,660
では、全ての教師サンプルを
一気に処理するために

59
00:03:26,299 --> 00:03:32,720
ベクトル化したらどうなるでしょう。

60
00:03:29,660 --> 00:03:35,030
式は今までと似たようなものです。

61
00:03:32,720 --> 00:03:40,060
一層目においては、

62
00:03:35,030 --> 00:03:48,410
大文字のZ[1]=W[1]X+b[1]で、

63
00:03:40,060 --> 00:03:54,650
A[1]=g[1](Z[1])となります。

64
00:03:48,410 --> 00:03:57,920
XはA[0]だということを

65
00:03:54,650 --> 00:03:59,959
覚えておいてください。

66
00:03:57,920 --> 00:04:01,850
これは、列ごとに教師サンプルが
並んでいるものです。

67
00:03:59,959 --> 00:04:05,450
これを取って

68
00:04:01,850 --> 00:04:08,269
A[0]と書き換えます。

69
00:04:05,450 --> 00:04:08,720
次の層も

70
00:04:08,269 --> 00:04:16,720
同じような感じです。

71
00:04:08,720 --> 00:04:21,980
Z[2]=W[2]A[1]+b[2]

72
00:04:16,720 --> 00:04:24,530
A[2]=g[2](Z[2])

73
00:04:21,980 --> 00:04:28,370
ベクトルzやaを取ってきて

74
00:04:24,530 --> 00:04:29,810
並べただけです。

75
00:04:28,370 --> 00:04:34,310
これは1つ目のサンプルのzベクトル、

76
00:04:29,810 --> 00:04:37,310
これは2つ目のサンプルのzベクトル…

77
00:04:34,310 --> 00:04:39,830
という感じでm個目まで続き、

78
00:04:37,310 --> 00:04:43,700
それらが列として並べられたものが

79
00:04:39,830 --> 00:04:47,390
大文字のZと呼ばれる行列です。

80
00:04:43,700 --> 00:04:50,000
大文字のAも、大文字のXと同様に

81
00:04:47,390 --> 00:04:52,040
教師サンプルが列になって

82
00:04:50,000 --> 00:04:53,720
左から右に並んでいるものです。

83
00:04:52,040 --> 00:04:59,450
この工程の最後には、

84
00:04:53,720 --> 00:05:03,200
Y hat=g(Z[4])が求められ、

85
00:04:59,450 --> 00:05:04,670
これはA[4]にも等しいです。

86
00:05:03,200 --> 00:05:08,000
これが、全ての教師サンプルの予測値を

87
00:05:04,670 --> 00:05:09,980
水平方向に並べたものです。

88
00:05:08,000 --> 00:05:12,590
では表記法を整理しましょう。

89
00:05:09,980 --> 00:05:17,720
上に書かれている表記法ですが、

90
00:05:12,590 --> 00:05:19,820
小文字のzとaを

91
00:05:17,720 --> 00:05:22,070
大文字に変えることができます。

92
00:05:19,820 --> 00:05:23,810
つまりここが大文字のZになりますね、

93
00:05:22,070 --> 00:05:25,790
ここまでで、ベクトル化を使って
教師サンプル全体に対して

94
00:05:23,810 --> 00:05:29,060
一気に順伝播を行う方法を見てきました。

95
00:05:25,790 --> 00:05:32,990
A[0]はXですね。

96
00:05:29,060 --> 00:05:35,240
このベクトル化した実装を見ると、

97
00:05:32,990 --> 00:05:37,670
ここにforループをつけたら

98
00:05:35,240 --> 00:05:40,370
よさそうです。

99
00:05:37,670 --> 00:05:44,360
for l=1...4、つまり

100
00:05:40,370 --> 00:05:47,000
lが1から大文字のLになるまでのループです。

101
00:05:44,360 --> 00:05:48,950
そして、一層目の活性化を計算し、

102
00:05:47,000 --> 00:05:51,860
二層目、三層目、四層目…と続きます。

103
00:05:48,950 --> 00:05:54,370
なので、ここにforループを置けば

104
00:05:51,860 --> 00:05:56,660
うまくいくでしょう。

105
00:05:54,370 --> 00:05:58,550
もちろん、
ニューラルネットワークを実装するとき

106
00:05:56,660 --> 00:06:00,770
なるべくforループを避けたいのは
わかっています。

107
00:05:58,550 --> 00:06:03,290
しかし、ここはforループを使う以外に

108
00:06:00,770 --> 00:06:05,060
実装する方法がないと

109
00:06:03,290 --> 00:06:06,590
思われる場所なので、

110
00:06:05,060 --> 00:06:09,080
順伝播を実装するときには

111
00:06:06,590 --> 00:06:10,700
forループを書いて全然構いません。

112
00:06:09,080 --> 00:06:12,740
こうすることで、

113
00:06:10,700 --> 00:06:15,050
一層目、二層目、三層目…と
計算することができます。

114
00:06:12,740 --> 00:06:17,210
forループを使う以外に

115
00:06:15,050 --> 00:06:19,970
lを１から大文字のL、

116
00:06:17,210 --> 00:06:23,060
つまりニューラルネットワークの
層の数だけ回せる方法を

117
00:06:19,970 --> 00:06:24,620
知っている人はいないし、

118
00:06:23,060 --> 00:06:27,830
私もわかりません。

119
00:06:24,620 --> 00:06:30,980
なので、ここでforループを使うのは

120
00:06:27,830 --> 00:06:32,690
全く問題ないのです。

121
00:06:30,980 --> 00:06:35,300
ここまでで、

122
00:06:32,690 --> 00:06:37,760
ディープニューラルネットワークの表記法と

123
00:06:35,300 --> 00:06:39,680
順伝播の方法が終わりました。

124
00:06:37,760 --> 00:06:41,900
これらのことにあなたが少し

125
00:06:39,680 --> 00:06:44,000
親しみを感じられたなら、

126
00:06:41,900 --> 00:06:45,830
隠れ層が1つだけのニューラルネットワークで

127
00:06:44,000 --> 00:06:47,750
やったことを取ってきて、

128
00:06:45,830 --> 00:06:50,750
それに似たものを

129
00:06:47,750 --> 00:06:53,420
繰り返しただけだからです。

130
00:06:50,750 --> 00:06:55,420
ディープニューラルネットワークを

131
00:06:53,420 --> 00:06:57,860
実装する中で、

132
00:06:55,420 --> 00:06:59,450
バグのない実装をする方法の一つは、

133
00:06:57,860 --> 00:07:01,580
行列のサイズについて

134
00:06:59,450 --> 00:07:03,500
秩序正しく慎重に

135
00:07:01,580 --> 00:07:05,300
考えることです。

136
00:07:03,500 --> 00:07:07,280
私が自分のコードを書くときには、

137
00:07:05,300 --> 00:07:08,960
ちょっと紙を取ってきて

138
00:07:07,280 --> 00:07:11,480
今扱っている行列のサイズを

139
00:07:08,960 --> 00:07:13,940
注意深く考えます。

140
00:07:11,480 --> 00:07:16,570
次の動画で、

141
00:07:13,940 --> 00:07:16,570
その方法を学びましょう。