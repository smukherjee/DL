1
00:00:00,390 --> 00:00:04,350
当你建立一个神经网络的时候

2
00:00:02,580 --> 00:00:06,720
你需要做的众多选择之一

3
00:00:04,350 --> 00:00:09,599
是对隐藏层使用什么样的激活函数

4
00:00:06,720 --> 00:00:11,490
以及对输出层的神经元使用什么样的激活函数

5
00:00:09,599 --> 00:00:13,139
到现在为止我们都（对输出层）

6
00:00:11,490 --> 00:00:16,080
使用了Sigmiod函数

7
00:00:13,139 --> 00:00:18,720
但是有时候使用别的激活函数会更好

8
00:00:16,080 --> 00:00:20,939
我们一起来看看

9
00:00:18,720 --> 00:00:23,279
其中的一些选择

10
00:00:20,939 --> 00:00:26,099
在神经网络正向传递步骤中

11
00:00:23,279 --> 00:00:28,710
我们在这三个步骤中使用了Sigmiod函数

12
00:00:26,099 --> 00:00:32,610
因此Sigmiod函数在这里也被称为激活函数

13
00:00:28,710 --> 00:00:37,590
这是一个大家熟悉的Sigmiod函数曲线

14
00:00:32,610 --> 00:00:40,680
a等于1除于1加欧拉常数e的负z次方

15
00:00:37,590 --> 00:00:42,600
在一般情况下

16
00:00:40,680 --> 00:00:49,739
我们可以使用其他的关于z的函数g

17
00:00:42,600 --> 00:00:53,309
我们把它写在这里

18
00:00:49,739 --> 00:00:56,010
g为一个非线性函数

19
00:00:53,309 --> 00:00:59,250
但是不一定是Sigmiod函数

20
00:00:56,010 --> 00:01:01,879
比如说，Sigmoid函数值的范围在0和1之间

21
00:00:59,250 --> 00:01:04,290
一个通常情况下效果比Sigmoid函数

22
00:01:01,879 --> 00:01:06,900
效果更好的激活函数

23
00:01:04,290 --> 00:01:10,320
是tanh函数

24
00:01:06,900 --> 00:01:14,189
也叫做双曲正切函数

25
00:01:10,320 --> 00:01:19,979
这是z，a和关于z的tanh函数

26
00:01:14,189 --> 00:01:25,710
函数值的范围在1和-1之间

27
00:01:19,979 --> 00:01:31,079
tanh函数的公式为

28
00:01:25,710 --> 00:01:37,799
e的z次方和e的-z次方之差

29
00:01:31,079 --> 00:01:40,140
除以他们的和

30
00:01:37,799 --> 00:01:43,890
它其实在数学上看是一个

31
00:01:40,140 --> 00:01:46,350
移位后的Sigmoid函数

32
00:01:43,890 --> 00:01:49,860
也就是一个普通的Sigmoid函数

33
00:01:46,350 --> 00:01:52,079
但是通过移位后

34
00:01:49,860 --> 00:01:54,570
经过原点并且重新调整比例

35
00:01:52,079 --> 00:01:58,530
使函数输出范围在-1和1之间

36
00:01:54,570 --> 00:02:05,340
如果在隐藏单元中
令关于z的函数g

37
00:01:58,530 --> 00:02:09,910
等于tanh(z)

38
00:02:05,340 --> 00:02:12,490
那么最终效果基本都会好过

39
00:02:09,910 --> 00:02:14,020
使用Sigmoid函数的结果
这是因为此时函数的值

40
00:02:12,490 --> 00:02:16,930
介于1和-1之间

41
00:02:14,020 --> 00:02:19,000
因此隐藏层激活函数输出的平均值

42
00:02:16,930 --> 00:02:21,550
会更加逼近于0

43
00:02:19,000 --> 00:02:23,020
有时候当你

44
00:02:21,550 --> 00:02:23,590
训练一个

45
00:02:23,020 --> 00:02:25,690
学习算法的时候

46
00:02:23,590 --> 00:02:29,709
你可能会中心化你的数据

47
00:02:25,690 --> 00:02:31,510
并且使用tanh替代Sigmiod函数

48
00:02:29,709 --> 00:02:34,750
以达到数据中心化的效果

49
00:02:31,510 --> 00:02:36,880
数据中心化使数据的平均值

50
00:02:34,750 --> 00:02:39,610
更加逼近零，而不是比如说0.5

51
00:02:36,880 --> 00:02:41,410
这个会使得下一层的学习

52
00:02:39,610 --> 00:02:43,510
变的更简单一点

53
00:02:41,410 --> 00:02:45,820
我们将在第二个课程中具体讨论

54
00:02:43,510 --> 00:02:47,380
彼时我们会在讨论优化算法时
进行进一步的研究

55
00:02:45,820 --> 00:02:50,739
这里面一个需要知道的是

56
00:02:47,380 --> 00:02:52,480
我几乎再也不使用Sigmoid函数

57
00:02:50,739 --> 00:02:54,250
作为激活函数

58
00:02:52,480 --> 00:02:56,410
再也不使用

59
00:02:54,250 --> 00:02:59,560
因为tanh函数在大多数情况下几乎

60
00:02:56,410 --> 00:03:03,550
都相当严格地优于Sigmoid
一个例外是

61
00:02:59,560 --> 00:03:07,420
在输出层上
因为y的值是0或者1

62
00:03:03,550 --> 00:03:10,570
因此可以理解

63
00:03:07,420 --> 00:03:13,989
你需要输出的y帽（拟合值）

64
00:03:10,570 --> 00:03:16,570
的输出在0和1之间

65
00:03:13,989 --> 00:03:19,360
而不是在-1和1之间

66
00:03:16,570 --> 00:03:21,430
在一个特例下我会使用Sigmoid

67
00:03:19,360 --> 00:03:24,670
作为激活函数，就是当你使用

68
00:03:21,430 --> 00:03:26,350
二元分类的时候

69
00:03:24,670 --> 00:03:29,709
在这种情况下你可以使用Sigmoid激活函数

70
00:03:26,350 --> 00:03:35,170
用于输出层

71
00:03:29,709 --> 00:03:37,180
关于z2的函数g等于Sigmoid(z2)

72
00:03:35,170 --> 00:03:40,299
你看到的例子这里

73
00:03:37,180 --> 00:03:43,920
你可能会在隐藏层

74
00:03:40,299 --> 00:03:47,769
用tanh作为激活函数

75
00:03:43,920 --> 00:03:49,299
将Sigmoid用作输出层的激活函数

76
00:03:47,769 --> 00:03:51,670
因此不同的层会使用不同的激活函数

77
00:03:49,299 --> 00:03:53,709
有时候需要标明

78
00:03:51,670 --> 00:03:55,690
每个层使用的激活函数

79
00:03:53,709 --> 00:03:58,510
都可能不同

80
00:03:55,690 --> 00:04:02,230
我们可以使用方括号的上标

81
00:03:58,510 --> 00:04:04,540
来表明G[1]

82
00:04:02,230 --> 00:04:06,940
和G[2]可能是不尽相同的函数

83
00:04:04,540 --> 00:04:09,340
上标标明在哪个层

84
00:04:06,940 --> 00:04:11,470
上标表示为层

85
00:04:09,340 --> 00:04:12,879
带有方括号的上标2表示输出层

86
00:04:11,470 --> 00:04:15,680
输出层

87
00:04:12,879 --> 00:04:18,109
Sigmoid和tanh函数的

88
00:04:15,680 --> 00:04:20,780
缺点之一是

89
00:04:18,109 --> 00:04:22,910
如果Z的值非常大或者非常小

90
00:04:20,780 --> 00:04:24,460
那么关于这个函数导数的梯度或者

91
00:04:22,910 --> 00:04:27,560
斜率会变的很小

92
00:04:24,460 --> 00:04:30,139
当Z很大或者很小的时候

93
00:04:27,560 --> 00:04:33,169
函数的斜率值

94
00:04:30,139 --> 00:04:35,270
会接近零

95
00:04:33,169 --> 00:04:38,360
这会使得梯度下降变的缓慢

96
00:04:35,270 --> 00:04:41,810
一个在机器学习界很流行的

97
00:04:38,360 --> 00:04:44,900
小工具叫做线性整流函数

98
00:04:41,810 --> 00:04:50,720
线性整流函数大概是这么样子的

99
00:04:44,900 --> 00:04:57,110
ReLU的公式为

100
00:04:50,720 --> 00:05:00,500
a等于0和z中较大一个值

101
00:04:57,110 --> 00:05:03,530
当z为正时导数为1

102
00:05:00,500 --> 00:05:05,990
反之当z为负的时候

103
00:05:03,530 --> 00:05:07,580
斜率，或说导数就为0

104
00:05:05,990 --> 00:05:10,190
当技术上去实现的时候

105
00:05:07,580 --> 00:05:12,349
虽然z正好为0时候的导数并不存在

106
00:05:10,190 --> 00:05:14,210
但是当你在计算机上实现的时候

107
00:05:12,349 --> 00:05:18,770
你通常会得到

108
00:05:14,210 --> 00:05:21,229
z正好为000...的非常小的数值

109
00:05:18,770 --> 00:05:22,940
所以你不需要去担心
（遇到z恰好为0的情况）

110
00:05:21,229 --> 00:05:25,610
在实际应用当中

111
00:05:22,940 --> 00:05:29,659
你可以认为当z为0时候的

112
00:05:25,610 --> 00:05:32,270
导数为1或者0

113
00:05:29,659 --> 00:05:35,479
这样做并不会有什么大的问题

114
00:05:32,270 --> 00:05:37,430
即使这个函数是不可微的

115
00:05:35,479 --> 00:05:40,010
这里有一些大致规则

116
00:05:37,430 --> 00:05:43,280
来选取激活函数

117
00:05:40,010 --> 00:05:45,620
当你处理的问题是二分类问题
此时的输出为0和1

118
00:05:43,280 --> 00:05:47,539
那么Sigmoid激活函数在输出层

119
00:05:45,620 --> 00:05:50,479
的使用是顺理成章的选择

120
00:05:47,539 --> 00:05:59,419
而在其他神经元上使用ReLU，或者

121
00:05:50,479 --> 00:06:04,460
也叫做线性整流函数

122
00:05:59,419 --> 00:06:07,190
会是一个用来做激活函数的更好选择

123
00:06:04,460 --> 00:06:10,280
如果你不确定用什么样的激活函数

124
00:06:07,190 --> 00:06:13,849
用在你的隐藏层上

125
00:06:10,280 --> 00:06:15,289
我推荐使用线性整流函数函数

126
00:06:13,849 --> 00:06:17,570
ReLU是目前广泛被人们使用的一个方法

127
00:06:15,289 --> 00:06:20,120
虽然有时候

128
00:06:17,570 --> 00:06:21,350
人们也会使用双曲函数

129
00:06:20,120 --> 00:06:23,150
作为激活函数

130
00:06:21,350 --> 00:06:26,270
ReLU的缺点之一是

131
00:06:23,150 --> 00:06:28,640
当z为负数的时候

132
00:06:26,270 --> 00:06:31,700
其导数为0,但在实际应用中并不是问题

133
00:06:28,640 --> 00:06:33,890
不过还有另外一个版本的ReLU

134
00:06:31,700 --> 00:06:35,420
叫做Leaky Relu

135
00:06:33,890 --> 00:06:38,690
我们将在下一张幻灯片上给出公式

136
00:06:35,420 --> 00:06:40,520
当z为负值时，函数值不是为0

137
00:06:38,690 --> 00:06:42,940
而是包含像这样的一个很小斜率

138
00:06:40,520 --> 00:06:47,900
这个叫做Leaky ReLU

139
00:06:42,940 --> 00:06:51,170
Leaky ReLU效果通常要好于

140
00:06:47,900 --> 00:06:53,900
ReLU激活函数

141
00:06:51,170 --> 00:06:54,860
虽然在实践中使用的相对少

142
00:06:53,900 --> 00:06:56,770
这是因为无论使用哪个，问题都是不大的

143
00:06:54,860 --> 00:06:59,330
如果你必须从2个中选取一个

144
00:06:56,770 --> 00:07:01,460
我通常会选用ReLU

145
00:06:59,330 --> 00:07:04,460
ReLU和Leaky ReLU的共有的优势是

146
00:07:01,460 --> 00:07:06,500
在z的数值空间里面

147
00:07:04,460 --> 00:07:08,150
激活函数的导数或者说

148
00:07:06,500 --> 00:07:11,870
激活函数的斜率

149
00:07:08,150 --> 00:07:13,970
离0比较远

150
00:07:11,870 --> 00:07:15,920
因此在实践当中使用普通的

151
00:07:13,970 --> 00:07:18,590
ReLU激活函数的话

152
00:07:15,920 --> 00:07:20,810
那么神经网络的学习速度通常会比使用

153
00:07:18,590 --> 00:07:23,840
双曲函数tanh或者Sigmoid函数来的更快

154
00:07:20,810 --> 00:07:26,420
主要原因是使学习变慢的斜率趋向0的现象

155
00:07:23,840 --> 00:07:28,700
变少了

156
00:07:26,420 --> 00:07:31,580
激活函数的导数趋向于0会降低学习的速度

157
00:07:28,700 --> 00:07:33,950
我们知道，一半z的数值范围

158
00:07:31,580 --> 00:07:36,710
ReLU的斜率为0

159
00:07:33,950 --> 00:07:39,050
但是在实际使用中

160
00:07:36,710 --> 00:07:41,120
大多数的隐藏单元的z值

161
00:07:39,050 --> 00:07:43,700
将会大于0，因此学习仍然可以很快

162
00:07:41,120 --> 00:07:45,800
让我们快速的回顾一下

163
00:07:43,700 --> 00:07:47,600
不同激活函数的优缺点

164
00:07:45,800 --> 00:07:50,030
这是一个Sigmoid激活函数

165
00:07:47,600 --> 00:07:52,790
我会建议

166
00:07:50,030 --> 00:07:54,410
不要使用这样的函数，除了输出层上并且

167
00:07:52,790 --> 00:07:56,330
你要解决的是二分类问题

168
00:07:54,410 --> 00:07:59,540
或者干脆完全不使用这个函数

169
00:07:56,330 --> 00:08:02,720
我几乎不使用这个函数的原因是

170
00:07:59,540 --> 00:08:05,060
tanh函数相比要好很多

171
00:08:02,720 --> 00:08:12,080
tanh激活函数

172
00:08:05,060 --> 00:08:13,430
大概是这个样子的

173
00:08:12,080 --> 00:08:15,490
而默认的最经常使用的激活函数

174
00:08:13,430 --> 00:08:19,100
则是ReLU函数，它是这个样子的

175
00:08:15,490 --> 00:08:23,660
当你没有特别好的选择的时候
可以使用ReLU

176
00:08:19,100 --> 00:08:26,600
你也可以尝试使用

177
00:08:23,660 --> 00:08:31,930
Leaky ReLU函数

178
00:08:26,600 --> 00:08:36,659
其a是0.01z，z值中的

179
00:08:31,930 --> 00:08:40,390
最大的0.01z和z中最大的那个

180
00:08:36,659 --> 00:08:43,810
0.01z和z使得函数略微弯曲

181
00:08:40,390 --> 00:08:46,200
你可能会说

182
00:08:43,810 --> 00:08:51,670
为什么要使用0.01这个常数

183
00:08:46,200 --> 00:08:53,380
你完全可以将其看作

184
00:08:51,670 --> 00:08:54,670
算法的另外一个参数

185
00:08:53,380 --> 00:08:58,480
有人说那样做效果更好

186
00:08:54,670 --> 00:08:59,649
但是我很少看到有人真正去做这件事

187
00:08:58,480 --> 00:09:01,360
如果你想在你的应用里面尝试一下

188
00:08:59,649 --> 00:09:03,430
大胆的去尝试吧！

189
00:09:01,360 --> 00:09:05,800
你能发现其是否有效

190
00:09:03,430 --> 00:09:08,290
如果效果好的话继续使用它

191
00:09:05,800 --> 00:09:09,880
我希望本课程给你一些

192
00:09:08,290 --> 00:09:11,620
可选的激活函数

193
00:09:09,880 --> 00:09:13,870
你可以用于你自己的神经网络

194
00:09:11,620 --> 00:09:15,940
事实上在深度学习中

195
00:09:13,870 --> 00:09:18,130
你通常会有很多选择来
建立你的深度神经网络

196
00:09:15,940 --> 00:09:20,110
你通常会有很多选择来
建立你的深度神经网络

197
00:09:18,130 --> 00:09:22,089
从隐藏单元的数量

198
00:09:20,110 --> 00:09:24,430
到激活函数的选择

199
00:09:22,089 --> 00:09:25,839
到我们将讨论的如何初始化权重

200
00:09:24,430 --> 00:09:28,480
有很多的选择

201
00:09:25,839 --> 00:09:30,880
还有很多相类似的选择

202
00:09:28,480 --> 00:09:33,279
多数时候很难得到现成的实践指导

203
00:09:30,880 --> 00:09:35,649
来精确的适配你的问题

204
00:09:33,279 --> 00:09:37,270
并最好的解决它

205
00:09:35,649 --> 00:09:39,070
通过这个系列的课程

206
00:09:37,270 --> 00:09:40,839
我会让你了解我在业界看到的方法

207
00:09:39,070 --> 00:09:43,450
哪些更流行，哪些少流行一些

208
00:09:40,839 --> 00:09:45,520
但是对于你的应用

209
00:09:43,450 --> 00:09:46,930
以及你应用的特征

210
00:09:45,520 --> 00:09:49,450
其实是很难事先去断定

211
00:09:46,930 --> 00:09:51,400
什么方法是最好的，

212
00:09:49,450 --> 00:09:52,930
通常的建议通常是

213
00:09:51,400 --> 00:09:54,940
如果你不确定哪个激活函数效果最好

214
00:09:52,930 --> 00:09:57,700
你可以逐个尝试使用他们

215
00:09:54,940 --> 00:10:00,010
并用交叉验证集或说开发数据集

216
00:09:57,700 --> 00:10:02,529
去验证他们的效果

217
00:10:00,010 --> 00:10:04,480
我们将在后面讲到

218
00:10:02,529 --> 00:10:08,350
看哪个方法的效果最好并选择它

219
00:10:04,480 --> 00:10:10,180
我认为通过测试

220
00:10:08,350 --> 00:10:13,510
这些不同的选择

221
00:10:10,180 --> 00:10:16,240
你能够更好的

222
00:10:13,510 --> 00:10:18,130
设计面向未来的神经网络架构

223
00:10:16,240 --> 00:10:20,550
来对应你问题的特征和算法的演进

224
00:10:18,130 --> 00:10:23,440
而不是我来告诉你

225
00:10:20,550 --> 00:10:25,630
例如使用ReLU激活函数

226
00:10:23,440 --> 00:10:27,339
而不要使用别的方法

227
00:10:25,630 --> 00:10:29,440
这样的建议不一定对于

228
00:10:27,339 --> 00:10:30,790
你想要在不久或者长远的将来

229
00:10:29,440 --> 00:10:32,410
解决的问题

230
00:10:30,790 --> 00:10:36,220
这个就是关于选择

231
00:10:32,410 --> 00:10:37,870
激活函数的内容

232
00:10:36,220 --> 00:10:39,310
你看到了最为流行

233
00:10:37,870 --> 00:10:41,459
的几种激活函数

234
00:10:39,310 --> 00:10:44,260
还有一个时常被提起的问题

235
00:10:41,459 --> 00:10:45,160
为什么你需要一个

236
00:10:44,260 --> 00:10:46,959
激活函数

237
00:10:45,160 --> 00:10:49,240
为什么不能干脆不用它

238
00:10:46,959 --> 00:10:49,779
我们会在下一个视频中讨论

239
00:10:49,240 --> 00:10:52,240
（音乐）

240
00:10:49,779 --> 00:10:54,430
你会理解

241
00:10:52,240 --> 00:10:58,259
为什么神经网络需要某种

242
00:10:54,430 --> 00:10:58,259
非线性的激活函数
GTC字幕组翻译