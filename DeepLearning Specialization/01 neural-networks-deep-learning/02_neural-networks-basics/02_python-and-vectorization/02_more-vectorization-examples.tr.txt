Bir önceki videoda vektörleştirme (vectorization) işlemini önemli ölçüde hızlandırmak için gömülmüş fonksiyonlar kullanımı ve fazla sayıda döngüden kurtulmanın nasıl olduğuyla ilgili birkaç video izlediniz. Şimdi bir kaç örneğe daha bakalım. Sinirsel ağ programlama veya lojistik regresyon programladığınız zaman asıl dikkat etmeniz gereken temel kural mümkün olduğunca açık durumdaki döngülerden kaçınmanızdır. Ve döngü kullanmadan bunu yapmanız her zaman mümkün değildir. Fakat gömülmüş fonksiyonlar kullanabildiğiniz zaman veya işlemi gerçekleştirmek için gereken neyse o yolu bulduğunuz zaman genellikle işler döngü kullanımına göre daha hızlı ilerler. Bir diğer örneğe birlikte bakalım. Eğer vektör U yu matrix A ve başka bir vektör V'nin çarpımının sonucu olarak elde etmek istersek bu durumda vektör U'yu yukarıdaki toplam sembolüyle ifade edilmiş olur. Right. Bu Ui yi nasıl tanımlayacağımızı gösterir Bu durumda vektörleştirmeme(non-vectorization) uygulaması U vektörünü NP nokta sıfırlar matrisi Sıfırlar (n,1) Bu durumda i sonsuza ve j de sonsuza Bu durumda yukarıdaki eşitlik elde edilir. Böylece elimizde i ve j üzerinden 2 tane döngü oldu İşte Bu vektörsüzleştirme versiyonudur. Vektörleştirme uygulamasındaki eşitlik böyledir. Ve bunun vektörleştirme uygulaması eldeki iki farklı for döngülerini eler ve işlemi ciddi anlamda hızlandırır Bir örnek üzerinden gidelim. Diyelim ki elinizde V vektörü var ve bunun her elemanını eksponansiyel (üstel) operasyonda uygulamak istiyorsunuz. u eşittir e üzeri v1 e üzeri v2 devam ediyor en son v  üzeri n olacak şekilde Gördüğünüz denklemler vektörsüzleştirme uygulamasında önce U'yu sıfır vektörüyle ilk değer atamaya ve sonrasında da for döngüsüyle elemanları bir kere de işleme sokar Python ve NumPy'de bir çok gömülmüş fonksiyonlar bize bu vektörlerin bir fonksiyonla işleme sokma kabiliyeti sağlar. Bu aşamada NumPy'yi NP olarak import ederiz. ve yukarıdaki denklemle U'yu tanımlarız. Bir önceki örneğe dönecek olursak bir tane açık for döngümüz vardı ve aynı işlemi V input U output olacak şekilde tek satır kod ile halettik. Böylece açık for döngüsü çıkarıldı ve uygulama çok daha hızlı bir şekilde konuldu. Aslında NumPy kütüphanesinin daha bir çok vektör değer fonksiyonları var Mesela Np.log V logaritmik işlem yapar Np.abs mutlak değer hesaplar. Np.maximum maksimum değer bulma işlemi yapar V with zero arasındaki bütün elemanların en büyüğünü bu fonksiyonla elde edebiliriz V**2 V'deki bütün elemanların karesini alır 1/V vektörün tersini alır Yani, ne zaman for loop yazma ihtiyacı duyduğunuzda dönüp bakmakta ve NumPY gömülmüş fonksiyonu ile yapılabilir mi diye kontrol etmekte fayda var. Tüm bu edinimlerimizi kullanıp bunu lojistik regresyon gradyen iniş uygulamasını yapalım ve bir veya iki döngüden kurtulabilir miyiz bakalım. Az önce üzerinde konuştuğumuz işlemin kodu burada ve içerisinde 2 adet for döngüsü var. Birisi burada ve diğeri de burada kurulmuş Örneğimizde, denklem bu şekilde ancak elinizde 2den fazla öznitelik varsa bu durumda DW1, DW2 ... üzerinde for döngülerine ihtiyacınız vardır. Eğer J= 1 ... nx DWJ'nin güncellenmesi gerekir. Yani ikinci for-loop'un elimine edilmesini istiyoruz Yapacağımız işlem bu satırda budur. Burada DW1'yı açıkça ilk değer atamak yerine dw2 ve sıfırları bundan kurtulacağız ve DW 'yı bir vektör yapacağız. dw 'yu np.zeros olarak kuracağız ve NX bir boyutlu vektör olarak atacağız. Ve burada özel bileşenler üzerinde for döngüsü kurmak yerine vektör değer operasyonu kullanacağız. dw + eşittir X (i) çarpı dz(i) ve bunun yerine dW bölü eşittir M. Ve şimdi iki for döngüsü yerine sadece bir for döngümüz var. Hala bir for döngümüz test örneğindeki özel bileşenler üzerinde Umarım bu video sayesinde vektörleştirme ve bir for loop'tan kurtulma ve kodun hızlı çalışması konusunda size bir fikir vermiştir. Bunun daha iyisini de yapabiliriz. Bir sonraki videoda lojistik regresyonun nasıl vektörleştirileceği üzerine konuşacağız ve hatta for loop kullanmadan hoş ve şaşırtıcı sonuç göreceksiniz. Test örneklerinde for döngüsüne gerek kalmadan tüm test veri setinin prosesi kodunu yazabileceksiniz. --hemen hemen aynı zamanda -- Bir sonraki videoda görüşmek üzere.