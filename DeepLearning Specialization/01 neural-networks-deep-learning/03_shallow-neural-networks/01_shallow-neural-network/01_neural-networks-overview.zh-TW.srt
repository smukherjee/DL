1
00:00:00,000 --> 00:00:02,190
歡迎回來。這個星期

2
00:00:02,190 --> 00:00:04,530
你學會建置一個神經網路。

3
00:00:04,530 --> 00:00:06,885
在深入探討技術細節之前,

4
00:00:06,885 --> 00:00:08,070
我想在這個影片中,

5
00:00:08,070 --> 00:00:12,000
為您提供本周影片中
您將看到的內容概述。

6
00:00:12,000 --> 00:00:14,910
如果您跟不上這些細節,

7
00:00:14,910 --> 00:00:18,810
別擔心,我們將在接下來的
幾個影片中深入探討技術細節。

8
00:00:18,810 --> 00:00:23,490
但是現在,讓我們快速概述一下,
如何建置你的網路。

9
00:00:23,490 --> 00:00:26,250
上周,我們討論了羅吉斯迴歸分析

10
00:00:26,250 --> 00:00:31,965
我們看到這個模型,如何對應于下面的計算圖形,

11
00:00:31,965 --> 00:00:35,640
在那裡,您加入特徵 x 和參數

12
00:00:35,640 --> 00:00:40,140
w 和 b, 那允許您計算 z,然後用於計算 a,

13
00:00:40,140 --> 00:00:43,280
我們使用 a 可互換為

14
00:00:43,280 --> 00:00:48,065
輸出 y-hat,然後您可以計算損失函數 L,

15
00:00:48,065 --> 00:00:51,390
一個神經網路,看起來像這樣.

16
00:00:51,390 --> 00:00:53,415
正如我之前已經提到的,

17
00:00:53,415 --> 00:00:58,640
你可以通過堆疊大量的小sigmoid單位,
來形成一個神經網路。

18
00:00:58,640 --> 00:01:03,785
而以前,此節點對應于兩個計算步驟。

19
00:01:03,785 --> 00:01:05,975
第一個是計算 z 值,

20
00:01:05,975 --> 00:01:08,630
其次,它計算這 a 值。

21
00:01:08,630 --> 00:01:10,460
在這個神經網路中,

22
00:01:10,460 --> 00:01:16,565
這堆節點將對應于,這樣類似的 z 的計算,

23
00:01:16,565 --> 00:01:20,165
以及,類似這樣 a 的計算。

24
00:01:20,165 --> 00:01:26,270
然後,該節點將對應于另一個 z 
和另一個 a 的類似計算。

25
00:01:26,270 --> 00:01:30,380
因此,我們稍後將介紹的標記法,如下所示。

26
00:01:30,380 --> 00:01:33,365
首先輸入特徵, X

27
00:01:33,365 --> 00:01:36,190
連同一些參數 W 和 b,

28
00:01:36,190 --> 00:01:39,840
這將允許您計算 z_1.。

29
00:01:39,840 --> 00:01:43,625
因此,我們將介紹的新標記法是,我們將使用

30
00:01:43,625 --> 00:01:47,060
上標方括弧 1 指的是

31
00:01:47,060 --> 00:01:51,185
與此節點堆疊關聯的數值,它被稱為層。

32
00:01:51,185 --> 00:01:54,080
稍後,我們將使用上標方括弧 2

33
00:01:54,080 --> 00:01:58,355
來標示與該節點關聯的數值。

34
00:01:58,355 --> 00:02:01,445
這稱為神經網路的另一層。

35
00:02:01,445 --> 00:02:03,680
上標方括弧,

36
00:02:03,680 --> 00:02:05,135
就像我們這裡一樣

37
00:02:05,135 --> 00:02:06,905
不要混淆於

38
00:02:06,905 --> 00:02:12,980
上標圓形括弧,我們用它來標示單ㄧ訓練例子。

39
00:02:12,980 --> 00:02:17,885
所以, x 上標圓括弧,我指的是第 i 個訓練示例,

40
00:02:17,885 --> 00:02:23,180
上標方括弧 1 和 2 指這些不同的層;

41
00:02:23,180 --> 00:02:27,530
第一層和第二層在這個神經網路。

42
00:02:27,530 --> 00:02:33,920
計算完 z[1] 後類似于羅吉斯迴歸分析之後,

43
00:02:33,920 --> 00:02:37,715
會有一個計算 a[1]的計算,

44
00:02:37,715 --> 00:02:40,970
那只是 sigmoid(z[1]),

45
00:02:40,970 --> 00:02:51,530
然後使用另一個線性方程式計算 z[2],然後計算 a[2]。

46
00:02:51,530 --> 00:02:55,325
a[2] 是最終的輸出

47
00:02:55,325 --> 00:02:59,270
在這神經網路,也將與 y-hat互換使用。

48
00:02:59,270 --> 00:03:02,390
所以,我知道這有很多的細節,但關鍵

49
00:03:02,390 --> 00:03:05,925
記住的是,對於羅吉斯迴歸分析,

50
00:03:05,925 --> 00:03:09,435
我們這個 z ,跟著是 a 的計算.

51
00:03:09,435 --> 00:03:10,665
在這個神經網路中,

52
00:03:10,665 --> 00:03:12,830
在這裡,我們只是做很多次,

53
00:03:12,830 --> 00:03:14,870
這個 z, 跟著是 a 的計算,

54
00:03:14,870 --> 00:03:17,745
這個 z, 跟著是 a 的計算

55
00:03:17,745 --> 00:03:21,590
最後計算損失。

56
00:03:21,590 --> 00:03:24,200
您還記得,對於羅吉斯迴歸分析

57
00:03:24,200 --> 00:03:27,800
我們有這個反向的計算來

58
00:03:27,800 --> 00:03:32,045
計算導數,或者在計算 da,

59
00:03:32,045 --> 00:03:33,365
dz 等等。

60
00:03:33,365 --> 00:03:34,805
所以,以同樣的方式,

61
00:03:34,805 --> 00:03:38,990
神經網路也會做向後計算,看起來像

62
00:03:38,990 --> 00:03:47,370
這樣,您會計算 da[2],

63
00:03:47,370 --> 00:03:51,060
dz[2],允許您計算 dw[2],

64
00:03:51,060 --> 00:03:56,130
db[2],等等。

65
00:03:56,130 --> 00:04:04,060
此從右至左向後計算,用紅色箭頭來表示。

66
00:04:04,060 --> 00:04:08,750
這為您提供了神經網路的快速概述。

67
00:04:08,750 --> 00:04:12,470
它基本上,採取羅吉斯迴歸分析和重複兩次。

68
00:04:12,470 --> 00:04:14,780
我知道有很多新的記號法,

69
00:04:14,780 --> 00:04:16,910
新的細節,不要擔心保存他們,

70
00:04:16,910 --> 00:04:21,575
遵循一切,最有可能,我們將在
接下來的幾個影片進入細節。

71
00:04:21,575 --> 00:04:23,060
因此,讓我們繼續下一個影片

72
00:04:23,060 --> 00:04:26,270
我們將開始討論怎樣表示神經網路。