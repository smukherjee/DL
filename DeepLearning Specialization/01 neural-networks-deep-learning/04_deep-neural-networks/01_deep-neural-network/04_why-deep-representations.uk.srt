1
00:00:00,000 --> 00:00:03,339
Ми всі чули що глибокі нейронні мережі (НМ) добре розв'язують багато

2
00:00:03,339 --> 00:00:07,073
задач. Але справа не лише в тому, що вони мають бути великими,

3
00:00:07,073 --> 00:00:10,718
а в тому, що вони мають бути глибокими, 
мати багато прихованих шарів.

4
00:00:10,718 --> 00:00:12,208
Чому це так?

5
00:00:12,208 --> 00:00:15,833
Давай розглянемо кілька прикладів і спробуємо зрозуміти чому

6
00:00:15,833 --> 00:00:17,720
глибокі НМ будуть добре працювати.

7
00:00:17,720 --> 00:00:22,181
Насамперед, що таке ці "глибокі" обчислення?

8
00:00:22,181 --> 00:00:25,393
Якщо ми будуємо систему розпізнавання облич або

9
00:00:25,393 --> 00:00:29,631
виявлення облич, то ось що може робити глибока НМ.

10
00:00:29,631 --> 00:00:35,059
Скажімо, ми вводимо зображення обличчя. Тоді 1-ий шар НМ

11
00:00:35,059 --> 00:00:40,000
можна сказати буде виявляти риси або краї.

12
00:00:40,000 --> 00:00:45,519
В цьому прикладі я зобразив як НМ зі, скажімо, 
20 прихованими вузлами

13
00:00:45,519 --> 00:00:48,017
буде "обчислювати" це зображення.

14
00:00:48,017 --> 00:00:52,357
Тож 20 прихованих вузлів візуалізовані в цих малих прямокутниках.

15
00:00:52,357 --> 00:00:57,325
Тож, наприклад, 
ця маленька візуалізація відображає як прихований вузол

16
00:00:57,325 --> 00:01:01,978
намагається знайти розташування краю в зображенні.

17
00:01:01,978 --> 00:01:05,914
І, можливо, цей прихований вузол намагається

18
00:01:05,914 --> 00:01:09,955
знайти горизонтальні краї зображення.

19
00:01:09,955 --> 00:01:13,184
Коли ми говоритимемо про згорткові НМ в наступних курсах

20
00:01:13,184 --> 00:01:16,129
ця часткова візуалізація матиме більший зміст.

21
00:01:16,129 --> 00:01:19,562
Щоб уявити обчислення 1-ого шару НМ, подивись

22
00:01:19,562 --> 00:01:22,690
на зображення і спробуй виділити у ньому краї. 
[Напевно маються на увазі перепади кольорів]

23
00:01:22,690 --> 00:01:27,356
Тепер давай спробуємо виділити краї у зображенні, групуючи

24
00:01:27,356 --> 00:01:28,730
пікселі, щоб сформувати ці краї.

25
00:01:28,730 --> 00:01:34,670
Після цього виявлені краї можна згрупувати і сформувати частини облич.

26
00:01:34,670 --> 00:01:40,289
Тож, наприклад, може знайтись глибший нейрон, що намагається знайти око.

27
00:01:40,289 --> 00:01:44,480
Або інший нейрон, що намагається знайти частину носа.

28
00:01:44,480 --> 00:01:47,463
Тож, групуючи багато країв,

29
00:01:47,463 --> 00:01:50,970
може початись виявлення різних частин облич.

30
00:01:50,970 --> 00:01:56,035
І, нарешті, групуючи різні частини облич

31
00:01:56,035 --> 00:02:01,006
(ока, носа чи підборіддя), шар може спробувати розпізнати або

32
00:02:01,006 --> 00:02:03,564
виявити різні типи облич.

33
00:02:03,564 --> 00:02:07,755
Тож інтуїтивно можна уявити, що попередні шари НМ

34
00:02:07,755 --> 00:02:10,190
виявляли прості функції, як от краї,

35
00:02:10,190 --> 00:02:14,573
потім групували їх в пізніших шарах НМ, тобто

36
00:02:14,573 --> 00:02:17,625
вивчали все складніші і складніші функції.

37
00:02:17,625 --> 00:02:23,640
Ці візуалізації матимуть більший зміст, 
коли ми говоритимемо про згорткові НМ.

38
00:02:23,640 --> 00:02:26,203
Одна технічна деталь цієї візуалізації:

39
00:02:26,203 --> 00:02:29,802
виявлячі країв розглядають відносно маленькі ділянки зображення,

40
00:02:29,802 --> 00:02:31,703
можливо дуже маленькі, як ось цей;

41
00:02:31,703 --> 00:02:36,616
а виявлячі облич, можливо, - набагато більші ділянки.

42
00:02:36,616 --> 00:02:41,308
Але з цього головне зрозуміти, що спочатку шукаються прості речі,

43
00:02:41,308 --> 00:02:43,675
як краї, а потім з них будуються

44
00:02:43,675 --> 00:02:47,216
або їх групують для виявлення більш складних речей, таких як око або ніс,

45
00:02:47,216 --> 00:02:50,530
а потім групують і ці речі, щоб знайте ще більш складні.

46
00:02:50,530 --> 00:02:55,665
І це ієрархічне представлення від простого до складного

47
00:02:55,665 --> 00:02:58,508
або композиційне представлення

48
00:02:58,508 --> 00:03:04,114
може застосовуватись не лише для зображень чи розпізнавання облич.

49
00:03:04,114 --> 00:03:08,593
Наприклад, коли ми будуємо систему розпізнавання мовлення, 
то візуалізувати це складно, але,

50
00:03:08,593 --> 00:03:10,908
якщо ввести аудіодоріжку,

51
00:03:10,908 --> 00:03:15,684
то, можливо, 1-ий шар НМ навчатиметься виявляти

52
00:03:15,684 --> 00:03:20,863
низькорівневі характеристики звукових хвиль, наприклад, піднімається

53
00:03:20,863 --> 00:03:21,703
чи спадає тон,

54
00:03:21,703 --> 00:03:26,869
білий шум це чи свистячий звук

55
00:03:26,869 --> 00:03:27,903
і наскільки стрімкі коливання

56
00:03:27,903 --> 00:03:31,124
в сенсі низькорівневих характеристик звукових хвиль.

57
00:03:31,124 --> 00:03:34,233
А далі [2-ий шар], групуючи низькорівневі характеристики, можливо,

58
00:03:34,233 --> 00:03:37,937
навчатиметься виявляти структурні елементи звуку,

59
00:03:37,937 --> 00:03:40,297
які в лінгвістиці називаються фонемами.

60
00:03:40,297 --> 00:03:45,098
Наприклад, в слові "кіт" "к" - фонема, "і" - фонема,

61
00:03:45,098 --> 00:03:46,787
"т" - ще одна фонема.

62
00:03:46,787 --> 00:03:49,987
Тож, можливо, навчатиметься знаходити структурні елементи звуку.

63
00:03:49,987 --> 00:03:54,688
А далі [3-ій шар], групуючи це, можливо, 
навчатиметься розпізнавати слова в звуковій доріжці.

64
00:03:54,688 --> 00:03:58,270
А далі [4-ий шар] групуватиме це,

65
00:03:58,270 --> 00:04:02,912
щоб розпізнати цілі фрази або речення.

66
00:04:02,912 --> 00:04:07,572
Тож в глибокій НМ з багатьма прихованими шарами, можливо, перші

67
00:04:07,572 --> 00:04:10,477
шари виявлятимуть оці низькорівневі прості характеристики, а далі

68
00:04:10,477 --> 00:04:15,339
глибші шари групуватимуть простіші речі, які вже виявлені,

69
00:04:15,339 --> 00:04:19,392
щоб виявляти більш складні речі, такі як окремі слова

70
00:04:19,392 --> 00:04:21,040
або навіть фрази і речення,

71
00:04:21,040 --> 00:04:24,745
які ми проговоримо, щоб здійснити розпізнавання мовлення.

72
00:04:24,745 --> 00:04:30,168
Тож ми бачимо, що перші шари обчислюють ніби

73
00:04:30,168 --> 00:04:35,673
відносно прості функції на вхідних даних, такі як пошук країв, а коли ми

74
00:04:35,673 --> 00:04:41,046
заглиблюємось в НМ, то починаємо обчислювати несподівано складні речі,

75
00:04:41,046 --> 00:04:44,876
такі як виявлення облич або виявлення слів, фраз чи речень.

76
00:04:44,876 --> 00:04:48,767
Дехто проводить аналогію між глибокими НМ і

77
00:04:48,767 --> 00:04:52,656
людським мозком, де ми віримо, 
чи нейронауковці вірять, що людський мозок

78
00:04:52,656 --> 00:04:57,162
так само починає виявляти прості речі - 
краї, що розрізняють очі -

79
00:04:57,162 --> 00:05:00,370
а потім складає ці речі в складніші, 
щоб виявити більш складні речі, такі як

80
00:05:00,370 --> 00:05:02,440
обличчя, що ми бачимо.

81
00:05:02,440 --> 00:05:05,038
Я думаю, що проведення аналогії між

82
00:05:05,038 --> 00:05:08,276
глибокими НМ і людським мозком інколи дещо небезпечне,

83
00:05:08,276 --> 00:05:13,301
хоча багато в чому ми не помиляємось, 
припускаючи як працює людський

84
00:05:13,301 --> 00:05:18,102
мозок і як він, можливо, 
насправді спочатку виявляє прості речі, такі як краї,

85
00:05:18,102 --> 00:05:22,598
а потім складає їх в більш складні об'єкти. Тож

86
00:05:22,598 --> 00:05:27,430
це послужило певною формою натхнення 
для деякого глибокого навчання (ГН).

87
00:05:27,430 --> 00:05:29,850
Ми поговоримо трохи більше про людський

88
00:05:29,850 --> 00:05:33,065
мозок або біологічний мозок пізніше на цьому тижні.

89
00:05:35,534 --> 00:05:40,407
Деяке розуміння того, чому глибокі НМ добре

90
00:05:40,407 --> 00:05:42,756
працюють, закладене ось тут.

91
00:05:42,756 --> 00:05:47,868
Цей висновок виведений з теорії електричних кіл, 
яка намагається визначити

92
00:05:47,868 --> 00:05:53,760
які типи функцій можуть бути обчислені за допомогою різних І, 
АБО та НЕ вентилів, тобто логічних вентилів.

93
00:05:53,760 --> 00:05:58,860
Грубо кажучи, функції, 
які ми обчислюємо відносно малими, але глибокими НМ

94
00:05:58,860 --> 00:06:03,595
(під малими я маю на увазі, 
що кількість прихованих вузлів відносно мала),

95
00:06:03,595 --> 00:06:07,553
проте, якщо ми спробуємо обчислити цю ж функцію мілкою НМ, тобто

96
00:06:07,553 --> 00:06:09,178
маючи досить небагато прихованих шарів,

97
00:06:09,178 --> 00:06:13,296
то, швидше за все, нам потрібно буде 
обчислити експоненційно більше прихованих вузлів.

98
00:06:13,296 --> 00:06:18,109
Тож, давай я спробую проілюструвати це в загальних рисах 
на одному прикладі.

99
00:06:18,109 --> 00:06:21,423
Скажімо, ми намагаємось обчислити Виключне АБО [XOR] 
або паритет

100
00:06:21,423 --> 00:06:23,349
всіх вхідних ознак.

101
00:06:23,349 --> 00:06:28,430
Тож ми намагаємось обчислити x₁ XOR x₂ XOR x₃ XOR і так до

102
00:06:28,430 --> 00:06:33,064
xₙ (тобто ми маємо n ознак x).

103
00:06:33,064 --> 00:06:39,924
Тож, якщо ми побудуємо XOR дерево ось так... 
Тобто спочатку обчислимо XOR для x₁

104
00:06:39,924 --> 00:06:44,586
та x₂, далі візьмемо x₃ та x₄ і обчислимо їхнє XOR...

105
00:06:44,586 --> 00:06:49,392
Технічно, якщо ми використовуємо лише вентилі І, АБО і НЕ, 
то нам потрібно буде кілька шарів, щоб обчислити

106
00:06:49,392 --> 00:06:54,196
функцію XOR, а не просто 1 шар. 
Проте, ми зможемо обчислити XOR

107
00:06:54,196 --> 00:06:58,791
за допомогою відносно невеликої кількості вентилів. 
І так далі.

108
00:06:58,791 --> 00:07:03,987
І так ми будемо будувати справжнє XOR дерево...

109
00:07:03,987 --> 00:07:12,090
Ось так. Поки, врешті-решт, не отримаємо (назвімо це y)

110
00:07:12,090 --> 00:07:15,236
вихідний вентиль ŷ, що =y,

111
00:07:15,236 --> 00:07:18,398
Виключному АБО або паритету усіх вхідних бітів.

112
00:07:18,398 --> 00:07:24,790
Тож при обчисленні Виключного АБО 
глибина НМ буде пропорційна log(n). [O(log(n))]

113
00:07:24,790 --> 00:07:27,410
[незрозуміло]

114
00:07:27,410 --> 00:07:30,836
Тож кількість вузлів або кількість кружечків,

115
00:07:30,836 --> 00:07:33,929
або кількість вентилів в цій НМ не така вже й велика.

116
00:07:33,929 --> 00:07:38,452
Нам непотрібна велика кількість вентилів, 
щоб обчислити Виключне АБО.

117
00:07:38,452 --> 00:07:43,458
Але що якщо ми не можемо використовувати НМ з багатьма

118
00:07:43,458 --> 00:07:48,203
прихованими шарами 
(в цьому випадку O(log(n)) прихованих шарів)? Що якщо нам необхідно

119
00:07:48,203 --> 00:07:53,382
обчислити цю функцію лише одним прихованим шаром? 
Тобто, якщо всі ці ознаки

120
00:07:53,382 --> 00:07:57,912
будуть спрямовані у всі ці приховані вузли,

121
00:07:57,912 --> 00:08:02,116
а потім всі ці вузли - у вихідне y,

122
00:08:02,116 --> 00:08:07,120
то, щоб обчислити паритет (функцію XOR), цей

123
00:08:07,120 --> 00:08:12,124
прихований шар повинен буде бути експоненційно великим, тому що

124
00:08:12,124 --> 00:08:18,397
ми будемо повинні вичерпно перерахувати 
всі від 2 до n можливих комбінацій.

125
00:08:18,397 --> 00:08:23,139
Тобто всі по черзі від 2 до n можливих комбінацій

126
00:08:23,139 --> 00:08:27,898
вхідних бітів, щоб отримати в результаті 1 або 0.

127
00:08:27,898 --> 00:08:32,213
Тобто наш прихований шар буде експоненційно рости

128
00:08:32,213 --> 00:08:33,554
по відношенню до кількості бітів.

129
00:08:33,554 --> 00:08:38,229
По-моєму, технічно це буде 2 в степені n-1 прихованих вузлів.

130
00:08:38,229 --> 00:08:43,948
А 2 в степені n і є експоненційною залежністю від кількості бітів.

131
00:08:43,948 --> 00:08:49,149
Тож, надіюсь, це дає розуміння, що математичні функції

132
00:08:49,149 --> 00:08:55,275
набагато легше обчислювати глибокими НМ ніж мілкими.

133
00:08:55,275 --> 00:09:01,028
Мушу зазначити, що для мене особисто цей висновок 
теорії електричних кіл не надто простий для

134
00:09:01,028 --> 00:09:05,985
розуміння, але його дуже часто

135
00:09:05,985 --> 00:09:11,223
цитують коли пояснюють важливість глибокого представлення.

136
00:09:11,223 --> 00:09:13,600
Тепер додам ще дещо до причин,

137
00:09:13,600 --> 00:09:16,897
через які надають перевагу глибоким НМ.

138
00:09:16,897 --> 00:09:22,204
Буду повністю відвертим. Мені здається, що ще однією з причин, 
чому поняття "глибоке навчання" так широко розповсюдилось є брендинг.

139
00:09:22,204 --> 00:09:26,776
Ми називаємо ці речі НМ з великою кількістю 
прихованих шарів, але словосполучення "глибоке навчання"

140
00:09:26,776 --> 00:09:31,198
стало відомим брендом. Воно просто "дуже глибоке".

141
00:09:31,198 --> 00:09:36,284
Тож я думаю, що цей термін з'явився в результаті 
ребрендингу "нейронних мереж" або

142
00:09:36,284 --> 00:09:39,622
ребрендингу "нейронних мереж з багатьма прихованими шарами",

143
00:09:39,622 --> 00:09:42,970
щоб захопити уяву мас.

144
00:09:42,970 --> 00:09:47,479
Вважається, що бренд "глибокі НМ" добре робить свою роботу.

145
00:09:47,479 --> 00:09:51,342
Інколи люди дуже захоплюються і наполягають 
на використанні безлічі прихованих шарів,

146
00:09:51,342 --> 00:09:55,500
але коли я починаю вирішувати нову задачу, я часто починаю навіть з

147
00:09:55,500 --> 00:09:58,803
логістичної регресії, а потім пробую 1 або 2 приховані шари,

148
00:09:58,803 --> 00:10:01,722
а потім ніби використовую це 
[кількість прихованих шарів] як гіперпараметр.

149
00:10:01,722 --> 00:10:05,731
Використовую це як параметр або гіперпараметр, 
який можна налагоджувати, щоб спробувати

150
00:10:05,731 --> 00:10:07,935
знайти правильну глибину НМ.

151
00:10:07,935 --> 00:10:12,800
Проте, в останні кілька років була тенденція: люди виявили, що

152
00:10:12,800 --> 00:10:17,590
для деяких застосунків потрібні дуже-дуже глибокі НМ, 
інколи з кількома десятками шарів.

153
00:10:17,590 --> 00:10:22,264
Інколи це може бути і найкраща модель для задачі.

154
00:10:22,264 --> 00:10:27,605
Тож це все щодо розуміння чому ГН добре працює.

155
00:10:27,605 --> 00:10:31,411
Тепер давай до прямого розглянемо ще й механіку реалізації

156
00:10:31,411 --> 00:10:33,769
зворотного поширення.