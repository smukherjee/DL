1
00:00:00,000 --> 00:00:03,339
हम सब सुन रहे हैं कि डीप न्यूरल नेटवर्क 
बहुत अच्छा काम करते हैं

2
00:00:03,339 --> 00:00:07,073
बहुत सी समस्याओं के लिए, और ऐसा नहीं है कि
 वे होने चाहिए एक बड़े नेटवर्क्स,

3
00:00:07,073 --> 00:00:10,718
कि विशेष रूप से, वे होने चाहिए डीप या 
होनी चाहिए बहुत सी हिडन लेयर्स.

4
00:00:10,718 --> 00:00:12,208
तो ऐसा क्यों है?

5
00:00:12,208 --> 00:00:15,833
चलो देखते हैं कुछ उदाहरण और कोशिश 
करते हैं समझने की कि

6
00:00:15,833 --> 00:00:17,720
क्यों डीप नेटवर्क्स शायद अच्छा चलते हैं.

7
00:00:17,720 --> 00:00:22,181
तो पहले, क्या है एक डीप नेटवर्क कम्प्यूटिंग?

8
00:00:22,181 --> 00:00:25,393
यदि आप बना रहे हैं एक सिस्टम
 फ़ेस रेकग्निशन के लिए या

9
00:00:25,393 --> 00:00:29,631
फ़ेस डिटेक्शन के लिए, यहाँ है कि 
क्या एक डीप न्यूरल नेटवर्क करेगा.

10
00:00:29,631 --> 00:00:35,059
शायद आप इनपुट करें एक छवि एक 
फ़ेस की तब पहली लेयर न्यूरल नेटवर्क की

11
00:00:35,059 --> 00:00:40,000
को आप सोच सकते हैं शायद एक फ़ीचर 
डिटेक्टर जैसे या एक एज डिटेक्टर जैसे.

12
00:00:40,000 --> 00:00:45,519
इस उदाहरण में, मैं प्लॉट कर रहा हूँ 
क्या है एक न्यूरल नेटवर्क 
शायद 20 हिडन यूनिट्स के साथ,

13
00:00:45,519 --> 00:00:48,017
कोशिश करेगा कम्प्यूट करने की इस छवि पर.

14
00:00:48,017 --> 00:00:52,357
तो 20 हिडन यूनिट्स विज़ूअलाइज़ करती हैं 
इन छोटे वर्ग बक्सों से.

15
00:00:52,357 --> 00:00:57,325
तो उदाहरण के लिए, यह छोटा 
विज़ूअलाइज़ेशन दर्शाता है एक हिडन यूनिट

16
00:00:57,325 --> 00:01:01,978
कोशिश कर रहा हैं समझने कि कहाँ है एजेज़ 
उस ऑरीएंटेशन में उस छवि में.

17
00:01:01,978 --> 00:01:05,914
और शायद यह हिडन यूनिट कोशिश 
कर रहा है समझने की

18
00:01:05,914 --> 00:01:09,955
कहाँ है हॉरिज़ॉंटल एजेज़ उस छवि में.

19
00:01:09,955 --> 00:01:13,184
और फिर जब हम बात करेंगे 
कॉन्वलूशनल नेटवर्क्स की बाद के एक कोर्स में,

20
00:01:13,184 --> 00:01:16,129
यह विशेष विज़ूअलाइज़ेशन 
थोड़ा ज़्यादा समझ आएगी.

21
00:01:16,129 --> 00:01:19,562
लेकिन फ़ॉर्म, आप समझ सकते हैं पहली 
लेयर न्यूरल नेटवर्क की जैसे कि देखता है

22
00:01:19,562 --> 00:01:22,690
छवि को और समझने की कोशिश करता है 
कहाँ है एजेज़ इस छवि में.

23
00:01:22,690 --> 00:01:27,356
अब, चलो सोचते हैं कहाँ है एजेज़ 
इस छवि में इकट्ठा करके

24
00:01:27,356 --> 00:01:28,730
पिक्सल्स को बनाने के लिए एजेज़.

25
00:01:28,730 --> 00:01:34,670
फिर यह लेता है डिटेक्ट किए हुए एजेज़ को और इकट्ठा 
करता है एजेज़ को बनाने के लिए हिस्से फ़ेस के.

26
00:01:34,670 --> 00:01:40,289
तो उदाहरण के लिए, आपके पास हो सकता है 
एक न्यूरल न्यूरॉन देखने के लिए कि क्या यह ढूँढ रहा है एक आँख,

27
00:01:40,289 --> 00:01:44,480
या एक भिन्न न्यूरॉन ढूँढने की कोशिश कर रहा है नाक.

28
00:01:44,480 --> 00:01:47,463
और इसलिए एक साथ रखने से बहुत से एजेज़,

29
00:01:47,463 --> 00:01:50,970
यह शुरू करता है डिटेक्ट करने विभिन्न हिस्से फ़ेस के.

30
00:01:50,970 --> 00:01:56,035
और फिर, अंत में, एक साथ रखते हुए
 भिन्न हिस्से फ़ेस के,

31
00:01:56,035 --> 00:02:01,006
जैसे आँख या एक नाक या एक कान या एक ठोड़ी, 
यह तब कोशिश करता है पहचानने की या

32
00:02:01,006 --> 00:02:03,564
डिटेक्ट करने की विभिन्न प्रकार के फ़ेस.

33
00:02:03,564 --> 00:02:07,755
तो स्वाभाविक रूप से, आप समझ सकते हैं शुरू की 
लेयर न्यूरल नेटवर्क की

34
00:02:07,755 --> 00:02:10,190
को डिटेक्ट केरते हुए सरल फ़ंक्शंज़ जैसे एजेज़.

35
00:02:10,190 --> 00:02:14,573
और फिर इकट्ठा करते हुए इन्हें बाद की लेयर्स में 
एक न्यूरल नेटवर्क की

36
00:02:14,573 --> 00:02:17,625
ताकि यह लर्न कर सके और अधिक जटिल फ़ंक्शंज़.

37
00:02:17,625 --> 00:02:23,640
ये विज़ूअलाइज़ेशन अधिक समझ आएँगी 
जब हम बात करेंगे कॉन्वलूशनल नेट्स की.

38
00:02:23,640 --> 00:02:26,203
और एक तकनीकी जानकारी इस विज़ूअलाइज़ेशन की,

39
00:02:26,203 --> 00:02:29,802
एज डिटेक्टर्स देख रहे हैं अपेक्षाकृत 
छोटे क्षेत्रों में एक छवि के,

40
00:02:29,802 --> 00:02:31,703
शायद बहुत छोटे क्षेत्रों में उस जैसे.

41
00:02:31,703 --> 00:02:36,616
और फिर फ़ेशल डिटेक्टर्स देख सकते हैं 
शायद बड़े क्षेत्रों में छवि के लेकिन

42
00:02:36,616 --> 00:02:41,308
मुख्य अनुभव जो आप लेते हैं इससे कि सिर्फ़ 
ढूँढने के लिए सरल चीज़ें

43
00:02:41,308 --> 00:02:43,675
जैसे एजेज़ और फिर बनाना उन्हें.

44
00:02:43,675 --> 00:02:47,216
बनाना उन्हें एक साथ डिटेक्ट करने के लिए
 अधिक जटिल चीज़ें जैसे एक 
आँख और नाक और

45
00:02:47,216 --> 00:02:50,530
फिर रखना उन्हें एक साथ ढूँढने के लिए 
और भी अधिक जटिल चीज़ें.

46
00:02:50,530 --> 00:02:55,665
और इस तरह की सरल से जटिल पदानुक्रमित निरूपण,

47
00:02:55,665 --> 00:02:58,508
या कॉम्पज़िशनल निरूपण,

48
00:02:58,508 --> 00:03:04,114
अप्लाई होता है अन्य तरह के डेटा को भी, 
छवियों और फ़ेस रेकग्निशन के अलावा.

49
00:03:04,114 --> 00:03:08,593
उदाहरण के लिए, यदि आप बना रहे हैं 
एक स्पीच रेकग्निशन सिस्टम,

50
00:03:08,593 --> 00:03:10,908
यह कठिन होता है दोबारा विज़ूअलाइज़ 
करना स्पीच लेकिन

51
00:03:10,908 --> 00:03:15,684
यदि आप इनपुट करते हैं एक ऑडियो क्लिप तब 
शायद पहला स्तर एक न्यूरल नेटवर्क का शायद

52
00:03:15,684 --> 00:03:20,863
सीखे डिटेक्ट करना निम्न स्तर का 
ऑडियो वेव फ़ॉर्म फ़ीचर, जैसे कि 
क्या यह टोन ऊपर जा रही है?

53
00:03:20,863 --> 00:03:21,703
क्या यह नीचे जा रही है?

54
00:03:21,703 --> 00:03:26,869
क्या यह वाइट नोईस है या आवाज़ जैसे स्सस्स.

55
00:03:26,869 --> 00:03:27,903
और पिच क्या है?

56
00:03:27,903 --> 00:03:31,124
और जब वह उस पर जाता है, डिटेक्ट करना 
निम्न स्तर का वेव फ़ॉर्म फ़ीचर उस जैसा.

57
00:03:31,124 --> 00:03:34,233
और फिर बनाकर निम्न स्तर वेव फ़ॉर्म्स,

58
00:03:34,233 --> 00:03:37,937
शायद आप सीखेंगे डिटेक्ट करना 
मूल यूनिट आवाज़ के.

59
00:03:37,937 --> 00:03:40,297
भाषाविज्ञान में उन्हें स्वनिम / फ़ोनीम्स कहते हैं.

60
00:03:40,297 --> 00:03:45,098
लेकिन, उदाहरण के लिए, शब्द cat में, 
C है एक स्वनिम, A है एक स्वनिम,

61
00:03:45,098 --> 00:03:46,787
T एक और स्वनिम है.

62
00:03:46,787 --> 00:03:49,987
लेकिन सीखता हैं ढूँढना शायद मूल यूनिटस 
आवाज़ की और

63
00:03:49,987 --> 00:03:54,688
फिर बनाकर उन्हें एक साथ शायद 
सीखता है पहचानना शब्द ऑडियो में.

64
00:03:54,688 --> 00:03:58,270
और फिर शायद बनाकर उन्हें एक साथ,

65
00:03:58,270 --> 00:04:02,912
पहचानने के लिए पूरे वाक्यांश या वाक्य.

66
00:04:02,912 --> 00:04:07,572
तो बहुत सी हिडन लेयर वाला डीप न्यूरल 
नेटवर्क में हो सकती है शुरू की

67
00:04:07,572 --> 00:04:10,477
लेयर्स सीखें ये निम्न स्तर के सरल फ़ीचर्ज़ और

68
00:04:10,477 --> 00:04:15,339
फिर बाद की लेयर्स फिर रखती है एक साथ 
सरलतर चीज़ें जो उसने डिटेक्ट की हैं

69
00:04:15,339 --> 00:04:19,392
डिटेक्ट करने के लिए अधिक जटिल चीज़ें 
जैसे पहचानना ख़ास शब्द या

70
00:04:19,392 --> 00:04:21,040
यहां तक कि वाक्यांश या वाक्य भी.

71
00:04:21,040 --> 00:04:24,745
जो बोला गया है करने के लिए स्पीच रेकग्निशन.

72
00:04:24,745 --> 00:04:30,168
और हम क्या देखते हैं कि जहाँ शुरू की लेयर्स 
कम्प्यूट कर रही है जो लगते हैं

73
00:04:30,168 --> 00:04:35,673
अपेक्षाकृत सरल फ़ंक्शन्स इनपुट के 
जैसे कहाँ हैं एजेज़, जब तक

74
00:04:35,673 --> 00:04:41,046
आप पहुँचते हैं आगे नेटवर्क में आप कर सकते हैं 
आश्चर्यजनक रूप से जटिल चीज़ें.

75
00:04:41,046 --> 00:04:44,876
जैसे फ़ेस डिटेक्ट करना या डिटेक्ट करना 
शब्द या वाक्यांश या वाक्य.

76
00:04:44,876 --> 00:04:48,767
कुछ समानता दिखाते हैं डीप न्यूरल नेटवर्कस और

77
00:04:48,767 --> 00:04:52,656
मानव मस्तिष्क में, जहां हम मानते हैं, 
या न्यूरोसाययंटिस्ट्स विश्वास करते हैं,

78
00:04:52,656 --> 00:04:57,162
कि मानव मस्तिष्क भी शुरू करता है 
सरल चीजों का पता लगाने से 
जैसे एजेज़ उनमें जो

79
00:04:57,162 --> 00:05:00,370
आपकी आँखें देखती हैं फिर बनाना उन्हें 
डिटेक्ट करने के लिए अधिक जटिल

80
00:05:00,370 --> 00:05:02,440
चीज़ें जैसे कि चेहरे जो आप देखते हैं.

81
00:05:02,440 --> 00:05:05,038
मुझे लगता है समानता डीप लर्निंग और

82
00:05:05,038 --> 00:05:08,276
मानव मस्तिष्क में कभी-कभार
 थोड़ा खतरनाक भी होता है.

83
00:05:08,276 --> 00:05:13,301
लेकिन वहां बहुत कुछ सच है, वह यह कि 
हम कैसे सोचते है कि मानव मस्तिष्क 
कैसे काम करता है और

84
00:05:13,301 --> 00:05:18,102
और कि मानव मस्तिष्क शायद डिटेक्ट करता है 
सरल चीज़ें जैसे एजेज़ और

85
00:05:18,102 --> 00:05:22,598
रखता है उन्हें एक साथ बनाने के अधिक 
जटिल चीज़ें और उसने

86
00:05:22,598 --> 00:05:27,430
दी है एक कुछ प्रेरणा 
कुछ लोगों को सीखने में भी.

87
00:05:27,430 --> 00:05:29,850
हम देखेंगे थोड़ा और मानव मस्तिष्क के बारे में या

88
00:05:29,850 --> 00:05:33,065
जैविक मस्तिष्क के बारे में बाद के 
वीडियो में इस सप्ताह के.

89
00:05:35,534 --> 00:05:40,407
अन्य समझ कि क्यों डीप न्यूरल 
नेटवर्क प्रतीत होते हैं

90
00:05:40,407 --> 00:05:42,756
सही काम करते हुए है निम्नलिखत.

91
00:05:42,756 --> 00:05:47,868
तो यह परिणाम आता है सर्किट थ्योरी से 
जो सम्बंधित है सोच से

92
00:05:47,868 --> 00:05:53,760
कि किस तरह के फ़ंक्शन आप कम्प्यूट 
कर सकते हैं विभिन्न AND गेट्स, OR गेट्स से 
मूल रूप से लॉजिक गेट्स से.

93
00:05:53,760 --> 00:05:58,860
तो अनौपचारिक रूप से, उनके फ़ंक्शन कम्प्यूट 
करते है एक अपेक्षाकृत छोटे लेकिन डीप न्यूरल

94
00:05:58,860 --> 00:06:03,595
नेटवर्क से और छोटे न्यूरल नेटवर्क से मेरा 
मतलब है संख्या हिडन यूनिट्स की अपेक्षाकृत कम.

95
00:06:03,595 --> 00:06:07,553
लेकिन यदि आप कम्प्यूट करने की कोशिश करें 
वही फ़ंक्शन एक उथले नेटवर्क में,

96
00:06:07,553 --> 00:06:09,178
जिसमें बहुत सी हिडन लेयर्स नहीं है,

97
00:06:09,178 --> 00:06:13,296
तब आपको शायद चाहिए होंगी 
एक्स्पोनेनशीयल रूप से अधिक हिडन यूनिट्स 
कम्प्यूट करने के लिए.

98
00:06:13,296 --> 00:06:18,109
तो मैं देता हूँ आपको केवल एक और उदाहरण 
और समझाता हूँ इसे थोड़ा और आसान ढंग से.

99
00:06:18,109 --> 00:06:21,423
लेकिन मान लो आप कोशिश कर रहे हैं 
कम्प्यूट करने की इक्स्क्लूसिव OR या पैरिटी

100
00:06:21,423 --> 00:06:23,349
पैरिटी सारे इनपुट फ़ीचर्ज़ की.

101
00:06:23,349 --> 00:06:28,430
तो आप कम्प्यूट करना चाह रहे हैं X1,
XOR, X2, XOR,

102
00:06:28,430 --> 00:06:33,064
X3, XOR, आगे Xn यदि आपके
 पास हैं n या n X फ़ीचर्ज़.

103
00:06:33,064 --> 00:06:39,924
तो यदि आप बनाते हैं XOR ट्री इस तरह का, 
तो हमारे लिए यह कम्प्यूट करता है 
XOR X1 और

104
00:06:39,924 --> 00:06:44,586
X2, फिर लेता है X3 और X4 और 
कम्प्यूट करता है उनका XOR.

105
00:06:44,586 --> 00:06:49,392
और तकनीकी तौर पर, यदि आप उपयोग कर 
रहे हैं AND गेट या NOT गेट, आप को 
आवश्यकता हो सकती है

106
00:06:49,392 --> 00:06:54,196
कुछ लेयर्स की कम्प्यूट करने की XOR
 फ़ंक्शन बजाय सिर्फ़ एक लेयर के, लेकिन

107
00:06:54,196 --> 00:06:58,791
एक अपेक्षाकृत छोटे सर्किट से, आप कम्प्यूट 
कर सकते हैं XOR, और इसी प्रकार आगे.

108
00:06:58,791 --> 00:07:03,987
और फिर आप बना सकते हैं बूलीयन, 
एक XOR ट्री इस तरह,

109
00:07:03,987 --> 00:07:12,090
जब तक अंत में, आपके पास है एक सर्किट यहाँ 
जो आउट्पुट करता है, चलो, उसे कहते हैं Y.

110
00:07:12,090 --> 00:07:15,236
Y हैट की आउट्पुट है बराबर Y के.

111
00:07:15,236 --> 00:07:18,398
इक्स्क्लूसिव OR, पैरिटी इन सब इनपुट बिट्स की.

112
00:07:18,398 --> 00:07:24,790
तो कम्प्यूट करने के लिए गहराई बाएँ नेटवर्क 
की होगी लॉग N की रेंज में.

113
00:07:24,790 --> 00:07:27,410
तो हमारे पास होगा इस तरह का XOR ट्री.

114
00:07:27,410 --> 00:07:30,836
तो नोड्स की संख्या या सर्किट घटकों की 
संख्या या

115
00:07:30,836 --> 00:07:33,929
गेट्स की संख्या इस नेटवर्क में उतनी 
बड़ी नहीं है.

116
00:07:33,929 --> 00:07:38,452
आपको नहीं चाहिए उतने गेट्स कम्प्यूट 
करने के लिए इक्स्क्लूसिव OR.

117
00:07:38,452 --> 00:07:43,458
लेकिन अब, यदि आपको अनुमति नहीं है 
न्यूरल नेटवर्क इस्तेमाल करने की जिसमें 
बहुत सी

118
00:07:43,458 --> 00:07:48,203
हिडन लेयर्स हो, इस केस में, 
लॉग N की रेंज में,

119
00:07:48,203 --> 00:07:53,382
यदि आपको मजबूर किया जाता है
 कम्प्यूट करने के लिए 
यह फ़ंक्शन सिर्फ़ एक हिडन लेयर से,

120
00:07:53,382 --> 00:07:57,912
तो आपके पास हैं ये सब चीज़ें जाते हुए अंदर, 
तो हिडन यूनिट्स.

121
00:07:57,912 --> 00:08:02,116
और फिर ये चीज़ें फिर आउट्पुट Y.

122
00:08:02,116 --> 00:08:07,120
फिर कम्प्यूट करने के लिए यह XOR फ़ंक्शन, 
इस हिडन लेयर

123
00:08:07,120 --> 00:08:12,124
को होना चाहिए एक्स्पोनेनशीयल रूप से बड़ा,
 क्योंकि मूलत:,

124
00:08:12,124 --> 00:08:18,397
आपको आवश्यकता है एक एक कर के सारे 
सम्भव कन्फ़िग्यरेशन बताने की.

125
00:08:18,397 --> 00:08:23,139
तो 2 की पावर N की रेंज के सभी 
सम्भव कन्फ़िग्यरेशन इनपुट

126
00:08:23,139 --> 00:08:27,898
बिट्स के जिनसे मिलता है इक्स्क्लूसिव 
OR 1 या 0.

127
00:08:27,898 --> 00:08:32,213
तो आपको लेनी पड़ती है एक हिडन लेयर जो 
एक्स्पोनेनशीयल रूप से बड़ी है

128
00:08:32,213 --> 00:08:33,554
बिट्स की संख्या में.

129
00:08:33,554 --> 00:08:38,229
मुझे लगता है 
तकनीकी रूप से आप कर सकते हैं इसे 
2 की पॉवर N घटा 1 हिडन यूनिट्स से.

130
00:08:38,229 --> 00:08:43,948
लेकिन वह है 2 की पॉवर N की रेंज जो 
एक्स्पोनेनशीयल रूप से बड़ी है 
बिट्स की संख्या से.

131
00:08:43,948 --> 00:08:49,149
तो मुझे आशा है कि यह आपको एक ज्ञान 
देता है कि कुछ गणितीय फ़ंक्शन्स है,

132
00:08:49,149 --> 00:08:55,275
जो आसान है कम्प्यूट करना डीप नेटवर्क्स से 
तुलना में उथले नेटवर्क्स से.

133
00:08:55,275 --> 00:09:01,028
वास्तव में, मैंने व्यक्तिगत रूप से सर्किट थ्योरी से 
परिणाम कम उपयोगी पाए

134
00:09:01,028 --> 00:09:05,985
समझने के लिए, लेकिन एक परिणाम 
जिसका लोग अक्सर

135
00:09:05,985 --> 00:09:11,223
उद्धरण करते हैं जब समझाते हैं 
मूल्य बहुत डीप रेप्रेज़ेंटेशन का.

136
00:09:11,223 --> 00:09:13,600
अब, इस कारण के अलावा

137
00:09:13,600 --> 00:09:16,897
पसंद करना डीप न्यूरल नेटवर्क है,

138
00:09:16,897 --> 00:09:22,204
मैं सोचता हूँ अन्य कारण हैं कि टर्म डीप लर्निंग 
प्रसिद्ध हो गया है केवल ब्राण्ड की तरह.

139
00:09:22,204 --> 00:09:26,776
इन चीज़ों को कहते थे बहुत सी 
हिडन लेयर वाला न्यूरल नेटवर्क.

140
00:09:26,776 --> 00:09:31,198
वाक्यांश डीप लर्निंग है केवल एक बड़ा ब्राण्ड, 
यह सिर्फ़ उतना गहरा है.

141
00:09:31,198 --> 00:09:36,284
तो मैं सोचता हूँ जब वह टर्म लोकप्रिय हो गयी 
तब वास्तव में न्यूरल नेटवर्क को 
दोबारा ब्राण्ड करने से या

142
00:09:36,284 --> 00:09:39,622
बहुत सी हिडन लेयर वाले न्यूरल नेटवर्क को 
दोबारा ब्राण्ड करने से

143
00:09:39,622 --> 00:09:42,970
सहायता हुई कैप्चर करने में लोकप्रिय 
कल्पना भी.

144
00:09:42,970 --> 00:09:47,479
वे इसे शुद्ध ब्रैंडिंग समझते हैं कि डीप नेटवर्क्स 
बेहतर काम करते हैं.

145
00:09:47,479 --> 00:09:51,342
कभी-कभी लोग हद से बाहर चले जाते हैं 
और ज़ोर देते हैं बहुत सी हिडन लेयर्स 
इस्तेमाल करने पर.

146
00:09:51,342 --> 00:09:55,500
लेकिन जब मैं शुरू करता हूँ एक नई समस्या,
 मैं अक्सर शुरू करता हूँ

147
00:09:55,500 --> 00:09:58,803
लॉजिसिटिक रेग्रेशन से, फिर प्रयास करता हूँ 
कुछ एक या

148
00:09:58,803 --> 00:10:01,722
दो हिडन लेयर्स से और इस्तेमाल करता हूँ
 वह एक हाइपर पेरमिटर जैसे.

149
00:10:01,722 --> 00:10:05,731
इस्तेमाल करता हूँ वह एक पेरमिटर या 
हाइपर पेरमिटर जैसे जो आप ट्यून करते हैं 
जानने के लिए

150
00:10:05,731 --> 00:10:07,935
सही गहराई आपके न्यूरल नेटवर्क की.

151
00:10:07,935 --> 00:10:12,800
लेकिन पिछले कई वर्षों से एक प्रवृत्ति है 
लोगों को खोजने की ओर कि

152
00:10:12,800 --> 00:10:17,590
कुछ ऐप्लिकेशन्स के लिए बहुत ज़्यादा डीप 
न्यूरल नेटवर्क्स यह जिसमें शायद बहुत

153
00:10:17,590 --> 00:10:22,264
दर्जनों लेयर्स हैं कभी-कभी, कर सकती है 
सबसे अच्छा मॉडल एक समस्या को.

154
00:10:22,264 --> 00:10:27,605
तो बस इतना ही समझने के लिए कि क्यों 
डीप लर्निंग प्रतीत होता है बेहतर 
काम करते हुए.

155
00:10:27,605 --> 00:10:31,411
चलो अब देखते हैं तकनीक को कि कैसे 
इम्प्लमेंट करना है न केवल फ़ॉर्वर्ड

156
00:10:31,411 --> 00:10:33,769
प्रॉपगेशन बल्कि बैक प्रॉपगेशन भी.