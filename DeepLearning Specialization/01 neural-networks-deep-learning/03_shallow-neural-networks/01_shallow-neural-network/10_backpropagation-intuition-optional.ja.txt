前回の動画では 逆伝播の式を見てきました 今回の動画では
計算グラフを使って その式がどうやって導けるのか
直感をつかみましょう この動画は完全に任意のものなので 見るかどうかは
ご自由にしてくださいね どちらにしても作業は
ちゃんとできるはずです ロジスティック回帰についてお話ししたとき zやaそして損失を計算するために 前方へ進んでいきましたよね そして微分を求めるために このように後方へ進んでいって
まずdaを計算し それからdzを計算し 更にdwとdbを計算しました 損失 L(a,y)の定義は -yloga-(1-y)log(1-a) でした もし微積分について詳しくて これのaについての微分を求めれば それはdaの公式となります daとはこれのことです そして本当に微分をしてみると これは-y/a+(1-y/1-a)だとわかります これの微分を取ったらこうなると
微積分学からわかるのです 後方にもう一歩進んでdzを計算すると dzはa-yだとわかります わけは前に説明しましたが 連鎖律によるとdz=da×g'(z)になります ここでg(z)とは zに
シグモイド関数を適用したものです ロジスティック回帰での
出力ユニットの活性化関数ですね これもまだロジスティック回帰だと
思い出してください x1、x2、x3と
一つのシグモイドユニットがあって そこから出てきたaから
yハットを求められるものです ここの活性化関数はシグモイド関数です 余談ですが 微積分学の連鎖律について
詳しい人に向けてです これの理由なのですが aはσ(z)ですよね なのでzについてのLの微分は aについてのLの微分×da/dzです このaはσ(z)に等しく これはd/dz×g(z)に等しく つまりg'(z)です そのためこの表現 コード内で
dzと書いてあるものは コードでdaと表されているもの
×g'(z)なのです これはただ
この式を表しているだけです 最後の式の導出は微積分学
特に連鎖律に慣れていないと 意味がわからないと思います わからなくても心配要りません 必要なときには直感的な説明を
するようにしますね ロジスティック回帰のために
dzを計算したので dwとdbを計算します
dwはdz・xで dbは教師サンプルが一つのときは
ただのdzです これがロジスティック回帰でした ニューラルネットワークで
逆伝播を計算するときには これに似た計算をしますが
計算を二回行うという点で異なります xは直接出力ユニットに行くのではなく まず隠れ層を通ってから
出力ユニットに行くからです ここでやった計算は
１回のステップのようなもので このような二層のニューラル
ネットワークでは二回行います このように入力層隠れ層と出力層のある 二層のニューラルネットワークを
考えます 計算の進め方を思い出してください まずこの式を使ってz[1]を計算し それからa[1]とz[2]を計算します z[2]はパラメータW[2]と
b[2]の影響も受けます 次はz[2]をもとにして a[2]が求まって
最後に損失を計算できます 誤差逆伝播法では反対方向に進みます
まずda[2] そしてdz[2] また戻ってdW[2] db[2]を計算します 更に戻ってda[1] dz[1]…というふうに続きます 入力xに対する微分を計算する
必要はありません 教師あり学習ではxは固定されていて 最適化する必要はないので 少なくとも教師あり学習では
xについての微分は計算しません da[2]の明示的な計算は飛ばします 実際にda[2]を計算して それを使って
dz[2]を求めることもできますが 実際にはこれらの二つを
一つの手順にまとめられます すると前と同じように
dz[2]=a[2]-y それからdW[2]とdb[2]もあります これらは下に書きますね dW[2]=dz[2]a[1]Tで db[2]=dz[2]です このあたりはロジスティック回帰と
よく似ていますね ロジスティック回帰では
dw=dz・xというのがありましたが 今回はa[1]がxの役割を担っています 大文字の行列Wと個別のパラメータwとの
関係を考えれば ここに転置がある意味もわかりますよね 出力が一つのロジスティック回帰では
wは行ベクトルだからです dW[2]も似たようなものですが 列ベクトルなので ロジスティック回帰のxとは違って
a[1]を転置するのです これで逆伝播の半分が終わりました ここでもda[1]を計算して構いませんが 実際はda[1]とdz[1]の計算は 一つにまとめてしまうことが多いです 実装するときには dz[1]は W[2]T×dz[2]とg[1]'(z[1])の 要素ごとの掛け算です ここで次元の確認をしてみましょう このようなニューラルネットワークがあって yハットがここにありますね n[0] これはnxに等しいですが
n[0]個の入力特徴量があって n[1]個の隠れユニット そしてn[2]があります この場合出力ユニットは一つなので n[2]は１です W[2]は(n[2],n[1])次元なので z[2] そしてdz[2]は(n[2],1)次元です 二項分類をするときには
(1,1)になりますね z[1]とdz[1]は (n[1],1)次元となります 任意の変数fooとdfooは
いつでも同じ次元を持ちます だからWとdWやbとdb、zとdzなども いつも同じ次元を持つのです これら全ての次元が合うかを
確かめてみましょう dz[1]=W[2]T×dz[2]がありますね これは要素同士の掛け算で
g[1]'(z[1])がかかっています 上記から dz[1]の次元は (n[1],1)次元になり W[2]は転置されているので
(n[1],n[2])次元となります dz[2]は(n[2],1)次元になって これはz[1]と同じ次元です なのでこれも(n[1],1)となって
要素同士の掛け算ですね これで次元はわかったでしょう (n[1],1)次元のベクトルが (n[1],n[2])次元の行列
×(n[2],1)から得られます なぜならこれら二つの掛け算で
(n[1],1)次元の行列ができて 二つの(n[1],1)次元のベクトル同士の
要素ごとの掛け算だからです そのため次元はぴったり合います 逆伝播を実装するときのアドバイスですが 行列の次元数が一致することを
確認したら つまりW[1]やW[2]、Z[1]、 Z[2]、A[1]、A[2]などの行列の次元数を しっかり考えて それらの次元数が合うことを
確認しておけば そのおかげで逆伝播のバグを
たくさん取り除けることがあります いいですね
これでdz[1]が求まりました 最後にdW[1]とdb[1]です ここに書いた方がいいのですが スペースが不足しているので
右側に書きますね dW[1]とdb[1]は次の式で求められます これはdz[1]・xTで こちらはdz[1]です この式とこの式はよく似ていますね これはただの偶然ではありません xがa[0]の役割を担っていて
xの転置はa[0]の転置なので 実際これらの式はとてもよく似ているのです これで逆伝播がどのように
得られるかわかったでしょうか dz[2]、dW[2]、db[2]、dz[1]、dW[1]
そしてdb[1]という ６つの大切な式があります これら６つの式を取って
次のスライドにコピーしますね ここまで 一つの教師サンプルが
あるときのための 逆伝播の方法を求めてきました もちろん 一度に一つの
教師サンプルを処理するより 全ての教師サンプルをベクトル化して
処理したいですよね 順伝播のときには 一つの教師サンプルを処理しているとき このような式がありました ベクトル化するためには 全てのzを取ってきて
このように列として並べ z[1](m)までありますね
これを大文字のZと呼びました 列として並べたものを 大文字バージョンとして扱うことで Z[1]=W[1]X+b[1]と A[1]=g[1](Z[1])であることが
わかりました このコースのために表記法を
注意深く定義したので サンプルを行列の列として並べることで
うまくいくようになっています 慎重に計算していけば
逆伝播のときも同じようにできるはずです ベクトル化した式は次の通りです まず全ての教師サンプルから
dzを取ってきて 行列のそれぞれの列として並べます
これらも同じようにすると これがベクトル化した実装になります これがdW[2]の計算方法です ここには1/mをつけました
なぜならコスト関数Jは iが１からmまでの損失を足して
mで割ったものだからです 微分を計算するときには ロジスティック回帰で重みを
更新したときと同じように 1/mをつけるのです db[2]はこのようになります これもdZの合計を1/mしたものです
dZ[1]は次のように計算できます 繰り返しますがこれは要素ごとの掛け算です 前のスライドで見たように
これは(n[1],1)次元のですが 今回は(n[1],m)になっています これらはどちらも(n[1],m)次元です そのためこのアスタリスクが
要素ごとの掛け算になるのです 残りの二つはこのようになります これらはそんなに
衝撃的な変化ではないですよね これで逆伝播アルゴリズムの導き方を
直感でわかってもらえていたら嬉しいです 機械学習のすべての中で 逆伝播のアルゴリズムの導き方が 私が見た中で最も複雑な
数学の一つだと思います 線形代数の知識が必要とされる上に 行列の微分を第一原理よりゼロから
求めなければなりません 行列の微積分学の専門家の方なら この過程を使って 自分で微分アルゴリズムを
証明できるかもしれませんが 深層学習を実践している人たちの中には この動画と同じレベルの
式の導出方法を見て 直感を持ってこのアルゴリズムを 効果的に実装できている人が
たくさんいると思います 微積分学のエキスパートだったら ゼロからこれらを求められるか
試してみるといいと思います これは機械学習で私が見てきた中で 最も難しい導出の一つです どちらにせよ これを実装すれば
うまくいくでしょうし これを調整するのに
十分な直感は持っていると思います ニューラルネットワークを実装する前に 最後にもう一つ
細かいことをお教えしようと思います それはニューラルネットワークの重みを
どうやって初期化するかです パラメータをゼロではなく ランダムに初期化することは ニューラルネットワークを訓練するうえで
大事であることがわかっています 次のビデオでそのわけを見てみましょう