1
00:00:00,000 --> 00:00:03,116
ニューラルネットワークの絵を
何回か描きました

2
00:00:03,116 --> 00:00:05,712
このビデオでは
これらが一体何を意味するのかを話します

3
00:00:05,712 --> 00:00:06,728
言い換えれば

4
00:00:06,728 --> 00:00:11,235
描いてきたニューラルネットワークが
なにを表現しているかです

5
00:00:11,235 --> 00:00:15,014
まずは隠れ層が１つの
ニューラルネットワークに

6
00:00:15,014 --> 00:00:17,290
焦点を当てて
始めたいと思います

7
00:00:17,290 --> 00:00:19,667
ニューラルネットワークの絵です

8
00:00:19,667 --> 00:00:22,986
それぞれの部分に
名前を付けます

9
00:00:22,986 --> 00:00:27,447
x1、x2、x3 が
縦に重ねられています

10
00:00:27,447 --> 00:00:30,694
これは入力層と
言われています

11
00:00:30,694 --> 00:00:35,764
当然ではありますが
ニューラルネットワークへの入力が格納されています

12
00:00:35,764 --> 00:00:37,990
この丸の層があります

13
00:00:37,990 --> 00:00:41,663
これは隠れ層と
呼ばれています

14
00:00:41,663 --> 00:00:45,414
なぜ「隠れ」なのか
すぐに説明しますが

15
00:00:45,414 --> 00:00:49,509
この１つのノードで形成されている最後の層は

16
00:00:49,509 --> 00:00:53,894
出力層と呼ばれています

17
00:00:53,894 --> 00:00:56,059
予測値のyハットを生成します

18
00:00:56,059 --> 00:00:59,932
教師あり学習で学習させるニューラルネットワークでは

19
00:00:59,932 --> 00:01:05,237
トレーニングセットには
入力値のx と目的出力値のy が含まれています

20
00:01:05,237 --> 00:01:09,239
隠れ層が意味しているのは
トレーニングセットの中では

21
00:01:09,239 --> 00:01:12,702
この中央部のノードの真の値は
観測されていません

22
00:01:12,702 --> 00:01:15,185
つまりトレーニングセットからは
どんな値であるべきかが分かりません

23
00:01:15,185 --> 00:01:16,640
入力は分かります

24
00:01:16,640 --> 00:01:18,094
出力も分かりますが

25
00:01:18,094 --> 00:01:20,992
トレーニングセットの中では
隠れ層のものは分かりません

26
00:01:20,992 --> 00:01:25,542
トレーニングセットの中では
見えないので隠れ層という呼び方を

27
00:01:25,542 --> 00:01:28,088
説明しているようなものです

28
00:01:28,088 --> 00:01:30,262
表記の方法を紹介しましょう

29
00:01:30,262 --> 00:01:35,542
ベクトルｘで入力特徴量を
表わしていましたが

30
00:01:35,542 --> 00:01:37,226
他の方法では

31
00:01:37,226 --> 00:01:41,987
入力特徴量の値は
a[0] と表記します

32
00:01:41,987 --> 00:01:44,934
ａはまた活性化を表わし

33
00:01:44,934 --> 00:01:47,733
これはニューラルネットワークの

34
00:01:47,733 --> 00:01:51,651
それぞれの層が次の層に値を渡すのを
表わします

35
00:01:51,651 --> 00:01:55,998
入力層はｘの値を
隠れ層に渡します

36
00:01:55,998 --> 00:02:01,110
このため入力層の活性化を
a(0)と呼びます

37
00:02:01,110 --> 00:02:05,990
次の層の隠れ層では
これを受けて活性化の値を生成します

38
00:02:05,990 --> 00:02:09,601
これをａ[1]と書きます

39
00:02:09,601 --> 00:02:13,306
この最初のユニット、ノードは

40
00:02:13,306 --> 00:02:17,824
ａ[1]1の値を生成します

41
00:02:17,824 --> 00:02:20,735
２つ目のノードは

42
00:02:20,735 --> 00:02:23,311
ａ[1]2の値を生成と
いうように続きます

43
00:02:23,311 --> 00:02:26,488
ａ[1]が意味するのは

44
00:02:26,488 --> 00:02:30,120
４次元のベクトルで

45
00:02:30,120 --> 00:02:34,707
Python では４ｘ１の列ベクトルで
このようなものです

46
00:02:34,707 --> 00:02:39,205
４次元なのは
４つの隠れノード、ユニットが

47
00:02:39,205 --> 00:02:42,684
隠れ層にあるからです

48
00:02:42,684 --> 00:02:46,302
そして最終的に
出力層が値を生成します

49
00:02:46,302 --> 00:02:47,948
ａ[2]で実数です

50
00:02:47,948 --> 00:02:51,658
yハットはａ[2]の値を取ります

51
00:02:51,658 --> 00:02:55,885
次にロジステック回帰で
ｙハット＝ａと書く理由です

52
00:02:55,885 --> 00:03:00,349
ロジステック回帰では１つしか出力層がないので

53
00:03:00,349 --> 00:03:03,583
[]を付けて表記しないのです

54
00:03:03,583 --> 00:03:07,916
ニューラルネットワークでは
どの層から来たかを

55
00:03:07,916 --> 00:03:11,653
明示的に表すために
[]を使っていきます

56
00:03:11,653 --> 00:03:15,468
表記の決まりで
おかしいところは

57
00:03:15,468 --> 00:03:20,194
この見ているネットワークは
２層ニューラルネットワークと呼ばれます

58
00:03:20,194 --> 00:03:23,541
この理由はニューラルネットワークの層を数えるときに

59
00:03:23,541 --> 00:03:25,321
入力層を数に入れないからです

60
00:03:25,321 --> 00:03:28,858
つまり隠れ層が１層目で
出力層は２層目です

61
00:03:28,858 --> 00:03:32,661
この表記方法の決まりでは
入力層を０層目と呼びます

62
00:03:32,661 --> 00:03:35,887
つまりこのニューラルネットワークは
厳密には３層でもあるかもしれません

63
00:03:35,887 --> 00:03:39,649
入力層、隠れ層、出力層と
あるからです

64
00:03:39,649 --> 00:03:43,357
研究論文や他のものを読んだ時もそうですが
このコースでは慣例の使われ方では

65
00:03:43,357 --> 00:03:47,489
この例のようなニューラルネットワークは
２層のニューラルネットワークと呼びます

66
00:03:47,489 --> 00:03:51,602
公式には入力層は
数にいれないからです

67
00:03:51,602 --> 00:03:55,912
最後に 後で出てくることに

68
00:03:55,912 --> 00:03:59,670
隠れ層と出力層には
関連付けられるパラメーターがあります

69
00:03:59,670 --> 00:04:03,447
隠れ層にはｗ、ｂというパラメーターが
関連付けられています

70
00:04:03,447 --> 00:04:08,218
これには[1]と書いてこのパラメーターが

71
00:04:08,218 --> 00:04:12,395
隠れ層の１層目と関連があることを示します

72
00:04:12,395 --> 00:04:15,416
後にこの例の場合 ｗは４ｘ３の行列で

73
00:04:15,416 --> 00:04:18,016
ｂは４ｘ１のベクトルに
なっているのを見ていきます

74
00:04:18,016 --> 00:04:21,754
座標の最初の４は
隠れ層に４つのノードが

75
00:04:21,754 --> 00:04:24,503
存在する事実から来ていて

76
00:04:24,503 --> 00:04:28,120
３は３つの入力特徴量がある事実から来ています

77
00:04:28,120 --> 00:04:31,980
後ほど この行列の配列について話します

78
00:04:31,980 --> 00:04:33,844
その時には より理解できているかもしれません

79
00:04:33,844 --> 00:04:37,813
しかし 出力層もまた関連付けられたパラメーターがあります

80
00:04:37,813 --> 00:04:41,663
パラメーターｗ[2]、ｂ[2]があります

81
00:04:41,663 --> 00:04:45,747
これらの配列は１ｘ４と１ｘ１となります

82
00:04:45,747 --> 00:04:49,297
１ｘ４なのは
隠れ層に４つの隠れユニットがあり

83
00:04:49,297 --> 00:04:51,177
出力層が１ユニットしかないためです

84
00:04:51,177 --> 00:04:56,378
繰り返しますが これらの行列やベクトルの配列を
今後のビデオで見ていきます

85
00:04:56,378 --> 00:04:59,839
さてここでは２層のニューラルネットワークが
どんなものか見てきました

86
00:04:59,839 --> 00:05:03,108
１つの隠れ層のニューラルネットワークでした

87
00:05:03,108 --> 00:05:04,260
見ての通り、ここある。

88
00:05:04,260 --> 00:05:08,513
このニューラルネットワークが
一体何を計算しているのか見ていきましょう

89
00:05:08,513 --> 00:05:11,223
ニューラルネットワークがｘを入力され

90
00:05:11,223 --> 00:05:14,169
ｙハットを計算するところまで
どのようにたどり着くかを見ましょう