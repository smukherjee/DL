1
00:00:00,000 --> 00:00:03,840
이번 비디오가 매우 흥미로웠던 것 같은데요, 

2
00:00:01,800 --> 00:00:06,240
이번 비디오에서는 여러분의 

3
00:00:03,840 --> 00:00:08,730
신경망에서 기울기 강하를 도입하는

4
00:00:06,240 --> 00:00:10,530
방법에 대해 배우겠습니다. 한개의 숨겨진 레이어의 경우에 말이죠. 

5
00:00:08,730 --> 00:00:12,809
이번 비디오에서는 여러분에게 

6
00:00:10,530 --> 00:00:14,639
도입을 위해 필요한 공식을

7
00:00:12,809 --> 00:00:17,039
드릴것인데요. 기울기 강하의 후 방향전파가

8
00:00:14,639 --> 00:00:19,410
작동하게끔 말입니다. 그리고 이 비디오 이후로

9
00:00:17,039 --> 00:00:21,510
더 직관적인 부분에 대해서 다룰텐데요

10
00:00:19,410 --> 00:00:24,150
왜 이런 특정 공식이 

11
00:00:21,510 --> 00:00:26,070
정확한 공식인지, 또는

12
00:00:24,150 --> 00:00:27,630
기울기를 계산하는데 올바른

13
00:00:26,070 --> 00:00:28,289
공식인지 신경망과 관련하여

14
00:00:27,630 --> 00:00:30,090
살펴보겠습니다.

15
00:00:28,289 --> 00:00:32,520
1개의 숨겨진 래이어로 이루어진 

16
00:00:30,090 --> 00:00:39,930
여러분의 신경망은 일단

17
00:00:32,520 --> 00:00:44,760
w1, v1, w2, b2등의 파라미터값을 가질 것입니다.

18
00:00:39,930 --> 00:00:50,399
만약 nx 또는

19
00:00:44,760 --> 00:00:56,640
n0 입력 특성을 갖는 경우, n1의

20
00:00:50,399 --> 00:00:59,149
숨겨진 유닛과 n2 결과값 유닛이 있는 경우, 

21
00:00:56,640 --> 00:01:05,670
여기 예제에서는 이 값들이 1이 되도록 합니다.

22
00:00:59,149 --> 00:01:08,880
그러면 w1 매트릭스는 (n1, n0), 

23
00:01:05,670 --> 00:01:11,250
그리고 b1은 (n, 1)차원 벡터인데요 

24
00:01:08,880 --> 00:01:13,350
그러므로 (n1, 1)로 표기해주면 됩니다.

25
00:01:11,250 --> 00:01:16,500
칼럼 벡터이죠. 

26
00:01:13,350 --> 00:01:20,750
w2의 디이멘션은 n2 x n1 인데요 

27
00:01:16,500 --> 00:01:26,759
그리고 b2의 다이멘션은 

28
00:01:20,750 --> 00:01:28,590
(n1, 1)입니다. 아직까지는

29
00:01:26,759 --> 00:01:30,930
n2가 1인 경우만 봤습니다.

30
00:01:28,590 --> 00:01:36,930
오직 1개의 숨겨진 유닛이 있는 경우 말이죠. 

31
00:01:30,930 --> 00:01:39,570
또한, 신경망에서는 

32
00:01:36,930 --> 00:01:41,340
비용함수도 있는데요, 

33
00:01:39,570 --> 00:01:44,220
현재까지는 일단 이진분류법으로만

34
00:01:41,340 --> 00:01:48,659
진행한다고 가정할 것입니다.

35
00:01:44,220 --> 00:01:51,740
이러한 경우, 파라미터들의 비용은

36
00:01:48,659 --> 00:01:57,090
이렇게 생길 텐데요, 1 나누기 m 

37
00:01:51,740 --> 00:01:59,969
그리고 loss 함수의 평균값, 

38
00:01:57,090 --> 00:02:03,420
여기 L은 신경망이 예측하는

39
00:01:59,969 --> 00:02:06,240
ŷ의 값인데요, 이것은 a2와

40
00:02:03,420 --> 00:02:07,649
마찬가지 인데요, ground true label 이 Y인 경우 말이죠, 

41
00:02:06,240 --> 00:02:09,629
그리고 만약 이진분류법을

42
00:02:07,649 --> 00:02:12,510
진행하면, loss함수는 

43
00:02:09,629 --> 00:02:15,030
이전에 이용했던 로지스틱 회귀

44
00:02:12,510 --> 00:02:18,420
함수와 동일할 수 있습니다.

45
00:02:15,030 --> 00:02:21,450
여러분의 알고리즘의 파라미터를 트레이닝 시키기 위해서는

46
00:02:18,420 --> 00:02:23,189
기울기 강하를 진행하여야 합니다.

47
00:02:21,450 --> 00:02:25,379
신경망을 트레이닝 시킬때는

48
00:02:23,189 --> 00:02:26,129
파라미터를 임의로 초기화하는데 굉장히

49
00:02:25,379 --> 00:02:28,140
중요합니다. 0으로 초기화하는 대신에 말이죠.

50
00:02:26,129 --> 00:02:30,030
이것이 왜 그런지는 나중에 보겠습니다.

51
00:02:28,140 --> 00:02:32,069
하지만 각각의 기울기 강하

52
00:02:30,030 --> 00:02:34,140
loop은 예측수치를

53
00:02:32,069 --> 00:02:36,780
산출할 텐데요, 

54
00:02:34,140 --> 00:02:42,359
기본적으로 i가 1에서 m 사이인 경우의

55
00:02:36,780 --> 00:02:44,519
ŷ i 를 구하는 것입니다. 

56
00:02:42,359 --> 00:02:49,440
그 다음으로는 derivative를 구해야 하는데요, 

57
00:02:44,519 --> 00:02:51,420
dw1을 구하고, 

58
00:02:49,440 --> 00:02:54,359
파라미터 w1에 대해서 

59
00:02:51,420 --> 00:02:56,489
비용함수의 derivative를 구하는데요, 

60
00:02:54,359 --> 00:02:59,220
또 하나의 변수를 구해야 하는데요, 

61
00:02:56,489 --> 00:03:00,870
이것을 db1이라고 할 텐데요, 

62
00:02:59,220 --> 00:03:04,109
이것은 비용함수의 기울기를 

63
00:03:00,870 --> 00:03:07,349
변수 B1에 대해서 구한 값인데요.

64
00:03:04,109 --> 00:03:10,170
이렇게하여 비슷하게

65
00:03:07,349 --> 00:03:12,629
파라미터 w1, b2를 구할 수도 있습니다.

66
00:03:10,170 --> 00:03:17,879
그리고 마지막으로 기울기 강하 업데이트는 

67
00:03:12,629 --> 00:03:22,709
w1을 w1 빼기 알파, 러닝속도죠, 

68
00:03:17,879 --> 00:03:26,129
곱하기 dw1 입니다. b1은 

69
00:03:22,709 --> 00:03:31,620
b1 빼기 러닝속도 곱하기 

70
00:03:26,129 --> 00:03:34,739
db1이 됩니다. w2 b2도 비슷하게 됩니다.

71
00:03:31,620 --> 00:03:36,299
가끔 저는 콜론을 사용하는데요, 

72
00:03:34,739 --> 00:03:38,489
=을 뜻하는 것으로 2개의 표기법

73
00:03:36,299 --> 00:03:40,829
모두 괜찮습니다. 

74
00:03:38,489 --> 00:03:42,510
이것이 그러면 기울기 강하의 iteration인데요, 

75
00:03:40,829 --> 00:03:44,280
이런 절차를 몇번 반복하여

76
00:03:42,510 --> 00:03:46,079
파라미터가 수렴하는 것처럼 

77
00:03:44,280 --> 00:03:48,150
보일때까지 진행합니다.

78
00:03:46,079 --> 00:03:50,099
이전 비디오에서는 예측값을 산출하는

79
00:03:48,150 --> 00:03:51,629
방법에 대해 이야기 했는데요, 

80
00:03:50,099 --> 00:03:54,060
이것을 어떻게 하는지 봤습니다.

81
00:03:51,629 --> 00:03:56,269
벡터화 방식으로 말이죠. 

82
00:03:54,060 --> 00:04:00,180
핵심은 바로 이런 partial derivative 항을

83
00:03:56,269 --> 00:04:04,079
산출하는 방법입니다. dw1, db1, 

84
00:04:00,180 --> 00:04:06,780
그리고 derivative인 bw2, dp2 같이 말이죠. 

85
00:04:04,079 --> 00:04:09,419
제가 하고 싶은 것은 여러분에게

86
00:04:06,780 --> 00:04:12,150
공식을 제공해서 derivative를

87
00:04:09,419 --> 00:04:14,699
산출할 수 있도록 하고, 

88
00:04:12,150 --> 00:04:17,430
다음 비디오로 넘어가서, 선택적인 비디오인데요, 

89
00:04:14,699 --> 00:04:19,090
이러한 공식이 어떻게 나왔는지에 대해

90
00:04:17,430 --> 00:04:21,400
설명하도록 하겠습니다.

91
00:04:19,090 --> 00:04:26,169
다시 요약하자면, 

92
00:04:21,400 --> 00:04:33,250
전 방향전파 공식을 말이죠, 

93
00:04:26,169 --> 00:04:37,900
이 공식에서는 z1=w1 x 더하기 b1 그리고

94
00:04:33,250 --> 00:04:41,680
여기 a1은 activation 함수인데요

95
00:04:37,900 --> 00:04:49,690
g1(z1)입니다.

96
00:04:41,680 --> 00:04:53,530
그리고 z2=w2 a1 더하기 b2입니다.

97
00:04:49,690 --> 00:04:55,180
그리고 마지막으로 

98
00:04:53,530 --> 00:05:01,210
이것은 트레이닝세트에 걸쳐 벡터화된 것인데요, 

99
00:04:55,180 --> 00:05:02,740
a2=g2(z2)입니다.

100
00:05:01,210 --> 00:05:04,870
다시 한번 여기서는 이진분류법을 이용한다는

101
00:05:02,740 --> 00:05:06,610
가정이 있습니다. 그러면 여기 activation

102
00:05:04,870 --> 00:05:08,560
함수는 사실 시그모이드 함수기 되야하겠죠. 

103
00:05:06,610 --> 00:05:11,080
여기다가 적겠습니다.

104
00:05:08,560 --> 00:05:13,870
이것이 전 방향전파 또는 

105
00:05:11,080 --> 00:05:15,729
왼쪽에서 오른쪽으로 전개되는 

106
00:05:13,870 --> 00:05:18,430
신경망 산출법인데요. 다음으로, 

107
00:05:15,729 --> 00:05:24,750
derivative를 계산해보겠습니다. 이것은

108
00:05:18,430 --> 00:05:30,750
후 방향전파 인데요, dz2를 계산할 

109
00:05:24,750 --> 00:05:33,610
것인데요, a2 - ground truth Y인데요, 

110
00:05:30,750 --> 00:05:36,580
다시 말씀 드리자면 이것은 

111
00:05:33,610 --> 00:05:41,289
예시에대해 vectorize된 것입니다.

112
00:05:36,580 --> 00:05:45,280
매트릭스 Y는 1 x m 매트릭스 인데요

113
00:05:41,289 --> 00:05:51,370
모든 M 예시에 대해 가로로 정리합니다.

114
00:05:45,280 --> 00:05:55,330
dw2는 이거와 같은데요, 

115
00:05:51,370 --> 00:05:58,870
여기 첫번째 3개의 공식은 

116
00:05:55,330 --> 00:06:00,900
로지스틱 회귀 의 기울기 강하와

117
00:05:58,870 --> 00:06:00,900
매우 유사합니다.

118
00:06:00,960 --> 00:06:12,610
x는 1 컴마, keepdimms=

119
00:06:07,419 --> 00:06:15,580
true인데요, 여기 상세 내용인데요, 

120
00:06:12,610 --> 00:06:18,070
NP.sum 은 파이썬 넘파이 명령어 인데요

121
00:06:15,580 --> 00:06:21,100
1차원의 매트릭스에서의 명령어입니다.

122
00:06:18,070 --> 00:06:24,810
이번 예제에서의 경우, 가로로 더하는 것을 뜻합니다.

123
00:06:21,100 --> 00:06:27,600
그리고 keepdimms는

124
00:06:24,810 --> 00:06:31,230
파이썬이 웃기게 생긴 rank 1 arra가 

125
00:06:27,600 --> 00:06:34,650
뜨지 않도록 방지해줍니다. 

126
00:06:31,230 --> 00:06:37,010
이것은 (n, ) 이렇게 생겼었죠. 

127
00:06:34,650 --> 00:06:41,280
여기 이 keepdimms=true를 진행함으로써 

128
00:06:37,010 --> 00:06:44,580
파이썬이 db2의 결과값으로 벡터

129
00:06:41,280 --> 00:06:47,820
(n, 1)의 값을 출력하게 해줍니다.

130
00:06:44,580 --> 00:06:50,130
엄밀히 이야기하면 이것은 (n2, 1)이 될것입니다.

131
00:06:47,820 --> 00:06:53,520
이 경우에는 어차피 (1, 1)

132
00:06:50,130 --> 00:06:56,790
이여서 상관은 없으나 나중에 

133
00:06:53,520 --> 00:06:58,500
이것이 중요한 때 다시 이야기하겠습니다.

134
00:06:56,790 --> 00:07:01,320
지금까지 한 것은 로지스틱 회귀와

135
00:06:58,500 --> 00:07:03,919
매우 흡사한데요, 산출하면서 

136
00:07:01,320 --> 00:07:14,370
계속 후 방향전파을 실행합니다.

137
00:07:03,919 --> 00:07:19,380
이것은 dz2 곱하기 z1 의 g1 프라임 

138
00:07:14,370 --> 00:07:20,880
여기 g1 프라임은 여러분이 

139
00:07:19,380 --> 00:07:22,919
결과값 층 L에 대한 숨겨진 레이어를

140
00:07:20,880 --> 00:07:25,770
사용한 activation 함수의 

141
00:07:22,919 --> 00:07:27,030
derivative인데요, 결과값 층 L에 대해서는

142
00:07:25,770 --> 00:07:29,400
이진분류법을 가정합니다.

143
00:07:27,030 --> 00:07:30,780
이 가정에는 시그모이드 함수가

144
00:07:29,400 --> 00:07:34,620
함께 내포하고 있습니다.

145
00:07:30,780 --> 00:07:39,090
여기 dz2 와 같이 말이죠, 그리고 여기 곱하기는

146
00:07:34,620 --> 00:07:43,050
element-wise product 입니다. 

147
00:07:39,090 --> 00:07:46,950
그러므로 여기 이 부분은 (n1, m) 매트릭스가 

148
00:07:43,050 --> 00:07:48,990
될 것인데요, 그리고 여기 

149
00:07:46,950 --> 00:07:52,680
element wise derivative 또한

150
00:07:48,990 --> 00:07:54,720
(n1, m) 매트릭스 일 것입니다. 

151
00:07:52,680 --> 00:07:59,669
여기 곱하기는 element wise product인데요 

152
00:07:54,720 --> 00:08:08,490
여기 2개의 매트릭스에 대해서 말이죠. 그리고 dw1은 이거와

153
00:07:59,669 --> 00:08:18,950
같고, db1은 이것과 동일합니다.

154
00:08:08,490 --> 00:08:21,900
여기 np 점 sum dz 1, 1과 같도, 

155
00:08:18,950 --> 00:08:23,430
keepdimms=true 입니다. 

156
00:08:21,900 --> 00:08:27,210
이전에는 keepdimss가

157
00:08:23,430 --> 00:08:28,590
n=2 였으므로 별로 중요하지 않았지만, 

158
00:08:27,210 --> 00:08:35,729
(1, 1)이 그냥 실수였기 때문이었습니다. 

159
00:08:28,590 --> 00:08:38,370
하지만 여기서는 db1이 (n1, 1) 벡터가 됨에 따라, 

160
00:08:35,729 --> 00:08:40,110
파이썬 명령이 np.sum이 

161
00:08:38,370 --> 00:08:43,110
이러한 디이멘션을 결과값으로 갖도록 

162
00:08:40,110 --> 00:08:46,529
해야할 것입니다. 웃기게 생긴 

163
00:08:43,110 --> 00:08:48,360
rank 1 array 의 디아멘션 이 나오지 않게 말이죠. 

164
00:08:46,529 --> 00:08:50,580
이것이 나오면 나중에 계산하는데서

165
00:08:48,360 --> 00:08:53,310
망가트릴 수 있습니다. keepdimms

166
00:08:50,580 --> 00:08:56,910
parameter 대신에 사용할 수 있는 방법은

167
00:08:53,310 --> 00:09:00,060
reshape를 불러오는 것입니다.

168
00:08:56,910 --> 00:09:04,400
NP.sum 결과값을 다시 reshape 하는 것입니다

169
00:09:00,060 --> 00:09:08,310
db가 원하는 디아멘션으로 말이죠. 

170
00:09:04,400 --> 00:09:11,339
전 방향전파에 대해 

171
00:09:08,310 --> 00:09:14,310
4가지 공식을 통해 알아봤는데요, 

172
00:09:11,339 --> 00:09:16,680
후 방향전파은 6개의 공식을 통해 알아봤습니다. 

173
00:09:14,310 --> 00:09:18,870
이번 강의에서는 이런 공식을 단순히 적었지만

174
00:09:16,680 --> 00:09:22,050
다음 선택적 비디오에서는 

175
00:09:18,870 --> 00:09:23,940
이런 6개의 후 방향전파 알고리즘 

176
00:09:22,050 --> 00:09:25,830
공식이 어떻게 파생되었는지 

177
00:09:23,940 --> 00:09:27,750
직관적인 부분을 다루겠습니다. 

178
00:09:25,830 --> 00:09:30,000
자유롭게 선택적으로 보시면 됩니다.

179
00:09:27,750 --> 00:09:32,730
이런 알고리즘을 도입하면 

180
00:09:30,000 --> 00:09:34,650
전 방향전파 과 후 방향전파이 

181
00:09:32,730 --> 00:09:36,750
올바르게 나올 수 있습니다.

182
00:09:34,650 --> 00:09:39,029
또한 신경망의 파라미터를 배우기 위해

183
00:09:36,750 --> 00:09:41,520
적용해야 하는 기울기 강하에서도

184
00:09:39,029 --> 00:09:43,680
derivative를 올바르게 산출할 수 있게 해줄 것입니다.

185
00:09:41,520 --> 00:09:45,209
미적분학에 대한 부분을 깊이 이해하지 않더라도 

186
00:09:43,680 --> 00:09:47,130
성공적으로 도입하여 잘 작동시킬 수 

187
00:09:45,209 --> 00:09:50,520
있는데요, 많은 성공한 사람들은

188
00:09:47,130 --> 00:09:52,320
이전부터 그렇게 하였습니다.

189
00:09:50,520 --> 00:09:54,180
원하시면 다음 비디오를 시청하셔도 좋습니다.

190
00:09:52,320 --> 00:09:56,580
조금더 이 공식들이 파생되었던

191
00:09:54,180 --> 00:09:58,820
직관적인 부분을 다룰 테니까요.

192
00:09:56,580 --> 00:09:58,820
방정식