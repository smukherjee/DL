1
00:00:00,000 --> 00:00:05,340
在上一节视频中<br />我们通过一些例子说明了如何通过内置函数

2
00:00:05,340 --> 00:00:07,770
和避免使用显式的for循环来实现向量化

3
00:00:07,770 --> 00:00:11,130
这可以有效地提高代码的运行速度

4
00:00:11,130 --> 00:00:13,558
让我们再看几个例子

5
00:00:13,558 --> 00:00:16,110
需要记住的经验之谈是

6
00:00:16,110 --> 00:00:19,792
当你在编写神经网络或逻辑回归时

7
00:00:19,792 --> 00:00:23,190
都要尽可能避免使用显式的for循环

8
00:00:23,190 --> 00:00:27,285
虽然有时候无法完全避免使用for循环

9
00:00:27,285 --> 00:00:29,520
但如果你能使用内置函数

10
00:00:29,520 --> 00:00:32,710
或者找到其他方式 来计算你想要的答案

11
00:00:32,710 --> 00:00:36,555
这通常会比直接使用for循环更快

12
00:00:36,555 --> 00:00:38,445
我们再来看一个例子

13
00:00:38,445 --> 00:00:44,412
比如你想要计算向量u 它是矩阵A和

14
00:00:44,412 --> 00:00:46,144
向量v的乘积

15
00:00:46,144 --> 00:00:50,840
根据矩阵乘法的定义

16
00:00:50,840 --> 00:00:55,830
u_i等于A_ij*v_j 对j求和

17
00:00:55,830 --> 00:00:57,050
对吧

18
00:00:57,050 --> 00:00:58,445
这就是u_i的定义

19
00:00:58,445 --> 00:01:01,500
非向量化的实现方法是

20
00:01:01,500 --> 00:01:06,140
u=np.zeros(n, 1)

21
00:01:06,140 --> 00:01:08,105
u=np.zeros(n, 1)

22
00:01:08,105 --> 00:01:11,745
然后对i循环

23
00:01:11,745 --> 00:01:14,250
接着对j循环

24
00:01:14,250 --> 00:01:22,950
然后 u[i]+=A[i][j]*v[j]

25
00:01:22,950 --> 00:01:27,725
这是一个两层的for循环 分别对i和j进行循环

26
00:01:27,725 --> 00:01:29,269
这是一种非向量化的实现方式

27
00:01:29,269 --> 00:01:37,355
向量化的实现方式是
u=np.dot(A,v)

28
00:01:37,355 --> 00:01:39,160
右边这种

29
00:01:39,160 --> 00:01:41,655
向量化的实现方式

30
00:01:41,655 --> 00:01:44,628
消除了两层for循环 运行速度要快得多

31
00:01:44,628 --> 00:01:46,655
我们再来看一个例子

32
00:01:46,655 --> 00:01:50,900
假设在内存中已经有了一个向量v

33
00:01:50,900 --> 00:01:55,190
你想要对向量v中的每一个元素进行指数运算

34
00:01:55,190 --> 00:01:59,250
计算u等于e^v_1 e^v_2直到

35
00:01:59,250 --> 00:02:04,085
e^v_n这样一个向量

36
00:02:04,085 --> 00:02:10,370
这是一种非向量化的实现方式

37
00:02:10,370 --> 00:02:13,700
即首先初始化u为一个零向量

38
00:02:13,700 --> 00:02:18,260
然后用一个for循环每次计算一个元素

39
00:02:18,260 --> 00:02:24,110
但实际上 Python和NumPy中有许多内置函数

40
00:02:24,110 --> 00:02:31,160
只需调用一个函数 就可以完成这些向量计算

41
00:02:31,160 --> 00:02:40,180
所以我的实现方式是<br />import numpy as np

42
00:02:40,180 --> 00:02:50,110
然后只需调用 u=np.exp(v)

43
00:02:50,110 --> 00:02:53,600
注意之前的这个显式的for循环

44
00:02:53,600 --> 00:02:57,317
在这里变成了一行代码 输入为向量v

45
00:02:57,317 --> 00:02:58,840
输出为向量u

46
00:02:58,840 --> 00:03:02,340
右边的实现方式避免了使用显式的for循环

47
00:03:02,340 --> 00:03:06,275
而且比直接使用for循环更快

48
00:03:06,275 --> 00:03:10,310
实际上 Numpy中有很多支持向量值的函数

49
00:03:10,310 --> 00:03:15,175
比如np.log(v)会对向量中每个元素进行对数计算

50
00:03:15,175 --> 00:03:19,220
np.abs()可以计算绝对值

51
00:03:19,220 --> 00:03:24,935
np.maximum(v,0)会将v中的每个元素

52
00:03:24,935 --> 00:03:29,850
与0相比求最大值

53
00:03:29,850 --> 00:03:31,290
v**2

54
00:03:31,290 --> 00:03:38,214
计算向量v中每个元素的平方

55
00:03:38,214 --> 00:03:42,932
1/v计算每个元素的倒数 等等

56
00:03:42,932 --> 00:03:45,635
所以每当你想要写for循环的时候

57
00:03:45,635 --> 00:03:48,440
先看看是否可以通过调用

58
00:03:48,440 --> 00:03:53,069
NumPy的内置函数来避免for循环

59
00:03:53,069 --> 00:03:55,640
接下来 让我们将这些知识

60
00:03:55,640 --> 00:03:59,105
运用到逻辑回归的梯度下降算法实现中

61
00:03:59,105 --> 00:04:02,955
看看我们是否可以至少摆脱两个for循环中的一个

62
00:04:02,955 --> 00:04:07,980
这是我们计算逻辑回归中的导数的代码

63
00:04:07,980 --> 00:04:09,335
这里有两个for循环

64
00:04:09,335 --> 00:04:13,065
一个在这里 另一个在这里

65
00:04:13,065 --> 00:04:15,422
在这个例子中

66
00:04:15,422 --> 00:04:17,810
n_x等于2

67
00:04:17,810 --> 00:04:19,910
但如果你的特征数量超过2个

68
00:04:19,910 --> 00:04:23,865
那就需要在for循环中处理

69
00:04:23,865 --> 00:04:25,676
dw_1 dw_2 dw_3等等

70
00:04:25,676 --> 00:04:30,305
相当于这里有一个<br />for j=1...n_x

71
00:04:30,305 --> 00:04:37,395
在循环中更新dw_j

72
00:04:37,395 --> 00:04:41,745
接下来我们要消灭这第二个for循环

73
00:04:41,745 --> 00:04:43,745
这就是我们要在这一页做的

74
00:04:43,745 --> 00:04:48,725
我们的做法是 不再将dw_1 dw_2等

75
00:04:48,725 --> 00:04:51,390
显式地初始化为零

76
00:04:51,390 --> 00:04:58,100
移除这一段 然后令dw成为一个向量

77
00:04:58,100 --> 00:05:05,600
令dw=np.zeros((n_x,1))

78
00:05:05,600 --> 00:05:10,490
即n_x乘1的向量

79
00:05:10,490 --> 00:05:13,175
接着在这里

80
00:05:13,175 --> 00:05:15,941
不再使用针对单个元素的for循环

81
00:05:15,941 --> 00:05:19,190
而是使用这个针对向量值的运算

82
00:05:19,190 --> 00:05:22,350
dw+=x^(i)dz^(i)

83
00:05:22,350 --> 00:05:32,031
dw+=x^(i)dz^(i)

84
00:05:32,031 --> 00:05:33,620
最后 不再使用这一段

85
00:05:33,620 --> 00:05:42,562
而是使用dw/=m<br />现在 我们从两个for循环简化为只有一个for循环

86
00:05:42,562 --> 00:05:48,864
我们保留了这层针对各个训练样本的for循环

87
00:05:48,864 --> 00:05:51,030
我希望本节视频能给你一种向量化的概念

88
00:05:51,030 --> 00:05:54,165
通过移除一个for循环

89
00:05:54,165 --> 00:05:56,505
你的代码会运行地更快

90
00:05:56,505 --> 00:05:58,320
但实际上我们还可以做得更好

91
00:05:58,320 --> 00:05:59,418
在下节视频中

92
00:05:59,418 --> 00:06:03,237
我们会进一步讨论如何对逻辑回归进行向量化

93
00:06:03,237 --> 00:06:07,365
你会惊讶地看到一种没有for循环的实现方式

94
00:06:07,365 --> 00:06:10,695
不需要对于训练样本的for循环

95
00:06:10,695 --> 00:06:14,760
你的代码几乎可以同时处理

96
00:06:14,760 --> 00:06:17,060
整个训练集

97
00:06:17,060 --> 00:06:19,000
我们下个视频再见