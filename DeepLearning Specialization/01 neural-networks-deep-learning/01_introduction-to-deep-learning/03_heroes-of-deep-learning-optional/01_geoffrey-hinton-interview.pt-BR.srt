1
00:00:00,620 --> 00:00:03,610
Como parte deste curso pelo site www.deeplearning.ai,

2
00:00:03,610 --> 00:00:07,590
espero não apenas lhes ensinar as ideias
técnicas da aprendizagem profunda, mas

3
00:00:07,590 --> 00:00:11,658
também apresentar a vocês algumas pessoas,
alguns heróis da aprendizagem profunda.

4
00:00:11,658 --> 00:00:13,160
As pessoas que inventaram tantas

5
00:00:13,160 --> 00:00:17,700
dessas ideias as quais você aprenderá
neste curso ou nesta especialização.

6
00:00:17,700 --> 00:00:21,420
Nesses vídeos, também espero
pedir a esses líderes da aprendizagem profunda

7
00:00:21,420 --> 00:00:24,990
para lhe dar conselhos de carreira sobre
como você pode invadir a aprendizagem profunda,

8
00:00:24,990 --> 00:00:27,805
sobre como você pode pesquisar ou
encontrar um trabalho de aprendizagem profunda.

9
00:00:27,805 --> 00:00:30,156
Como primeiro desta série de entrevistas,

10
00:00:30,156 --> 00:00:34,228
estou encantado em apresentar a você
uma entrevista com Geoffrey Hinton.

11
00:00:38,427 --> 00:00:44,150
Bem-vindo Geoff, e obrigado por fazer
esta entrevista com a deeplearning.ai.

12
00:00:44,150 --> 00:00:46,550
— Obrigado por me convidar.

13
00:00:46,550 --> 00:00:50,088
— Eu creio que neste ponto você, mais
que qualquer outra pessoa neste planeta,

14
00:00:50,088 --> 00:00:52,835
inventou tantas
das ideias por trás da aprendizagem profunda.

15
00:00:52,835 --> 00:00:57,650
E muitas pessoas têm lhe chamado de
padrinho da aprendizagem profunda.

16
00:00:57,650 --> 00:01:01,529
Embora não tenha sido, até conversarmos
alguns minutos atrás, que percebi

17
00:01:01,529 --> 00:01:05,600
que você pensa que sou o primeiro a chamá-lo assim,
o que estou bastante feliz por ter feito.

18
00:01:06,780 --> 00:01:11,320
Mas o que quero perguntar é:
muitas pessoas lhe conhecem como uma lenda,

19
00:01:11,320 --> 00:01:15,030
eu quero lhe perguntar sobre sua
história pessoal por trás da lenda.

20
00:01:15,030 --> 00:01:19,980
Então, como você se envolveu, fazendo o caminho
de volta, como você se envolveu na AI e

21
00:01:19,980 --> 00:01:21,520
aprendizagem de máquina e redes neurais?

22
00:01:22,730 --> 00:01:26,960
— Bem, quando eu estava no colegial,
eu tinha um colega de sala que era sempre

23
00:01:26,960 --> 00:01:31,220
melhor que eu em tudo,
ele era um matemático brilhante.

24
00:01:31,220 --> 00:01:37,010
E ele veio para a escola um dia e disse: 
você sabia que o cérebro usa hologramas?

25
00:01:38,190 --> 00:01:44,161
E eu acho que era meados de 1966, e
eu disse tipo: o que é um holograma?

26
00:01:44,161 --> 00:01:47,390
E ele explicou que em um holograma
você pode cortá-lo pela metade e

27
00:01:47,390 --> 00:01:49,730
você ainda tem a figura inteira.

28
00:01:49,730 --> 00:01:53,466
E aquelas memórias do cérebro podem ser
distribuídas sobre todo o cérebro.

29
00:01:53,466 --> 00:01:56,022
E, então, eu acho que ele leu
sobre os experimentos de Lashley,

30
00:01:56,022 --> 00:01:57,939
nos quais você corta pedaços
do cérebro de um rato e

31
00:01:57,939 --> 00:02:01,740
descobre que é muito difícil encontrar um
pedaço no qual ele estoca uma memória particular.

32
00:02:04,411 --> 00:02:08,920
Então isso foi o que me interessou primeiro
em como o cérebro armazena as memórias.

33
00:02:10,180 --> 00:02:12,220
E, depois, quando fui para a faculdade,

34
00:02:12,220 --> 00:02:15,130
comecei a estudar fisiologia e
física.

35
00:02:16,400 --> 00:02:17,731
Penso que quando estava em Cambridge

36
00:02:17,731 --> 00:02:20,260
eu era o único graduando
cursando fisiologia e física.

37
00:02:21,888 --> 00:02:25,270
E depois eu desisti disso e

38
00:02:25,270 --> 00:02:29,170
tentei cursar filosofia, porque eu
pensava que poderia me dar mais conhecimento.

39
00:02:29,170 --> 00:02:32,780
Mas isso me pareceu, na verdade,

40
00:02:32,780 --> 00:02:37,130
falta de meios para distinguir
quando eles diziam algo falso.

41
00:02:37,130 --> 00:02:39,420
E, então, eu troquei para psicologia.

42
00:02:41,988 --> 00:02:45,920
Em psicologia eles tinham teorias muito,
muito simples e isso me parecia

43
00:02:45,920 --> 00:02:49,620
meio que completamente inadequado
para explicar o que o cérebro estava fazendo.

44
00:02:49,620 --> 00:02:52,737
Então, eu fiz um intervalo e
me tornei carpinteiro.

45
00:02:52,737 --> 00:02:57,169
E, então, decidi que eu tentaria AI,
e fui para Edimburgo

46
00:02:57,169 --> 00:02:59,580
estudar AI com Langer Higgins.

47
00:02:59,580 --> 00:03:02,662
E ele havia feito um trabalho
legal com redes neurais, e

48
00:03:02,662 --> 00:03:07,830
ele havia acabado de desistir das redes neurais,
e estava muito impressionado pela tese de Winograd;

49
00:03:07,830 --> 00:03:11,460
então, quando eu cheguei, ele pensou que eu estava,
meio que, fazendo essa coisa antiquada, e

50
00:03:11,460 --> 00:03:14,210
eu teria que começar em AI simbólica.

51
00:03:14,210 --> 00:03:18,210
E tivemos muitas brigas sobre isso,
mas eu apenas continuei fazendo aquilo que acreditava.

52
00:03:18,210 --> 00:03:21,138
— E então o quê?

53
00:03:21,138 --> 00:03:28,033
— Eu, finalmente, obtive um PhD em AI, e
então eu não conseguia um emprego na Inglaterra.

54
00:03:28,033 --> 00:03:30,979
Mas eu vi este anúncio muito legal para

55
00:03:30,979 --> 00:03:36,070
Bolsa de Estudo Sloan na Califórnia, e
consegui obter uma delas.

56
00:03:36,070 --> 00:03:40,625
Fui para Califórnia e
tudo era diferente lá.

57
00:03:40,625 --> 00:03:46,685
Então, na Inglaterra,
redes neurais eram consideradas um pouco bobas,

58
00:03:46,685 --> 00:03:50,272
e na Califórnia, Don Norman e

59
00:03:50,272 --> 00:03:56,640
David Rumelhart estavam muito abertos
a ideias sobre redes neurais.

60
00:03:56,640 --> 00:04:00,720
Foi a primeira vez que eu estava em algum lugar
onde pensar sobre como o cérebro funciona

61
00:04:00,720 --> 00:04:03,290
e pensar sobre como ele poderia
se relacionar com a psicologia

62
00:04:03,290 --> 00:04:05,650
era visto como algo muito positivo.

63
00:04:05,650 --> 00:04:06,936
E foi muito divertido lá,

64
00:04:06,936 --> 00:04:09,792
em particular, colaborar
com David Rumelhart foi ótimo.

65
00:04:09,792 --> 00:04:12,968
— Entendo, ótimo.
Então isso foi quando você esteve na UCSD
[Universidade da Califórnia em San Diego], e

66
00:04:12,968 --> 00:04:16,177
você e Rumelhart, por volta de 1982,

67
00:04:16,177 --> 00:04:20,182
acabaram escrevendo o artigo criador da retro-propagação, certo?

68
00:04:20,182 --> 00:04:23,292
— Na verdade,
foi mais complicado que isso.

69
00:04:23,292 --> 00:04:24,796
— O que aconteceu?

70
00:04:24,796 --> 00:04:28,214
— Eu acho que no início de 1982

71
00:04:28,214 --> 00:04:32,900
David Rumelhart e eu, e Ron Williams,

72
00:04:32,900 --> 00:04:37,967
entre nós, desenvolvemos
o algoritmo de retro-propagação,

73
00:04:37,967 --> 00:04:42,291
e foi principalmente idea de David Rumelhart.

74
00:04:42,291 --> 00:04:46,390
Descobrimos mais tarde que muitas
outras pessoas o tinham inventado.

75
00:04:46,390 --> 00:04:52,798
David Parker o tinha inventado, 
provavelmente depois de nós, mas antes que o publicássemos.

76
00:04:52,798 --> 00:04:56,425
Paul Werbos já o tinha publicado
alguns anos antes, mas

77
00:04:56,425 --> 00:04:58,860
ninguém deu muita atenção.

78
00:04:58,860 --> 00:05:01,923
Houve outras pessoas que tinham
desenvolvido algoritmos muito semelhantes.

79
00:05:01,923 --> 00:05:04,340
Não está claro o que se entende por
 retro-propagação.

80
00:05:04,340 --> 00:05:08,055
Mas usar a regra da cadeia para se obter 
as derivadas não era uma ideia nova.

81
00:05:08,055 --> 00:05:12,484
— Entendi. Por que você pensa que foi
seu artigo que ajudou

82
00:05:12,484 --> 00:05:15,940
tanto a comunidade a se agarrar
 à retro-propagação?

83
00:05:15,940 --> 00:05:20,540
Parece que seu artigo marcou
um contaminação na aceitação desse

84
00:05:20,540 --> 00:05:22,934
algoritmo, quem quer que o tenha aceitado.

85
00:05:22,934 --> 00:05:26,675
— Então, conseguimos um artigo na revista Nature em 1986.

86
00:05:26,675 --> 00:05:30,580
E eu fiz muito trabalho político
para que o artigo fosse aceito.

87
00:05:30,580 --> 00:05:34,622
Eu descobri que um dos peritos
provavelmente seria Stuart Sutherland,

88
00:05:34,622 --> 00:05:36,992
que era um psicólogo
bem conhecido na Inglaterra.

89
00:05:36,992 --> 00:05:38,815
E eu fui falar com ele
por um longo tempo, e

90
00:05:38,815 --> 00:05:41,480
expliquei exatamente
o que estava acontecendo.

91
00:05:41,480 --> 00:05:44,140
E ele ficou muito impressionado pelo fato

92
00:05:44,140 --> 00:05:48,970
de que mostramos que a retro-propagação
poderia aprender representações para palavras.

93
00:05:48,970 --> 00:05:52,490
E que você poderia olhar para aquelas
representações, que são como pequenos vetores,

94
00:05:52,490 --> 00:05:55,950
e poderia compreender o significado
das características individuais.

95
00:05:55,950 --> 00:06:01,600
Então, na verdade, o treinamos em pequenas
tríades de palavras sobre árvores genealógicas,

96
00:06:01,600 --> 00:06:06,420
como Maria tinha
como mãe Victória.

97
00:06:06,420 --> 00:06:11,550
E você poderia dar as primeiras duas palavras,
e ele teria que prever a última palavra.

98
00:06:11,550 --> 00:06:12,970
E depois que você o tenha treinado,

99
00:06:12,970 --> 00:06:17,780
você poderia ver todo tipo de características nas
representações das palavras individuais.

100
00:06:17,780 --> 00:06:19,950
Como a nacionalidade da pessoa lá,

101
00:06:19,950 --> 00:06:25,180
de que geração elas eram, que ramo da
árvore genealógica elas estavam, e assim por diante.

102
00:06:25,180 --> 00:06:27,680
Foi o que deixou Stuart Sutherland
realmente impressionado com isso, e

103
00:06:27,680 --> 00:06:29,666
eu acho que foi por isso que o artigo foi aceito.

104
00:06:29,666 --> 00:06:33,905
—Incorporando as palavras muito cedo,
e você já está vendo recursos

105
00:06:33,905 --> 00:06:38,390
aprendidos de significados semânticos
emergir do algoritmo de treinamento.

106
00:06:38,390 --> 00:06:44,090
— Sim, então do ponto de vista de um psicólogo,
o que foi interessante foi unificar

107
00:06:44,090 --> 00:06:49,740
duas vertentes completamente diferentes
de ideias sobre como era o conhecimento.

108
00:06:49,740 --> 00:06:53,460
Então, havia a velha visão do psicólogo
que um conceito era um grande

109
00:06:53,460 --> 00:06:56,810
conjunto de recursos, e
havia muitas evidências para isso.

110
00:06:56,810 --> 00:07:02,180
E depois havia a visão da AI do tempo,
que é uma visão estruturalista formal.

111
00:07:02,180 --> 00:07:06,190
Que era a que um conceito é
como ele se relaciona com outros conceitos.

112
00:07:06,190 --> 00:07:09,820
E para capturar um conceito, você teria que
fazer algo como estrutura gráfica ou

113
00:07:09,820 --> 00:07:11,640
talvez uma rede semântica.

114
00:07:11,640 --> 00:07:15,875
E o que esse exemplo da retro-propagação
mostrou foi que você poderia dar

115
00:07:15,875 --> 00:07:21,070
a informação que entraria na estrutura gráfica
ou, neste caso, uma árvore genealógica.

116
00:07:22,080 --> 00:07:26,920
E isso poderia converter a informação em
características de tal modo que ela poderia, então,

117
00:07:26,920 --> 00:07:33,470
usar as características para auferir novas
informações consistentes, isto é, generalizar.

118
00:07:33,470 --> 00:07:38,438
Mas o ponto crucial era esse vai e vem
entre a representação gráfica ou

119
00:07:38,438 --> 00:07:43,000
a representação estruturada tipo árvore,
da árvore genealógica, e

120
00:07:43,000 --> 00:07:46,715
a representação de pessoas
em grandes vetores de características.

121
00:07:46,715 --> 00:07:50,873
E, na verdade, a partir da representação
tipo gráfico você pode obter características em

122
00:07:50,873 --> 00:07:51,469
vetores,

123
00:07:51,469 --> 00:07:54,995
E a partir dos vetores das características, você poderia
obter mais da representação tipo gráfico.

124
00:07:54,995 --> 00:07:57,730
—Então isso é 1986?

125
00:07:57,730 --> 00:08:02,430
No início dos anos 90, [Yoshua] Bengio mostrou que
realmente podia obter dados reais,

126
00:08:02,430 --> 00:08:07,420
você poderia obter um texto em Inglês e
aplicar as mesmas técnicas lá, e

127
00:08:07,420 --> 00:08:13,980
obter incorporações para palavras reais de textos
em Inglês, e que impressionaram muito as pessoas.

128
00:08:13,980 --> 00:08:18,682
— Recentemente eu acho que temos falado
muito sobre computadores rápidos como as UPGs
 –Unidades de Processamento Gráfico– e

129
00:08:18,682 --> 00:08:21,750
supercomputadores que estão
conduzindo a aprendizagem profunda.

130
00:08:21,750 --> 00:08:26,376
Eu não percebi que voltar entre 1986
e o início dos anos 90, soasse que entre

131
00:08:26,376 --> 00:08:29,570
você e Bengio já havia
o início dessa tendência.

132
00:08:30,600 --> 00:08:32,630
— Sim, foi um avanço imenso.

133
00:08:32,630 --> 00:08:41,440
Em 1986 eu estava usando uma máquina LISP que
era menos que o décimo de um mega flop.

134
00:08:41,440 --> 00:08:47,720
E cerca de 1993 ou por perto,
as pessoas estavam vendo dez mega flops.

135
00:08:47,720 --> 00:08:49,600
— Entendo.
— Então havia um fator de 100,

136
00:08:49,600 --> 00:08:51,770
e esse foi o ponto
em que foi fácil usar,

137
00:08:51,770 --> 00:08:53,580
porque os computadores estavam
apenas ficando mais rápidos.

138
00:08:53,580 --> 00:08:56,960
— Nas últimas décadas,
você inventou

139
00:08:56,960 --> 00:08:59,970
tantas peças de redes neurais e
aprendizagem profunda.

140
00:08:59,970 --> 00:09:02,670
Na verdade estou curioso,
de todas as coisas que você inventou,

141
00:09:02,670 --> 00:09:05,050
de quais delas você ainda
está mais animado hoje?

142
00:09:06,940 --> 00:09:09,590
— Então, eu acho que o mais bonito
é o trabalho que fiz com

143
00:09:09,590 --> 00:09:12,620
Terry Sejnowski nas máquinas Boltzmann.

144
00:09:12,620 --> 00:09:14,500
Nós descobrimos que havia este algoritmo

145
00:09:14,500 --> 00:09:18,830
de aprendizagem muito simples
que se aplicava a grandes

146
00:09:18,830 --> 00:09:23,550
redes de grande densidade conectadas onde você
poderia apenas ver alguns dos nós.

147
00:09:23,550 --> 00:09:27,730
Então, ele aprenderia representações escondidas
e era uma algoritmo muito simples.

148
00:09:27,730 --> 00:09:31,130
E isso pareceu como aquele tipo de coisa que você
deveria ser capaz de obter em uma cérebro pois

149
00:09:31,130 --> 00:09:34,210
cada sinapse apenas precisava saber
sobre o comportamento dos dois

150
00:09:34,210 --> 00:09:35,940
neurônios aos quais ele estava diretamente conectado.

151
00:09:37,010 --> 00:09:41,230
E a informação que fora
propagada era a mesma.

152
00:09:41,230 --> 00:09:45,160
Houve duas fases diferentes,
as quais chamamos acordar e dormir.

153
00:09:45,160 --> 00:09:46,820
Mas nas duas fases diferentes,

154
00:09:46,820 --> 00:09:48,760
você está propagando informação
da mesma maneira.

155
00:09:48,760 --> 00:09:52,360
Onde há algo como retro-propagação,
 há uma passagem para a frente e

156
00:09:52,360 --> 00:09:54,820
uma passagem para trás, e
eles funcionam de forma diferente.

157
00:09:54,820 --> 00:09:56,379
Eles estão mandando tipos
diferentes de sinais.

158
00:09:58,100 --> 00:10:01,190
Então eu acho que isso é
a coisa mais bonita.

159
00:10:01,190 --> 00:10:03,730
E por muitos anos pareceu
simplesmente como uma curiosidade,

160
00:10:03,730 --> 00:10:05,090
pois parecia como
se fosse lento demais.

161
00:10:06,210 --> 00:10:10,420
Mas então, mais tarde, eu me livrei um
pouco da beleza, e isso começou a me deixar

162
00:10:10,420 --> 00:10:13,730
estabelecer e usar apenas uma iteração,
em uma rede de alguma forma mais simples.

163
00:10:13,730 --> 00:10:16,570
E isso resultou nas máquinas
de Boltzmann restritas,

164
00:10:16,570 --> 00:10:19,430
que na verdade funcionaram
efetivamente na prática.

165
00:10:19,430 --> 00:10:21,586
Bom, na competição Netflix,
por exemplo,

166
00:10:21,586 --> 00:10:26,170
as máquinas de Boltzmann restritas foram
um dos ingredientes da entrada vencedora.

167
00:10:26,170 --> 00:10:30,210
— Na verdade, muito do ressurgimento
recente da rede neural e da

168
00:10:30,210 --> 00:10:34,790
aprendizagem profunda, começando em 2007,
foi a máquina de Boltzmann restrita,

169
00:10:34,790 --> 00:10:37,710
e o trabalho da máquina não restrita de Boltzmann
que você e seu laboratório fizeram.

170
00:10:38,940 --> 00:10:42,130
— Sim, então essa é outra das peças
do trabalho com o qual estou muito feliz,

171
00:10:42,130 --> 00:10:46,290
a ideia de que você poderia treinar sua
máquina de Boltzmann restrita, que apenas

172
00:10:46,290 --> 00:10:51,120
tinha uma camada de características escondidas e
que você podia aprender uma camada de características.

173
00:10:51,120 --> 00:10:54,850
E, depois, você poderia tratar essas
características como dados e fazer de novo e,

174
00:10:54,850 --> 00:10:57,953
então, você poderia tratar os novas características
que você aprendeu como dados e fazer de novo,

175
00:10:57,953 --> 00:10:59,570
tantas vezes quantas você quisesse.

176
00:10:59,570 --> 00:11:03,060
Isso era legal, funcionou na prática.

177
00:11:03,060 --> 00:11:08,709
E, então, Uy Tay percebeu que a coisa
toda poderia ser tratada como um único modelo,

178
00:11:08,709 --> 00:11:11,110
mas foi um tipo de modelo estranho.

179
00:11:11,110 --> 00:11:15,946
Foi um modelo onde, no topo, que você tinha
uma máquina de Boltzmann restrita, mas

180
00:11:15,946 --> 00:11:20,626
abaixo dela você tinha uma rede de crenças
sigmoide que era algo que foi

181
00:11:20,626 --> 00:11:23,060
inventada muitos anos antes.

182
00:11:23,060 --> 00:11:24,620
Então, era um modelo dirigido e

183
00:11:24,620 --> 00:11:28,651
o que conseguimos criar treinando
essas máquinas de Boltzmann

184
00:11:28,651 --> 00:11:32,760
restritas foi uma maneira eficiente de fazer
deduções nas redes de crenças sigmoide.

185
00:11:33,830 --> 00:11:36,870
Então, em torno desse tempo,

186
00:11:36,870 --> 00:11:41,270
havia pessoas fazendo redes neurais,
que poderiam usar redes densamente conectadas, mas

187
00:11:41,270 --> 00:11:45,500
não tiveram nenhuma forma boa de fazer
impressões probabilísticas nelas.

188
00:11:45,500 --> 00:11:50,050
E você tinha pessoas fazendo modelos gráficos,
ao contrário dos meus filhos,

189
00:11:50,050 --> 00:11:55,603
que poderiam deduzir devidamente, mas
apenas em redes levemente conectadas.

190
00:11:55,603 --> 00:12:01,140
E o que conseguimos mostrar foi
a maneira de aprender essas

191
00:12:01,140 --> 00:12:06,280
redes de crença profundas, de modo que há formas
aproximadas de dedução que são muito rápidas,

192
00:12:06,280 --> 00:12:10,578
são eficientes em uma única passagem adiante
e isso foi um resultado muito bonito.

193
00:12:10,578 --> 00:12:14,890
E você poderia garantir que cada vez que
você aprende aquela camada extra de características,

194
00:12:16,010 --> 00:12:19,980
houve um ligação, cada vez que você aprendeu
uma nova camada, você conseguiu uma nova ligação, e

195
00:12:19,980 --> 00:12:22,700
a nova ligação sempre foi
melhor que a velha ligação.

196
00:12:22,700 --> 00:12:25,810
— As ligações variacionais,
mostradas conforme você adiciona camadas.

197
00:12:25,810 --> 00:12:26,970
Sim, eu me lembro desse vídeo.

198
00:12:26,970 --> 00:12:29,680
— Então, essa foi a segunda coisa
sobre a qual eu estava realmente animado.

199
00:12:29,680 --> 00:12:35,600
E eu acho que a terceira coisa foi o trabalho que
fiz com os métodos variacionais.

200
00:12:35,600 --> 00:12:40,750
Acontece que as pessoas em estatística
fizeram um trabalho semelhante anteriormente,

201
00:12:40,750 --> 00:12:43,100
mas não sabíamos disso.

202
00:12:44,610 --> 00:12:47,260
Então, conseguimos fazer

203
00:12:47,260 --> 00:12:50,250
o EM ‑Expectation-Maximization‑ funcionar muito melhor, mostrando
que você não precisava fazer um passo E perfeito.

204
00:12:50,250 --> 00:12:52,800
Você podia fazer um passo E aproximado.

205
00:12:52,800 --> 00:12:55,320
E EM era um grande algoritmo em estatística.

206
00:12:55,320 --> 00:12:58,380
E nós mostramos uma grande
generalização dele.

207
00:12:58,380 --> 00:13:02,490
E, em particular, em 1993,
eu acho, com Van Camp,

208
00:13:02,490 --> 00:13:07,040
eu escrevi um artigo, acho que
 o primeiro artigo variacional de Bayes,

209
00:13:07,040 --> 00:13:12,090
onde mostramos que você poderia de fato
fazer uma versão do aprendizado Bayesiano

210
00:13:12,090 --> 00:13:17,950
que era de longe mais tratável,
por aproximar a verdade posterior com a adivinhação.

211
00:13:17,950 --> 00:13:20,320
E você poderia fazer isso na rede neural.

212
00:13:20,320 --> 00:13:22,600
E eu estava muito animado por ela.

213
00:13:22,600 --> 00:13:23,680
— Entendi.
Uau, certo.

214
00:13:23,680 --> 00:13:26,670
Sim, eu acho que eu me lembro
de todos esses artigos.

215
00:13:26,670 --> 00:13:32,630
Neil e Hinton[...], o artigo sobre [Modelo] Proximal
[de Redes Profundas] de Ian [Goodfellow] [...],
passei muitas horas lendo isso.

216
00:13:32,630 --> 00:13:36,070
E eu acho que alguns dos
algoritmos que você usa hoje,

217
00:13:36,070 --> 00:13:41,110
ou algum dos algoritmos que muitas pessoas
usam quase todo dia, são o que,

218
00:13:41,110 --> 00:13:46,570
coisas do tipo desligamentos, ou eu acho que 
as ativações ReLU vieram do seu grupo?

219
00:13:46,570 --> 00:13:47,390
— Sim e não.

220
00:13:47,390 --> 00:13:51,470
Então outras pessoas pensaram
sobre as unidades lineares retificadas, ReLU.

221
00:13:51,470 --> 00:13:56,860
E nós, na verdade, fizemos algum trabalho com
as máquinas de Boltzmann restritas mostrando

222
00:13:56,860 --> 00:14:02,880
que a ReLU era quase que exatamente equivalente
a uma pilha inteira de unidades logísticas.

223
00:14:02,880 --> 00:14:05,190
E isso é uma das coisas
que ajudou as ReLUs a se popularizarem.

224
00:14:05,190 --> 00:14:07,440
— Eu fiquei realmente curioso com isso.

225
00:14:07,440 --> 00:14:12,570
O artigo sobre ReLU teve muita
matemática mostrando que esta função

226
00:14:12,570 --> 00:14:15,530
pode ser aproximada com essa
fórmula realmente complicada.

227
00:14:15,530 --> 00:14:19,140
Você fez aquela matemática de modo que seu artigo
fosse aceito numa conferência acadêmica,

228
00:14:19,140 --> 00:14:24,840
ou toda aquela matemática realmente influenciou
o desenvolvimento do limite do 0 e do X?

229
00:14:26,450 --> 00:14:30,440
— Esse foi um dos casos onde,
na verdade, a matemática foi importante

230
00:14:30,440 --> 00:14:32,350
para o desenvolvimento da ideia.

231
00:14:32,350 --> 00:14:35,262
Então, eu sabia sobre as unidades lineares retificadas
obviamente, e

232
00:14:35,262 --> 00:14:36,821
eu sabia sobre das unidades logísticas.

233
00:14:36,821 --> 00:14:39,250
E por causa do trabalho nas
máquinas de Boltzmann,

234
00:14:39,250 --> 00:14:42,720
todo o trabalho básico foi
feito usando unidades logísticas.

235
00:14:42,720 --> 00:14:45,120
Então, a questão era:

236
00:14:45,120 --> 00:14:49,070
o algoritmo de aprendizagem poderia funcionar
em algo com unidades lineares retificadas?

237
00:14:49,070 --> 00:14:54,400
E mostrar as unidades lineares retificadas
foi quase exatamente o equivalente a uma pilha

238
00:14:54,400 --> 00:15:00,350
de unidades logísticas, mostramos que
toda a matemática passaria.

239
00:15:00,350 --> 00:15:01,508
Entendo.

240
00:15:01,508 --> 00:15:05,890
E isso forneceu a inspiração para
hoje, dezenas de pessoas usam ReLU e

241
00:15:05,890 --> 00:15:08,000
ela funciona sem...-
—Sim.

242
00:15:08,000 --> 00:15:12,130
— ... sem necessariamente precisar
compreender a mesma motivação.

243
00:15:13,150 --> 00:15:16,850
— Sim, uma das coisas que notei
mais tarde, quando fui para o Google,

244
00:15:16,850 --> 00:15:22,796
acho que em 2014, eu dei uma palestra
no Google sobre usar ReLUs e

245
00:15:22,796 --> 00:15:26,660
iniciei com a matriz identidade,

246
00:15:26,660 --> 00:15:30,300
porque a parte legal das ReLUs é
que se você continuar replicando as camadas

247
00:15:30,300 --> 00:15:32,667
escondidas e
você iniciar com a identidade,

248
00:15:32,667 --> 00:15:35,050
ela apenas copia o padrão
na camada abaixo.

249
00:15:36,140 --> 00:15:40,120
Então, eu estava mostrando que você pode treinar
redes com 300 camadas escondidas e

250
00:15:40,120 --> 00:15:44,760
você poderia treiná-las de modo eficiente
se você inicializar com a identidade delas.

251
00:15:44,760 --> 00:15:48,065
Mas eu não persegui mais isso e
realmente me arrependo de não ter perseguido.

252
00:15:48,065 --> 00:15:52,507
Publicamos um artigo mostrando
que você poderia inicializar [...]

253
00:15:52,507 --> 00:15:55,565
mostrando que você podia iniciar
recorrências como essa.

254
00:15:55,565 --> 00:16:00,370
Mas eu deveria ter perseguido mais isso
porque, mais tarde, nessas redes

255
00:16:00,370 --> 00:16:03,572
residuais isso é realmente aquele tipo de coisa.

256
00:16:03,572 --> 00:16:06,660
— Ao longo dos anos, eu ouvi
você falar muito sobre o cérebro.

257
00:16:06,660 --> 00:16:09,447
Eu ouvi você falar sobre a relação
ser a retro-propagação e o cérebro.

258
00:16:09,447 --> 00:16:13,720
Quais são seus pensamentos atuais sobre isso?

259
00:16:13,720 --> 00:16:16,910
— Na verdade, estou trabalhando em
um artigo sobre isso agora mesmo.

260
00:16:18,250 --> 00:16:21,160
Eu acho que meu pensamento principal é este:

261
00:16:21,160 --> 00:16:25,570
se ocorrer de a retro-propagação ser de fato
um bom algoritmo para se fazer aprendizado,

262
00:16:26,620 --> 00:16:31,610
então, com certeza, a evolução poderia ter
descoberto como melhorá-la.

263
00:16:32,730 --> 00:16:37,270
Quero dizer, você tem células que poderiam
se transformar em globos oculares ou dentes.

264
00:16:37,270 --> 00:16:42,440
Agora, se as células podem fazer isso, com certeza
elas podem melhorar a retro-propagação e,

265
00:16:42,440 --> 00:16:45,860
presumidamente, esta enorme
pressão seletiva para tanto.

266
00:16:45,860 --> 00:16:50,490
Então, eu penso que a ideia neuro-cientista
dela não parecer ser plausível, é simplesmente tola.

267
00:16:50,490 --> 00:16:52,890
Deve haver alguma implementação
sutil disso.

268
00:16:52,890 --> 00:16:56,000
E penso que o cérebro provavelmente tem
algo que pode não ser exatamente a

269
00:16:56,000 --> 00:16:58,620
retro-propagação, mas
está bem próximo disso.

270
00:16:58,620 --> 00:17:02,566
E ao longo dos anos, eu sugeri
inúmeras ideias sobre como isso poderia funcionar.

271
00:17:02,566 --> 00:17:06,994
Então, em 1987, trabalhando com Jay McClelland,

272
00:17:06,994 --> 00:17:11,202
Eu sugeri o
algoritmo de recirculação,

273
00:17:11,202 --> 00:17:16,090
onde a ideia é você mandar
informação ao redor de um circuito repetitivo.

274
00:17:17,470 --> 00:17:18,686
E você tenta fazer isso de modo

275
00:17:18,686 --> 00:17:22,206
que as coisas não mudem conforme
a informação percorre este circuito repetidas vezes.

276
00:17:22,206 --> 00:17:26,490
A versão mais simples seria você ter
unidades de entrada e unidades escondidas, e

277
00:17:26,490 --> 00:17:31,046
você mandar informação da entrada para
a escondida e depois de volta para a da entrada, e

278
00:17:31,046 --> 00:17:34,388
depois de volta para a escondida e
depois de volta para a entrada e assim por diante.

279
00:17:34,388 --> 00:17:38,001
E o que você quer,
você quer treinar um codificador automático,

280
00:17:38,001 --> 00:17:42,300
mas você quer treiná-lo sem
ter que fazer retro-propagação,

281
00:17:42,300 --> 00:17:47,250
então, você apenas o treina e se livra
de todas as variações nas atividades.

282
00:17:47,250 --> 00:17:51,922
A ideia é que a regra da aprendizagem para

283
00:17:51,922 --> 00:17:57,930
sinapse seja... Mude a proporção
de peso para a entrada pré-sináptica e

284
00:17:57,930 --> 00:18:01,780
em proporção à taxa de
mudança na entrada pós sináptica.

285
00:18:01,780 --> 00:18:04,060
Mas, na recirculação, você está tentando
fazer a entrada pós sináptica,

286
00:18:04,060 --> 00:18:08,330
você está tentando fazer o velho
ser bom e o novo ser ruim,

287
00:18:08,330 --> 00:18:09,620
então, você está mudando naquela direção.

288
00:18:11,010 --> 00:18:14,472
Inventamos este algoritmo antes de
os neurocientistas sugerirem

289
00:18:14,472 --> 00:18:16,521
o "Spike-timing-dependent plasticity" - STDP
(plasticidade sináptica dependente do tempo
de disparos entre os neurônios).

290
00:18:16,521 --> 00:18:20,700
O "STDP" é, na verdade,
o mesmo algoritmo, mas

291
00:18:20,700 --> 00:18:26,220
do contrário, em que o novo é algo bom e
o velho é algo ruim, na regra da aprendizagem.

292
00:18:26,220 --> 00:18:30,010
Então, você está mudando as proporções
de peso para a atividade pré-sináptica

293
00:18:30,010 --> 00:18:35,690
multiplicada pela nova atividade pré-sináptica
menos a velha.

294
00:18:37,060 --> 00:18:42,020
Mais tarde, eu percebi em 2007,
que se você pegar uma pilha de

295
00:18:42,020 --> 00:18:47,830
máquinas Boltzmann Restritas e
você se treinar nelas,

296
00:18:47,830 --> 00:18:52,620
depois de treinado, você, então, teria
exatamente as condições certas para

297
00:18:52,620 --> 00:18:56,450
implementar a retro-propagação
reconstruir.

298
00:18:56,450 --> 00:19:01,124
Se você olhasse para a era da reconstrução,
que a era da reconstrução deveria,

299
00:19:01,124 --> 00:19:05,728
na verdade, lhe dizer a derivada
da performance discriminativa.

300
00:19:05,728 --> 00:19:12,079
E na primeira oficina de Aprendizagem profunda,
em 2007, eu palestrei sobre isso.

301
00:19:12,079 --> 00:19:16,454
Isso foi quase que completamente ignorado.

302
00:19:16,454 --> 00:19:19,799
Mais tarde, Yoshua Bengio,
assumiu a ideia e

303
00:19:19,799 --> 00:19:24,340
isso, na verdade, funcionou
mais que a oficina.

304
00:19:24,340 --> 00:19:26,490
E eu mesmo tenho
trabalhado mais nisso.

305
00:19:26,490 --> 00:19:33,280
E eu acho que esta ideia de que se você tiver
uma pilha de codificadores automáticos, então você pode

306
00:19:33,280 --> 00:19:38,440
obter as derivadas enviando atividades
para trás e localizar reconstituintes,

307
00:19:38,440 --> 00:19:42,520
é uma ideia realmente interessante e
bem pode ser como o cérebro funciona.

308
00:19:42,520 --> 00:19:47,520
— Um outro tópico que eu sei que você segue
e que eu ouvi que você ainda está

309
00:19:47,520 --> 00:19:51,930
trabalhando nele, é como lidar com 
múltiplas escalas de tempo na aprendizagem profunda?

310
00:19:51,930 --> 00:19:54,468
Então, você pode compartilhar seus 
pensamentos sobre isso?

311
00:19:54,468 --> 00:19:58,910
— Sim, então, na verdade, isso volta aos
meus primeiros anos de aluno de graduação.

312
00:19:58,910 --> 00:20:04,040
A primeira palestra que eu dei foi sobre
usar o que eu chamava de pesos rápidos.

313
00:20:04,040 --> 00:20:07,560
Pesos que se adaptam rapidamente,
mas caem rapidamente.

314
00:20:07,560 --> 00:20:08,832
E, portanto, pode conter memória de curto prazo.

315
00:20:08,832 --> 00:20:13,496
E eu mostrei em um sistema muito
simples em 1973 que você pode fazer

316
00:20:13,496 --> 00:20:16,590
recursão verdadeira com esses pesos.

317
00:20:16,590 --> 00:20:23,010
E o que eu quero dizer por recursão verdadeira
é que os neurônios que são usados

318
00:20:23,010 --> 00:20:28,470
para representar as coisas são reusados para
representar coisas no núcleo recursivo.

319
00:20:30,210 --> 00:20:31,750
E os pesos que são usados para

320
00:20:31,750 --> 00:20:34,388
representar o conhecimento se tornam re-usados
no núcleo recursivo.

321
00:20:34,388 --> 00:20:39,170
E isso leva a questão de
quando você faz uso do núcleo recursivo,

322
00:20:39,170 --> 00:20:41,600
como você se lembra do que
é que você estava fazendo?

323
00:20:41,600 --> 00:20:42,970
Onde está essa memória?

324
00:20:42,970 --> 00:20:45,015
Porque você usou os neurônios para
o núcleo recursivo.

325
00:20:46,080 --> 00:20:49,240
E a resposta é que você pode colocar essa
memória em pesos rápidos e

326
00:20:49,240 --> 00:20:53,940
você pode recuperar os estados de atividade
dos neurônios a partir desses pesos rápidos.

327
00:20:53,940 --> 00:20:56,151
E, mais recentemente,
trabalhando com Jimmy Ba,

328
00:20:56,151 --> 00:21:00,141
na verdade, escrevemos um artigo sobre NIPS, Sist. de Proc.
de Informação Neural, usando pesos rápidos para recursão como essa.

329
00:21:00,141 --> 00:21:00,898
Entendo.

330
00:21:00,898 --> 00:21:04,145
— Então isso foi uma grande lacuna.

331
00:21:04,145 --> 00:21:08,746
O primeiro modelo foi
"não publicado" em 1973 e

332
00:21:08,746 --> 00:21:14,966
então o modelo de Jimmy Ba foi em 2015,
eu acho, ou 2016.

333
00:21:14,966 --> 00:21:16,469
Então é cerca de 40 anos mais tarde.

334
00:21:16,469 --> 00:21:22,840
— E, eu acho que...
Uma das ideias que eu gostaria que você falasse agora,

335
00:21:22,840 --> 00:21:29,350
ao longo de cinco anos, eu acho que são as cápsulas,
onde você está com elas?

336
00:21:29,350 --> 00:21:34,150
— Ok, então estou de volta ao
estado em que costumava estar.

337
00:21:34,150 --> 00:21:39,320
Que é eu ter essa ideia e realmente
acreditar nela e ninguém mais acreditar.

338
00:21:39,320 --> 00:21:42,120
E eu submeto artigos sobre ela e
eles são rejeitados.

339
00:21:42,120 --> 00:21:45,938
Mas eu realmente acredito nessa ideia e
vou continuar impelindo-a.

340
00:21:45,938 --> 00:21:53,880
Então ela depende de, ah...
Há umas duas ideias chave.

341
00:21:53,880 --> 00:22:00,000
Uma é sobre como você representa
entidades multidimensionais, e você

342
00:22:00,000 --> 00:22:05,070
pode representar entidades multidimensionais
por apenas algumas atividades ocultas.

343
00:22:05,070 --> 00:22:07,630
Contanto que você saiba
que há qualquer uma delas.

344
00:22:07,630 --> 00:22:12,150
Então, a ideia é em cada região da imagem,
você assumirá que há no máximo

345
00:22:12,150 --> 00:22:14,000
um dos tipos particulares de característica.

346
00:22:15,200 --> 00:22:18,020
E, então, você usará uma pilha de neurônios,
e

347
00:22:18,020 --> 00:22:23,190
suas atividades representarão
os diferentes aspectos daquela característica,

348
00:22:24,230 --> 00:22:27,270
como naquela região, exatamente
o que são as coordenadas x e y?

349
00:22:27,270 --> 00:22:28,780
Em que orientação está?

350
00:22:28,780 --> 00:22:29,930
Quão rápido ela está se movendo?

351
00:22:29,930 --> 00:22:30,630
De que cor é?

352
00:22:30,630 --> 00:22:31,270
Quão brilhosa é?

353
00:22:31,270 --> 00:22:32,590
E coisas assim.

354
00:22:32,590 --> 00:22:36,350
Então, você pode usar uma pilha de neurônios
para representar diferentes dimensões da

355
00:22:36,350 --> 00:22:37,710
a mesma coisa.

356
00:22:37,710 --> 00:22:39,410
Desde que haja apenas um deles.

357
00:22:40,490 --> 00:22:46,110
Essa é uma maneira muito diferente
de fazer a representação

358
00:22:46,110 --> 00:22:48,155
a partir do que nós, normalmente,
estamos acostumados nas redes neurais.

359
00:22:48,155 --> 00:22:49,820
Normalmente, nas redes neurais
apenas temos uma grande camada,

360
00:22:49,820 --> 00:22:52,080
e todas as unidades saem e
fazem o que quer que façam.

361
00:22:52,080 --> 00:22:55,770
Mas você não pensa em agrupá-las
em pequenos grupos que representam

362
00:22:55,770 --> 00:22:57,310
diferentes coordenadas da mesma coisa.

363
00:22:58,660 --> 00:23:02,080
Então eu acho que devemos vencer
essa estrutura extra.

364
00:23:02,080 --> 00:23:05,020
E, depois, a outra ideia
que vem com isso.

365
00:23:05,020 --> 00:23:07,410
— Então isso quer dizer que na
representação distribuída,

366
00:23:07,410 --> 00:23:09,280
você dividiu, particionou,
 a representação.

367
00:23:09,280 --> 00:23:11,270
— Sim. —Em diferentes subconjuntos.

368
00:23:11,270 --> 00:23:13,900
— Sim. — Para representar, certo, ao invés de...

369
00:23:13,900 --> 00:23:15,600
— Eu chamo cada uma dessas estruturas de cápsula.

370
00:23:15,600 --> 00:23:16,180
Entendo.

371
00:23:16,180 --> 00:23:21,078
— E a idea é que uma cápsula seja capaz de
representar uma instância de uma característica, mas

372
00:23:21,078 --> 00:23:21,794
apenas uma.

373
00:23:21,794 --> 00:23:27,130
E isso representa todas as propriedades
diferentes daquela característica.

374
00:23:27,130 --> 00:23:29,880
É uma característica que tem
um monte de propriedades ao invés de

375
00:23:29,880 --> 00:23:34,530
um neurônio normal e uma rede de neurônios,
que tem apenas uma escala de propriedade.

376
00:23:34,530 --> 00:23:36,240
— Sim, entendo sim.

377
00:23:36,240 --> 00:23:41,423
— E, então, o que você pode fazer se conseguir obter
isso é: você pode fazer algo que redes neurais

378
00:23:41,423 --> 00:23:48,980
normais sejam muito ruins, o que quer dizer que você
pode fazer o que chamo de rotina de concordância.

379
00:23:48,980 --> 00:23:52,960
Então, vamos supor que você
queira fazer segmentação e

380
00:23:52,960 --> 00:23:56,660
você tem algo que pode ser uma boca
e algo a mais que pode ser um nariz.

381
00:23:57,910 --> 00:24:02,179
E você quer saber se você deveria
colocá-los junto e fazer uma coisa.

382
00:24:02,179 --> 00:24:03,879
Então, a ideia deveria ter uma cápsula para

383
00:24:03,879 --> 00:24:06,040
uma boca que tem
os parâmetros da boca.

384
00:24:06,040 --> 00:24:10,582
E você tem uma cápsula para um nariz
que tem os parâmetros do nariz.

385
00:24:10,582 --> 00:24:13,797
E depois, para decifrar se
colocá-los juntos

386
00:24:13,797 --> 00:24:18,670
ou não, você obtém cada um deles para votar
quais deveriam ser os parâmetros para um rosto.

387
00:24:19,930 --> 00:24:23,718
Agora, se a boca e o nariz estão
na relação espacial certa,

388
00:24:23,718 --> 00:24:24,725
eles concordarão.

389
00:24:24,725 --> 00:24:28,888
Quando você obtém duas capturas em nível um
votando pelo mesmo conjunto de parâmetros

390
00:24:28,888 --> 00:24:32,106
no próximo nível,
você pode supor que eles estão provavelmente certos,

391
00:24:32,106 --> 00:24:35,350
porque a concordância em um espaço
dimensional alto é muito improvável.

392
00:24:36,950 --> 00:24:42,109
E essa é uma maneira bem diferente
de fazer filtragem,

393
00:24:42,109 --> 00:24:46,130
do que o que usamos normalmente nas redes neurais.

394
00:24:46,130 --> 00:24:50,708
Então, eu acho que esta rotina por concordância
será crucial para

395
00:24:50,708 --> 00:24:56,700
obter redes neurais para generalizar
muito melhor a partir de dados limitados.

396
00:24:56,700 --> 00:24:59,797
Eu penso que será muito bom
conseguir as mudanças no ponto de vista,

397
00:24:59,797 --> 00:25:01,500
muito bom para fazermos segmentação.

398
00:25:01,500 --> 00:25:04,794
E eu espero que seja muito mais
eficiente estatisticamente do que o que

399
00:25:04,794 --> 00:25:06,147
atualmente fazemos nas redes neurais.

400
00:25:06,147 --> 00:25:08,575
Que é: se você quer lidar
com mudanças no ponto de vista,

401
00:25:08,575 --> 00:25:12,000
você apenas dá um monte de mudanças
no ponto de vista e treinamento em todas elas.

402
00:25:12,000 --> 00:25:16,460
— Sei, certo, então ao invés de propagação
para frente na aprendizagem supervisionada,

403
00:25:16,460 --> 00:25:19,120
você pode aprender isso de uma maneira diferente.

404
00:25:20,220 --> 00:25:24,120
— Bem, eu ainda planejo fazer isso
com aprendizagem supervisionada, mas

405
00:25:24,120 --> 00:25:27,720
as mecânicas do passo 
para frente são muito diferentes.

406
00:25:27,720 --> 00:25:32,010
Não é simplesmente uma propagação para frente
 no sentido que há pequenos trechos de iteração

407
00:25:32,010 --> 00:25:36,550
acontecendo, onde você pensa que encontrou
uma boca e você pensa que encontrou um nariz.

408
00:25:36,550 --> 00:25:39,127
E usar pequenos pedaços
de iteração para decidir

409
00:25:39,127 --> 00:25:42,530
se eles deveriam realmente
se unir para formar um rosto.

410
00:25:42,530 --> 00:25:46,352
E você pode fazer retro-propagações
a partir dessa iteração.

411
00:25:46,352 --> 00:25:50,286
Você pode tentar e
fazer isso um pouco de forma discriminativa,

412
00:25:50,286 --> 00:25:54,417
e estamos trabalhando nisso
agora no meu grupo em Toronto.

413
00:25:54,417 --> 00:26:00,260
Agora eu tenho uma pequena equipe Google
em Toronto, parte da equipe do cérebro.

414
00:26:00,260 --> 00:26:02,127
É por isso que estou animado agora.

415
00:26:02,127 --> 00:26:02,891
— Entendi, ótimo, sim.

416
00:26:02,891 --> 00:26:05,366
Estou esperando esse artigo
quando ele sair.

417
00:26:05,366 --> 00:26:10,750
— Sim, se ele sair [risos].

418
00:26:10,750 --> 00:26:13,040
— Você trabalhou em aprendizagem profunda
por muitas décadas.

419
00:26:13,040 --> 00:26:15,330
Na verdade estou bastante ansioso,
como era seu pensamento,

420
00:26:15,330 --> 00:26:18,760
sua compreensão de AI
mudou ao longo desses anos?

421
00:26:20,380 --> 00:26:27,678
— Então, acho que muita da minha história
intelectual tem sido em torno da propagação retroativa,

422
00:26:27,678 --> 00:26:33,531
e como usar propagação retroativa,
como fazer uso de seu poder.

423
00:26:33,531 --> 00:26:36,966
Para começar, em meados dos anos 80,
a estávamos usando para

424
00:26:36,966 --> 00:26:40,203
aprendizagem discriminativa e
ela estava funcionando bem.

425
00:26:40,203 --> 00:26:42,405
Então, eu decidi, no início dos anos 90,

426
00:26:42,405 --> 00:26:46,749
que, na verdade, a maioria da aprendizagem humana
seria aprendizagem não-supervisionada.

427
00:26:46,749 --> 00:26:50,138
E fiquei muito mais interessado
em aprendizagem não-supervisionada, e

428
00:26:50,138 --> 00:26:54,300
é quando eu trabalhei em coisas
como o algoritmo Wegstein.

429
00:26:54,300 --> 00:26:58,306
— E seus comentários naquela época
realmente influenciaram meu pensamento também.

430
00:26:58,306 --> 00:27:03,010
Quando eu estava liderando o 'Google Brain',
nosso primeiro projeto passou muito tempo

431
00:27:03,010 --> 00:27:07,900
trabalhando em aprendizagem não-supervisionada
por causa da sua influência.

432
00:27:07,900 --> 00:27:09,740
— Certo, e eu posso ter lhe induzido ao erro.

433
00:27:09,740 --> 00:27:11,470
Porque, a longo prazo,

434
00:27:11,470 --> 00:27:13,840
eu penso que a aprendizagem não-supervisionada
será absolutamente crucial.

435
00:27:15,160 --> 00:27:19,376
Mas você tem que,
meio que, enfrentar a realidade.

436
00:27:19,376 --> 00:27:24,107
E o que funcionou ao longo dos últimos dez
anos ou mais, foi a aprendizagem supervisionada.

437
00:27:24,107 --> 00:27:27,179
Treinamento discriminativo,
onde você tem rótulos, ou

438
00:27:27,179 --> 00:27:31,810
você está tentando prever a próxima coisa
nas séries, então isso atua como o rótulo.

439
00:27:31,810 --> 00:27:33,769
E isso funcionou incrivelmente bem.

440
00:27:37,528 --> 00:27:42,266
Eu ainda acredito que a aprendizagem não-supervisionada
será crucial, e as coisas funcionarão

441
00:27:42,266 --> 00:27:47,145
incrivelmente muito melhor do que elas funcionam
agora quando conseguirmos que funcione corretamente,

442
00:27:47,145 --> 00:27:48,200
mas ainda não conseguimos.

443
00:27:49,990 --> 00:27:53,225
— Sim, eu acho que muitos dos veteranos
em aprendizagem profunda,

444
00:27:53,225 --> 00:27:56,074
incluindo eu mesmo,
continuamos muito animados em relação a isso.

445
00:27:56,074 --> 00:28:01,513
É que simplesmente nenhum de nós
ainda teve uma ideia de como fazer isso.

446
00:28:01,513 --> 00:28:04,983
Talvez você tenha, eu não sinto que eu tenho.

447
00:28:04,983 --> 00:28:08,160
— O código de alteração variacional é onde
você usa os truques de reparametrização.

448
00:28:08,160 --> 00:28:10,120
Pareceu-me como uma boa ideia.

449
00:28:10,120 --> 00:28:15,260
E as redes contraditórias generativas também
me pareceram ser uma ideia muito legal.

450
00:28:15,260 --> 00:28:18,645
Eu acho que redes
contraditórias generativas são uma

451
00:28:18,645 --> 00:28:23,430
das maiores ideias na
aprendizagem profunda que é realmente nova.

452
00:28:23,430 --> 00:28:26,363
Espero que eu possa fazer
cápsulas de igual sucesso, mas

453
00:28:26,363 --> 00:28:31,740
agora as redes contraditórias generativas,
eu acho, têm sido um grande avanço.

454
00:28:31,740 --> 00:28:34,439
— O que aconteceu com a dispersão e
as características lentas,

455
00:28:34,439 --> 00:28:38,806
que eram dois dos outros princípios para
a construção de modelos não supervisionados?

456
00:28:41,556 --> 00:28:47,788
Nunca fui tão bom em
dispersão como você foi, amigo.

457
00:28:47,788 --> 00:28:52,672
Mas características lentas, eu acho, são um erro.

458
00:28:52,672 --> 00:28:53,660
Você não deveria dizer lentas.

459
00:28:53,660 --> 00:28:57,880
A ideia básica está certa, mas você não deveria
buscar por características que não mudam,

460
00:28:57,880 --> 00:29:00,660
você não deveria buscar
características que mudam de formas previsíveis.

461
00:29:01,680 --> 00:29:07,060
Então, aqui está um tipo de princípio básico
sobre como você modela qualquer coisa.

462
00:29:08,620 --> 00:29:13,391
Você toma as suas medidas,
e você está aplicando transformações

463
00:29:13,391 --> 00:29:17,612
não-lineares para suas
medidas até que você consiga

464
00:29:17,612 --> 00:29:22,672
uma representação como um vetor estado
na qual de fato seja linear.

465
00:29:22,672 --> 00:29:26,103
Então, você simplesmente não finge que é linear
como você faz com filtros comuns.

466
00:29:26,103 --> 00:29:29,625
Mas você de fato encontra uma transformação
partindo das observáveis para

467
00:29:29,625 --> 00:29:32,616
as variáveis subjacentes
onde as operações lineares,

468
00:29:32,616 --> 00:29:37,480
como multiplicadores de matriz nas variáveis
subjacentes, farão o trabalho.

469
00:29:37,480 --> 00:29:39,700
Então, por exemplo,
se você quiser mudar pontos de vistas,

470
00:29:39,700 --> 00:29:42,890
se você quiser produzir a imagem
a partir de outro ponto de vista,

471
00:29:42,890 --> 00:29:46,900
o que você deveria fazer é ir partir
dos pixels para as coordenadas.

472
00:29:47,950 --> 00:29:50,686
E, uma vez que você consegue
a representação da coordenada,

473
00:29:50,686 --> 00:29:54,120
que é um tipo de coisa que
espero que as cápsulas descubram,

474
00:29:54,120 --> 00:29:57,350
você pode, então, fazer um multiplicador de matriz
para mudar o ponto de vista e,

475
00:29:57,350 --> 00:29:59,210
então, você pode mapeá-lo de volta para os pixels.

476
00:29:59,210 --> 00:29:59,893
— Certo, é por isso que você fez tudo isso.

477
00:29:59,893 --> 00:30:02,170
— Eu acho que é um princípio
muito geral.

478
00:30:02,170 --> 00:30:04,773
— É por isso que você fez todo esse
trabalho na síntese facial, certo?

479
00:30:04,773 --> 00:30:09,355
Onde você pega um rosto e o comprime
a um vetor de dimensões bem pequenas e, então,

480
00:30:09,355 --> 00:30:12,450
você pode passear por isso e
voltar para outros rostos.

481
00:30:12,450 --> 00:30:15,950
— Eu tinha um aluno que trabalhou nisso,
eu não trabalhei muito nisso eu mesmo.

482
00:30:17,100 --> 00:30:19,180
— Agora, tenho certeza que você ainda
é questionado todo o tempo,

483
00:30:19,180 --> 00:30:23,920
se alguém quer entrar na
aprendizagem profunda, o que eles deveriam fazer?

484
00:30:23,920 --> 00:30:25,040
Então, que tipo de conselho você daria?

485
00:30:25,040 --> 00:30:28,938
Tenho certeza que você tem dado muitos conselhos
para pessoas em cada uma das configurações, mas para

486
00:30:28,938 --> 00:30:31,550
o público global de
pessoas assistindo a este vídeo.

487
00:30:31,550 --> 00:30:35,999
Que conselho você teria para
eles entrarem na aprendizagem profunda?

488
00:30:35,999 --> 00:30:42,171
— Ok, então, meu conselho é leia a
literatura, mas não muito sobre ela.

489
00:30:42,171 --> 00:30:48,030
Este conselho eu obtive do meu orientador,
que é muito diferente do que a maioria das pessoas dizem.

490
00:30:48,030 --> 00:30:52,474
A maioria das pessoas deveriam gastar vários
anos lendo a literatura e,

491
00:30:52,474 --> 00:30:55,421
então, você deveria começar
trabalhando nas suas próprias ideias.

492
00:30:55,421 --> 00:31:00,295
E isso pode ser verdade para alguns pesquisadores,
mas para pesquisadores criativos, eu acho

493
00:31:00,295 --> 00:31:03,803
que o que você quer é ler
um pouco de literatura

494
00:31:03,803 --> 00:31:07,792
e notar algo que você pensa que
todo mundo está fazendo errado.

495
00:31:07,792 --> 00:31:10,340
Eu sou contrário nesse sentido.

496
00:31:10,340 --> 00:31:13,568
Você olha para aquilo e
aquilo apenas não parece certo.

497
00:31:13,568 --> 00:31:15,660
E, então, descobre como fazê-lo certo.

498
00:31:16,890 --> 00:31:22,476
E, depois, quando as pessoas lhe dize
"isso não é bom", apenas mantenha sua posição.

499
00:31:22,476 --> 00:31:26,339
E eu tenho um princípio muito bom para
ajudar as pessoas a manterem suas posições,

500
00:31:26,339 --> 00:31:29,996
ainda que suas intuições
sejam boas ou não.

501
00:31:29,996 --> 00:31:32,030
Se suas intuições são boas,
você deveria segui-las e

502
00:31:32,030 --> 00:31:34,060
você será, eventualmente, bem sucedido.

503
00:31:34,060 --> 00:31:36,478
Se suas intuições não são boas,
não importa o que você faz.

504
00:31:36,478 --> 00:31:40,329
— Entendo [Risos].

505
00:31:40,329 --> 00:31:43,420
Conselho inspirador, posso segui-lo também.

506
00:31:43,420 --> 00:31:45,410
— Você também deve
confiar nas suas intuições.

507
00:31:45,410 --> 00:31:47,847
Não faz sentido não confiar nelas.

508
00:31:47,847 --> 00:31:49,420
— Entendo, sim.

509
00:31:49,420 --> 00:31:55,193
Eu geralmente aconselho as pessoas a não apenas ler,
mas replicar artigos publicados.

510
00:31:55,193 --> 00:31:58,161
E talvez isso coloque um limite
natural em quantos você pode fazer,

511
00:31:58,161 --> 00:32:00,800
porque replicar resultados
consome muito tempo.

512
00:32:01,910 --> 00:32:05,312
Sim, é verdade que quando você está
tentando replicar uma publicação

513
00:32:05,312 --> 00:32:08,100
você descobre todos os pequenos
truques necessários para fazê-lo funcionar.

514
00:32:08,100 --> 00:32:11,938
O outro conselho que tenho é
nunca pare de programar.

515
00:32:11,938 --> 00:32:15,577
Porque se você dá a um aluno
algo para fazer, se eles estiverem trapaceando,

516
00:32:15,577 --> 00:32:18,550
eles voltarão e dirão que não funcionou.

517
00:32:18,550 --> 00:32:22,030
E a razão pela qual não funcionou deveria
ser alguma decisão pequena que eles tomaram,

518
00:32:22,030 --> 00:32:25,100
que eles não perceberam que era crucial.

519
00:32:25,100 --> 00:32:28,850
E se você der a um bom aluno,
por exemplo,

520
00:32:28,850 --> 00:32:31,120
você pode dar a ele qualquer coisa e
ele voltará e dirá que funcionou.

521
00:32:32,670 --> 00:32:36,420
Eu me lembro de fazer isso uma vez,
e eu disse, mas espere um minuto.

522
00:32:36,420 --> 00:32:37,330
Desde a última vez que falamos,

523
00:32:37,330 --> 00:32:40,380
eu percebi que possivelmente não funcionaria
pela seguinte razão;

524
00:32:40,380 --> 00:32:43,586
e disse, sim, eu percebi isso de imediato,
então eu suponho que você não quis dizer isso.

525
00:32:43,586 --> 00:32:47,627
— [Risos] Entendi, sim,
isso é ótimo, sim.

526
00:32:47,627 --> 00:32:51,575
Vamos ver, qualquer outro conselho para

527
00:32:51,575 --> 00:32:57,782
pessoas que queiram entrar na AI e
aprendizagem profunda?

528
00:32:57,782 --> 00:33:02,000
— Penso que, basicamente, leia o suficiente,
para começar a desenvolver intuições.

529
00:33:02,000 --> 00:33:05,811
E, depois, confie nas suas intuições e
as siga,

530
00:33:05,811 --> 00:33:10,783
mas não fique tão preocupado se todo
mundo disser que não faz sentido.

531
00:33:10,783 --> 00:33:14,352
— E eu acho que não há como
saber se outros estão certos ou

532
00:33:14,352 --> 00:33:19,950
errados quando eles dizem que não faz sentido, mas você
apenas tem que segui-la, e então descobrir.

533
00:33:19,950 --> 00:33:24,350
— Certo, mas há uma coisa, que
é, se você pensa que é de fato uma boa ideia,

534
00:33:24,350 --> 00:33:27,201
e outras pessoas lhe dizem
que não faz nenhum sentido,

535
00:33:27,201 --> 00:33:29,761
então você está
realmente em algo.

536
00:33:29,761 --> 00:33:33,960
Então, um exemplo disso é quando Richard Zemel e eu
primeiro surgimos com os métodos variacionais.

537
00:33:35,420 --> 00:33:40,690
Eu enviei e-mails explicando isso para um ex-aluno
meu chamado Peter Brown,

538
00:33:40,690 --> 00:33:42,560
que sabia muito sobre isso.

539
00:33:43,570 --> 00:33:46,967
E ele mostrou isso às pessoas
que trabalhavam com ele,

540
00:33:46,967 --> 00:33:51,253
chamados de os irmãos Pietro,
eles eram gêmeos, eu acho.

541
00:33:51,253 --> 00:33:55,914
E ele, então, me disse mais tarde o que eles disseram,
e eles disseram

542
00:33:55,914 --> 00:34:00,277
ou esse cara está bêbado,
ou ele ´´e simplesmente imbecil, então

543
00:34:00,277 --> 00:34:04,260
eles realmente,
de fato pensaram que não fazia sentido.

544
00:34:04,260 --> 00:34:06,460
Agora, poderia ter sido parcialmente
como eu expliquei,

545
00:34:06,460 --> 00:34:08,043
porque eu expliquei isso em termos intuitivos.

546
00:34:09,150 --> 00:34:13,100
Mas quando você tem o que você
pensa que é uma boa ideia e

547
00:34:13,100 --> 00:34:16,810
outras pessoas pensam que é completamente lixo,
isso é sinal de uma ideia realmente boa.

548
00:34:18,026 --> 00:34:21,555
— Entendi, e tópicos de pesquisa,

549
00:34:21,555 --> 00:34:26,183
novos alunos universitários deveriam
trabalhar em cápsulas e

550
00:34:26,183 --> 00:34:30,707
talvez aprendizagem não supervisionada, alguma outra?

551
00:34:30,707 --> 00:34:34,078
— Um bom conselho para
novos alunos universitários é:

552
00:34:34,078 --> 00:34:38,344
veja se você pode encontrar um orientador
que tenha crenças semelhantes as suas.

553
00:34:38,344 --> 00:34:42,637
Porque se você trabalha em coisas que
seu orientador se ligue profundamente,

554
00:34:42,637 --> 00:34:47,170
você terá muitos bons conselhos e
tempo do seu orientador.

555
00:34:47,170 --> 00:34:50,590
Se você trabalha em coisa que seu
orientador não se interessa,

556
00:34:50,590 --> 00:34:55,262
tudo o que você terá será algum conselho,
mas não será quase tão útil.

557
00:34:55,262 --> 00:34:58,386
— Entendo, e
por último, um conselho para os aprendizes.

558
00:34:58,386 --> 00:35:02,440
Como você se sente em relação às pessoas
 que estão iniciando um programa de PhD?

559
00:35:02,440 --> 00:35:09,687
Versus se juntar a uma grande empresa,
ou a um excelente grupo de pesquisa?

560
00:35:09,687 --> 00:35:13,890
— Sim, é complicado,
eu acho que agora, o que está acontecendo é

561
00:35:13,890 --> 00:35:18,727
que não há suficiente acadêmicos treinados na
aprendizagem profunda para ensinar todas as pessoas

562
00:35:18,727 --> 00:35:21,125
que precisamos que aprendam isso nas universidades.

563
00:35:21,125 --> 00:35:25,011
Só não existe o corpo
docente amplo lá, mas

564
00:35:25,011 --> 00:35:27,780
acho que isso será temporário.

565
00:35:27,780 --> 00:35:32,410
Eu acho que o que aconteceu é que
a maioria dos departamentos tem sido bem lenta

566
00:35:32,410 --> 00:35:34,890
para entender o tipo de
revolução que está acontecendo.

567
00:35:34,890 --> 00:35:38,720
Eu meio que concordo com você, que não é
uma segunda revolução industrial, mas

568
00:35:38,720 --> 00:35:41,000
é algo quase nessa escala.

569
00:35:41,000 --> 00:35:43,691
E há uma enorme mudança acontecendo,

570
00:35:43,691 --> 00:35:47,980
basicamente porque nossa relação
com computadores tem mudado.

571
00:35:47,980 --> 00:35:53,920
Ao invés de programá-los,
agora nós os mostramos e eles descobrem.

572
00:35:53,920 --> 00:35:56,570
É uma maneira completamente
diferente de usar computadores, e

573
00:35:56,570 --> 00:36:01,210
departamentos de computação científica são construídos
ao redor de ideias de programação de computadores

574
00:36:01,210 --> 00:36:03,480
e eles não entendem esse tipo de,

575
00:36:05,000 --> 00:36:09,330
essa mostra de que computadores
serão tão grandes quanto programação de computadores.

576
00:36:09,330 --> 00:36:13,940
Exceto se eles compreenderem que metade das
pessoas no departamento deveriam ser pessoas

577
00:36:13,940 --> 00:36:16,510
que conseguem que computadores façam
coisas ao mostrá-las.

578
00:36:16,510 --> 00:36:22,183
Então, meu departamento se recusa a reconhecer
que deva ter muitas

579
00:36:22,183 --> 00:36:24,790
e muitas pessoas fazendo isso.

580
00:36:24,790 --> 00:36:28,730
Elas pensam que eles têm umas duas,
talvez um pouco mais, mas não tantas.

581
00:36:31,260 --> 00:36:32,452
E nesta situação,

582
00:36:32,452 --> 00:36:36,510
você tem que se lembrar das grandes empresas
que proporcionam muito treinamento.

583
00:36:36,510 --> 00:36:40,335
Então, a Google agora está treinando as pessoas,
chamamos de residência de cérebros.

584
00:36:40,335 --> 00:36:43,792
Eu suspeito que as universidades
eventualmente irão alcançar.

585
00:36:43,792 --> 00:36:48,360
— Entendi, certo, de fato, talvez muitos
alunos tenham descoberto isso.

586
00:36:48,360 --> 00:36:53,131
Dos muitos dos 50 melhores programas de PhD,
mais da metade dos candidatos estão, na verdade,

587
00:36:53,131 --> 00:36:57,079
querendo trabalhar mostrando,
ao invés de programando.

588
00:36:57,079 --> 00:37:00,720
Sim, legal, sim, de fato,
dar crédito onde é devido,

589
00:37:00,720 --> 00:37:04,930
assim como deeplearning.ai está criando
uma especialização em aprendizagem profunda.

590
00:37:04,930 --> 00:37:09,239
Até onde eu sei, o primeiro CMOA ‑[Curso Massivo Online Aberto]‑ deles
em aprendizagem profunda foi, na verdade o seu

591
00:37:09,239 --> 00:37:11,752
no Coursera, em 2012, também.

592
00:37:12,828 --> 00:37:14,430
E, de alguma forma estranha,

593
00:37:14,430 --> 00:37:18,900
foi quando você publicou pela primeira vez o algoritmo
RMS Prop, que também é um rascunho.

594
00:37:20,240 --> 00:37:25,910
— Certo, sim, bem, como você sabe, isso aconteceu
porque você me convidou para fazer o CMOA.

595
00:37:25,910 --> 00:37:30,239
E, então, quando eu estava bem duvidoso em
fazer, você continuou me incentivando a fazer, então,

596
00:37:30,239 --> 00:37:34,340
isso foi muito bom que eu fiz,
embora tenha dado muito trabalho.

597
00:37:34,340 --> 00:37:37,409
— Sim, e obrigado por fazer isso,
eu me lembro de você reclamando comigo

598
00:37:37,409 --> 00:37:38,351
do quão trabalhoso foi.

599
00:37:38,351 --> 00:37:42,413
E você ficando fora até tarde da noite,
mas eu acho que muito alunos foram

600
00:37:42,413 --> 00:37:47,330
beneficiados pelo seu primeiro CMOA, então,
sou muito grato a você por isso.

601
00:37:47,330 --> 00:37:49,260
— Isso é bom, sim,
— Sim, ao longo dos anos,

602
00:37:49,260 --> 00:37:53,290
tenho visto você envolvido em debates
sobre paradigmas para AI, e

603
00:37:53,290 --> 00:37:57,030
se tem havido uma mudança de paradigmas para
AI.

604
00:37:57,030 --> 00:37:59,984
Quais são seus,
você pode dividir seus pensamentos sobre isso?

605
00:37:59,984 --> 00:38:05,157
— Sim, alegremente, então, eu acho que nos
primeiros dias, de volta aos anos 50,

606
00:38:05,157 --> 00:38:10,335
pessoas como von Neumann e Turing
não acreditavam em AI simbólica,

607
00:38:10,335 --> 00:38:14,220
eles estavam muito mais inspirados pelo cérebro.

608
00:38:14,220 --> 00:38:20,127
Infelizmente, ambos morreram muito
jovens, e a voz deles não foi ouvida.

609
00:38:20,127 --> 00:38:21,806
E nos primeiros dias da AI,

610
00:38:21,806 --> 00:38:26,259
as pessoas estavam completamente convencidas de que
as representações que você precisa para

611
00:38:26,259 --> 00:38:30,500
a inteligência eram expressões
simbólicas de algum tipo.

612
00:38:30,500 --> 00:38:35,509
Um tipo de lógica limpa, onde você poderia
fazer coisas não-monotônicas e não muito

613
00:38:35,509 --> 00:38:41,143
lógicas, mas algo parecido com a lógica, e que
a essência da inteligência seja o raciocínio.

614
00:38:41,143 --> 00:38:45,662
O que aconteceu agora é que
há uma visão completamente diferente,

615
00:38:45,662 --> 00:38:50,984
a de que um pensamento é apenas
um grande vetor de atividade neural,

616
00:38:50,984 --> 00:38:55,200
então, contraste isso com a de um pensamento
ser uma expressão simbólica.

617
00:38:55,200 --> 00:38:59,087
E eu acho que as pessoas que pensam que
pensamentos são apenas expressões simbólicas

618
00:38:59,087 --> 00:39:00,140
cometem um erro enorme.

619
00:39:01,210 --> 00:39:07,030
O que entra é uma sequência de palavras, e
o que sai é uma sequência de palavras.

620
00:39:08,140 --> 00:39:12,580
E por causa disso, sequências de palavras
são a maneira óbvia de representar as coisas.

621
00:39:12,580 --> 00:39:15,710
Então, eles pensaram que o que deveria estar
no meio era uma sequência de palavras,

622
00:39:15,710 --> 00:39:18,360
ou algo como uma sequência de palavras.

623
00:39:18,360 --> 00:39:21,310
E eu acho que o que está no meio é
não se parece nada com uma sequência de palavras.

624
00:39:21,310 --> 00:39:26,060
Eu acho que a ideia de que pensamentos devem ser,
e em algumas linguagens são, tão bobos quanto

625
00:39:26,060 --> 00:39:30,980
a ideia de que compreender
o layout de uma cena espacial

626
00:39:30,980 --> 00:39:34,280
deve ser em pixels, pixels entram.

627
00:39:34,280 --> 00:39:37,930
E se pudéssemos, se tivéssemos uma impressora de
matriz de pontos anexada a nós,

628
00:39:37,930 --> 00:39:41,929
então os pixels sairiam, mas
o que está no meio não são pixels.

629
00:39:43,210 --> 00:39:46,620
E, então, penso que os pensamentos são apenas
esses ótimos vetores grandes,

630
00:39:46,620 --> 00:39:48,460
e que os vetores grandes têm poderes causais.

631
00:39:48,460 --> 00:39:50,490
Eles causam outros vetores grandes,

632
00:39:50,490 --> 00:39:56,100
e isso é completamente diferente da visão padrão de AI
que os pensamentos são expressões simbólicas.

633
00:39:56,100 --> 00:39:56,700
— Entendi, bom,

634
00:39:57,740 --> 00:40:01,560
eu acho que a AI está certamente chegando
a este novo ponto de vista desses dias.

635
00:40:01,560 --> 00:40:02,660
— Algo do tipo,

636
00:40:02,660 --> 00:40:08,230
eu acho que muitas pessoas em AI ainda pensam
que pensamentos têm que ser expressões simbólicas.

637
00:40:08,230 --> 00:40:09,780
— Muito obrigado por
conceder esta entrevista.

638
00:40:09,780 --> 00:40:12,970
Foi fascinante ouvir como a aprendizagem
profunda tem evoluído ao longo dos anos,

639
00:40:12,970 --> 00:40:17,680
assim como você ainda está ajudando a direcioná-la
para o futuro, então, obrigado Jeff.

640
00:40:17,680 --> 00:40:19,038
— Bem, obrigado por me dar essa oportunidade.
[Tradução: Simone Tateishi | Revião: Carlos Lage.]

641
00:40:19,038 --> 00:40:20,147
>> Obrigado.