1
00:00:00,000 --> 00:00:02,389
欢迎大家来学习第四周的课程

2
00:00:02,389 --> 00:00:06,354
在之前的课程中 我们学习了单隐藏层神经网络中的

3
00:00:06,354 --> 00:00:10,807
前向传播和反向传播 逻辑回归

4
00:00:10,807 --> 00:00:13,064
我们还学习了向量化

5
00:00:13,064 --> 00:00:15,936
也了解了对权重参数进行随机初始化的重要性

6
00:00:15,936 --> 00:00:19,417
如果你完成了前几周的作业 那你应该已经

7
00:00:19,417 --> 00:00:21,378
实现并亲眼看到了某些理念实现后的效果

8
00:00:21,378 --> 00:00:21,977
迄今为止

9
00:00:21,977 --> 00:00:26,718
我们已经学过实现深度神经网络需要的大部分知识

10
00:00:26,718 --> 00:00:30,453
本周的任务是 把这些理念组合起来

11
00:00:30,453 --> 00:00:33,669
并利用它们实现你自己的深度神经网络

12
00:00:33,669 --> 00:00:36,406
完成本周的习题所需要的时间会比较长

13
00:00:36,406 --> 00:00:39,457
可能要多花一点心思 我会压缩

14
00:00:39,457 --> 00:00:43,784
本周的视频课程 让你观看视频的时间稍微短一些

15
00:00:43,784 --> 00:00:48,107
这样你就能有更多的时间去完成编程作业

16
00:00:48,107 --> 00:00:52,537
我希望这个作业可以加深你对神经网络的理解<br />并能得到一个让你引以为豪的深度学习模型

17
00:00:52,537 --> 00:00:55,576
到底什么是深度神经网络呢?

18
00:00:55,576 --> 00:00:59,225
我们已经见过这个逻辑回归的结构图了

19
00:00:59,225 --> 00:01:03,439
也已经学习过单隐藏层的神经网络结构

20
00:01:03,439 --> 00:01:07,925
先来看一个例子 这里有一个双隐层的神经网络模型

21
00:01:07,925 --> 00:01:10,661
还有一个含有5个隐藏层的神经网络模型

22
00:01:10,661 --> 00:01:15,744
我们一般会说逻辑回归 是非常“浅”的模型

23
00:01:15,744 --> 00:01:19,947
而右下角这个模型的层数更深

24
00:01:19,947 --> 00:01:23,585
浅和深 只是程度的区别

25
00:01:23,585 --> 00:01:26,952
这是单隐藏层的神经网络

26
00:01:26,952 --> 00:01:30,052
也就是2层神经网络

27
00:01:30,052 --> 00:01:34,880
要记住 当我们计算神经网络的层数时 不包括输入层

28
00:01:34,880 --> 00:01:38,076
我们只计算隐藏层和输出层

29
00:01:38,076 --> 00:01:42,849
所以这是2层神经网络 它仍然比较浅

30
00:01:42,849 --> 00:01:45,961
但比逻辑回归要深一点

31
00:01:45,961 --> 00:01:50,086
从技术上来说 逻辑回归是单层神经网络

32
00:01:50,086 --> 00:01:53,536
但是过去几年 在人工智能领域

33
00:01:53,536 --> 00:01:58,561
机器学习社区 认识到有一些函数

34
00:01:58,561 --> 00:02:03,590
只有很深的神经网络才能学习<br />而比较浅的网络模型无法做到这点

35
00:02:03,590 --> 00:02:08,119
虽然对于任何特定的问题来说 可能很难事先得知

36
00:02:08,119 --> 00:02:10,163
你需要多深的网络

37
00:02:10,163 --> 00:02:14,305
所以一般我们会先尝试逻辑回归 然后再尝试

38
00:02:14,305 --> 00:02:19,200
一个 两个隐藏层 可以把隐藏层的数量作为

39
00:02:19,200 --> 00:02:22,739
另一个超参数 你可以尝试很多不同的值

40
00:02:22,739 --> 00:02:27,515
然后通过交叉验证或者 开发集进行评估

41
00:02:27,515 --> 00:02:29,447
之后我们会深入讨论

42
00:02:29,447 --> 00:02:33,998
现在我们来看看 描述深度网络使用的符号

43
00:02:33,998 --> 00:02:39,147
这是一 二 三 四 四层神经网络

44
00:02:40,974 --> 00:02:45,729
有三个隐藏层 隐藏层中

45
00:02:45,729 --> 00:02:50,842
的单元数量是5 5 3 然后有一个输出单元

46
00:02:50,842 --> 00:02:52,731
我们使用大写的L来表示

47
00:02:52,731 --> 00:02:56,591
神经网络中的层数

48
00:02:56,591 --> 00:03:03,881
这里 L = 4 即层数为4

49
00:03:03,881 --> 00:03:11,880
我们用n上标小写l来表示

50
00:03:11,880 --> 00:03:17,101
第l层上的单元数

51
00:03:17,101 --> 00:03:22,501
让我们来标上序号 输入层是第0层

52
00:03:22,501 --> 00:03:28,950
这是第1层 这是第2层 这是第3层 这是第4层

53
00:03:28,950 --> 00:03:33,822
然后我们可以得出 n[1] 的标记是这样的

54
00:03:33,822 --> 00:03:39,529
第一个隐藏层这儿 n[1]=5 因为在这儿有5个隐藏单元

55
00:03:39,529 --> 00:03:43,623
对于这个 我们可以写成n[2]

56
00:03:43,623 --> 00:03:48,810
代表第二个隐藏层中的单元数量

57
00:03:48,810 --> 00:03:53,315
也等于5 n[3] = 3

58
00:03:53,315 --> 00:03:59,459
n[4] = n[L] 因为大写的L等于4

59
00:03:59,459 --> 00:04:04,101
输出层的单元数为1

60
00:04:04,101 --> 00:04:08,878
我们再来看看输入层

61
00:04:08,878 --> 00:04:13,003
输入层n[0] = nx = 3

62
00:04:13,003 --> 00:04:17,879
这就是我们用来描述不同层中单元数

63
00:04:17,879 --> 00:04:18,463
的符号

64
00:04:18,463 --> 00:04:23,913
对于每一层l 我们还会使用

65
00:04:23,913 --> 00:04:30,196
a[l]代表第l层中的激活函数

66
00:04:30,196 --> 00:04:34,669
之后我们会看到 在前向传播的场景中

67
00:04:34,669 --> 00:04:40,791
你最后要计算的 激活函数g(z[l])的结果a[l]

68
00:04:40,791 --> 00:04:46,440
我们也可以用层数l来对激活函数进行标注

69
00:04:46,440 --> 00:04:51,736
另外 我们用W[l]来表示

70
00:04:51,736 --> 00:04:55,973
计算第l层的中间值z[l]时使用的权重

71
00:04:55,973 --> 00:05:00,714
类似的 b[l]表示计算z[l]时用到的偏置参数

72
00:05:00,714 --> 00:05:07,114
最后 我们来总结一下所使用的符号 我们把输入特征称为x

73
00:05:07,114 --> 00:05:12,215
而x同时也是第零层的激活函数 所以a[0] = x

74
00:05:12,215 --> 00:05:17,133
最后一层的激活函数 a[L] = y帽

75
00:05:17,133 --> 00:05:25,275
所以a[L]等于神经网络的预测输出<br />也就是等于预测值y帽

76
00:05:25,275 --> 00:05:28,200
现在我们已经知道了 神经网络看起来是什么样的

77
00:05:28,200 --> 00:05:32,427
以及在构建深度神经网络时会使用的各种符号

78
00:05:32,427 --> 00:05:36,457
我知道在本节课中我们已经 介绍了很多这样的符号

79
00:05:36,457 --> 00:05:40,916
如果你不经意间忘记了某个符号的含义
不用担心 我们在课程网站上公布了符号列表

80
00:05:40,916 --> 00:05:45,089
你可以在那儿找到这些符号的定义

81
00:05:45,089 --> 00:05:48,958
后面我会跟大家讲讲 在这种类型的网络中

82
00:05:48,958 --> 00:05:49,620
前向传播究竟是什么样的

83
00:05:49,620 --> 00:05:51,019
我们下节课见
翻译 | 审阅：Cousera Global Translator Community