我們已經談過向量化讓您明顯的加快您程式的速度 在這段影片中, 我們將談您如何可以向量化 羅吉斯迴歸分析的建置 所以用他們來處理整個訓練集 也就是建置一個單一循環的梯度下降對於 整個訓練集甚至不需要一個迴圈 我對這項技術非常興奮 當我們以後談論到有關於神經網路甚至 不用ㄧ個迴圈 讓我們開始吧, 首先先檢驗羅吉斯迴歸分析的正向傳播步驟 如果您有 m 個訓練例子 為了要預測的第一個例子 您需要計算 計算 z 我用這個熟悉的方法 計算這個啟動的 您計算 y-hat 在第一個例子 然後做第二個訓練例子地預測 您需要計算這個 然後做第三個例子的預測 您需要計算這個, 等等 而您也許需要這樣做 m 次 如果您有 m 個例子 實際上為了做正向傳播步驟 也就是計算這些 m 訓練例子的預測 有一種方式 不需要明顯的迴圈 我們來看怎麼做 首先, 記得我們定義矩陣大寫 X 為訓練輸入 不同的欄疊在一起像這樣 所以這是一個矩陣 是一個 nx 乘 m 矩陣 我這樣寫成 Python numpy shpae 的格式 這只是說 X 是 nx 乘 m 維矩陣 現在第一件事是我想展示您如何計算z1, z2, z3等等 所有在一個步驟中 實際上只用一行程式 我將建立一個 1 乘 m 矩陣實際是一個行向量我將計算z1 z2 ...等等 一直到zm, 一次解決 實際上這個可以表示成 W轉置 大寫矩陣X 加這個向量 B B, ... 等等 而這個 B, B. B, B 東西是一個 1乘m 向量或者說 1乘m 矩陣有 m 維度行向量 依照您對於矩陣乘法的熟悉程度 您也許把 W 轉置 X1, X2,... 到 Xm W轉置當作一個行向量 這個W轉置會是一個行向量像這樣 第一個項目會是計算W轉置X1, W轉置X2等等... W轉置Xm, 然後加第二項 B B, B, ..等等 您在每個項目最後加上B 您最終得到另一個 1 乘m 向量 而第一個項目 第二個項目等等 總共 m 個元素 而如果您對應上面的定義 第一個元素剛好是 z1 第二個元素剛好是 z2 等等 就像我們得到大寫X 您拿您的訓練例子 將他們一個疊一個, 水平疊起來一樣 我將定義大寫Z 像 您用這個小寫 z 將它們水平疊在一起 就像您水平疊這些小寫 x 相對應到每個不同的訓練例子 您得到大寫 X 一樣的方式當您拿這些小寫 z 水平疊在一起 您得到這個變數大寫 Z 而實際上, 為了建置這個 numpy 的指令是大寫 Z 等於 np.dot(W.T, X) + B 就是 W 轉置X 加 B Python 有一個微妙的地方 也就是 b 是一個實數或者您可以說 1乘1矩陣 就是一個平常的實數 但, 當您加這個向量跟這個實數 Python 自動拿這個實數 b 展成一個 1乘m 行向量 如果這個運算看起來有點奧妙 這是稱為 Python 廣播 (broadcasting) 您現在先不用擔心 我們會在下ㄧ段影片談到 但請記住只用一行程式, 就是這一行程式 您可以計算大寫 Z 而大寫 Z 將是一個 1乘m 的矩陣包含所有的小寫 z 小寫 z1 直到小寫 zm 所以這是 Z, 那 A 呢? 我們接著想要做的是 找一個方式來計算 A1 A2...到 Am 在同一時間 就像堆疊小寫 x 成為 大寫 X, 堆疊小寫 z 成為大寫 Z 一樣 堆疊小寫 a 將會造成新的變數 我們將其定義為大寫A 而在程式作業中 您會看到如何建立一個向量值S型函數 所以用S型函數 輸入這個大寫 Z 為變數會非常有效率輸出大寫 A 您會在程式作業中看到所有細節 總結一下 我們在這張投影片看到的是與其使用迴圈來經過 m 個訓練例子計算小寫 z 跟小寫 a 一次一個, 您可以使用這一行程式 來同時計算所有這些 z 而用這一行程式 使用適當的建置 小寫 Sigma 來同時計算所有小寫 a 所以這是您如何同時來建立 向量化建置的 正向傳播於所有 m 個訓練例子 總結一下, 您剛看過您如何使用 很有效的向量化來計算所有的啟動 同時計算小寫 a 接下來, 您也可以很有效率使用向量化 來計算反向傳播 跟計算導數 我們在下一段影片看如何做到