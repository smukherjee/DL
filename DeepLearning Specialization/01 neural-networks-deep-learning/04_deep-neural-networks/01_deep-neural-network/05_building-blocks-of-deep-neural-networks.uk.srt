1
00:00:00,000 --> 00:00:02,705
В попередніх відео цього тижня, як і

2
00:00:02,705 --> 00:00:05,735
у відео кількох попередніх тижнів,

3
00:00:05,735 --> 00:00:08,075
ти вже бачив/ла основні структурні блоки

4
00:00:08,075 --> 00:00:10,456
прямого і зворотного поширення -

5
00:00:10,456 --> 00:00:14,090
ключові компоненти, які потрібні 
для реалізації глибокої нейронної мережі (НМ).

6
00:00:14,090 --> 00:00:18,303
Давай подивимось як об'єднати ці блоки, щоб побудувати глибоку НМ.

7
00:00:18,303 --> 00:00:20,333
Ось НМ з кількома шарами.

8
00:00:20,333 --> 00:00:27,290
Давай виокремимо 1 шар і сфокусуємось лише на 
обчисленнях, що в ному проходять.

9
00:00:27,290 --> 00:00:33,410
Тож для шару l ми маємо деякі параметри w[l] і b[l].

10
00:00:33,410 --> 00:00:35,645
І для прямого поширення

11
00:00:35,645 --> 00:00:42,180
ми введемо активатори a[l-1]

12
00:00:42,180 --> 00:00:48,895
з попереднього шару і виведемо a[l].

13
00:00:48,895 --> 00:01:00,105
Ось як ми це раніше обчислювали: z[l]=w[l]a[l-1]+b[l],

14
00:01:00,105 --> 00:01:08,025
потім a[l]=g(z[l]). Правильно?

15
00:01:08,025 --> 00:01:12,990
Тож отак ми переходимо від вхідного a[l-1] до вихідного a[l].

16
00:01:12,990 --> 00:01:15,330
Виявляється, що для подальшого використання

17
00:01:15,330 --> 00:01:20,640
буде корисно кешувати значення z[l]. [cache z[l]]

18
00:01:20,640 --> 00:01:24,060
Тож давай я включу також і кеш, бо

19
00:01:24,060 --> 00:01:28,170
зберігання значення z[l] буде корисним для кроку зворотного

20
00:01:28,170 --> 00:01:31,290
поширення пізніше. І далі

21
00:01:31,290 --> 00:01:32,745
для зворотного кроку

22
00:01:32,745 --> 00:01:34,785
(для кроку зворотного поширення), знову ж,

23
00:01:34,785 --> 00:01:37,390
фокусуємось на обчисленнях l-ого шару,

24
00:01:37,390 --> 00:01:41,635
ми збираємось реалізувати функцію, яка отримує

25
00:01:41,635 --> 00:01:51,840
da[l] і видає da[l-1].

26
00:01:51,840 --> 00:01:53,650
Уточню, що

27
00:01:53,650 --> 00:01:56,050
що на вході маємо як da[l],

28
00:01:56,050 --> 00:01:59,130
так і кеш.

29
00:01:59,130 --> 00:02:04,040
Тож нам доступно значення z[l], яке ми обчислили.

30
00:02:04,040 --> 00:02:06,755
І далі разом з виведенням da[l-1]

31
00:02:06,755 --> 00:02:09,940
ми можемо вивести і градієнти,

32
00:02:09,940 --> 00:02:14,025
щоб реалізувати Градієнтний спуск для навчання.

33
00:02:14,025 --> 00:02:19,772
Тож це - базова структура реалізації як прямого кроку,

34
00:02:19,772 --> 00:02:22,680
який ми називаємо прямою функцією, так і зворотного кроку,

35
00:02:22,680 --> 00:02:24,970
яку ми мали б називати зворотною функцією.

36
00:02:24,970 --> 00:02:27,535
Тож резюмуємо. В l-ому шарі

37
00:02:27,535 --> 00:02:32,650
ми матимемо прямий крок або пряме поширення, 
або пряму функцію,

38
00:02:32,650 --> 00:02:38,986
що отримує a[l-1] і видає a[l].

39
00:02:38,986 --> 00:02:42,070
А щоб здійснити обчислення,

40
00:02:42,070 --> 00:02:45,650
нам потрібно використати w[l] і b[l] і також

41
00:02:45,650 --> 00:02:54,185
вивести і кеш, який містить z[l]. А далі зворотна функція,

42
00:02:54,185 --> 00:02:56,357
крок зворотного поширення,

43
00:02:56,357 --> 00:03:01,650
буде іншою функцією, яка отримує

44
00:03:01,650 --> 00:03:08,065
da[l] і видає da[l-1].

45
00:03:08,065 --> 00:03:13,820
Тобто каже нам, маючи похідні від активаторів

46
00:03:13,820 --> 00:03:16,950
(тобто da[l]), які ці похідні.

47
00:03:16,950 --> 00:03:20,400
Тобто наскільки ми хочемо щоб змінилось a[l-1].

48
00:03:20,400 --> 00:03:24,590
Тобто обчислює похідні від активаторів з попереднього шару.

49
00:03:24,590 --> 00:03:29,870
І в цьому квадраті ми повинні використати w[l] і b[l].

50
00:03:29,870 --> 00:03:31,200
І, виявляється, що по дорозі

51
00:03:31,200 --> 00:03:34,144
ми обчислили dz[l].

52
00:03:34,144 --> 00:03:36,400
І потім цей квадрат,

53
00:03:36,400 --> 00:03:43,515
ця зворотна функція, може також вивести dw[l] і db[l].

54
00:03:43,515 --> 00:03:47,740
Інколи я використовую червоний колір, 
щоб позначити зворотні обчислення.

55
00:03:47,740 --> 00:03:51,668
Тож якщо хочеш, ми можемо їх намалювати червоним.

56
00:03:51,668 --> 00:03:55,305
Тож якщо ми можемо реалізувати обидві ці функції,

57
00:03:55,305 --> 00:03:59,390
то базові обчислення НМ будуть наступними.

58
00:03:59,390 --> 00:04:02,453
Ми беремо вхідні ознаки a[0]

59
00:04:02,453 --> 00:04:07,710
і даємо їх на вхід. А це обчислить активатори 1-ого шару.

60
00:04:07,710 --> 00:04:09,624
Позначимо їх a[1].

61
00:04:09,624 --> 00:04:14,535
Щоб це зробити нам потрібно w[1] і b[1].

62
00:04:14,535 --> 00:04:21,090
Потім ми також кешуємо z[1].

63
00:04:21,090 --> 00:04:22,475
Зробивши це,

64
00:04:22,475 --> 00:04:25,290
ми даємо це на вхід 2-ого шару.

65
00:04:25,290 --> 00:04:28,026
Використовуємо w[2] і b[2].

66
00:04:28,026 --> 00:04:32,355
Обчислюємо активатори наступного шару a[2]

67
00:04:32,355 --> 00:04:36,475
і т.д. аж поки

68
00:04:36,475 --> 00:04:40,065
не дійдемо до a[велике L],

69
00:04:40,065 --> 00:04:42,655
що =ŷ.

70
00:04:42,655 --> 00:04:51,595
І по дорозі кешуємо всі значення z.

71
00:04:51,595 --> 00:04:55,161
Тож це крок прямого поширення.

72
00:04:55,161 --> 00:04:57,585
Тепер для кроку зворотного поширення

73
00:04:57,585 --> 00:05:03,220
ми побудуємо зворотну послідовність

74
00:05:03,220 --> 00:05:12,200
ітерацій, в яких ми рухаємось назад 
і обчислюємо градієнти. Ось так.

75
00:05:12,200 --> 00:05:17,350
Тож тут на вході ми маємо da[L].

76
00:05:17,350 --> 00:05:21,674
І цей квадрат дає нам da[L-1]

77
00:05:21,674 --> 00:05:30,424
і т.д. доки ми не отримаємо da[2], da[1].

78
00:05:30,424 --> 00:05:35,500
Ми можемо, насправді, піти далі і обчислити da[0],

79
00:05:35,500 --> 00:05:40,250
але це похідна від вхідних ознак, яка нам непотрібна.

80
00:05:40,250 --> 00:05:46,237
Принаймі для тренування коефіцієнтів цієї контрольованої НМ.

81
00:05:46,237 --> 00:05:48,160
Тож ми можемо просто зупинитись тут.

82
00:05:48,160 --> 00:05:54,365
По дорозі зворотне поширення теж виводить dw[L], db[L].

83
00:05:54,365 --> 00:05:58,800
Тут використовуються параметри w[l] і b[l].

84
00:05:58,800 --> 00:06:04,810
Тут виведуться dw[3], db[3].

85
00:06:04,810 --> 00:06:09,238
І так далі.

86
00:06:09,238 --> 00:06:16,660
І ми обчислимо всі похідні, які нам потрібні.

87
00:06:16,660 --> 00:06:21,235
Тепер просто трохи заповнимо ці блоки.

88
00:06:21,235 --> 00:06:28,500
Ці квадрати будуть також використовувати ці параметри.

89
00:06:28,500 --> 00:06:34,560
І, виявляється, ми ще побачимо пізніше, 
що всередині цих квадратів

90
00:06:34,560 --> 00:06:37,460
обчислюються також і dz.

91
00:06:37,460 --> 00:06:43,305
Тож ось з чого складається одна ітерація тренування НМ: 
починається з a[0], що є x,

92
00:06:43,305 --> 00:06:46,725
проходить через пряме поширення ось так,

93
00:06:46,725 --> 00:06:50,715
обчислює ŷ, а далі використовує його, щоб обчислити оце,

94
00:06:50,715 --> 00:06:55,680
а потім - зворотне поширення.

95
00:06:55,680 --> 00:06:59,880
Тепер ми маємо всі ці рівняння похідних.

96
00:06:59,880 --> 00:07:08,830
w буде скориговано як w мінус навчальний темп 
помножений на dw в кожному шарі.

97
00:07:08,830 --> 00:07:12,570
І аналогічно для b.

98
00:07:12,570 --> 00:07:17,757
І тепер обчислюється зворотне поширення і 
обчислюються всі ці похідні.

99
00:07:17,757 --> 00:07:21,845
Тож це - одна ітерація Градієнтного спуску для НМ.

100
00:07:21,845 --> 00:07:25,125
Перед тим як піти далі - одна маленька деталь.

101
00:07:25,125 --> 00:07:29,665
Згідно концепції, буде дуже корисно 
розуміти під кешем - зберігання

102
00:07:29,665 --> 00:07:34,361
значення z для зворотної функції, але

103
00:07:34,361 --> 00:07:35,635
при реалізації

104
00:07:35,635 --> 00:07:38,496
(ти побачиш це в програмувальній вправі)

105
00:07:38,496 --> 00:07:40,840
можна побачити, що кеш може бути зручним способом

106
00:07:40,840 --> 00:07:43,730
отримати також і значення параметрів w[1] і b[1]

107
00:07:43,730 --> 00:07:46,648
всередині зворотної функції.

108
00:07:46,648 --> 00:07:47,868
В програмувальній вправі

109
00:07:47,868 --> 00:07:49,795
ти зберігатимеш як кеш z,

110
00:07:49,795 --> 00:07:52,239
так і w, і b.

111
00:07:52,239 --> 00:07:57,715
Зберігатимуться z[2], w[2], b[2].

112
00:07:57,715 --> 00:07:59,860
А з реалізаційної точки зору,

113
00:07:59,860 --> 00:08:04,120
я бачу в цьому ще й зручний спосіб отримання копій параметрів

114
00:08:04,120 --> 00:08:08,975
там, де вони використовуватимуться пізніше 
при реалізації зворотного поширення.

115
00:08:08,975 --> 00:08:15,446
Тож це просто підказка для реалізації, яку ти побачиш, 
коли виконуватимеш програмувальне завдання.

116
00:08:15,446 --> 00:08:20,061
Тож тепер ти ознайомився/лась з основними 
структурними блоками для реалізації глибокої НМ.

117
00:08:20,061 --> 00:08:21,640
В кожному шарі є крок прямого поширення

118
00:08:21,640 --> 00:08:24,260
і відповідний крок зворотного поширення,

119
00:08:24,260 --> 00:08:27,675
і кеш для передачі інформації від одного до іншого.

120
00:08:27,675 --> 00:08:28,930
У наступному відео

121
00:08:28,930 --> 00:08:32,250
ми поговоримо як практично реалізувати ці структурні блоки.

122
00:08:32,250 --> 00:08:33,500
Давай перейдемо до наступного відео.