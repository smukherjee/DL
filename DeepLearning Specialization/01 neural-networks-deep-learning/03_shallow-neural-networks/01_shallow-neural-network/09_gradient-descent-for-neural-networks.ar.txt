حسنًا، أعتقد أن هذا الفيديو سيكون مثيرًا. في هذا الفيديو، سترون كيفية تنفيذ انحدار التدرج للشبكة العصبية
بطبقة مخفية واحدة لديكم. في هذا الفيديو، سأمنحكم فقط
المعادلات التي تحتاجون إلى تنفيذها لتفعيل الانتشار الخلفي
أو انحدار التدرج، وبعد ذلك في الفيديو الذي يليه، سأوضح لكم الأسباب التي تجعل هذه المعادلات بالذات هي
المعادلات الدقيقة، والمعادلات الصحيحة لحساب
التدرجات التي تحتاجون إليها لشبكتكم العصبية. لذا فإن الشبكة العصبية، التي تحتوي على طبقة مخفية واحدة حتى الآن، ستحتوي على المتغيرات W1 وB1 وW2 وB2. لذلك، كتذكير، إذا كان لديكم NX أو بدلاً من ذلك ميزات الإدخال N0، والوحدات المخفية N1، ووحدات الإخراج N2 في الأمثلة لدينا. حتى الآن لدي فقط N2 يساوي 1، وستكون المصفوفة W1 هي N1، N0. B1 ستكون المتجه ذو الأبعاد N1، لذا، يمكننا كتابة ذلك كـ N1
في مصفوفة أحادية الأبعاد، ومتجه العمود بالفعل. ستكون أبعاد W2 هي N2، N1، وسيكون بُعد B2 هو N2، 1. حسنًا، رأينا فقط حتى الآن
أمثلة يكون خلالها N2 يساوي 1، حيث تكون لديكم وحدة مخفية واحدة فقط. لذا، لديكم أيضًا دالة تكلفة
لشبكة عصبية. حتى الآن، سأفترض
أنكم تقومون بالتصنيف الثنائي. وفي تلك الحالة، تكلفة المتغيرات الخاصة بكم
كما يلي ستكون 1 على M في متوسط دالة الخسارة هذه. إذًا، L هنا هي الخسارة عندما
تتوقع الشبكة العصبية لديكم Y hat، حسنًا. هذا حقًا A2 عندما تكون
تسمية التدرج تساوي Y. إذا كنتم تقومون بإجراء التصنيف الثنائي، فإن دالة الخسارة ستكون تحديدًا ما
تستخدمونه للانحدار اللوجستي في وقت سابق. لذا، لتدريب متغيرات الخوارزمية لديكم، فإنكم تحتاجون إلى تنفيذ انحدار التدرج. عند تدريب شبكة عصبية، يكون من المهم تهيئة المتغيرات
عشوائيًا بدلاً من جميع الأصفار. سنرى لاحقًا سبب هذه الحالة، لكن بعد تهيئة
المتغير إلى شيء ما، كل حلقة أو انحدار تدرج
مع التوقعات المحسوبة. لذا، فأنتم تحسبون Y hat I بشكل أساسي، لـ I يساوي 1 إلى M، مثلاً. ثم، تحتاجون إلى حساب المشتقات. وتحتاجون إلى حساب DW1، وهذا هو المشتقة لدالة التكلفة
مع مراعاة المتغير W1، يمكنكم حساب متغير آخر، وسأطلق عليه DB1، وهو مشتقة أو انحدار
دالة التكلفة لديكم مع مراعاة المتغير B1 وما إلى ذلك. وبالمثل للمتغيرات الأخرى W2 وB2. وأخيرًا، تحديث انحدار التدرج
سيكون عبارة عن تحديث W1 كـ W1 ناقص ألفا. معدل التعلم مضروبًا في D, W1. سيتم تحديث B1 إلى B1 ناقص معدل التعلم مضروبًا في DB1، وبالمثل لـ W2 وB2. أحيانًا، أستخدم نقطتين يساوي
وأحيانًا يساوي، أي رمز يعمل جيدًا. لذا، سيكون هذا
تكرارًا للانحدار التدريجي، وتكررون ذلك مرات عديدة حتى تبدو المتغيرات
وكأنها تتقارب. لذا، في الفيديوهات السابقة، تحدثنا حول كيفية
حساب التوقعات، كيفية حساب المخرجات، ورأينا كيفية القيام بذلك
من خلال طريقة تنطوي على التحويل إلى متجه أيضًا. لذا، فإن الأمر الرئيسي هو معرفة كيفية حساب
مصطلحات المشتقات الجزئية هذه، DW1 وDB1 وكذلك
المشتقات DW2 وDB2. لذا ما أريد القيام به هو إعطاءكم المعادلات التي تحتاجون إليها
لحساب هذه المشتقات. وسأقوم بتأجيل ذلك إلى الفيديو التالي،
وهو فيديو اختياري للتعمق أكثر فيما يتعلق بكيفية
التوصل إلى هذه الصيغ. لذا، دعوني فقط
ألخص معادلات الانتشار الأمامي. لذا، لديكم Z1 يساوي W1X زائد B1، ثم A1 يساوي دالة التنشيط
في هذه الطبقة المطبقة حسب العناصر كـ Z1، ثم Z2 يساوي W2، A1 زائد V2، ثم أخيرًا، مثل كل المتغيرات المحولة إلى متجهات على مستوى مجموعتكم التدريبية، حسنًا؟ A2 يساوي G2 (Z2)‎. ومرة أخرى، إذا كنا نفترض الآن
أننا نقوم بالتصنيف الثنائي، فإن دالة التنشيط هذه،
يجب أن تكون الدالة السينية حقًا، نفس الشيء لهذه النهاية العصبية. فهذا هو الانتشار الأمامي أو الاتجاه من اليسار إلى اليمين للحساب لشبكتكم العصبية. بعد ذلك، دعونا نحسب المشتقات. إذًا، هذه هي خطوة الانتشار الخلفي، ثم أحسب DZ2 يساوي A2
ناقص تدرج Y، وفقط كتذكير، تكون كل هذه المتغيرات محولة إلى متجهات عبر الأمثلة. إذًا، المصفوفة Y هي هذه المصفوفة 1 على M التي تدرج كل أمثلة M
الخاصة بكم مكدسة أفقيًا. ثم تبين أن DW2 تساوي ذلك، وفي الواقع، هذه المعادلات الثلاث الأولى مماثلة تمامًا للانحدار التدريجي
المرتبط بالانحدار اللوجستي. X تساوي 1، فاصلة، keep dims=true. لنتعرف على تفاصيل بسيطة وهي أن np.sum هو أمر Python NumPy للجمع
على مستوى بُعد واحد بالمصفوفة. في هذه الحالة، يتم الجمع أفقيًا، وما تفعله keepdims هو
أنها تمنع بايثون من إخراج واحدة من هذه المصفوفات
المرحة المصنفة رقم واحد، حسنًا؟ حيث كانت الأبعاد N فاصلة. لذلك من خلال وجود keepdims = true، فإن هذا يضمن أن بايثون ستخرج
لـ DB متجهًا N, 1. في الواقع، أعتقد أن ذلك من الناحية التقنية سيكون N2, 1. في هذه الحالة، سيكون رقمًا تلو الآخر، وربما لا يكون مهمًا، ولكن لاحقًا، سنرى عندما يكون ذلك مهمًا فعلاً. لذا، فإن ما فعلناه حتى الآن
هو مماثل جدًا للانحدار اللوجستي. ولكن الآن بينما تواصلوا
تشغيل الانتشار الخلفي، ستحسبون ذلك، DZ2 مضروبًا في G1 '(Z1)‎. لذا فإن هذه الكمية G1' هي المشتقة الخاص بما إذا كانت دالة التنشيط
التي تستخدمونها للطبقة المخفية، ولطبقة الإخراج، أفترض أنكم تنفذون التصنيف
الثنائي من خلال الدالة السينية. لذلك، فإن هذا مضمن بالفعل
في هذه الصيغة لـ DZ2، وفي هذه المرة هذا المنتج يعتمد على العناصر. لذلك، ستكون هذه عبارة عن
مصفوفة N1, M، وسيكون هذا المشتقة التي تعتمد على العناصر أيضًا
عبارة عن مصفوفة N1, N، وبالتالي سيكون في هذه المرة هناك
ناتجًا يعتمد على العناصر للمصفوفتين. وأخيرًا، DW1 يساوي هذا، وDB1 يساوي هذا، وp.sum DZ1، المحور يساوي 1، keepdims = true. لذا، بينما في السابق ربما كانت
أهمية keepdims أقل إذا كان N2 يساوي 1. فإن النتيجة هي واحد تلو الآخر،
وهو مجرد عدد حقيقي. هنا، DB1 ستكون المتجه N1, 1، لذا، فأنتم تريدون بايثون، وتريدون Np.sons. سأضع شيئًا من هذا البعد بدلاً من
مصفوفة مرحة من التصنيف الأول لهذا البعد والذي قد ينتهي به الحال
من خلال الخطأ في بعض حسابات البيانات. وستكون الطريقة الأخرى هي
عدم الاحتفاظ بالمتغيرات، ولكن تتم إعادة تشكيل إخراج NP.sum
إلى هذا البُعد بوضوح، وهو ما تود أن تحتوي عليه DB. حسنًا، فقد كان هذا هو الانتشار الأمامي
في أربع معادلات على ما أعتقد، والانتشار الخلفي في ست معادلات على ما أعتقد. أعرف أنني قمت بتدوين هذه المعادلات، ولكن في الفيديو الاختياري الآخر، سوف نقوم بتوضيح بعض الأمور لكيفية اشتقاق المعادلات الست لخوارزمية
الانتشار الخلفي. لكم مطلق الحرية سواء أردتم مشاهدته أم لا. ولكن على أي حال،
إذا قمتم بتطبيق هذه الخوارزميات، فسيكون لديكم تطبيق صحيح
للانتشار الأمامي والانتشار الخلفي. وسيكون بإمكانكم حساب المشتقات
التي تحتاجون إليها لتطبيق انحدار التدرج، لتعلم متغيرات الشبكة العصبية الخاصة بكم. ويمكن تطبيق هذه الخوارزمية وجعلها ملائمة للعمل بدون
الفهم العميق لحساب التفاضل والتكامل. ويقوم بذلك الكثير من
ممارسي التعلم العميق الناجحين. ولكن، إذا أردتم، يمكنكم أيضًا مشاهدة الفيديو التالي، للتمتع بفهم أكبر للاشتقاق
الخاص بهذه المعادلات.