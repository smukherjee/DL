Щоб ефективно розробляти глибокі нейронні мережі (НМ), необхідно добре налаштовувати не лише параметри, але й гіперпараметри. Тож що таке гіперпараметри? Давай подивимось. Тож параметрами нашої моделі є W і b. Але є й інші речі, які потрібно повідомляти навчальному алгоритму, такі як навчальний темп α - тому що нам потрібно вказати α і це буде визначати як розвиваються параметри; а також кількість ітерацій Градієнтного спуску, який ми втілюємо. Наш навчальний алгоритм має й інші величини, які потрібно встановити. Такі як кількість прихованих шарів, яку ми позначаємо великою L, кількість прихованих вузлів - n[1], n[2] і т.д., також потрібно вибрати функцію активації - чи то ВЛВ [ReLU], чи гіперболічний тангенс, чи сигмоїду, чи щось інше для використання в прихованих шарах. І, отже, всі ці величини є величинами, які треба повідомити нашому навчальному алгоритму. Вони є параметрами, які контролюють остаточні параметри W і b, тож ми називаємо всі ці величини внизу гіперпараметрами. Через те, що ці величини - навчальний темп α, кількість ітерацій, кількість прихованих шарів та ін. - є параметрами, що контролюють W і b, ми називаємо їх гіперпараметрами. Тому, що ці гіперпараметри певним чином визначають остаточні значення параметрів W і b. Фактично, глибоке навчання (ГН) має багато різноманітних гіперпараметрів. Пізніше, в подальших курсах ми побачимо інші гіперпараметри. Такі як імпульсний коефіцієнт, розмір міні-серії, різноманітні форми параметрів регуляризації і т.д. Якщо ці поняття внизу поки що незрозумілі, не переймайся, ми про них поговоримо в 2-ому курсі. Через те, що ГН має так багато гіперпараметрів, порівняно з ранніми етапами машинного навчання (МН), я буду старатись завжди називати навчальний темп α гіперпараметром, а не просто параметром. Я думаю, що на ранніх етапах МН, коли не було так багато гіперпараметрів, більшість з нас зазвичай називали α параметром. Технічно, α і є параметром, але параметром, який визначає справжні параметри. Тож тепер ми намагаємось називати ці величини - α, кількість ітерацій та ін. - гіперпараметрами. Отже, коли ми тренуємо глибоку НМ для свого застосунку, то виявляємо, що є багато можливих значень гіперпараметрів, які просто потрібно перевірити на практиці. Тож застосування ГН сьогодні - дуже емпіричний процес. Часто ми просто маємо ідею, наприклад, вона може полягати в тому, щоб знайти найкраще значення для навчального темпу. Наприклад, ми припускаємо, що α має бути 0,01. І таким його і встановлюємо. Потім перевіряємо, дивимось як воно працює. А потім, отримавши результат, ми можемо змінити наше припущення і збільшити навчальний темп до 0,05. Тож, якщо ми не впевнені яке найкраще значення для навчального темпу використати, то можна спробувати одне значення навчального темпу α і побачити, що функція Витрат J прямує вниз ось так, потім можна спробувати більше значення для навчального темпу α і побачити, що функція Витрат йде вгору або розбігається, а потім можна спробувати ще якесь значення і побачити, що вона різко спускається вниз, але збігається до більшого значення. Можна спробувати ще якесь значення і побачити, що функція Витрат J робить отак. Потім, після кількох спроб, можна буде зробити висновок, що, наприклад, оце значення α дає нам досить швидке навчання і дозволяє функції Витрат J збігатись до меншого значення J, тож його ми й будемо використовувати. На попередньому слайді ми бачили багато різноманітних гіперпараметрів. Виявляється, що коли ми починаємо вирішувати робити новий застосунок, то буває дуже складно точно знати наперед які найкращі значення мають мати гіперпараметри. Тож часто ми просто перевіряємо багато різних значень і весь час проходимо оце коло. Ми пробуємо певні значення. Можливо, 5 прихованих шарів з певною кількістю прихованих вузлів, реалізуємо їх і дивимось чи вони працюють. І знову по колу. Тож назва цього слайду - "Прикладне ГН - дуже емпіричний процес". Емпіричний процес, можливо, - вигадливий спосіб сказати, що потрібно просто перевіряти багато значень і дивитись які спрацюють. Також я помітив, що ГН зараз застосовується до величезної кількості задач: від комп'ютерного бачення до розпізнавання мовлення, до обробки людської мови, до багатьох застосунків для структурованих даних, таких як онлайн реклами або інтернет пошуку, або рекомендації продуктів і т.д. Я помітив, що, по-перше, дослідники з будь-якої з цих дисциплін часом переходять до іншої і інколи їх розуміння гіперпараметрів може бути застосовним там, а інколи - ні. Тож я часто раджу людям, особливо коли вони починають розв'язувати нову задачу, пробувати набір значень і дивитись як це працює. І в наступному курсі ми побачимо систематизований спосіб, кілька систематичний способів підбору набору величин. А, по-друге, навіть якщо ти працюєш над одним застосунком довгий час, скажімо, ти працюєш над онлайн рекламою, то, досягши певного прогресу в задачі, дуже ймовірно, що найкращі величини для навчального темпу α, кількості прихованих вузлів та ін., можуть змінитись. Тож, навіть якщо ти налаштував систему з найкращими значеннями гіперпараметрів сьогодні, то, можливо, через рік ти відмітиш, що найкращі значення змінились. Можливо це станеться через обчислювальну інфраструктуру - зміниться кількість бітів ЦП [CPU] або тип ГП [GPU], на якому проводиться навчання, чи ще щось. Через це одним з практичних правил є час від часу (можливо раз в кілька місяців, якщо робота над задачею триватиме досить довго, скажімо, багато років) просто підставляй кілька величин в значення гіперпараметрів і перевіряй чи нема кращих значень для гіперпараметрів. Роблячи так, ти поступово здобудеш розуміння того, які гіперпараметри працюють для твого типу задач. Я знаю, що здається ніби це не надто приємна частина ГН, бо потрібно перепробувати так багато значень для цих гіперпараметрів, але, можливо, це - одна з областей, в якій дослідження ГН все ще просувається вперед і, можливо, через деякий час ми зможемо давати кращі поради для підбору найкращих гіперпараметрів. Проте, також може бути, що через те, що ЦП, ГП, НМ і набори даних постійно змінюються, інструкції не будуть структуруватись ще певний час і нам необхідно продовжувати пробувати різні значення і оцінювати їх набором перехресної перевірки або чимсь іншим і знаходити працюючі значення, які підходять під конкретну задачу. Отже, це був короткий огляд гіперпараметрів. В 2-ому курсі ми дамо також кілька підказок як систематично досліджувати набір гіперпараметрів. Тож на даний момент ти вже маєш практично всі потрібні інструменти, щоб виконати програмувальну вправу. Перед тим як ти приступиш я поділюсь ще одним набором ідей, які часто обдумую, а саме, наскільки ГН подібне до людського мозку.