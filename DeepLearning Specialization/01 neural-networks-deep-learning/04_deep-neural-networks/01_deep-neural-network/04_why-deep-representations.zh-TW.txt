我們已經聽過深度
神經網路作用得很好對於 很多的問題, 這不僅僅是
需要大型的神經網路 具體來說，需要的是
深層或者說很多隱藏層的 為什麼會這樣? 我們來看一些例子
試著得到一些直觀 為什麼深度網路作用得很好 首先
什麼是深度網路計算? 如果您建立一個系統來
人臉辨識或 人臉偵測, 這是深度
神經網路可以做的 或許您可以輸入一個人臉的照片
那第一層的神經網路 您可以想成也許是
特徵偵測或者邊緣偵測 在這個例子我畫一個
神經網路也許有 20 個隱藏單元 也許是一種這個圖形的計算 所以這 20 個隱藏單元視覺化
成為這小小的正方盒 舉個例子, 這小小的視覺化
代表一個隱藏單元是 試著找出是否
這個影像有邊緣是在這個方向 也許這個隱藏單元
或許試著找出 是否有水平
邊緣在這個影像 當我們在以後課程談到
卷積網路 這個特別的視覺化
會比較有感覺 這種樣式, 您可以想像第一層
的神經網路是看 這個圖形試著找出
這個圖形的邊緣在那裡 想像這個圖形的邊緣
是集合 一些像素成為邊緣 它可以偵測這些邊緣然後匯集
這些邊緣成為部分人臉 舉個例子, 您也許有一些神經元
試著看是否可以找到眼睛 或者不同的神經元試著
找出部分的鼻子 將很多的邊緣放在一起 它可以開始來偵測
人臉不同的部位 最後將
人臉不同的部分放在一起 像是眼睛或者鼻子或者
下巴, 它可以試著辨識 或偵測不同的人臉 直觀上，您可以想像這些
神經網路前面幾層是 偵測簡單函數, 像是邊緣 然後組合在一起
在這些神經網路的後面幾層 它就可以學越來越複雜的函數 這些視覺化會比較合理
當我們談到卷積網路 而視覺化的一個技術細節 邊緣偵測是看著
影像中很小的區域 以許很小的區域像這樣 而人臉辨識您看到
也許是影像中大很多的區域但 但是主要的重點是
從這裡只是很簡單的事情 像是邊緣然後將他們建立起來 將它們組合來偵測
更複雜的東西像是眼睛, 鼻子 然後再將它們組合
來找尋更複雜的東西 而這種簡單到複雜
階層表現 或者組合表現 也應用到其他類型
而不僅是影像跟人臉辨識 舉個例子, 如果您試著
建立一個語音辨識系統 很難將語音視覺化, 但 如果您輸入一段語音或許
第一層神經網路也許 學著偵測低階語音波型
特徵, 像是這音調上升 或是下降 它是白噪音或者
嘶聲像是[聲音] 音階是如何 當在這個階層
偵測低階音波特徵像這些 然後結合
低階音波樣式 您會學習偵測
聲音基本單元 在語言學他們叫音位 舉個例子, cat 這個字
 c 是音位, a 是音位 t 是另一個音位 學著尋找或許是
聲音的基本單元 組合在一起或許
學習辨識音訊中的單字 也許將這些組合在一起 為了辨識整個短句或者句子 所以深度學習網路有很多隱藏
層也許能夠在早先 的層學著
低階簡單的特徵 然後在比較深層時
將這些簡單的東西放在一起偵測 為了偵測更複雜的東西
像是辨識特別字或者 甚至是短句或句子 最終是為了
語音辨識 我們看的是在早期的
階層計算的似乎是 相對簡單的函數對於輸入
像是邊緣在哪裡, 等到了 您進到深一點網路您
真的會驚訝它們做的複雜事情 像是偵測人臉或者
偵測單字或短句或句子 一些人喜歡做一種比喻
介於深度神經網路跟 人類的大腦, 我們相信
或者說神經學家相信 人腦也是開始
偵測簡單的東西像是您 眼睛看到的邊緣然後建立
成偵測更複雜 的東西像是您看到的人臉 我想這種比喻介於
深度學習跟 人腦有時候
有一點點危險 但有很多的理由相信
這是我們認為人腦如此工作 人腦也許
先偵測簡單的東西像是邊緣 然後將他們放在一起變成
更複雜的東西所以 這也是一種鬆散方式的啟發
對於一些深度學習 我們會看到多一些
有關人腦或者 有關生物大腦在
這個禮拜晚一點的影片中 其他直觀
有關於深度網路似乎 作用得很好如下 這個結果來自於電路
理論, 屬於一種思考 有關於您可以計算何種函數
使用不同的 AND閘, OR閘, NOT 閘不同的閘 非正式而言, 它們這些函數
相對的"小"深度神經 網路, 小的意思是
隱藏單元的數目很小 但如果您計算同樣
函數用一個淺的網路 沒有足夠的隱藏層 那您需要指數般
多的隱藏單元來做計算 讓我用一個例子
來一點點非正式的解釋 假設您試著
計算 互斥或 您輸入特徵的同位 您試著計算 x1 
XOR x2 XOR x3 XOR 一直到 xn 如果您有 n 或者
 nx 特徵 如果您建一個 XOR 樹像這樣
我們用它來計算 XOR of x1 跟 x2, 然後 x3 跟
x4 計算它們的 XOR 技術上來說如果您只用
AND, OR, NOT 閘您或許需要 幾層來計算 XOR 
函數而不只一層 但是相對小的電路
您可以計算 XOR 等等 然後您可以建
真的, XOR 樹像這樣 直到最後您有一的電路
輸出，我們稱為 y 輸出為 y-hat 等於 y 這個 互斥或
對於所有輸入位元的同位 為了計算 XOR, 
左邊網路的深度是 log N 等級 我們用一個  XOR 樹 節點的數目或者
電路元件的數目或者 閘的數目在這個
網路, 並不是很大 您並不需要很多閘
為了要計算 互斥或 現在, 如果您不允許
神經網路使用多重 隱藏層, 在這個例子
log n 層的隱藏層 如果您強迫計算這個
函數只用一層隱藏層 您讓所有東西都進入
隱藏層 然後這個東西輸出 y 然後為了計算這個
XOR 函數, 這個隱藏層 會需要指數般大
因為基本上 您需要詳盡列舉我們
2 的 n 次方可能組合 所以 2 的 n次方等級
可能的組合的輸入位元 結果是窮舉
所有的 1 或者 0 您最終需要隱藏層
是指數般大的 總共的位元 我想技術上您可以做到
 2 的 n - 1 次方 但這也是 2 的 n 次方等級
相當大的位元數目 我希望這給您一點感覺
用數學函數 比較容易用深度網路計算
比起淺層網路 實際上, 我個人認為
用電路理論的結果比較不能 得到直觀, 但
人們通常引用這種結果 來解釋
深度表達的價值 現在, 除了這個理由 比較喜歡深度神經網路
老實說 我想其它理由這個名詞深度
學習起飛只是因為品牌效應 這個東西我們之前稱作神經
網路有很多層隱藏層，但 這個名稱"深度學習" 
就是一個好品牌, 它是如此之深 我想一旦一個名詞抓住了
將神經網路重新包裝或者 將神經網路有很多
隱藏層重新包裝 幫助捕捉大眾
的想像 但對於品牌公關而言
深度網路做得很好 有時候人們玩過火
堅持用一大堆隱藏層 但當我開始新的問題時
我真的通常開始於 羅吉斯迴歸分析
然後試用一個或是 兩個隱藏層
用此當超參數 用這個當作參數或者超參數
來調整為了試著找到 正確的深度對於您的神經網路 但過去幾年
有一種趨勢人們發現 對於一些應用很深很深
神經網路也許 數打的層級有時候
是最佳的模型對於那些問題 所以這是一些直觀對於
為什麼深度學習似乎作用很好 現在讓我們來看一下如何建置不只正向 傳播, 還有反向傳播的機制