1
00:00:00,000 --> 00:00:03,810
在之前的影片中,您看到了如何計算導數和建立

2
00:00:03,810 --> 00:00:08,325
梯度下降,在僅只一個例子的羅吉斯迴歸分析。

3
00:00:08,325 --> 00:00:11,370
現在,我們要用 m 訓練例子,執行此操作。

4
00:00:11,370 --> 00:00:15,420
首先,讓我們提醒我們成本函數 J 的定義.

5
00:00:15,420 --> 00:00:19,815
J, 成本函數 w,b,你關心的是這個平均值,

6
00:00:19,815 --> 00:00:23,505
是 1/m，總和從 i = 1 到 m

7
00:00:23,505 --> 00:00:28,545
當您在 y 的例子上, 相對輸出值 a_i 時的損失,

8
00:00:28,545 --> 00:00:36,450
其中 a_i 是第 i 個訓練例子的預測, 等於 sigma(z_i),

9
00:00:36,450 --> 00:00:45,270
等於 sigma(w轉置 x_i, 加 b)。

10
00:00:45,270 --> 00:00:49,835
因此,我們在上一張投影片中展示的
是任何單一訓練例子,

11
00:00:49,835 --> 00:00:57,005
當您只有一個訓練例子時,如何計算導數。

12
00:00:57,005 --> 00:01:02,460
所以 dw_1, dw_2 和 d_b,

13
00:01:02,460 --> 00:01:04,670
現在加上上標 i 來表示

14
00:01:04,670 --> 00:01:09,140
你得到的對應值，如同你在上一張幻燈片上所做的那樣

15
00:01:09,140 --> 00:01:12,665
但那只是使用一個訓練例子,

16
00:01:12,665 --> 00:01:15,065
x_i y_i,對不起,

17
00:01:15,065 --> 00:01:16,840
錯過了那裡一個 i 。

18
00:01:16,840 --> 00:01:22,110
所以,現在你注意到總成本函數
作為一個總和，其實是平均值,

19
00:01:22,110 --> 00:01:25,805
因為 1/ m ,對於每一個損失項目.

20
00:01:25,805 --> 00:01:28,865
實際上這個導數,

21
00:01:28,865 --> 00:01:35,435
相對於 w_1的總體成本函數,也將是

22
00:01:35,435 --> 00:01:45,100
相對於每一個損失項目的 w_1 導數的平均值。

23
00:01:45,100 --> 00:01:51,420
但之前,我們已經演示如何
計算這個項目為 dw_1_i,

24
00:01:52,100 --> 00:01:55,530
在上一張投影片上,

25
00:01:55,530 --> 00:01:58,275
展示如何在單一訓練例子中計算此項。

26
00:01:58,275 --> 00:02:00,635
所以,你真正需要做的是計算

27
00:02:00,635 --> 00:02:06,020
這些導數,正如我們在前面的
訓練例子中展示的,並將它們平均,

28
00:02:06,020 --> 00:02:07,280
這會給你

29
00:02:07,280 --> 00:02:12,005
總體的梯度,您可以用來實現梯度下降.

30
00:02:12,005 --> 00:02:14,330
我知道那是很多細節,

31
00:02:14,330 --> 00:02:17,180
但讓我們把這一切,並包裝一起到

32
00:02:17,180 --> 00:02:19,760
具體的演算法,直到你能够

33
00:02:19,760 --> 00:02:23,480
實現羅吉斯迴歸分析使用梯度下降。

34
00:02:23,480 --> 00:02:29,105
所以,你可以這樣做:讓我們初始化 J 等於零,

35
00:02:29,105 --> 00:02:38,650
dw_1 等於零,dw_2 等於零,d_b 等於零。

36
00:02:38,650 --> 00:02:43,580
我們要做的是在訓練集上,使用 for loop,

37
00:02:43,580 --> 00:02:47,995
計算每個訓練例子的導數,然後將它們相加。

38
00:02:47,995 --> 00:02:50,340
所以,這是我們怎麼做的,對 i 等於 1 到 m,

39
00:02:50,340 --> 00:02:52,320
所以m是訓練例子的數量,

40
00:02:52,320 --> 00:02:56,705
我們計算 z_i 等於 w轉置 x_i 加 b。

41
00:02:56,705 --> 00:03:00,650
預測值 a_i 等於 Sigma(z_i),

42
00:03:00,650 --> 00:03:03,590
然後,讓我們把 J 加起來,

43
00:03:03,590 --> 00:03:11,580
J += -[(y_i )log(a_i) + (1-y_i)log(1-a_i)]

44
00:03:11,580 --> 00:03:14,405
然後把負號放在整個的前面,

45
00:03:14,405 --> 00:03:15,710
然後我們先前看到,

46
00:03:15,710 --> 00:03:20,615
我們有 dz_i = a_i - y_i,

47
00:03:20,615 --> 00:03:25,910
和 dw_1 += ( x1_i )( dz_i ),

48
00:03:25,910 --> 00:03:32,065
dw_2 += ( x2_i )( dz_i ),

49
00:03:32,065 --> 00:03:36,640
我做這個計算,假設你只有兩個特徵,

50
00:03:36,640 --> 00:03:38,530
n = 2, 否則,

51
00:03:38,530 --> 00:03:39,849
你要做 dw_1,

52
00:03:39,849 --> 00:03:41,755
dw_2, dw_3 ..等等,

53
00:03:41,755 --> 00:03:44,750
然後 db += dz_i,

54
00:03:44,750 --> 00:03:47,445
這是這個 for loop 的終點

55
00:03:47,445 --> 00:03:50,815
最後,對所有的 m 訓練例子都做了這些操作,

56
00:03:50,815 --> 00:03:55,720
您仍然需要除以 m,因為我們的計算平均值。

57
00:03:55,720 --> 00:03:58,870
所以,dw_1 /= m ,

58
00:03:58,870 --> 00:04:01,465
dw_2 /= m ,

59
00:04:01,465 --> 00:04:03,520
db /= m ,

60
00:04:03,520 --> 00:04:05,200
以計算平均值。

61
00:04:05,200 --> 00:04:08,080
所以,用所有這些計算,

62
00:04:08,080 --> 00:04:11,710
您計算了成本函數 J 的導數,相對於

63
00:04:11,710 --> 00:04:15,595
每個參數 w_1、w_2 和 b。

64
00:04:15,595 --> 00:04:17,685
關於一些我們正在做的幾點細節,

65
00:04:17,685 --> 00:04:24,250
我們使用 dw_1 和 dw_2 和 db 作為累加數,

66
00:04:24,250 --> 00:04:26,450
這樣,經過這個計算之後,

67
00:04:26,450 --> 00:04:30,700
dw_1 等於,

68
00:04:30,700 --> 00:04:36,160
總成本函數的導數對於 w_1,
 dw_2 和 db 同樣的。

69
00:04:36,160 --> 00:04:39,880
請注意,dw_1 和 dw_2 沒有上標 i,

70
00:04:39,880 --> 00:04:41,620
因為我們用他們在程式中當作

71
00:04:41,620 --> 00:04:44,300
累加數,以整個訓練集加起來。

72
00:04:44,300 --> 00:04:46,595
相較之下，dz_i

73
00:04:46,595 --> 00:04:51,190
這 dz 只是相對於單一的訓練例子。

74
00:04:51,190 --> 00:04:55,030
所以,這就是為什麼有一個上標 i 
對應到一個訓練的例子,

75
00:04:55,030 --> 00:04:56,710
i 是用來計算的

76
00:04:56,710 --> 00:04:59,745
所以,在完成了所有這些計算後,

77
00:04:59,745 --> 00:05:02,080
在實施梯度下降的一個步驟,

78
00:05:02,080 --> 00:05:03,730
您將更新 w_1,

79
00:05:03,730 --> 00:05:08,300
為: w_1 減 (學習率 乘 dw_1)

80
00:05:08,300 --> 00:05:12,515
w_2 更新為: w_2 減 (學習率 乘 dw_2)

81
00:05:12,515 --> 00:05:17,390
b 更新為: b 減 (學習率 乘 db)

82
00:05:17,390 --> 00:05:22,250
其中 dw_1、dw_2 和 db 是計算結果。

83
00:05:22,250 --> 00:05:27,530
最後,此處的 J 也將是成本函數的正確值。

84
00:05:27,530 --> 00:05:32,150
所以,投影片上的所有內容都僅只
實現了梯度下降的一個步驟而以,

85
00:05:32,150 --> 00:05:35,270
所以你必須重複這張投影片上的所有內容

86
00:05:35,270 --> 00:05:38,815
多次,以便採取梯度下降的多重步驟。

87
00:05:38,815 --> 00:05:42,700
如果這些細節看起來太複雜了,再次,

88
00:05:42,700 --> 00:05:44,485
現在不要太擔心,

89
00:05:44,485 --> 00:05:47,215
希望所有這些會變得比較清楚,當你

90
00:05:47,215 --> 00:05:49,850
在程式設計作業中實現此內容。

91
00:05:49,850 --> 00:05:53,425
但事實證明,有兩個弱點

92
00:05:53,425 --> 00:05:57,975
在我們這裡計算的做法,

93
00:05:57,975 --> 00:06:01,180
也就是說,以這種方式做羅吉斯迴歸分析,

94
00:06:01,180 --> 00:06:03,250
您需要編寫兩個 for loop。

95
00:06:03,250 --> 00:06:06,340
第一個 for loop 是這個對於迴圈的 m 訓練例子,

96
00:06:06,340 --> 00:06:11,360
第二個 for loop 是此處所有特徵的 for loop。

97
00:06:11,360 --> 00:06:12,600
所以,在這個例子中,

98
00:06:12,600 --> 00:06:14,040
只有兩個特徵,

99
00:06:14,040 --> 00:06:16,695
n 等於 2, nx 等於 2,

100
00:06:16,695 --> 00:06:18,240
但如果您有更多的特徵,

101
00:06:18,240 --> 00:06:20,940
你寫在這裡dw_1 dw_2,

102
00:06:20,940 --> 00:06:23,295
您有類似的計算對於 dw_3

103
00:06:23,295 --> 00:06:25,275
等等...到 dw_n。

104
00:06:25,275 --> 00:06:31,310
因此,似乎您需要 for loop 對所有特徵, 對 n 個特徵。

105
00:06:31,310 --> 00:06:34,415
當您建置深度學習演算法時,

106
00:06:34,415 --> 00:06:37,070
你會發現,明顯的使用 for loop 在

107
00:06:37,070 --> 00:06:41,255
您的程式,會使演算法運行效率降低。

108
00:06:41,255 --> 00:06:43,130
所以在深度學習時代,

109
00:06:43,130 --> 00:06:46,130
我們將會移到一個越來越大的資料集,

110
00:06:46,130 --> 00:06:50,180
因此,能夠實現您的演算法,
而無需明顯的使用

111
00:06:50,180 --> 00:06:54,800
for loop 是非常重要,它將
幫助您擴展到更大的資料集。

112
00:06:54,800 --> 00:06:58,550
因此,事實證明,有一組技術稱為向量化

113
00:06:58,550 --> 00:07:03,610
允許您在程式中免除這些明顯的 for loop。

114
00:07:03,610 --> 00:07:06,570
我認為在深度學習時代以前,

115
00:07:06,570 --> 00:07:08,595
那是在深度學習興起之前,

116
00:07:08,595 --> 00:07:10,810
向量化是可有可無,

117
00:07:10,810 --> 00:07:14,780
因此,有時您可以這樣做來
加快程式的速度,但有時不會。

118
00:07:14,780 --> 00:07:17,450
但在深度學習時代,向量化,

119
00:07:17,450 --> 00:07:19,250
以去掉 for loop,

120
00:07:19,250 --> 00:07:20,920
像這樣,像這樣,

121
00:07:20,920 --> 00:07:22,930
變得非常重要,

122
00:07:22,930 --> 00:07:26,350
因為我們在非常大的資料集上,
需要越來越多的訓練,

123
00:07:26,350 --> 00:07:28,975
因此,您確實需要非常有效率的程式

124
00:07:28,975 --> 00:07:30,680
所以,在接下來的幾個影片中,

125
00:07:30,680 --> 00:07:33,590
我們將討論向量化以及如何

126
00:07:33,590 --> 00:07:38,485
實現所有這些,甚至不使用一個 for loop。

127
00:07:38,485 --> 00:07:41,660
所以有了這些,我希望您有點概念

128
00:07:41,660 --> 00:07:45,305
去實現羅吉斯迴歸分析或羅吉斯迴歸分析的梯度下降。

129
00:07:45,305 --> 00:07:48,515
當您實現程式設計練習時,
事情會更加清晰。

130
00:07:48,515 --> 00:07:51,170
但在實際做程式設計練習之前,

131
00:07:51,170 --> 00:07:55,580
讓我們先來談談向量化,
這樣你就可以實現這整件事,

132
00:07:55,580 --> 00:08:00,210
實現一個反覆運算的梯度下降,而無需使用任何 for loop。