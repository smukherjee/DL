1
00:00:00,000 --> 00:00:03,116
Ви бачили схеми нейронних мереж (НМ), що я малював.

2
00:00:03,116 --> 00:00:05,712
У цьому відео поговоримо по суті
 про значення тих малюнків.

3
00:00:05,712 --> 00:00:06,728
Інакше кажучи,

4
00:00:06,728 --> 00:00:11,235
що саме ті замальовки НМ відображають.

5
00:00:11,235 --> 00:00:15,014
А почнемо ми з НМ, що

6
00:00:15,014 --> 00:00:17,290
зветься одношаровою.

7
00:00:17,290 --> 00:00:19,667
Ось схема НМ.

8
00:00:19,667 --> 00:00:22,986
Давай назвемо окремі частини цієї схеми.

9
00:00:22,986 --> 00:00:27,447
Ми маємо вхідні ознаки x₁, x₂, x₃ складені по вертикалі.

10
00:00:27,447 --> 00:00:30,694
Це називається вхідним шаром НМ.

11
00:00:30,694 --> 00:00:35,764
Напевно не буде сюрпризом факт, що він містить вхідні дані НМ.

12
00:00:35,764 --> 00:00:37,990
А ось інший шар, з колами.

13
00:00:37,990 --> 00:00:41,663
Він називається прихованим шаром НМ.

14
00:00:41,663 --> 00:00:45,414
Через хвильку я до цього повернусь і поясню, 
що мається на увазі під прихованим.

15
00:00:45,414 --> 00:00:49,509
Останній шар, в цьому випадку, сформований одним вузлом.

16
00:00:49,509 --> 00:00:53,894
Цей одновузловий шар називається вихідним шаром. 
Він відповідальний за

17
00:00:53,894 --> 00:00:56,059
генерацію передбачень ŷ.

18
00:00:56,059 --> 00:00:59,932
В НМ, що тренується контрольованим навчанням,

19
00:00:59,932 --> 00:01:05,237
тренувальний набір містить значення вхідних x і цільових вихідних y.

20
00:01:05,237 --> 00:01:09,239
Тож термін "прихований шар" відсилає до факту, 
що в тренувальному наборі

21
00:01:09,239 --> 00:01:12,702
дійсні значення вказаних вузлів не відстежуються.

22
00:01:12,702 --> 00:01:15,185
Тобто їх немає у тренувальному наборі.

23
00:01:15,185 --> 00:01:16,640
Ми бачимо вхідні дані.

24
00:01:16,640 --> 00:01:18,094
Ми знаємо, якими повинні бути вихідні дані.

25
00:01:18,094 --> 00:01:20,992
Проте, інформація про прихований шар 
відсутня в тренувальному наборі.

26
00:01:20,992 --> 00:01:25,542
Це трохи пояснює назву "прихований". Ми просто

27
00:01:25,542 --> 00:01:28,088
не бачимо його в тренувальному наборі. 
Давай введемо ще одне позначення.

28
00:01:28,088 --> 00:01:30,262
До цього часу ми використовували

29
00:01:30,262 --> 00:01:35,542
вектор x для позначення вхідних даних. Інше позначення

30
00:01:35,542 --> 00:01:37,226
значень

31
00:01:37,226 --> 00:01:41,987
вхідних даних буде a з верхнім індексом 0 в квадратних дужках.

32
00:01:41,987 --> 00:01:44,934
Термін a також означає "активатори" і

33
00:01:44,934 --> 00:01:47,733
відповідає значенням, що

34
00:01:47,733 --> 00:01:51,651
передаються іншим шарам НМ, які також передаються наступним шарам.

35
00:01:51,651 --> 00:01:55,998
Тож вхідний шар передає значення x прихованому шару.

36
00:01:55,998 --> 00:02:01,110
Ми називаємо це активаторами вхідного шару a з верхнім індексом [0].

37
00:02:01,110 --> 00:02:05,990
Наступний (прихований) шар, в свою чергу, генерує певні активатори,

38
00:02:05,990 --> 00:02:09,601
які я запишу як a з верхнім індексом [1].

39
00:02:09,601 --> 00:02:13,306
Зокрема, оцей перший вузол шару

40
00:02:13,306 --> 00:02:17,824
генеруватиме значення a[1] з нижнім індексом 1.

41
00:02:17,824 --> 00:02:20,735
Наступний вузол цього ж шару генеруватиме значення

42
00:02:20,735 --> 00:02:23,311
a[1] з нижнім індексом 2 і так далі.

43
00:02:23,311 --> 00:02:26,488
Тож a з верхнім індексом в квадратних дужках [1]

44
00:02:26,488 --> 00:02:30,120
є чотиримірним вектором або,

45
00:02:30,120 --> 00:02:34,707
в Python, матриця 4*1, або 4-елементний вектор-стовпчик, що

46
00:02:34,707 --> 00:02:39,205
виглядає ось так. 
І він є 4-елементним, бо в цьому випадку ми маємо 4 вузли,

47
00:02:39,205 --> 00:02:42,684
або 4 одиниці, чи вірніше, 4 прихованих одиниці в цьому прихованому шарі.

48
00:02:42,684 --> 00:02:46,302
І, нарешті, вихідний шар генеруватиме деякі значення a[2],

49
00:02:46,302 --> 00:02:47,948
що є дійсним числом,

50
00:02:47,948 --> 00:02:51,658
а ŷ прийме значення a[2].

51
00:02:51,658 --> 00:02:55,885
Тож це схоже на регресію, де ми маємо

52
00:02:55,885 --> 00:03:00,349
ŷ=a, і в моделі ми маємо лише один, вихідний шар, тож,

53
00:03:00,349 --> 00:03:03,583
не використовуємо там верхній індекс в квадратних дужках.

54
00:03:03,583 --> 00:03:07,916
Але тепер в НМ будемо використовувати 
верхній індекс в квадратних дужках,

55
00:03:07,916 --> 00:03:11,653
щоб наочно вказувати, до якого шару це відноситься.

56
00:03:11,653 --> 00:03:15,468
Дещо дивна річ конвенції позначень НМ.

57
00:03:15,468 --> 00:03:20,194
НМ, що ти зараз бачиш, називається двошаровою НМ.

58
00:03:20,194 --> 00:03:23,541
Це тому, що рахуючи шари НМ,

59
00:03:23,541 --> 00:03:25,321
ми не враховуємо

60
00:03:25,321 --> 00:03:28,858
вхідний шар. Тож прихований шар - це шар 1,

61
00:03:28,858 --> 00:03:32,661
вихідний шар - це шар 2. 
В нашій конвенції позначень вхідний шар

62
00:03:32,661 --> 00:03:35,887
називається шаром 0. Тож технічно тут

63
00:03:35,887 --> 00:03:39,649
3 шари, бо тут є вхідний шар, прихований шар і

64
00:03:39,649 --> 00:03:43,357
вихідний шар. Але, в загальноприйнятому вживанні, 
у дослідницьких статтях,

65
00:03:43,357 --> 00:03:47,489
матеріалах курсу, таку НМ всі називатимуть

66
00:03:47,489 --> 00:03:51,602
двошаровою. Ми не вважаємо вхідний шар

67
00:03:51,602 --> 00:03:55,912
за справжній. 
Зрештою, пізніше ми будемо говорити, що прихований шар

68
00:03:55,912 --> 00:03:59,670
і вихідні шари матимуть параметри пов'язані з ними.

69
00:03:59,670 --> 00:04:03,447
Тож прихований шар матиме пов'язані з ним параметри w та b

70
00:04:03,447 --> 00:04:08,218
і я буду надписувати верхні індекси 1 в квадратних дужках, щоб позначити,

71
00:04:08,218 --> 00:04:12,395
що ці параметри пов'язані з шаром 1 (з прихованим шаром).

72
00:04:12,395 --> 00:04:15,416
Ми побачимо пізніше, що w - це матриця 4*3,

73
00:04:15,416 --> 00:04:18,016
а b - вектор 4*1 в цьому випадку.

74
00:04:18,016 --> 00:04:21,754
Де перша розмірність "4" відображає,

75
00:04:21,754 --> 00:04:24,503
що ми маємо 4 вузли (або приховані одиниці)

76
00:04:24,503 --> 00:04:28,120
в шарі, а трійка відображає, що ми маємо 3 вхідні ознаки.

77
00:04:28,120 --> 00:04:31,980
Ми поговоримо пізніше про розмірність цих матриць.

78
00:04:31,980 --> 00:04:33,844
Тоді для цього буде більш вдалий час. Аналогічно

79
00:04:33,844 --> 00:04:37,813
вихідний шар теж має пов'язані з ним параметри

80
00:04:37,813 --> 00:04:41,663
w з верхнім індексом 2 в квадратних дужках і 
b з верхнім індексом 2 в квадратних дужках.

81
00:04:41,663 --> 00:04:45,747
І, виявляється, що їхні розміри - 1*4 і 1*1.

82
00:04:45,747 --> 00:04:49,297
1*4 тому що прихований шар має 4 прихованих вузли,

83
00:04:49,297 --> 00:04:51,177
а вихідний шар має лише 1 вузол.

84
00:04:51,177 --> 00:04:56,378
Але, повторю, 
ми поговоримо про розмірність цих матриць у подальших відео.

85
00:04:56,378 --> 00:04:59,839
Тож щойно ти бачив/ла як виглядає двошарова НМ.

86
00:04:59,839 --> 00:05:03,108
Ця мережа має один прихований шар.

87
00:05:03,108 --> 00:05:04,260
У наступному відео

88
00:05:04,260 --> 00:05:08,513
ми детальніше розберемо обчислення НМ.

89
00:05:08,513 --> 00:05:11,223
Тобто як НМ приймає на вхід x і

90
00:05:11,223 --> 00:05:14,169
весь шлях до обчислення на виході ŷ.