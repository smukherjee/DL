1
00:00:00,000 --> 00:00:01,619
当你开始训练神经网络时

2
00:00:01,619 --> 00:00:03,955
将权重参数进行随机初始化非常重要

3
00:00:03,955 --> 00:00:08,426
在逻辑回归的问题中
把权重参数初始化为零是可行的

4
00:00:08,426 --> 00:00:12,258
但把神经网络的权重参数全部初始化为零
并使用梯度下降

5
00:00:12,258 --> 00:00:14,706
将无法获得预期的效果

6
00:00:14,706 --> 00:00:15,289
让我们来看看为什么

7
00:00:15,289 --> 00:00:20,173
这里有两个输入样本参数

8
00:00:20,173 --> 00:00:25,953
因此n[0]等于2 还有两个隐藏单元
因此n[1]也等于2

9
00:00:25,953 --> 00:00:31,547
所以与隐藏层关联的权重w[1]

10
00:00:31,547 --> 00:00:35,373
是一个2x2的矩阵

11
00:00:35,373 --> 00:00:41,230
现在我们将这个矩阵的初始值都设为0

12
00:00:41,230 --> 00:00:45,531
同样我们将矩阵b[1]的值也都初始化为零

13
00:00:45,531 --> 00:00:50,788
偏离度矩阵b[1]的初始值都是0
不会影响最终结果

14
00:00:50,788 --> 00:00:54,240
但是将权重参数矩阵w[1]初始值都设为零
会引起某些问题

15
00:00:54,240 --> 00:00:59,625
这样的初始权重参数会导致

16
00:00:59,625 --> 00:01:05,522
无论使用什么样的样本进行训练

17
00:01:05,522 --> 00:01:09,253
a[1]1与a[1]2始终是相同的

18
00:01:09,253 --> 00:01:12,613
这第一个激活函数和这第二个激活函数将是完全一致的

19
00:01:12,613 --> 00:01:17,170
因为这些隐藏神经元在进行完全相同的计算工作

20
00:01:17,170 --> 00:01:21,810
当你进行反向传播的计算时

21
00:01:21,810 --> 00:01:24,478
由于对称问题

22
00:01:24,478 --> 00:01:30,165
这些隐藏单元将会在同样的条件下被初始化

23
00:01:30,165 --> 00:01:33,720
最终导致z[1]1的导数和dz[1]2的导数也不会有差别

24
00:01:33,720 --> 00:01:36,080
同样的

25
00:01:36,080 --> 00:01:39,851
我假设输出的权重也是相同的

26
00:01:39,851 --> 00:01:45,122
所以输出权重参数矩阵w[2]也等于[0,0]

27
00:01:45,122 --> 00:01:48,691
但如果你使用这种方法来初始化神经网络

28
00:01:48,691 --> 00:01:53,590
那么上面这个隐藏单元和下面这个隐藏单元也是相同的

29
00:01:53,590 --> 00:01:57,011
它们实现的是完全相同的功能

30
00:01:57,011 --> 00:02:01,687
有时候我们也称这是“对称”的

31
00:02:01,687 --> 00:02:03,765
我们来归纳一下这个结果

32
00:02:03,765 --> 00:02:08,064
经过每一次训练迭代 你将会得到两个

33
00:02:08,064 --> 00:02:11,272
实现完全相同功能的隐藏单元

34
00:02:11,272 --> 00:02:17,521
在之前的视频中W的导数将会是一个矩阵

35
00:02:17,521 --> 00:02:20,681
大概看上去是这样 每一行都是相同的

36
00:02:20,681 --> 00:02:23,318
然后我们进行一次权重更新

37
00:02:23,318 --> 00:02:30,163
当你在实际操作时 w[1]将被更新成w[1]-α*dw

38
00:02:30,163 --> 00:02:33,740
而你将会发现 经过每一次迭代后

39
00:02:33,740 --> 00:02:37,616
w[1]的第一行与第二行是相同的

40
00:02:37,616 --> 00:02:41,487
所以根据上述信息来归纳 我们可以得到一个证明结果

41
00:02:41,487 --> 00:02:44,688
如果你在神经网络中
将所有权重参数矩阵w的值初始化为零

42
00:02:44,688 --> 00:02:49,164
由于两个隐藏单元肩负着相同的计算功能

43
00:02:49,164 --> 00:02:53,541
并且也将同样的影响作用在输出神经元上

44
00:02:53,541 --> 00:02:57,542
经过一次迭代后 依然会得到相同的结果

45
00:02:57,542 --> 00:03:00,273
这两个隐藏神经元依然是“对称”的

46
00:03:00,273 --> 00:03:04,507
同样推导下去 经过两次迭代
三次迭代 以及更多次迭代

47
00:03:04,507 --> 00:03:07,013
无论你将这个神经网络训练多久

48
00:03:07,013 --> 00:03:10,373
这两个隐藏单元仍然在使用同样的功能进行运算

49
00:03:10,373 --> 00:03:15,212
在这个例子中 由于隐藏单元实现的都是相同的功能

50
00:03:15,212 --> 00:03:17,692
所以我们使用一个就够了

51
00:03:17,692 --> 00:03:22,378
在更大的神经网络中 假设我们有三个输入特征值

52
00:03:22,378 --> 00:03:24,972
以及非常多的隐藏单元

53
00:03:24,972 --> 00:03:29,239
一个类似的结论也同样成立

54
00:03:29,239 --> 00:03:34,107
就像我现在画的这样 我不会画出所有的连接线

55
00:03:34,107 --> 00:03:37,103
如果你依然将权重参数矩阵初始化为零
那么无论你

56
00:03:37,103 --> 00:03:40,603
运行梯度下降多长时间
所有的隐藏神经元都将是“对称”的

57
00:03:40,603 --> 00:03:44,037
它们依然将运行在完全相同的功能下

58
00:03:44,037 --> 00:03:48,785
而这并不能给我们带来任何帮助

59
00:03:48,785 --> 00:03:52,835
因为我们希望两个不同的隐藏单元能实现不同的功能

60
00:03:52,835 --> 00:03:57,748
因此只有进行随机初始化能够解决这样的问题

61
00:03:57,748 --> 00:03:58,677
让我们看看如何在Python中操作

62
00:03:58,677 --> 00:04:04,053
我们通常使用w[1]=np.random.randn((2,2))*0.01

63
00:04:04,053 --> 00:04:07,037
这样的写法来对这个2*2的矩阵进行随机初始化

64
00:04:07,037 --> 00:04:12,358
并乘上一个非常小的数 比如0.01 这样操作后

65
00:04:12,358 --> 00:04:14,951
你已经将权重参数矩阵赋予了非常小的随机初始值

66
00:04:14,951 --> 00:04:20,590
然后对于b来说 b并不会由于初始值为零而产生对称问题

67
00:04:20,590 --> 00:04:24,735
或称之为对称失效问题

68
00:04:24,735 --> 00:04:29,370
所以使用b[i]=np.zeros((2,1))将b矩阵初始值设为零

69
00:04:29,370 --> 00:04:32,166
好了 现在权重参数矩阵w已经完成了随机初始化

70
00:04:32,166 --> 00:04:36,769
不同的隐藏单元会承担不同的计算工作

71
00:04:36,769 --> 00:04:40,912
我们也不会再遇到类似前面说的对称失效问题了

72
00:04:40,912 --> 00:04:43,795
然后我们可以用同样的方法来将w[2]进行随机初始化

73
00:04:43,795 --> 00:04:48,858
b[2]也依然可以初始化为零

74
00:04:48,858 --> 00:04:55,321
好了 现在你想知道的可能是
为什么使用这个常量 为什么是0.01

75
00:04:55,321 --> 00:04:58,478
为什么我们不把它设置为100或1000？

76
00:04:58,478 --> 00:05:02,313
主要原因是 我们通常比较喜欢使用

77
00:05:02,313 --> 00:05:05,763
非常非常小的随机初始值

78
00:05:05,763 --> 00:05:10,443
而当你使用tanh或者sigmoid的激活函数时

79
00:05:10,443 --> 00:05:14,047
或者在输出层使用了sigmoid函数

80
00:05:14,047 --> 00:05:17,922
如果步长过大

81
00:05:17,922 --> 00:05:23,967
那么当你要计算激活值的时候

82
00:05:23,967 --> 00:05:28,621
你应该还记得z[1]=w[1]*x+b[1]

83
00:05:28,621 --> 00:05:34,094
而a[1]=g[1]*(z[1])

84
00:05:34,094 --> 00:05:39,097
所以 当w非常大的时候

85
00:05:39,097 --> 00:05:44,235
z（的绝对值）也相应的会非常大<br />或者说z可能是一个非常大的数或是一个非常小的数

86
00:05:44,235 --> 00:05:49,789
在这样的情况下 你可能最终会发现图上

87
00:05:49,789 --> 00:05:55,699
tanh和sigmoid函数中 这些相对平坦的部分<br />梯度的斜率非常小

88
00:05:55,699 --> 00:05:58,302
这意味着梯度下降会非常缓慢

89
00:05:58,302 --> 00:05:59,730
所以整个学习过程也会变得尤为缓慢

90
00:05:59,730 --> 00:06:04,133
概括一下 如果w过大 你很容易在开始时就

91
00:06:04,133 --> 00:06:08,633
得出一个非常大的z

92
00:06:08,633 --> 00:06:13,525
而这会导致你的tanh和sigmoid激活函数学习进度缓慢

93
00:06:13,525 --> 00:06:15,418
无法实现预期功能

94
00:06:15,418 --> 00:06:17,231
如果在你的神经网络中未使用

95
00:06:17,231 --> 00:06:22,149
任何sigmoid或者tanh激活函数 这种情况可能不明显

96
00:06:22,149 --> 00:06:26,506
但是如果你使用二分类 并且你的输出神经元

97
00:06:26,506 --> 00:06:30,806
使用了sigmoid函数 那么你不会希望初始参数过大

98
00:06:30,806 --> 00:06:35,435
所以这就是为什么我们说在上述公式中需要乘以0.01

99
00:06:35,435 --> 00:06:36,872
或者其他比较小的数值

100
00:06:36,872 --> 00:06:38,536
对权重参数矩阵w2来说也适用

101
00:06:38,536 --> 00:06:44,295
我们可以表示成

102
00:06:44,295 --> 00:06:49,545
w[2]=np.random.randn((1,2))*0.01

103
00:06:49,545 --> 00:06:51,404
噢上面应该是zeros

104
00:06:51,404 --> 00:07:00,085
这里还有另外一个结论分享给大家
有时候会有比0.01更为合适的数值

105
00:07:00,085 --> 00:07:04,304
当你在训练一个仅含一个隐藏层的神经网络时

106
00:07:04,304 --> 00:07:09,129
显而易见 0.01这个数值在类似于这样不含过多隐藏层的

107
00:07:09,129 --> 00:07:12,392
浅层神经网络中是非常合适的

108
00:07:12,392 --> 00:07:15,705
但当你要训练一个非常非常复杂的深度神经网络时

109
00:07:15,705 --> 00:07:19,294
我们会使用一个不同的数值

110
00:07:19,294 --> 00:07:23,642
在下周的课程中我们会简单说一下

111
00:07:23,642 --> 00:07:27,925
如何根据不同的情况
来挑一个与0.01不同的数值

112
00:07:27,925 --> 00:07:32,008
无论如何
通常情况我们的计算结果都会是一个相对小的数值

113
00:07:32,008 --> 00:07:34,584
所以这周我们就先讲到这里

114
00:07:34,584 --> 00:07:38,348
从这期的教学视频中
你应该学到了如何设计一个

115
00:07:38,348 --> 00:07:42,430
含有单独隐藏层的神经网络
参数初始化 使用前向传播进行预测

116
00:07:42,430 --> 00:07:45,445
以及在梯度下降时使用反向传播中

117
00:07:45,445 --> 00:07:46,275
涉及的导数计算

118
00:07:46,275 --> 00:07:48,654
所以现在你应该能完成课后测试和

119
00:07:48,654 --> 00:07:51,166
本周的编程练习

120
00:07:51,166 --> 00:07:52,143
祝你好运 我希望各位能喜欢我的课程
在其中学到知识

121
00:07:52,143 --> 00:07:54,802
也希望在后几期课程中能继续见到各位

122
00:07:54,802 --> 00:07:57,728
翻译 | 审阅
Cousera Global Translator Community