1
00:00:00,060 --> 00:00:04,380
在上节课中 我们介绍了

2
00:00:02,250 --> 00:00:06,150
L层的深度神经网络

3
00:00:04,380 --> 00:00:08,550
以及用于描述深度神经网络所使用的

4
00:00:06,150 --> 00:00:10,650
各种标记符号 在本次视频中 我们会讲

5
00:00:08,550 --> 00:00:13,769
怎样在深度神经网络中进行前向传播和反向传播

6
00:00:10,650 --> 00:00:16,440
和以往一样 我们先来看一下

7
00:00:13,769 --> 00:00:18,660
对于单独的训练样本X

8
00:00:16,440 --> 00:00:21,330
如何实现前向传播

9
00:00:18,660 --> 00:00:22,920
之后我们会讨论

10
00:00:21,330 --> 00:00:24,810
向量化的情况

11
00:00:22,920 --> 00:00:26,849
这种情况下我们会学到如何同时对

12
00:00:24,810 --> 00:00:29,660
整个训练集进行前向传播

13
00:00:26,849 --> 00:00:32,579
假设给定一个单一的训练样本x

14
00:00:29,660 --> 00:00:34,800
我们会这样来计算第一个隐藏层的激活函数

15
00:00:32,579 --> 00:00:42,329
对这里的第一个隐藏层

16
00:00:34,800 --> 00:00:48,239
先计算z[1]=w[1]*x+b[1]

17
00:00:42,329 --> 00:00:51,120
在这里划出来的w[1]和b[1]是

18
00:00:48,239 --> 00:00:53,879
影响第一层激活函数的参数

19
00:00:51,120 --> 00:00:56,899
这是神经网络中的第一个

20
00:00:53,879 --> 00:00:59,280
隐藏层 然后来计算

21
00:00:56,899 --> 00:01:04,979
这一层的激活函数 等于

22
00:00:59,280 --> 00:01:06,810
g(z[1]) 激活函数g

23
00:01:04,979 --> 00:01:09,090
的上标取决于你现在在哪一层

24
00:01:06,810 --> 00:01:11,010
我们现在正在计算第一个隐藏层的

25
00:01:09,090 --> 00:01:12,689
激活函数 所以上标为1

26
00:01:11,010 --> 00:01:13,320
现在我们已经得出了

27
00:01:12,689 --> 00:01:18,360
第一个隐藏层的计算公式

28
00:01:13,320 --> 00:01:24,470
第二个隐藏层怎么处理呢 我把它框起来

29
00:01:18,360 --> 00:01:32,189
这里就是z[2]=w[2]a[1]+b[2]

30
00:01:24,470 --> 00:01:34,950
所以第二层的激活函数

31
00:01:32,189 --> 00:01:39,180
就是权重参数矩阵w乘以

32
00:01:34,950 --> 00:01:44,270
第一层的输出函数 也就是我划出的这个值

33
00:01:39,180 --> 00:01:49,579
加上第二层的偏差向量

34
00:01:44,270 --> 00:01:55,770
然后a[2]等于z[2]代入激活函数

35
00:01:49,579 --> 00:01:57,990
这就是第二层的计算公式

36
00:01:55,770 --> 00:02:00,299
后面几层以此类推 直到输出层

37
00:01:57,990 --> 00:02:06,240
这个例子中输出层是第四层

38
00:02:00,299 --> 00:02:09,959
在第四层的计算中

39
00:02:06,240 --> 00:02:11,780
z[4]等于权重参数矩阵w[4]

40
00:02:09,959 --> 00:02:14,569
乘以上一层的激活函数

41
00:02:11,780 --> 00:02:23,930
再加上这个偏差向量

42
00:02:14,569 --> 00:02:26,720
类似的 将z[4]代入激活函数就是a[4]

43
00:02:23,930 --> 00:02:29,900
这样你就推导出了

44
00:02:26,720 --> 00:02:35,390
输出值y帽 另外要注意一下

45
00:02:29,900 --> 00:02:38,270
这里的X也等于a[0]

46
00:02:35,390 --> 00:02:41,209
因为我们也可以将输入的特征向量x

47
00:02:38,270 --> 00:02:44,000
看作是第0层的激活函数输出

48
00:02:41,209 --> 00:02:47,000
所以我们这里就用a[0]来取代x

49
00:02:44,000 --> 00:02:48,709
然后你就会发现

50
00:02:47,000 --> 00:02:53,980
所有屏幕上的等式看起来形式都相同

51
00:02:48,709 --> 00:03:02,750
我们来总结一下通用规则

52
00:02:53,980 --> 00:03:05,750
z[l]=w[l]*a[l-1]+b[l]

53
00:03:02,750 --> 00:03:10,630
其次是l层的激活函数

54
00:03:05,750 --> 00:03:16,850
a[l]就是将z[l]代入到激活函数g中

55
00:03:10,630 --> 00:03:20,120
这两个就是通用的前向传播等式

56
00:03:16,850 --> 00:03:23,540
我们已经完成了对单一训练样本

57
00:03:20,120 --> 00:03:26,299
的前向传播通用公式地推导

58
00:03:23,540 --> 00:03:29,660
下面我们来看看怎样将其

59
00:03:26,299 --> 00:03:32,720
向量化后应用在整个训练集中

60
00:03:29,660 --> 00:03:35,030
公式看起来和之前很类似

61
00:03:32,720 --> 00:03:40,060
我们先来看第一层

62
00:03:35,030 --> 00:03:48,410
使用大写字母

63
00:03:40,060 --> 00:03:54,650
Z[1]=W[1]X+b[1] 而A[1]=g[1](Z[1])

64
00:03:48,410 --> 00:03:57,920
要记住X=A[0]

65
00:03:54,650 --> 00:03:59,959
其实这些大写字母只是意味着把

66
00:03:57,920 --> 00:04:01,850
训练样本放入了矩阵的不同列中

67
00:03:59,959 --> 00:04:05,450
这里我们一样可以

68
00:04:01,850 --> 00:04:08,269
用A[0]来代替X

69
00:04:05,450 --> 00:04:08,720
下一层的公式看起来

70
00:04:08,269 --> 00:04:16,720
很相似

71
00:04:08,720 --> 00:04:21,980
Z[2]=W[2]A[1]+b[2]

72
00:04:16,720 --> 00:04:24,530
A[2]=g[2](Z[2])

73
00:04:21,980 --> 00:04:28,370
我们将这些向量 例如z和a等

74
00:04:24,530 --> 00:04:29,810
把它们堆叠起来变成矩阵

75
00:04:28,370 --> 00:04:34,310
这是第一个训练样本的z向量

76
00:04:29,810 --> 00:04:37,310
这是第二个训练样本的z向量

77
00:04:34,310 --> 00:04:39,830
以此类推直到第m个训练样本

78
00:04:37,310 --> 00:04:43,700
把这些都堆叠在列中形成矩阵

79
00:04:39,830 --> 00:04:47,390
将Z大写 我们称之为Z矩阵

80
00:04:43,700 --> 00:04:50,000
对矩阵A做类似的操作

81
00:04:47,390 --> 00:04:52,040
将所有的训练样本

82
00:04:50,000 --> 00:04:53,720
从左到右堆叠起来形成矩阵

83
00:04:52,040 --> 00:04:59,450
然后在整体向量化完成之后

84
00:04:53,720 --> 00:05:03,200
我们能得到Yhat=g(Z[4])

85
00:04:59,450 --> 00:05:04,670
这也等于A[4]

86
00:05:03,200 --> 00:05:08,000
这就是将所有训练样本经过深度神经网络

87
00:05:04,670 --> 00:05:09,980
生成的预测值水平堆叠起来

88
00:05:08,000 --> 00:05:12,590
下面来总结一下我们用过的的标记符号

89
00:05:09,980 --> 00:05:17,720
我在这里修改一下

90
00:05:12,590 --> 00:05:19,820
把小写z和a替换成大写的Z和A

91
00:05:17,720 --> 00:05:22,070
噢这个小写的z看上去

92
00:05:19,820 --> 00:05:23,810
已经像是大写的Z了

93
00:05:22,070 --> 00:05:25,790
这样我们就在整个训练集上

94
00:05:23,810 --> 00:05:29,060
得到了向量化的前向传播公式

95
00:05:25,790 --> 00:05:32,990
这里的A[0]=X

96
00:05:29,060 --> 00:05:35,240
如果你仔细观察

97
00:05:32,990 --> 00:05:37,670
这个向量化的过程

98
00:05:35,240 --> 00:05:40,370
就会发现看起来很像for循环

99
00:05:37,670 --> 00:05:44,360
当i取值范围是1到4时

100
00:05:40,370 --> 00:05:47,000
当l取值范围是1到总层数大写L时

101
00:05:44,360 --> 00:05:48,950
去计算第一层的激活函数

102
00:05:47,000 --> 00:05:51,860
接着是第二层

103
00:05:48,950 --> 00:05:54,370
然后类推至第三层和第四层

104
00:05:51,860 --> 00:05:56,660
看起来在这里就有个for循环

105
00:05:54,370 --> 00:05:58,550
我们之前讲过 在使用神经网络时

106
00:05:56,660 --> 00:06:00,770
要尽可能避免使用for循环

107
00:05:58,550 --> 00:06:03,290
但这是唯一的一处

108
00:06:00,770 --> 00:06:05,060
我觉得除了使用for循环以外

109
00:06:03,290 --> 00:06:06,590
并没有更好的实现方法

110
00:06:05,060 --> 00:06:09,080
所以当你在实现前向传播的时候

111
00:06:06,590 --> 00:06:10,700
使用一个for循环是可以接受的

112
00:06:09,080 --> 00:06:12,740
它被用来计算第一层的激活函数

113
00:06:10,700 --> 00:06:15,050
然后是第二层和第三层

114
00:06:12,740 --> 00:06:17,210
至今为止没有人提出

115
00:06:15,050 --> 00:06:19,970
而且我也不觉得有别的方法

116
00:06:17,210 --> 00:06:23,060
可以让我们不通过for循环来完成

117
00:06:19,970 --> 00:06:24,620
从1到L层的计算 也就是整个

118
00:06:23,060 --> 00:06:27,830
深度神经网络的计算工作

119
00:06:24,620 --> 00:06:30,980
所以在这里使用for循环

120
00:06:27,830 --> 00:06:32,690
是没有问题的

121
00:06:30,980 --> 00:06:35,300
到这里 深度神经网络的标记符号

122
00:06:32,690 --> 00:06:37,760
和如何在该网络中进行

123
00:06:35,300 --> 00:06:39,680
前向传播已经讲完了

124
00:06:37,760 --> 00:06:41,900
如果你觉得有些内容

125
00:06:39,680 --> 00:06:44,000
看起来很眼熟

126
00:06:41,900 --> 00:06:45,830
那是因为其实这些内容

127
00:06:44,000 --> 00:06:47,750
和我们之前提到的只有

128
00:06:45,830 --> 00:06:50,750
一个隐藏神经层的神经网络很相似

129
00:06:47,750 --> 00:06:53,420
只是将它重复了几次而已

130
00:06:50,750 --> 00:06:55,420
下期预告 当我们

131
00:06:53,420 --> 00:06:57,860
在构建自己的神经网络时

132
00:06:55,420 --> 00:06:59,450
如果试图尽量减少

133
00:06:57,860 --> 00:07:01,580
发生问题的概率

134
00:06:59,450 --> 00:07:03,500
就必须要很系统很仔细地

135
00:07:01,580 --> 00:07:05,300
处理相关矩阵的维度

136
00:07:03,500 --> 00:07:07,280
当我自己在写代码的时候

137
00:07:05,300 --> 00:07:08,960
通常会拿一张纸

138
00:07:07,280 --> 00:07:11,480
仔细地考虑我正在使用的矩阵的维度

139
00:07:08,960 --> 00:07:13,940
我们会在下一个视频教程中

140
00:07:11,480 --> 00:07:16,570
学习如何使用这种方法

141
00:07:13,940 --> 00:07:16,570
GTC字幕组翻译