1
00:00:00,000 --> 00:00:03,224
深層学習と脳には<br />どのような関係があるのでしょうか？

2
00:00:03,224 --> 00:00:06,918
実際のところ、<br />ほとんど関係はないと言えます。

3
00:00:06,918 --> 00:00:11,539
ではなぜ人々が深層学習と人間の<br />脳を関連づけるのか

4
00:00:11,539 --> 00:00:13,400
少し見てみましょう。

5
00:00:13,400 --> 00:00:17,624
ニューラルネットワークを実装する場合、<br />フォワードプロパゲーションと

6
00:00:17,624 --> 00:00:18,302
バックプロパゲーションを実装します。

7
00:00:18,302 --> 00:00:22,774
そして、これらの方程式が複雑な関数の<br />最急降下を行なっていることを

8
00:00:22,774 --> 00:00:27,122
直感的に理解するのが困難であることから、

9
00:00:27,122 --> 00:00:30,857
脳のようであるという類推が、

10
00:00:30,857 --> 00:00:34,380
実際に行われていることに対して過剰に<br />単純化された説明になっているのだと思います。

11
00:00:34,380 --> 00:00:37,707
しかしこの単純さは、<br />例えばメディアや公に発表する

12
00:00:37,707 --> 00:00:41,898
立場にある人にとってはある種魅力的で、

13
00:00:41,898 --> 00:00:44,808
実際のところ大衆の想像力を<br />かき立てる役に立っています。

14
00:00:44,808 --> 00:00:49,214
ロジスティック回帰と<br />シグモイド活性関数の間には

15
00:00:49,214 --> 00:00:54,468
非常に緩い類似性があり、

16
00:00:54,468 --> 00:00:58,427
ここに単一のニューロンの絵があります。

17
00:00:58,427 --> 00:01:02,326
この生物学的なニューロンは脳の細胞であり、

18
00:01:02,326 --> 00:01:07,941
x1, x2, x3 もしくは a1, a2, a3 など

19
00:01:07,941 --> 00:01:12,056
他のニューロンから電気信号を受信します。

20
00:01:12,056 --> 00:01:17,419
そしてシンプルな閾値計算を行い、<br />もしこのニューロンが発火した場合、

21
00:01:17,419 --> 00:01:23,366
軸索からこの長いワイヤーを通じて、<br />他のニューロンに電気信号を送信します。

22
00:01:23,366 --> 00:01:28,181
つまり、ニューラルネットワーク内の<br />単一ニューロンと、

23
00:01:28,181 --> 00:01:31,365
右図の生物学的なニューロンとの間には

24
00:01:31,365 --> 00:01:34,809
非常に単純な類似性があると言えます。

25
00:01:34,809 --> 00:01:39,356
しかし今日、神経科学者でさえ、<br />たった一つのニューロンが一体何をやっているのか、

26
00:01:39,356 --> 00:01:41,441
ほとんど理解できていない<br />という状況だと思います。

27
00:01:41,441 --> 00:01:45,532
単一のニューロンは、<br />神経科学で説明できることよりも

28
00:01:45,532 --> 00:01:48,141
はるかに複雑なもののように見えます。

29
00:01:48,141 --> 00:01:52,719
ロジスティック回帰と部分的に<br />似たところはあるものの、

30
00:01:52,719 --> 00:01:56,866
単一のニューロンが何をしているかに関して、

31
00:01:56,866 --> 00:01:59,101
今日誰も理解していない部分が多くあります。

32
00:01:59,101 --> 00:02:00,091
例えば、

33
00:02:00,091 --> 00:02:05,713
脳内のニューロンがどのように学習するのかは、<br />未だ解明されていないプロセスです。

34
00:02:05,713 --> 00:02:09,897
そして、人の脳がバックプロパゲーションや<br />最急降下法のようなアルゴリズムに基づくのか、

35
00:02:09,897 --> 00:02:14,269
あるいは根本的に異なる<br />学習原理に基づくのかは、

36
00:02:14,269 --> 00:02:18,277
全く解明されていません。

37
00:02:18,277 --> 00:02:23,515
私が深層学習について考えるとき、<br />それは教師あり学習において、

38
00:02:23,515 --> 00:02:28,679
柔軟かつ複雑な関数の学習、<br />入力に対する出力のマッピングの学習に

39
00:02:28,679 --> 00:02:32,614
優れているものとして考えます。

40
00:02:32,614 --> 00:02:37,205
脳との類推に関しては、<br />一時期は有用だったものの、

41
00:02:37,205 --> 00:02:41,960
この類推がもはや成り立たない段階まで<br />この分野が進展してきたと考えます。

42
00:02:41,960 --> 00:02:45,633
私はこの類推をあまり<br />使用しないようにしています。

43
00:02:45,633 --> 00:02:48,809
ニューラルネットワークと脳に関しては以上です。

44
00:02:48,809 --> 00:02:52,554
画像処理の分野においては、<br />他の深層学習の適用分野と比較して

45
00:02:52,554 --> 00:02:56,684
脳から得たインスピレーションが<br />多少多いかも

46
00:02:56,684 --> 00:02:57,333
しれませんが、

47
00:02:57,333 --> 00:03:02,835
私個人的には以前ほど<br />脳との類推は使用していません。

48
00:03:02,835 --> 00:03:05,216
このビデオは以上です。

49
00:03:05,216 --> 00:03:08,423
ここまでで、深層学習における<br />フォワードプロパゲーション、

50
00:03:08,423 --> 00:03:11,182
バックプロパゲーション、<br />最急降下法について学習しました。

51
00:03:11,182 --> 00:03:13,408
ではプログラミング課題で<br />最善を尽くしてください。そして

52
00:03:13,408 --> 00:03:17,109
２番目のコースで<br />お会いできるのを楽しみにしています。