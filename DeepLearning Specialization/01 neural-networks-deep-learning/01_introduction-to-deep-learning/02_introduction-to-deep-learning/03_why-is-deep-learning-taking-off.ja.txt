深層学習の裏にある基本的な考え方が 何十年も前からあったのなら、 なぜ今になって
これほど人気が出たのでしょうか。 この動画では、 何が深層学習の繁栄に貢献したのかについて
いくつかお話しします。 これは、あなたが実際にこれらのツールを使う機会を
見つける助けになるはずです。 この二、三年、私は多くの人から
このような質問を受けました。 「Andrew、何で深層学習が
急にうまく行きだしたの？」 こう聞かれたとき、 私はたいてい次のような図を描きます。 水平方向の軸が私たちが持っているデータの量を示す というグラフを描いてみます。 そして垂直方向の軸は、全ての学習アルゴリズムの
パフォーマンスを表します。 これは、スパム分類の正確さだったり、 広告クリック予測機能の性能だったり、 自動運転車が他の車の場所を推測するための ニューラルネットワークの
正確さだったりというものです。 昔からあった学習アルゴリズム、
例えばロジスティック回帰などの パフォーマンスを 持っているデータの量の関数としてグラフにすると、 このような曲線になるでしょう。 データを増やすと
しばらくパフォーマンスは上がりますが、 それ以降はあまり変わりません。 うまく描けませんでしたが、
水平な線だと思ってください。 そんなにデータがたくさんあっても
どうしたらいいのかわからないという感じです。 この社会では、 いくつもの問題に対して、 20年ほど前は比較的少ないデータ量しか
なかったのですが、 今ではかなり多くのデータを
持つことができるようになりました。 そのほとんどは、デジタル化のおかげです。 今日、人間の活動の多くがデジタルの領域にあります。 コンピュータやウェブサイト、モバイルのアプリで
多くの時間を費やしますし、 そのようなデジタルデバイスでの活動は
データを作り出します。 携帯に備え付けられた
安いカメラや加速度センサーなど、 「モノのインターネット」における
あらゆる種類のセンサーのおかげで、 更にもっと多くのデータを
集めることができています。 ここ20年間にわたって、 従来の学習アルゴリズムが
効率的に活用できる範囲を超えた 膨大な量のデータを蓄積できるようになったのです。 ニューラルネットワークについて見てみます。 小さいニューラルネットワークを訓練すれば パフォーマンスはこのようになります。 もう少し大きなニューラルネットワークを
訓練すれば、 これを中サイズの
ニューラルネットワークと呼びますね、 そのパフォーマンスは少し良くなります。 そしてとても大きなニューラルネットワークを
訓練すれば、 そのパフォーマンスは大抵どんどん良くなります。 ここから、いくつかのことがわかります。 まず、高レベルのパフォーマンスを得たければ、
二つのことが必要だということです。 一つは、膨大な量のデータを活用できるような 十分な大きさのニューラルネットワークを
訓練できなければなりません。 二つ目に、
x軸でこの位置にいなければなりません。 つまり、多くの量のデータが必要なのです。 「スケール」が深層学習の進歩を導いた
とよく言います。 「スケール」というのは、 ニューラルネットワークの大きさ、
つまり隠れ層が多くて 多くのパラメータがあり、 多くの繋がりがあるニューラルネットワークに加えて、
データの量のことも指しています。 実際、今日ニューラルネットワークにおいて
良いパフォーマンスを得るための最も確実な方法は、 より大きなネットワークを訓練するか
データを増やすかであることが多いです。 それもある程度で終わってしまいます、
なぜならデータが尽きるか ニューラルネットワークが大きいため
訓練時間が長くなりすぎるので。 しかし、ただスケールを改善するということが
深層学習においては非常に重要なのです。 この図をもう少し技術的に正確にするために、 少し付け足させてください。 x軸にデータの量を書きました。 専門的に言うと
これはラベル付けされたデータ、 つまり入力であるxとラベルyの両方を持つ 教師サンプルのデータを意味しています。 このコースで後ほど使うことになる
いくつかの表記法を紹介します。 教師サンプルの数を小文字のmで表します。 すると水平方向の軸がmとなります。 この図についてもう少し詳しく言うと、 ここの教師サンプルが少ない範囲では、 アルゴリズムの順番は
はっきりとは決まっていません。 なので、もし教師サンプルが少なければ パフォーマンスは、あなたが特徴量を
どれだけうまく操作できるかにかかっています。 もしSVMを使っていて
特徴量をよりうまく操作している人と 大きなニューラルネットワークを
使っている人がいれば、 この教師サンプルが少ない範囲の中では SVMの方がうまくいくということも
ありえます。 なので、図の左側のこの領域では、 アルゴリズム同士の相対的な位置は
それほどしっかり決まっていませんし、 パフォーマンスはあなたの特徴量を操作する技術と アルゴリズムの細かい部分を操作する力に
かかっています。 この教師サンプルが大きい領域、 つまりmが大きくなる右側の領域において はじめて大きなニューラルネットワークが 一貫して強くなるのです。 もしあなたが友だちに 「どうしてニューラルネットワークは
こんなに成功しているの？」と聞かれたら ぜひこの図を友だちにも
描いてあげてくださいね。 深層学習の発展の初期には、 大切なのは データのスケールと計算のスケールでした。 CPUやGPUで大きなニューラルネットワークを
訓練できるというだけで 大きな発展を遂げることができたのです。 しかし、特にここ数年で、 アルゴリズムの凄まじい革新も起こっています。 そのことも軽視することはできません。 興味深いことに、
アルゴリズムの革新の多くは ニューラルネットワークをもっと速く
動作させようとするものです。 具体的な例としては、 ニューラルネットワークにおける
大きな進歩の一つは こんな形のシグモイド関数から
ReLU関数に変えたことです。 ReLU関数については前の動画で触れましたね、
こういう形のものです。 今から言うことについては、細かい意味が
わからなくても気にしないでください。 シグモイド関数を機械学習で使うことによって
起こる問題の一つは、 このような傾きがほぼ0の領域が あるということです。 そのせいで、学習はとても遅くなってしまいます。 なぜなら、
最急降下法を実装したときに勾配が0に近いと、 パラメータはゆっくりとしか変わらないので
学習も遅くなるからです。 それに対して、ニューラルネットワークの
活性化関数と呼ばれるものを Rectified Linear Unit、
略してReLU関数に変えることで、 入力が正の場合は勾配は常に1になります。 なので、勾配が徐々に0になっていく
というようなことがなくなります。 ここの左側での線の傾きは0ですが、 ただシグモイド関数からReLU関数に変えるだけで 最急降下法と呼ばれるアルゴリズムは
とても速く動くようになるのです。 これはアルゴリズムの革新の中でも
比較的シンプルな例です。 しかしこの革新は最終的に、 計算を大きく手助けしました。 このように、速くコードを走らせるために アルゴリズムを変更した例は多くあります。 そのおかげで、私たちは 大きなニューラルネットワークや
多くのデータがあったとしても ニューラルネットワークを手頃な時間で
訓練できるようになったのです。 速い計算が大切なもう一つの理由は、 ニューラルネットワークを訓練する過程が
反復であるということです。 まずニューラルネットワークの
構造のアイデアがあり、 そのアイデアをコードにして実装します。 そしてそれを実験的に実行し、 それによってニューラルネットワークが
どれほどうまくいっているかがわかり、 それを見てまたニューラルネットワークに
細かい変更を加えるために戻ります。 そうやってこのサイクルを何度も繰り返します。 ニューラルネットワークの訓練に
長い時間がかかれば、 このサイクルを回すのにも
長い時間がかかります。 アイデアが浮かんでからそれを試すのに 10分かかるか一日かかるか、 もしくはひと月かかるかでは、 生産性に大きな違いが出てきます。 実際、ひと月かかることもたまにあります。 10分か一日で結果がわかれば、
より多くのアイデアを試すことができて、 そのアプリケーションにおいて
うまくいくニューラルネットワークを 発見できる可能性が高くなります。 このように、速い計算は 実験の結果を得るスピードを速くするのを
大いに助けてきました。 これは、ニューラルネットワークを実践する人が、 そして深層学習の研究者が 速く反復し、速くアイデアを改善するのに
役立ってきました。 そしてもちろん、これら全てが 驚くべきことに
毎月のように新しいアルゴリズムを開発している 深層学習の研究コミュニティ全体の力となりました。 これらのこと全てが、 驚くべきほど新しいアルゴリズムを発明し、 日進月歩で発展している 深層学習の研究コミュニティ全体にとって
大きな恩恵となっています。 ここまでのことが、
深層学習を興隆させてきた要因です。 素晴らしいことに、これらの要因は 深層学習を更に改善するために
活発にはたらいています。 データにしても、この社会は多くのデジタルデータを
生み出し続けています。 計算にしても、GPUやより速いネットワーク、 いろいろな種類のハードウェアなど、 特化したハードウェアが盛んになっています。 計算という点においては、 私たちが巨大なニューラルネットワークを
構築する能力は 上がり続けていると自信を持って言えます。 アルゴリズムにしても、 深層学習の研究コミュニティ全体の前線が
驚異的に進歩しているので、 楽観的になっていいと思うのです。 私自身も、深層学習はこれから何年も 改善を続けるだろうと
楽観的に考えています。 では、このセクションの最後の動画に行って このコースで何を学んだかについて
もう少しお話ししましょう。