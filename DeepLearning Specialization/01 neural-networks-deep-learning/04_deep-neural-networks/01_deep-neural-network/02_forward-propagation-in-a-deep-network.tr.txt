Son videoda, şu konuları tarif etmiştik genel derin yapay sinir ağlarında ve bu gibi ağlarda kullanılan gösterim hakkında konuştuk. Bu videoda ve ileri yayılmayı nasıl yapacağınızı her zamanki gibi derin ağlar olacak. Gelin önce ileri yayılım nedir, nasıl yapılır Verilen tek X girdisine göre nasıl hesaplanır üzerinde konuşacağız. vektör halindeki girdi üzerinde ileri yayılımın nereye uygulanacağını tüm eğitim seti aynı zamanda Verilen tek X girdisinde inceleyeceğiz. Daha sonra aktivasyon değerlerini nasıl hesaplayacağımızı göreceğiz, İlk katmanda aktivasyon hesaplarken z1 eşittir W1 çarpı X artı b1, yani W1 ve b1 parametreleri bu birinci katmanda aktivasyonları etkiler. burada tam yerinde onlardan bir tanesi yapay sinir ağıdır, ve hesaplayacağınız ilgili katmana ait aktivasyonlar Z1'in G'sine eşit olmalıdır ve hedef fonksiyon G, bulunduğun katmana bağlıdır ve AB indeksinin aktivasyon fonksiyonu 1. tabakadan başlar. Eğer ilk katmandan başlayarak aktivasyonları hesapladıysanız 2. katman ne olacak? Bu katman için Z2'yi hesaplamak gerekir bu da W2 ile A1 ve B2'nin toplanmasıdır. Sonuçta 2. katmanın aktivasyonu Ağırlık (W) matrisi ile 1. katman sonuçlarının çarpılmına . 2. katmanın bias vektörünün eklenmesiyle bulunur. A2, aktivasyon fonksiyonudur ve z2'ye uygulanır. Bu 2. katman için böyledir, diğer katmanlar için de çıktı katmanına gelene kadar aynı şekilde devam edilir. Burada çıktı katmanı 4. katmanda Z4, 4. katmanın bu katmana özgü parametreleri ile önceki katmandan gelen aktivasyonların çarpımının ile bias vektörüyle toplanması ve A4, G(z4) fonksiyonuna eşittir. Bu şekilde tahmini Y (şapkalı) çıktısını hesaplarsınız. Buradaki X sıfıra eşittir, çünkü girdi olan özellik vektörü aynı zamanda 0. katmanın aktivasyonlarıdır. Burada X'in üzerini çizip yerine 0 yazarsak tüm bu denklemler aynı görünecektir. Genel bir kural olarak, Z[L] W[L] ile a[L-1]'in çarpımına B[L] eklenmesidir. Ve bu katmanın aktivasyonları Z değerlerine uygulanan aktivasyonlara eşittir. Bu yayılım denklemleri için genel kuraldır. Tek öğrenme örneği için tüm bu işlemleri yaptık. Bütün veriseti için bu işlemleri vektör olarak nasıl yapmalıyız? Denklemler öncekiler gibi görünecektir, ilk katman için büyük harf Z1, büyük harf W1 ile büyük harf X ile çarpılarak B1 ile toplanmasıdır, ve A1 G(Z1) fonksiyonudur. Burada aklınızda X'in 0'a eşit olduğunu aklınızda bulundurun ve bunlar sadece eğitim örnekleridir. farklı sütunlarda bulunan X'in üzerini çizelim ve yerine 0 koyalım, sonraki katman benzerdir Z2, W2 ile A2 çarpımına B2 eklenmesi ve a2 G(Z2) dir. Bu Z ve A vektörlerini alıp üst üste yuğduktan sonra bu ilk eğitim örneği için Z vektörü ikinci eğitim örneği için ve böyle devam eder ta ki M. eğitim örneğine kadar bunlar sütunlar halinde birikir ve büyük harf Z ile gösterilir. Büyük harf A, ve büyük harf X için de benzer şekildedir. Bütün eğitim örnekleri sütun vektörüdür. Soldan sağa doğru birbirine eklenerek büyürler ve bu işlemler sonunda Y(şapka) elde edilir bu da G(Z4) ve bu da A4'a eşittir. Bu tahminlerdir ki onlar horizontal olarak dizilen eğitim örneklerinin birikimidir. Notasyonu özetlemek için burada bazı şeyleri değiştireceğim, küçük harf z ve a'yı büyük harf karşılıklarıyla değiştireceğim bu da büyük Z'nin karşılığı ve ileri yayılımın hesapladığınız vektör halini verecektir, tüm veri seti için, A0'ın X olduğu durumda. Eğer bu vektör yapma kodlamasına bakarsanız burada bir "for " döngüsü olması gerekir. L eşittir 1'den 4'e, L 1'e eşitken büyük L iken ve hesaplanmalıdır ki (her katman) katman 1, sonra katman 2 için aktivasyonlar sonra katman 3 ve katman 4 için bu nedenle "for" döngüsüne ihtiyaç vardır. Biliyorum ki ağlarınızı kodlarken biz genellikle belirgin "for" döngülerinden kurtulmayı isteriz ama bu örnek tek yerdir ki burada bence başka alternatifi yoktur belirgin "for" döngüsünden bence başka kodlama haricinde. İleri yayılımı kodlarken bu durum tam olarak doğrudur, for döngüsünün aktivasyonları hesaplamada kullanımı 1. katman, sonra 2. katman ve sonra 3. katman ve bu nedenle bu işi yapmanın başka yolu olduğunu sanmıyorum "for" döngüsü kullanmadan 1 den büyük harf L'ye, 1'den tüm katmanlara ağınızda bulunan, yani bu yer için belirgin for döngüleri normaldir. Derin sinir ağları için notasyon bu kadar, aynı şekilde nasıl ileri yayılım hesaplanacağını gösterdik. Gösterdiğimiz parçalar size biraz tanıdık geliyorsa sebebi bahsettiğimiz her parça daha önce gördüklerinize tek katmanlı sinir ağına benzerdir ve bunu çok kere tekrarlıyoruz. Sonuçta biz derin sinir ağını kodladık, diğer bir yol olarak bug ihtimalini artırmamak için kodlamada düşünmeliyiz çok sistematik ve dikkatli bir şekilde çalıştığınız matris boyutları hakkında Kendi kodumu debug ederken genelde bir kağıt alıp dikkatlice düşünürüm çalıştığım matrisin boyutları üzerinde Bunu nasıl yapacağınızı sonraki videoda öğrenelim.