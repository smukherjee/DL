1
00:00:00,000 --> 00:00:03,840
Ну що ж. Я думає це буде захоплююче

2
00:00:01,800 --> 00:00:06,240
відео. В цьому відео ти побачиш як

3
00:00:03,840 --> 00:00:08,730
реалізувати Градієнтний спуск для нашої

4
00:00:06,240 --> 00:00:10,530
нейронної мережі (НМ) з одним прихованим шаром.

5
00:00:08,730 --> 00:00:12,809
В цьому відео я дам

6
00:00:10,530 --> 00:00:14,639
рівняння, які необхідні для реалізації

7
00:00:12,809 --> 00:00:17,039
зворотного поширення, щоб змусити

8
00:00:14,639 --> 00:00:19,410
Градієнтний спуск працювати. А потім,

9
00:00:17,039 --> 00:00:21,510
в наступному відео, я дам більш

10
00:00:19,410 --> 00:00:24,150
розширене розуміння чому саме ці

11
00:00:21,510 --> 00:00:26,070
рівняння є точними або

12
00:00:24,150 --> 00:00:27,630
правильними рівняннями для обчислення

13
00:00:26,070 --> 00:00:28,289
градієнтів, які необхідні для

14
00:00:27,630 --> 00:00:30,090
нашої НМ.

15
00:00:28,289 --> 00:00:32,520
Тож наша НМ з поки єдиним

16
00:00:30,090 --> 00:00:39,930
прихованим шаром матиме

17
00:00:32,520 --> 00:00:44,760
параметри w[1], b[1], w[2] і b[2]. Просто

18
00:00:39,930 --> 00:00:50,399
нагадаю, що якщо ми маємо nₓ або

19
00:00:44,760 --> 00:00:56,640
n[0] вхідних ознак і n[1] прихованих

20
00:00:50,399 --> 00:00:59,149
вузлів, і n[2] вихідних вузлів (а в нашому

21
00:00:56,640 --> 00:01:05,670
випадку поки n[2]=1),

22
00:00:59,149 --> 00:01:08,880
то матриця w[1] матиме розмір n[1]*n[0].

23
00:01:05,670 --> 00:01:11,250
b[1] буде вектором розміру n[1], який

24
00:01:08,880 --> 00:01:13,350
можна записати як матрицю

25
00:01:11,250 --> 00:01:16,500
розміру n[1]*1, яка є вектором-стовпцем.

26
00:01:13,350 --> 00:01:20,750
Розмір w[2] буде n[2]*n[1].

27
00:01:16,500 --> 00:01:26,759
І розмір b[2] буде n[2]*1.

28
00:01:20,750 --> 00:01:28,590
Повторю, що досі ми

29
00:01:26,759 --> 00:01:30,930
бачили лише приклади, де n[2]=1,

30
00:01:28,590 --> 00:01:36,930
тобто де був один єдиний

31
00:01:30,930 --> 00:01:39,570
прихований вузол. Також ми маємо

32
00:01:36,930 --> 00:01:41,340
функцію Витрат для НМ.

33
00:01:39,570 --> 00:01:44,220
І поки, мушу нагадати, ми

34
00:01:41,340 --> 00:01:48,659
реалізовуємо двійкову класифікацію. Тож

35
00:01:44,220 --> 00:01:51,740
в цьому випадку Витрати наших параметрів

36
00:01:48,659 --> 00:01:57,090
наступні: середнє - 1/m -

37
00:01:51,740 --> 00:01:59,969
від оцієї функції Втрати. Тож

38
00:01:57,090 --> 00:02:03,420
L тут є втратою коли НМ

39
00:01:59,969 --> 00:02:06,240
передбачає ŷ. Насправді це - a[2]

40
00:02:03,420 --> 00:02:07,649
коли емпірична позначка [ground truth label] =y.

41
00:02:06,240 --> 00:02:09,629
Тож, реалізуючи двійкову

42
00:02:07,649 --> 00:02:12,510
класифікацію, функція Втрати може бути

43
00:02:09,629 --> 00:02:15,030
точно такою ж, 
яку ми вже використовували для логістичної регресії (ЛР).

44
00:02:12,510 --> 00:02:18,420
Отже, щоб натренувати параметри нашого

45
00:02:15,030 --> 00:02:21,450
алгоритму, нам потрібно застосувати Градієнтний

46
00:02:18,420 --> 00:02:23,189
спуск. Коли ми тренуємо НМ

47
00:02:21,450 --> 00:02:25,379
дуже важливо ініціалізувати

48
00:02:23,189 --> 00:02:26,129
параметри випадковим чином, а не

49
00:02:25,379 --> 00:02:28,140
рівними 0.

50
00:02:26,129 --> 00:02:30,030
Ми побачимо пізніше чому це так.

51
00:02:28,140 --> 00:02:32,069
Тож після ініціалізації параметрів

52
00:02:30,030 --> 00:02:34,140
кожен цикл Градієнтного спуску

53
00:02:32,069 --> 00:02:36,780
обчислюватиме передбачення

54
00:02:34,140 --> 00:02:42,359
або, по суті, обчислюватиме ŷ⁽ⁱ⁾

55
00:02:36,780 --> 00:02:44,519
для i від 1 до m. А потім

56
00:02:42,359 --> 00:02:49,440
нам потрібно буде обчислити похідні.

57
00:02:44,519 --> 00:02:51,420
Тож нам треба буде обчислити dw[1] (а ми вже

58
00:02:49,440 --> 00:02:54,359
це бачили), тобто похідну функції Витрат

59
00:02:51,420 --> 00:02:56,489
по параметру w[1].

60
00:02:54,359 --> 00:02:59,220
Також потрібно обчислити ще одну змінну,

61
00:02:56,489 --> 00:03:00,870
що називається db[1] і що є

62
00:02:59,220 --> 00:03:04,109
похідною або нахилом функції Витрат

63
00:03:00,870 --> 00:03:07,349
по змінній b[1] і т.д.

64
00:03:04,109 --> 00:03:10,170
Аналогічно для інших

65
00:03:07,349 --> 00:03:12,629
параметрів w[2] і b[2]. І, нарешті,

66
00:03:10,170 --> 00:03:17,879
корекцію Градієнтного спуску, тобто

67
00:03:12,629 --> 00:03:22,709
скоригувати w[1] як w[1] мінус

68
00:03:17,879 --> 00:03:26,129
навчальний темп α помножений на dw[1]

69
00:03:22,709 --> 00:03:31,620
і скоригувати b[1] як b[1] мінус навчальний темп

70
00:03:26,129 --> 00:03:34,739
помножений на db[1]. І, аналогічно, для w[2] і b[2].

71
00:03:31,620 --> 00:03:36,299
Інколи я використовую тут :=, а

72
00:03:34,739 --> 00:03:38,489
інколи просто =. Обидва

73
00:03:36,299 --> 00:03:40,829
позначення підходять. І оце буде

74
00:03:38,489 --> 00:03:42,510
однією ітерацією Градієнтного спуску і

75
00:03:40,829 --> 00:03:44,280
ми будемо її повторювати певну кількість

76
00:03:42,510 --> 00:03:46,079
разів допоки наші параметри почнуть

77
00:03:44,280 --> 00:03:48,150
збігатись. Отже, в попередніх відео

78
00:03:46,079 --> 00:03:50,099
ми говорили як обчислити

79
00:03:48,150 --> 00:03:51,629
передбачення, як обчислити вихідні дані.

80
00:03:50,099 --> 00:03:54,060
І побачили також як це зробити

81
00:03:51,629 --> 00:03:56,269
у векторизований спосіб. Тож основний момент -

82
00:03:54,060 --> 00:04:00,180
це знати як обчислити ці частинні

83
00:03:56,269 --> 00:04:04,079
похідні dw[1], db[1] і, звичайно,

84
00:04:00,180 --> 00:04:06,780
похідні dw[2] і db[2]. Тож

85
00:04:04,079 --> 00:04:09,419
я дам тобі

86
00:04:06,780 --> 00:04:12,150
рівняння, які потрібні для обчислення

87
00:04:09,419 --> 00:04:14,699
цих похідних і відкладу до

88
00:04:12,150 --> 00:04:17,430
наступного відео, що буде необов'язковим,

89
00:04:14,699 --> 00:04:19,090
більш глибокий огляд того, як ми прийшли

90
00:04:17,430 --> 00:04:21,400
до цих формул.

91
00:04:19,090 --> 00:04:26,169
Давай згадаємо ще раз

92
00:04:21,400 --> 00:04:33,250
рівняння прямого поширення. Тож ми

93
00:04:26,169 --> 00:04:37,900
маємо: Z[1]=w[1]X+b[1],

94
00:04:33,250 --> 00:04:41,680
A[1]= функції активації

95
00:04:37,900 --> 00:04:49,690
відповідного шару, що застосовується поелементно до Z[1],

96
00:04:41,680 --> 00:04:53,530
Z[2]=w[2]A[1]+b[2],

97
00:04:49,690 --> 00:04:55,180
і, нарешті (це все

98
00:04:53,530 --> 00:05:01,210
векторизовано для всього тренувального набору),

99
00:04:55,180 --> 00:05:02,740
A[2]=g[2](Z[2]). І ще раз повторю,

100
00:05:01,210 --> 00:05:04,870
якщо ми реалізуємо двійкову

101
00:05:02,740 --> 00:05:06,610
класифікацію, то ця функція

102
00:05:04,870 --> 00:05:08,560
активації, насправді, має бути сигмоїдою.

103
00:05:06,610 --> 00:05:11,080
Тож я просто допишу це.

104
00:05:08,560 --> 00:05:13,870
Тож це - пряме поширення або

105
00:05:11,080 --> 00:05:15,729
пряме обчислення зліва-направо

106
00:05:13,870 --> 00:05:18,430
нашої НМ. Тепер давай обчислимо

107
00:05:15,729 --> 00:05:24,750
похідні. Тож це - крок зворотного

108
00:05:18,430 --> 00:05:30,750
поширення, який обчислює

109
00:05:24,750 --> 00:05:33,610
dZ[2]=A[2] мінус емпіричне y.

110
00:05:30,750 --> 00:05:36,580
Просто нагадаю, що все це

111
00:05:33,610 --> 00:05:41,289
векторизовано для всіх зразків. Тож матриця Y

112
00:05:36,580 --> 00:05:45,280
має розмір 1*m і містить

113
00:05:41,289 --> 00:05:51,370
по горизонталі всі наші m зразків. 
[y⁽ⁱ⁾ для всіх зразків]

114
00:05:45,280 --> 00:05:55,330
dw[2]= оцьому. Фактично,

115
00:05:51,370 --> 00:05:58,870
оці перші три рівняння дуже

116
00:05:55,330 --> 00:06:00,900
подібні до Градієнтного спуску логістичної

117
00:05:58,870 --> 00:06:00,900
регресії.

118
00:06:00,960 --> 00:06:12,610
...axis=1, keepdims=True...

119
00:06:07,419 --> 00:06:15,580
Невеличке уточнення:

120
00:06:12,610 --> 00:06:18,070
np.sum - команда Python/NumPy, яка

121
00:06:15,580 --> 00:06:21,100
обчислює по певній заданій осі

122
00:06:18,070 --> 00:06:24,810
матриці. В нашому випадку - горизонтально.

123
00:06:21,100 --> 00:06:27,600
А keepdims перешкоджає

124
00:06:24,810 --> 00:06:31,230
Python-у вивести один з тих дивних

125
00:06:27,600 --> 00:06:34,650
масивів 1-ого порядку, що мають розмір

126
00:06:31,230 --> 00:06:37,010
(n,). Тож, присвоюючи keepdims

127
00:06:34,650 --> 00:06:41,280
True, ми пересвідчуємось, що

128
00:06:37,010 --> 00:06:44,580
Python виведе db[2] як вектор

129
00:06:41,280 --> 00:06:47,820
розміру n*1 (технічно, це буде

130
00:06:44,580 --> 00:06:50,130
(n[2], 1)). В цьому випадку це -

131
00:06:47,820 --> 00:06:53,520
просто число 1*1, тобто просто число, тож, можливо,

132
00:06:50,130 --> 00:06:56,790
це не має значення, проте, пізніше, ми побачимо

133
00:06:53,520 --> 00:06:58,500
коли це дійсно має значення. Тож до цього моменту

134
00:06:56,790 --> 00:07:01,320
ми робили щось дуже схоже до ЛР.

135
00:06:58,500 --> 00:07:03,919
Проте, тепер, коли ми продовжуємо

136
00:07:01,320 --> 00:07:14,370
обчислювати зворотне поширення,

137
00:07:03,919 --> 00:07:19,380
необхідно обчислити dZ[1]=w[2]ᵀdZ[2]*g[1]'(Z[1]).

138
00:07:14,370 --> 00:07:20,880
Тож цей вираз g[1]' є

139
00:07:19,380 --> 00:07:22,919
похідною будь-якої

140
00:07:20,880 --> 00:07:25,770
функції активації, яку ми використовуємо для

141
00:07:22,919 --> 00:07:27,030
прихованого шару. А для вихідного шару

142
00:07:25,770 --> 00:07:29,400
я нагадую, що ми реалізуємо двійкову

143
00:07:27,030 --> 00:07:30,780
класифікацію з сигмоїдою,

144
00:07:29,400 --> 00:07:34,620
тож воно вже враховано в

145
00:07:30,780 --> 00:07:39,090
формулі для dZ[2]. Ось це множення є

146
00:07:34,620 --> 00:07:43,050
поелементним. Оцей вираз

147
00:07:39,090 --> 00:07:46,950
буде матрицею n[1]*m.

148
00:07:43,050 --> 00:07:48,990
А оцей вираз - поелементна

149
00:07:46,950 --> 00:07:52,680
похідна - також буде

150
00:07:48,990 --> 00:07:54,720
матрицею n[1]*m. Тож оце множення є

151
00:07:52,680 --> 00:07:59,669
поелементним добутком двох

152
00:07:54,720 --> 00:08:08,490
матриць. І, нарешті, dw[1]= оцьому,

153
00:07:59,669 --> 00:08:18,950
а db[1]= оцьому: ...np.sum(

154
00:08:08,490 --> 00:08:21,900
dz[1], axis=1, keepdims

155
00:08:18,950 --> 00:08:23,430
=True. Тож якщо

156
00:08:21,900 --> 00:08:27,210
в попередньому рівнянні keepdims, можливо,

157
00:08:23,430 --> 00:08:28,590
не грав значної ролі (коли n[2] мало розмір

158
00:08:27,210 --> 00:08:35,729
1*1, тобто було просто дійсним

159
00:08:28,590 --> 00:08:38,370
числом), то тут db[1] є

160
00:08:35,729 --> 00:08:40,110
вектором n[1]*1, тож, ми хочемо, щоб Python команда

161
00:08:38,370 --> 00:08:43,110
np.sum вивела щось такого ж

162
00:08:40,110 --> 00:08:46,529
розміру, а не масив першого

163
00:08:43,110 --> 00:08:48,360
порядку, що має отой розмір [(n,)] і може

164
00:08:46,529 --> 00:08:50,580
зіпсувати подальші

165
00:08:48,360 --> 00:08:53,310
обчислення. Інший спосіб

166
00:08:50,580 --> 00:08:56,910
не вдаватись до параметрів keepdims - це

167
00:08:53,310 --> 00:09:00,060
явно викликати reshape, щоб перебудувати

168
00:08:56,910 --> 00:09:04,400
результат np.sum у потрібний

169
00:09:00,060 --> 00:09:08,310
розмір, який має мати db.

170
00:09:04,400 --> 00:09:11,339
Отже, це були пряме поширення, що має

171
00:09:08,310 --> 00:09:14,310
4 рівняння, і зворотне поширення, що має

172
00:09:11,339 --> 00:09:16,680
6 рівнянь. Я знаю, що просто записав

173
00:09:14,310 --> 00:09:18,870
ці рівняння, але в наступному

174
00:09:16,680 --> 00:09:22,050
необов'язковому відео давай отримаємо певне

175
00:09:18,870 --> 00:09:23,940
розуміння того, як ці 6 рівнянь

176
00:09:22,050 --> 00:09:25,830
алгоритму зворотного поширення були

177
00:09:23,940 --> 00:09:27,750
виведені. Незалежно від того, подивишся ти відео

178
00:09:25,830 --> 00:09:30,000
чи ні, якщо ти реалізуєш

179
00:09:27,750 --> 00:09:32,730
ці алгоритми, то матимеш правильну

180
00:09:30,000 --> 00:09:34,650
реалізацію прямого і зворотного поширень

181
00:09:32,730 --> 00:09:36,750
і зможеш обчислити

182
00:09:34,650 --> 00:09:39,029
похідні, що потрібні для застосування

183
00:09:36,750 --> 00:09:41,520
Градієнтного спуску, щоб навчати параметри

184
00:09:39,029 --> 00:09:43,680
своєї НМ. Можна

185
00:09:41,520 --> 00:09:45,209
реалізувати ці алгоритми і заставити їх працювати

186
00:09:43,680 --> 00:09:47,130
і без глибокого розуміння

187
00:09:45,209 --> 00:09:50,520
диф.числення. Багато успішних практиків

188
00:09:47,130 --> 00:09:52,320
глибокого навчання так роблять. Проте, якщо

189
00:09:50,520 --> 00:09:54,180
хочеш, то можеш подивитись наступне відео

190
00:09:52,320 --> 00:09:56,580
просто щоб отримати краще розуміння

191
00:09:54,180 --> 00:09:58,820
виведення цих

192
00:09:56,580 --> 00:09:58,820
рівнянь.