1
00:00:00,000 --> 00:00:03,116
之前你见过我画的一些神经网络的图

2
00:00:03,116 --> 00:00:05,712
在这个视频中让我们具体讨论这些图的内容

3
00:00:05,712 --> 00:00:06,728
换句话说

4
00:00:06,728 --> 00:00:11,235
就是那些神经网络究竟代表着什么

5
00:00:11,235 --> 00:00:15,014
我们从集中于神经网络的

6
00:00:15,014 --> 00:00:17,290
单隐藏层案例开始

7
00:00:17,290 --> 00:00:19,667
这是一个神经网络的图

8
00:00:19,667 --> 00:00:22,986
让我们给这图里的几个构件一些名字

9
00:00:22,986 --> 00:00:27,447
这里有输入功能 x1、x2、x3 垂直堆叠

10
00:00:27,447 --> 00:00:30,694
也就是神经网络输入层

11
00:00:30,694 --> 00:00:35,764
所以不足为奇的，这包含了神经网络的输入

12
00:00:35,764 --> 00:00:37,990
然后这里是另一层的圈子

13
00:00:37,990 --> 00:00:41,663
也就是神经网络隐藏层

14
00:00:41,663 --> 00:00:45,414
我待回会解释‘隐藏’的意思

15
00:00:45,414 --> 00:00:49,509
而这里的最后一层，就只是一个节点

16
00:00:49,509 --> 00:00:53,894
这一节点层称为输出层

17
00:00:53,894 --> 00:00:56,059
并负责生成的估計值 ŷ

18
00:00:56,059 --> 00:00:59,932
在神经网络中，带有监督学习的

19
00:00:59,932 --> 00:01:05,237
训练集包含输入的 x 值，以及输出 y 值

20
00:01:05,237 --> 00:01:09,239
所以术语‘隐藏层’是指在训练集中

21
00:01:09,239 --> 00:01:12,702
中间这一层节点的真实值并没有所观察

22
00:01:12,702 --> 00:01:15,185
这就是你看不到他们为什么应该在训练集里

23
00:01:15,185 --> 00:01:16,640
你看到的输入是什么

24
00:01:16,640 --> 00:01:18,094
你看到输出的是什么

25
00:01:18,094 --> 00:01:20,992
但训练集中间隐藏层的东西是看不见的

26
00:01:20,992 --> 00:01:25,542
所以这解释‘隐藏’的意思

27
00:01:25,542 --> 00:01:28,088
因为你训练集中看不见这些

28
00:01:28,088 --> 00:01:30,262
让我们介绍这些符号

29
00:01:30,262 --> 00:01:35,542
之前说过，我们用向量 X 来表示输入功能和

30
00:01:35,542 --> 00:01:37,226
替代符号

31
00:01:37,226 --> 00:01:41,987
的输入值就是 A 上标方括号 0

32
00:01:41,987 --> 00:01:44,934
A这个词也代表激活

33
00:01:44,934 --> 00:01:47,733
它指的是不同层次的值

34
00:01:47,733 --> 00:01:51,651
从神经网络的各层传递到下一层

35
00:01:51,651 --> 00:01:55,998
所以，输入层把 x 值传递到隐藏层

36
00:01:55,998 --> 00:02:01,110
这就是输入层 A 上标方 0 的激活

37
00:02:01,110 --> 00:02:05,990
下一层的隐藏层将生成一些激活

38
00:02:05,990 --> 00:02:09,601
我把它称为 A 上标方括号 1

39
00:02:09,601 --> 00:02:13,306
所以是这第一个单位或节点

40
00:02:13,306 --> 00:02:17,824
生成一个 A 值上标方括号 1 下标 1

41
00:02:17,824 --> 00:02:20,735
第二个节点我们生成一个值

42
00:02:20,735 --> 00:02:23,311
是下标 2 如此类推

43
00:02:23,311 --> 00:02:26,488
所以，上标方括号1

44
00:02:26,488 --> 00:02:30,120
这是个四维向量的或如你想的

45
00:02:30,120 --> 00:02:34,707
Python因为 4.1 的矩阵是普通向量，像这样

46
00:02:34,707 --> 00:02:39,205
它是四维的因为在这种情况下我们有四个节点

47
00:02:39,205 --> 00:02:42,684
或在这个隐藏层里的四个隐藏单位

48
00:02:42,684 --> 00:02:46,302
最后，在输出层再生成一些 A2 值

49
00:02:46,302 --> 00:02:47,948
这是一个实数

50
00:02:47,948 --> 00:02:51,658
所以 ŷ 就承接了 A2 的值

51
00:02:51,658 --> 00:02:55,885
所以这里的回归我们有 ŷ 等于 a 

52
00:02:55,885 --> 00:03:00,349
而立法回归我们只有一个输出层

53
00:03:00,349 --> 00:03:03,583
所以我们不使用上标方括号

54
00:03:03,583 --> 00:03:07,916
但在新网络，我们现在要使用上标方括号

55
00:03:07,916 --> 00:03:11,653
来明确指出其来自哪一层

56
00:03:11,653 --> 00:03:15,468
关于符号约定有个趣的事情

57
00:03:15,468 --> 00:03:20,194
就是这儿看到这个网络是一个两层的神经网络

58
00:03:20,194 --> 00:03:23,541
原因就是在神经网络计算分层时

59
00:03:23,541 --> 00:03:25,321
我们不包刮输入层

60
00:03:25,321 --> 00:03:28,858
所以隐藏层是第一层而输出层是第二层

61
00:03:28,858 --> 00:03:32,661
在我们的符号约定中，输入层是零

62
00:03:32,661 --> 00:03:35,887
所以就算这个神经网络有三层

63
00:03:35,887 --> 00:03:39,649
输入层、隐藏层、和输出层

64
00:03:39,649 --> 00:03:43,357
在惯例上，如果你读研究论文或

65
00:03:43,357 --> 00:03:47,489
在这课其它地方，你会发现这个神经网络是

66
00:03:47,489 --> 00:03:51,602
两层神经网络，因为我们不包刮输入层为正式层

67
00:03:51,602 --> 00:03:55,912
最后，我们去了解隐藏层和

68
00:03:55,912 --> 00:03:59,670
输出层的相关参数

69
00:03:59,670 --> 00:04:03,447
所以隐藏层会有相关参数 w 和 b

70
00:04:03,447 --> 00:04:08,218
那我要写上标方括号 1 来表示这些

71
00:04:08,218 --> 00:04:12,395
是隐藏层的相关参数

72
00:04:12,395 --> 00:04:15,416
稍后会看到，w将是一个4乘3矩阵

73
00:04:15,416 --> 00:04:18,016
而在这个例子中，b将是一个4乘1向量

74
00:04:18,016 --> 00:04:21,754
这里的第一个坐标四来自于

75
00:04:21,754 --> 00:04:24,503
隐藏层的四个节点和一个层

76
00:04:24,503 --> 00:04:28,120
而三个来自三个输入功能

77
00:04:28,120 --> 00:04:31,980
稍后我们再谈谈这些矩阵的维数

78
00:04:31,980 --> 00:04:33,844
那时可能比较容易了解

79
00:04:33,844 --> 00:04:37,813
但是在输出层的一些相关参数

80
00:04:37,813 --> 00:04:41,663
w 上标方括号 2 和 b 上标方括号 2

81
00:04:41,663 --> 00:04:45,747
而它们的维数各别是 1乘4 和 1乘1

82
00:04:45,747 --> 00:04:49,297
而这个 1乘4 是因为隐藏层有四个隐藏单位

83
00:04:49,297 --> 00:04:51,177
输出层已经只是一个单位

84
00:04:51,177 --> 00:04:56,378
在后面的视频中我们会复习
这些矩阵和向量的维数

85
00:04:56,378 --> 00:04:59,839
刚才看到神经网络的两个分层

86
00:04:59,839 --> 00:05:03,108
那是一个隐藏层的神经网络

87
00:05:03,108 --> 00:05:04,260
在下一个视频中，

88
00:05:04,260 --> 00:05:08,513
让我们深入了解神经网络的运算方式

89
00:05:08,513 --> 00:05:11,223
那就是神经网络的输入 x 

90
00:05:11,223 --> 00:05:14,169
通过运算方式而得到输出的 ŷ
GTC字幕组翻译