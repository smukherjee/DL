在上一段影片中 您看到羅吉斯迴歸分析模型 為了訓練參數W跟B對於羅吉斯迴歸分析模型 您需要定義成本函數 讓我們看看您可以用來訓練羅吉斯迴歸分析的成本函數 回顧一下, 這是我們在上一節課談過的 您的輸出y-hat是S型函數對於w 轉置x加上b而這裡的S型函數of z 定義在這裡 所以為了學習模型的參數,您給予 一個訓練集有m個訓練例子而它 似乎自然而然的您會想找到參數W 跟B至少在訓練集裡, 您得到的輸出 從您在訓練集的預測 我們寫成y-hat(i)會趨近於 真正的標籤值y_i 從您訓練及的資料得到的 所以再進一步的細節利用上面方程式 我們說y-hat定義在上面的 訓練例子x, 當然對於每一個例子 我們使用這個上標用 圓括弧內用索引來區分不同的例子 您的預測在訓練例子(i)會是y-hat(i)將是 通過S型函數用於W轉置X (i) 訓練例子的輸入加上B, 而您可以定義Z(i) 為 Z(i) 等於W轉置x(i)加b 所以這整個課程裡 我們將用這個符號約定 也就是上標括號i代表資料 X或Y或Z是一些相關於第i個訓練例子 相關於第i個例子 這就是上標i在括號中的意義 現在讓我們看看什麼是損失函數或者錯誤 函數我們可以用來衡量我們的演算法有多好 一種方式是您可以定義損失為當您的演算法輸出 y-hat而真正的標籤是y也許平方誤差或者是平方誤差的一半 事實上您可以這樣做 但在羅吉斯迴歸分析, 人們並不這樣做 因為當您用來學習參數時 您會發現當我們以後會談論到優化問題時會變成非凸 所以這樣會在優化問題時有很多的局部最佳 所以梯度下降可能無法找到全域最佳解 如果你不明白最後幾點意見 別擔心，我們會在以後的影片談到 但直覺上要記住的是 這個函數L稱為損失函數是一個函數您會 需要去定義衡量我們的輸出y-hat有多好當這真正的標籤是y 誤差平方看起來像它可能是一個合理的選擇 除了它會讓梯度下降不可行 所以在羅吉斯迴歸分析, 我們實際上定義 不一樣的損失函數扮演類似平方誤差的角色 會給我們在優化時是 凸形而我們將在往後的影片中變得比較容易做優化 因此，我們在羅吉斯迴歸分析中實際上是使用 底下的損失函數就像上面這裡 是負y log y-hat 加 1 減 y log 1 減 y-hat 這裡是一些直覺為什麼這個損失函數是合理的 請記住，如果我們使用 平方誤差，那麼你想要的平方誤差要盡可能小 同樣地用這個羅吉斯迴歸分析損失函數 我們也會想這要它盡可能小 了解它為什麼是合理的 讓我們看兩種情況 第一種情況 假設Y等於 1 那損失 函數y-hat 逗點就只是第一項也就是 負的 log y-hat 如果 y 等於 1, 因為如果 y 等於 1 那第二項 1 減 y 會等於 0 所以這是說如果 y 等於 1 您希望 負的 log y-hat 越小越好 也就是說 log y-hat 越大越好 越大表示您希望 y-hat 越大 但因為您知道 在S型函數中,它不會大於1 所以這也就是說如果 y 等於 1, 您 希望 y-hat 越大越好 但它不會大於 1 所以這說明了您希望 y-hat 盡量接近 1 另一種情況是如果 y 等於 0 如果 y 等於 0 那損失函數第一項會等於 0 因為 y 是 0, 然後第二項定義了損失函數 所以損失變成負的 log 1 減 y-hat 所以如果在學習過程中您試著讓損失函數最小 也就是您希望 log 1 減 y-hat 最大 因為這裡有一個負號而 經過類似的原因您可以得到結論 說這個損失函數試著讓 y-hat 越小越好 再一次因為 y-hat 是介於 0 跟 1 這說明了如果 y 等於 0 那 您的損失函數會推向那些使得 y-hat 接近於 0 的參數 現在有很多的函數具備 Rafidah 效果, 也就是 y 等於 1 我們試著讓 y-hat 越大, y 如果等於 0 我們試著讓 y-hat 越小 我們在綠色部分 是一個非正式的理解對於這個損失函數, 會在以後提供 一個額外的影片來給與正式的證明 為什麼在羅吉斯迴歸分析我們喜歡用這個損失函數用這個特別樣式 最後這損失函數是定義相對於單一的訓練例子 它衡量了您作用於單一訓練例子好不好 我現在將定義稱為成本函數 來衡量您在整個訓練集作用得好不好 所以成本函數 J 是適用於 您的參數W 跟 B 將是平均於 1/m 的損失函數的和應用到每一個訓練例子 而這裡的 y-hat 當然是 應用您的羅吉斯迴歸分析演算法的預測輸出, 使用了 一組特別的參數 W 跟 B 而將這個展開 就是等於負的 1 / m 取和從 i 等於 1 到 m 對於這個損失函數的定義 所以這是 y(i) log y-hat(i) 加 1 減 y(i) log 1 減 y-hat (i) 我想我可以放一個方括號在這裡 所以負號在外面 所以我將使用這些專有名詞 損失函數是應用在單一的訓練例子 而成本函數是您參數的成本 所以在訓練您的羅吉斯迴歸分析模型時 我們將試著找尋參數 W 跟 B 使得 讓所有成本最小對於 在最底下的 J 函數 所以, 您剛剛看到對羅吉斯迴歸分析演算法的設定 訓練例子的損失函數跟 整個成本函數對於您演算法的參數 實際上羅吉斯迴歸分析可以視為非常非常小的神經網路 在下一段影片我們會談論到所以您可以開始 得到一些直觀對於神經網路 所以讓我們進入下一段影片對於如何 視羅吉斯迴歸分析為一個小的神經網路