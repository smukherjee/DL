1
00:00:00,360 --> 00:00:04,530
이번 비디오에서는, 신경망 네트워크네서
예측수치를 계산하는 방법을 배웠습니다.

2
00:00:04,530 --> 00:00:06,610
싱글 트레이닝 샘플에 대해서 말이죠.

3
00:00:06,610 --> 00:00:11,520
이번 비디오에서는, 여러개의 트레이닝 샘플에서
벡터화시키는 방법을 배우겠습니다.

4
00:00:11,520 --> 00:00:15,350
로지스특 회귀분석에서 본 결과와
비슷할텐데요,

5
00:00:15,350 --> 00:00:19,050
다른 트레이닝 샘플을 매트릭스 줄별로 쌓아서

6
00:00:19,050 --> 00:00:23,630
이전 비디오에서 이용한 공식을 다시
이용 가능한데요,

7
00:00:23,630 --> 00:00:27,860
이전 공식을 살짝 변형시켜,
신경망이 모든 샘플에 대해

8
00:00:27,860 --> 00:00:32,340
결과값을 산출할 수 있도록 해줍니다.

9
00:00:32,340 --> 00:00:35,080
어떻게 하는지 한번 상세 내용을 보겠습니다.

10
00:00:35,080 --> 00:00:40,192
이전 비디오에서 z1, a1, z2, a2의 값을 구하는 방법에 대한

11
00:00:40,192 --> 00:00:41,348
4개 공식이였는데요,

12
00:00:41,348 --> 00:00:46,867
벡터 x 입력 특성에 따라서,

13
00:00:46,867 --> 00:00:53,810
이것을 이용해서, a2 = ŷ인 것을 생성할 수 있습니다.
하나의 트레이닝 샘플에 대해서 말이죠.

14
00:00:54,920 --> 00:01:00,050
여러분이 m 트레이닝 샘플이 있는 경우,
반복적으로 진행하는데요,

15
00:01:00,050 --> 00:01:01,870
첫번째 샘플의 경우,

16
00:01:01,870 --> 00:01:06,600
x 위첨자 소괄호 1로

17
00:01:06,600 --> 00:01:11,062
ŷ을 계산합니다.
첫번째 트레이닝 샘플에 대한 예상수치입니다.

18
00:01:11,062 --> 00:01:16,537
x2 를 이용해서
ŷ 2를 예상합니다.

19
00:01:16,537 --> 00:01:23,050
이렇게 x(m)까지 내려가
ŷ m을 구합니다.

20
00:01:23,050 --> 00:01:28,349
activation 함수로 표기하기 위해,

21
00:01:28,349 --> 00:01:31,669
이것을 a[2](1)으로 표기하겠습니다.

22
00:01:31,669 --> 00:01:36,676
그리고 이것은 a[2](2)이구요,

23
00:01:36,676 --> 00:01:40,640
이것은 a(2)(m)입니다.

24
00:01:40,640 --> 00:01:46,830
여기서 a[2](i)로 표기되는데요,

25
00:01:46,830 --> 00:01:52,520
소괄호 i는 트레이닝 샘플 i를 뜻합니다.

26
00:01:52,520 --> 00:01:57,220
대괄호 2는 2번 층을 나타내구요.

27
00:01:58,530 --> 00:02:02,460
이렇게 대괄호와 소괄호가 사용되는 것입니다.

28
00:02:04,170 --> 00:02:07,920
unvectorized 도입이 있는 경우에는,

29
00:02:07,920 --> 00:02:11,000
모든 트레이닝 샘플에 대해서
그 예측값을 계산하고 싶으면,

30
00:02:11,000 --> 00:02:15,630
i가 1에서 m까지 인 경우 말이죠,

31
00:02:15,630 --> 00:02:18,260
그러면 여기 4개의 공식을 도입합니다.
아시겠죠?

32
00:02:18,260 --> 00:02:24,162
z[1](i)를

33
00:02:24,162 --> 00:02:30,064
W(1) x(i) + b[1]와 일치하게 해줍니다.

34
00:02:30,064 --> 00:02:38,253
a[1](i) 는 z[1](1)의 시그모이드입니다.

35
00:02:38,253 --> 00:02:43,683
z[2](i) 는 w[2]a[1](i)

36
00:02:43,683 --> 00:02:50,099
+ b[2] 입니다. 그리고, z2i는 w2a1i 더하기
b2이구요

37
00:02:50,099 --> 00:02:56,686
a[2](i) 는 z[2](i)의 시그모이드 입니다.

38
00:02:56,686 --> 00:03:03,172
그러면 여기 4개 공식에
트레이닝 샘플에 의존하는 모든 변수에

39
00:03:03,172 --> 00:03:08,788
위첨자 소괄호를 더하는 것이라고 생각하면 됩니다.

40
00:03:08,788 --> 00:03:12,612
여기 위첨자와 소괄호 i 를
x, z, a에 더합니다.

41
00:03:12,612 --> 00:03:18,570
여러분이 모든 결과값을 m트레이닝 샘플에 대해
계산하고 싶으면

42
00:03:18,570 --> 00:03:23,930
여기 전체 값을 벡터화시킵니다.

43
00:03:23,930 --> 00:03:27,680
제가 만약에 이런 선형 수학 부분이

44
00:03:27,680 --> 00:03:31,170
많이 나온다고 생각이 드시면,
알아두실 부분은,

45
00:03:31,170 --> 00:03:34,580
딥러닝에서 이러한 부분이 올바른 값이 되도록하는 것이
중요하다는 것입니다.

46
00:03:34,580 --> 00:03:38,160
그리고 이 코스에서는
표기를 신중히 합니다.

47
00:03:38,160 --> 00:03:41,460
이 벡터화를 최대한 쉽게 만들기 위해서 말이죠.

48
00:03:41,460 --> 00:03:46,140
이런 핵심부분은

49
00:03:46,140 --> 00:03:49,750
여러분의 알고리즘이 잘 작동할 수 있도록
빨리 도입되는데 도움을 줄 것입니다.

50
00:03:51,060 --> 00:03:56,210
자 그럼, 여기 코드부분을
다음 슬라이드에 모두 복사하겠습니다.

51
00:03:56,210 --> 00:03:57,880
그리고나서 벡터화시키는 방법을 살펴보겠습니다.

52
00:03:59,130 --> 00:04:02,154
이전 슬라이드에 보이는 부분인데요,

53
00:04:02,154 --> 00:04:04,324
4개의 트레이닝 샘플에 m개의
트레이닝 샘플과 연결되는 부분입니다.

54
00:04:04,324 --> 00:04:09,769
다시 한번 보겠습니다.
x 매트릭스를

55
00:04:09,769 --> 00:04:16,860
여기 이 줄들에 쌓은 트레이닝 샘플이라고 했죠.

56
00:04:16,860 --> 00:04:20,180
그러면 트레이닝 샘플을 모두 여기 줄에 쌓도록 합니다.

57
00:04:20,180 --> 00:04:23,220
그러면 이것은 n이 되구요,

58
00:04:23,220 --> 00:04:27,860
정확히 얘기하면 nx이죠, 그리고 m이죠,
이것이 매트릭스의 다이멘션이 됩니다.

59
00:04:29,198 --> 00:04:32,630
핵심부분 펀치라인을 얘기하겠습니다.
여기 for loop의 벡터화된 도입을 위해 어떤 것을

60
00:04:32,630 --> 00:04:35,760
도입해야하는지에 관한 내용 말이죠.

61
00:04:35,760 --> 00:04:41,394
여러분이 계산해야 하는 것은,

62
00:04:41,394 --> 00:04:46,035
Z[1] = W[1] X + b[1] 그리고

63
00:04:46,035 --> 00:04:50,692
A[1]= z[1] 의 시그모이드 입니다..

64
00:04:50,692 --> 00:04:56,157
그리고 z[2] = w[2]입니다

65
00:04:56,157 --> 00:05:01,348
A[1] + b[2] 이고,

66
00:05:01,348 --> 00:05:10,100
A[2] 는 Z[2]의 시그모이드입니다.

67
00:05:10,100 --> 00:05:16,440
유사한 점은, 저희가 소문자 x의 벡터에서

68
00:05:16,440 --> 00:05:23,480
대문자 x 매트릭스로 갔는데요,
다른 줄별로 소문자 x의 값을 쌓아서 말이죠.

69
00:05:23,480 --> 00:05:28,494
z도 이와 같이 똑같이 진행하면,
예를 들어,

70
00:05:28,494 --> 00:05:33,509
z[1](i), z[1](2), 등등

71
00:05:33,509 --> 00:05:40,290
세로줄의 벡터인데요,
z[1](m) 까지 말이죠. 맞죠.

72
00:05:40,290 --> 00:05:46,270
여기 이 첫번째 것과 동일한데 모두 m까지를 나타낸 것인데요.

73
00:05:46,270 --> 00:05:50,045
그러면 이 것은 매트릭스 z1이 됩니다.

74
00:05:50,045 --> 00:05:55,299
비슷하게 a에도 적용할 수 있습니다.

75
00:05:55,299 --> 00:06:00,957
a[1](1), a[1](2) 등등 이렇게 말이죠.

76
00:06:00,957 --> 00:06:06,980
a[1](m) 까지해서 줄별로 쌓아진 것을 나타낼 수 있습니다.

77
00:06:06,980 --> 00:06:11,610
그러면 저희가 소문자 x에서 대문자 X로 간 것과 같이,

78
00:06:11,610 --> 00:06:13,280
소문자 z에서 대문자 Z가 된 것과 같이 말이죠,

79
00:06:13,280 --> 00:06:20,920
이것은 소문자 a벡터에서, 대문자 A1이 됩니다.

80
00:06:20,920 --> 00:06:26,685
여기 A1과 같이 말이죠.
또 비슷하게, z[2] 와 a[2]는

81
00:06:26,685 --> 00:06:30,141
여기 이 벡터들을 쌓아서 얻어지는데요,

82
00:06:30,141 --> 00:06:32,016
가로로 쌓아서 말이죠.

83
00:06:32,016 --> 00:06:37,326
그리고 여기 이런 벡터들을 가로로 쌓아서 말이죠.

84
00:06:37,326 --> 00:06:40,840
Z[2] 와 E[2]를 얻기까지 말이죠.

85
00:06:40,840 --> 00:06:44,042
여러분이 생각하는데 도움을 줄만한

86
00:06:44,042 --> 00:06:47,391
표기에 대한 내용은,
여기 Z와 A와 같은 매트릭스는

87
00:06:47,391 --> 00:06:51,420
트레이닝 샘플들을 가로로 인덱싱할 것인데요,

88
00:06:51,420 --> 00:06:55,631
그렇기 때문에 가로 인덱스가 다른 트레이닝 샘플을
나타내는 것입니다.

89
00:06:55,631 --> 00:06:59,730
만약 왼쪽에서 오른쪽을 가는 경우,
트레이닝 세트를 스케닝하는 것과 마찬가지입니다.

90
00:06:59,730 --> 00:07:04,617
세로로는 여기 세로 인덱스가
여러분의 신경망에 있는

91
00:07:04,617 --> 00:07:06,130
여러개의 노드를 나타냅니다.

92
00:07:06,130 --> 00:07:11,077
예를 들어, 여기 이 노드는,
가장 위에 있는데요,

93
00:07:11,077 --> 00:07:16,554
가장 위 코너에 있는 매트릭스 값은

94
00:07:16,554 --> 00:07:21,633
첫번때 샘플에서 숨겨진 유닛의 activation을
나타냅니다.

95
00:07:21,633 --> 00:07:25,812
그 다음 값은 두번째 첫번째 샘플에서
두번째 숨겨진 유닛의 activation을

96
00:07:25,812 --> 00:07:27,525
나타냅니다.

97
00:07:27,525 --> 00:07:31,505
그리고 첫번째 샘플의 세번째 숨겨진 유닛
도 이렇게 나타나겠죠.

98
00:07:31,505 --> 00:07:37,540
이렇게 스캔을 해보면, 이것이
숨겨진 유닛에 해당하는 인덱스 숫자이구요

99
00:07:39,670 --> 00:07:42,564
반대로 가로로 움직이는 경우,
첫번째 트레이닝 샘플의
첫번째 숨겨진 유닛에서,

100
00:07:42,564 --> 00:07:45,450
두번째 트레이닝 샘플의 첫번째
숨겨진 유닛으로

101
00:07:45,450 --> 00:07:48,240
이렇게 세번째 트레이닝 샘플 등등 말이죠.

102
00:07:48,240 --> 00:07:53,718
여기 이 노드가
마지막 트레이닝 샘플의 첫번째 숨겨진 유닛의

103
00:07:53,718 --> 00:07:59,030
activation까지 나타날때까지 n번째 트레이닝 샘플로
이어집니다.

104
00:08:00,760 --> 00:08:07,663
가로로는 A매트릭스가 n개의
트레이닝 샘플로 이어지는데요,

105
00:08:10,150 --> 00:08:14,195
세로로는 A 메트릭스의 인덱스

106
00:08:14,195 --> 00:08:17,589
가 숨겨진 유닛의 숫자를 나타냅니다.

107
00:08:22,342 --> 00:08:26,870
z 매트릭스도 동일한 직관이 적용됩니다.

108
00:08:26,870 --> 00:08:31,840
x 와 마찬가지고 가로에서는
n개의 트레이닝 샘플을 나타내듯이 말이죠.

109
00:08:31,840 --> 00:08:36,227
세로로는 입력 특성의 숫자가
나타나구요.

110
00:08:36,227 --> 00:08:41,180
이 숫자는 신경망의 입력 층과는 다릅니다.

111
00:08:42,750 --> 00:08:46,600
이제 여기서 배운 공식을 통해서

112
00:08:46,600 --> 00:08:51,320
벡터화를 통해 복수의 샘플이 있는 경우
벡터화 시키는 방법을 배웠는데요,

113
00:08:51,320 --> 00:08:55,130
다음 비디오에서는 어떻게 이런 공식이

114
00:08:55,130 --> 00:08:59,070
정의되는 것인지 조금 더 자세히
벡터화에 대한 정확한 도입에 대한 정의를
다루겠습니다.

115
00:08:59,070 --> 00:09:03,468
공식의 타당성은 지금 본 내용과 그리 크게 다르진 않을 것입니다.

116
00:09:03,468 --> 00:09:05,300
다음 비디오로 넘어가겠습니다.