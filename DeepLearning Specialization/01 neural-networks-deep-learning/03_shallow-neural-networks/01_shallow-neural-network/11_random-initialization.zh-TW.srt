1
00:00:00,000 --> 00:00:01,619
當您訓練您的神經網路

2
00:00:01,619 --> 00:00:03,955
怎樣隨機初始
這些權重是很重要的

3
00:00:03,955 --> 00:00:08,426
對於羅吉斯迴歸分析, 
初始這些權重為0 是可以的

4
00:00:08,426 --> 00:00:12,258
但對於神經網路初始
這些參數權重都為0

5
00:00:12,258 --> 00:00:14,706
然後應用到梯度下降
這樣是不可行的

6
00:00:14,706 --> 00:00:15,289
讓我們看看為什麼

7
00:00:15,289 --> 00:00:20,173
您有兩個輸入特徵

8
00:00:20,173 --> 00:00:25,953
n0 = 2, 兩個隱藏單元, n1 = 2

9
00:00:25,953 --> 00:00:31,547
相關於隱藏層的矩陣

10
00:00:31,547 --> 00:00:35,373
w1 將會是 2 乘 2

11
00:00:35,373 --> 00:00:41,230
假設您初始化它們
都為0, 0 0 0 0, 2 乘 2 矩陣

12
00:00:41,230 --> 00:00:45,531
假設 b1 也是等於 0

13
00:00:45,531 --> 00:00:50,788
實際上初始這個偏差值
 b 設為 0 是可以的

14
00:00:50,788 --> 00:00:54,240
但初始 w 全為  0 是個問題

15
00:00:54,240 --> 00:00:59,625
這種樣子的初始化的問題是對於

16
00:00:59,625 --> 00:01:05,522
任何例子您給予
您會有 a1, 1, 跟

17
00:01:05,522 --> 00:01:09,253
a1, 2 會是相等的, 對吧?

18
00:01:09,253 --> 00:01:12,613
這個啟動跟
這個啟動會一樣

19
00:01:12,613 --> 00:01:17,170
因為這兩個隱藏單元
用同樣的函數來計算

20
00:01:17,170 --> 00:01:21,810
然後
當您計算反向傳播時

21
00:01:21,810 --> 00:01:24,478
會變成 dz1,1跟

22
00:01:24,478 --> 00:01:30,165
dz1, 2 會是ㄧ樣
像是對稱般

23
00:01:30,165 --> 00:01:33,720
兩個隱藏單元
用一樣的方式初始

24
00:01:33,720 --> 00:01:36,080
技術上來說

25
00:01:36,080 --> 00:01:39,851
我假設輸出權重也是相同

26
00:01:39,851 --> 00:01:45,122
也就是 w2 等於 0 0

27
00:01:45,122 --> 00:01:48,691
但如果您用這樣初始
神經網路

28
00:01:48,691 --> 00:01:53,590
這個隱藏單元跟
這個隱藏單元是完全相等

29
00:01:53,590 --> 00:01:57,011
有時候您說它們
完全對稱

30
00:01:57,011 --> 00:02:01,687
也就是說它們
完全相同的函數

31
00:02:01,687 --> 00:02:03,765
然後用數學歸納法證明

32
00:02:03,765 --> 00:02:08,064
實際上經過單一
遞迴的訓練您的兩個隱藏

33
00:02:08,064 --> 00:02:11,272
單元還是計算
完全一樣的函數

34
00:02:11,272 --> 00:02:17,521
因為這是可以證明 dw 
會是一個矩陣像這樣

35
00:02:17,521 --> 00:02:20,681
每一個行的值都相同

36
00:02:20,681 --> 00:02:23,318
我們在執行權重更新時

37
00:02:23,318 --> 00:02:30,163
我們在執行權重更新時
w1 更新為 w1 - alpha 乘 dw

38
00:02:30,163 --> 00:02:33,740
您發現到 w1 經過每次遞迴

39
00:02:33,740 --> 00:02:37,616
都會是第一行
等於第二行

40
00:02:37,616 --> 00:02:41,487
所以這是可以用
數學歸納法證明如果您

41
00:02:41,487 --> 00:02:44,688
初始所有 w 的值為 0

42
00:02:44,688 --> 00:02:49,164
因為兩個隱藏單元開始於
一樣的函數計算

43
00:02:49,164 --> 00:02:53,541
兩個隱藏單元有
同樣的影響到輸出單元

44
00:02:53,541 --> 00:02:57,542
經過一次的遞迴
同樣的情況

45
00:02:57,542 --> 00:03:00,273
兩個隱藏單元還是對稱

46
00:03:00,273 --> 00:03:04,507
所以用數學歸納法, 經過兩次
遞迴, 三次遞迴等等

47
00:03:04,507 --> 00:03:07,013
不管您
訓練您個神經網路多久

48
00:03:07,013 --> 00:03:10,373
兩個隱藏單元還是
用同樣的函數計算

49
00:03:10,373 --> 00:03:15,212
這種情況下, 真的沒有
意義使用一個以上的隱藏單元

50
00:03:15,212 --> 00:03:17,692
因為它們都
計算同樣的東西

51
00:03:17,692 --> 00:03:22,378
當然對於大型的神經網路
假設說有三個特徵跟

52
00:03:22,378 --> 00:03:24,972
也許很多個隱藏單元

53
00:03:24,972 --> 00:03:29,239
同樣的議論顯示
像這樣的神經網路

54
00:03:29,239 --> 00:03:34,107
[聽不清]畫所有的線
如果您初始權重為 0

55
00:03:34,107 --> 00:03:37,103
那所有您的隱藏
單元是對稱的

56
00:03:37,103 --> 00:03:40,603
不管您跑多久的
梯度下降

57
00:03:40,603 --> 00:03:44,037
還是持續的
計算同樣的函數

58
00:03:44,037 --> 00:03:48,785
這並沒有幫助
因為您要不同的

59
00:03:48,785 --> 00:03:52,835
隱藏單元來計算
不同函數

60
00:03:52,835 --> 00:03:57,748
解決方式是
隨機初始您的參數

61
00:03:57,748 --> 00:03:58,677
您可以這樣做

62
00:03:58,677 --> 00:04:04,053
您可以設 w1 = np.random.randn

63
00:04:04,053 --> 00:04:07,037
這會產生一個高斯
隨機變數 (2,2)

64
00:04:07,037 --> 00:04:12,358
然後通常您會乘上一個
很小的數字, 比如說 0.01

65
00:04:12,358 --> 00:04:14,951
所以您初始它們
為很小的隨機值

66
00:04:14,951 --> 00:04:20,590
然後對於 b, 實際上 b
並沒有對稱問題

67
00:04:20,590 --> 00:04:24,735
稱為對稱
斷裂問題

68
00:04:24,735 --> 00:04:29,370
初始
b 都為 0 是沒有關係的

69
00:04:29,370 --> 00:04:32,166
因為
只要 w 隨機初始

70
00:04:32,166 --> 00:04:36,769
您開始用不同的隱藏
單元計算不同的事

71
00:04:36,769 --> 00:04:40,912
您不再有這種
對稱斷裂問題

72
00:04:40,912 --> 00:04:43,795
同樣地, 對於 w2
您將它們隨機初始

73
00:04:43,795 --> 00:04:48,858
b2 您可以初始為 0

74
00:04:48,858 --> 00:04:55,321
您也許會疑惑, 這個
常數來自何方, 為什麼是 0.01？

75
00:04:55,321 --> 00:04:58,478
為什麼不是 100 或是 1000？

76
00:04:58,478 --> 00:05:02,313
實際上我們通常
喜歡初始

77
00:05:02,313 --> 00:05:05,763
這些權重為很小的隨機值

78
00:05:05,763 --> 00:05:10,443
因為如果您使用 tanh 或
S型啟動函數

79
00:05:10,443 --> 00:05:14,047
或者其他S型函數
即使只在輸出層

80
00:05:14,047 --> 00:05:17,922
如果權重太大

81
00:05:17,922 --> 00:05:23,967
當您計算
啟動值

82
00:05:23,967 --> 00:05:28,621
記得 z[1] = w1 x + b

83
00:05:28,621 --> 00:05:34,094
然後 a1 是用啟動
函數應用到 z1

84
00:05:34,094 --> 00:05:39,097
如果 w 值很大
z 也會很大, 或者說至少

85
00:05:39,097 --> 00:05:44,235
z 的值不是很大
就是很小

86
00:05:44,235 --> 00:05:49,789
在這種情況下, 您大概會
在這個平坦的部分在 tanh

87
00:05:49,789 --> 00:05:55,699
函數或者S型函數, 而
那裡的斜率或者說梯度會很小

88
00:05:55,699 --> 00:05:58,302
意思是梯度
下降會很慢

89
00:05:58,302 --> 00:05:59,730
所以學習會很慢

90
00:05:59,730 --> 00:06:04,133
總結一下, 如果 w 很大
您很可能

91
00:06:04,133 --> 00:06:08,633
在開始學習時
有很大的 z 值

92
00:06:08,633 --> 00:06:13,525
會使得您的 tanh 或者 sigmoid 
啟動函數卡住

93
00:06:13,525 --> 00:06:15,418
因此放慢學習

94
00:06:15,418 --> 00:06:17,231
如果您不用S型函數或

95
00:06:17,231 --> 00:06:22,149
tanh 啟動函數在您的
神經網路這比較不會是問題

96
00:06:22,149 --> 00:06:26,506
但如果您處理二元分類問題
您的輸出單元是 sigmoid

97
00:06:26,506 --> 00:06:30,806
函數, 那您不希望
這些初始參數的值太大

98
00:06:30,806 --> 00:06:35,435
這是為什麼乘上 0.01會是
合理的值試試看

99
00:06:35,435 --> 00:06:36,872
或者其他小的數字

100
00:06:36,872 --> 00:06:38,536
w2 也是一樣, 對吧?

101
00:06:38,536 --> 00:06:44,295
這個可以是 random.randn

102
00:06:44,295 --> 00:06:49,545
我猜這會是 1 乘 2
在這個例子， 乘上 0.01

103
00:06:49,545 --> 00:06:51,404
這裡少了個 s

104
00:06:51,404 --> 00:07:00,085
最後, 實際上有時候
有可能比 0.01 有更好的常數

105
00:07:00,085 --> 00:07:04,304
當您訓練神經
網路只有一個隱藏層時

106
00:07:04,304 --> 00:07:09,129
這真的是淺層的神經網路
並沒有很多的隱藏層

107
00:07:09,129 --> 00:07:12,392
設這個為 0.01 也許是可行的

108
00:07:12,392 --> 00:07:15,705
但當您訓練一個
很深的神經網路時

109
00:07:15,705 --> 00:07:19,294
您也許需要選
不同的常數而不是 0.01

110
00:07:19,294 --> 00:07:23,642
在下一個禮拜的課程
我們將會提到一點點如何跟

111
00:07:23,642 --> 00:07:27,925
何時您需要選擇
不同於 0.01 的常數

112
00:07:27,925 --> 00:07:32,008
但不管如何, 通常
這都會是一個很小的數字

113
00:07:32,008 --> 00:07:34,584
所以這是這一週的影片

114
00:07:34,584 --> 00:07:38,348
您現在知道了如何去設定一個神經
網路有一層的隱藏層

115
00:07:38,348 --> 00:07:42,430
初始化參數
使用正向傳播做預估

116
00:07:42,430 --> 00:07:45,445
跟計算導數及
建置梯度下降

117
00:07:45,445 --> 00:07:46,275
使用反向傳播

118
00:07:46,275 --> 00:07:48,654
所以
您應該可以做測驗

119
00:07:48,654 --> 00:07:51,166
跟這個禮拜的
程式練習

120
00:07:51,166 --> 00:07:52,143
祝您幸運

121
00:07:52,143 --> 00:07:54,802
我希望您覺得
程式練習很好玩

122
00:07:54,802 --> 00:07:57,728
期望在第四週的課程再見到您