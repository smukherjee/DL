1
00:00:00,000 --> 00:00:02,250
欢迎回来这个视频中

2
00:00:02,250 --> 00:00:04,980
我们将讨论 在实现逻辑回归时

3
00:00:04,980 --> 00:00:08,330
如何计算导数来实现梯度下降

4
00:00:08,330 --> 00:00:11,040
重点在于

5
00:00:11,040 --> 00:00:13,230
逻辑回归中

6
00:00:13,230 --> 00:00:17,725
梯度下降的关键方程

7
00:00:17,725 --> 00:00:22,185
在这个视频里 我将用计算图来进行计算

8
00:00:22,185 --> 00:00:25,320
我必须承认 使用计算图

9
00:00:25,320 --> 00:00:29,342
对于逻辑回归的梯度下降来说
有些大材小用

10
00:00:29,342 --> 00:00:31,183
但我想通过这种方式

11
00:00:31,183 --> 00:00:33,975
让你们熟悉这些想法

12
00:00:33,975 --> 00:00:38,370
希望会对你学习经过神经网络有所帮助

13
00:00:38,370 --> 00:00:44,235
现在我们深入探讨逻辑回归的梯度下降充分

14
00:00:44,235 --> 00:00:49,070
之前 我们建立了这样的逻辑回归方程

15
00:00:49,070 --> 00:00:53,220
预测值y_hat的定义如图

16
00:00:53,220 --> 00:00:56,490
z的定义如图

17
00:00:56,490 --> 00:01:01,800
我们关注这一个例子

18
00:01:01,800 --> 00:01:03,630
损失函数关于这个例子

19
00:01:03,630 --> 00:01:05,190
的定义如图

20
00:01:05,190 --> 00:01:07,855
其中a是逻辑回归的输出

21
00:01:07,855 --> 00:01:10,535
y是真实值

22
00:01:10,535 --> 00:01:15,735
我们通过计算图表示它

23
00:01:15,735 --> 00:01:20,520
例如 有两个特征x1和x2

24
00:01:20,520 --> 00:01:22,860
为了计算z

25
00:01:22,860 --> 00:01:27,030
我们要输入w1,w2和b

26
00:01:27,030 --> 00:01:31,130
还有特征x1和x2的值

27
00:01:31,130 --> 00:01:33,705
这些东西在计算图里

28
00:01:33,705 --> 00:01:36,910
用来计算z

29
00:01:36,910 --> 00:01:41,588
z=w1x1+w2x2+b

30
00:01:41,588 --> 00:01:45,380
用方框框起来

31
00:01:45,380 --> 00:01:48,555
接着计算y_hat

32
00:01:48,555 --> 00:01:52,244
y_hat=a=sigma(z)

33
00:01:52,244 --> 00:01:55,740
这是计算图里接下来的一步

34
00:01:55,740 --> 00:01:58,725
最后我们计算L(a,y)

35
00:01:58,725 --> 00:02:01,840
我不再抄一遍公式了

36
00:02:01,840 --> 00:02:06,900
在逻辑回归中 我们要做的就是修改参数w和b

37
00:02:06,900 --> 00:02:12,830
来减少损失函数

38
00:02:12,830 --> 00:02:15,870
之前讲前向传播的步骤中

39
00:02:15,870 --> 00:02:19,280
讲了如何计算单个样本的损失函数

40
00:02:19,280 --> 00:02:23,940
现在我们讲讲如何反向计算导数

41
00:02:23,940 --> 00:02:26,025
这是一个整理后的框图

42
00:02:26,025 --> 00:02:30,690
因为我们要计算关于损失函数的导数

43
00:02:30,690 --> 00:02:33,570
反向传播时 首先要做的是

44
00:02:33,570 --> 00:02:38,010
计算损失函数对于a的导数

45
00:02:38,010 --> 00:02:41,940
所以

46
00:02:41,940 --> 00:02:43,570
在代码中

47
00:02:43,570 --> 00:02:49,000
你只要用da来表示dL/da

48
00:02:49,000 --> 00:02:52,725
如果你对微积分熟悉

49
00:02:52,725 --> 00:03:02,004
你可以得到dL/da=-y/a+(1-y)/(1-a)

50
00:03:02,004 --> 00:03:06,185
得到的方法是 通过loss的表达式

51
00:03:06,185 --> 00:03:07,535
然后如果你熟悉微积分的话

52
00:03:07,535 --> 00:03:10,515
你可以计算关于变量a的导数

53
00:03:10,515 --> 00:03:12,792
然后得到这个式子

54
00:03:12,792 --> 00:03:15,280
但你不熟悉的话也别担心

55
00:03:15,280 --> 00:03:17,960
我们会提供导数表

56
00:03:17,960 --> 00:03:20,100
和其他的这门课需要的东西

57
00:03:20,100 --> 00:03:21,185
所以如果你特别熟悉微积分的话

58
00:03:21,185 --> 00:03:24,590
我鼓励你从之前的讲义中

59
00:03:24,590 --> 00:03:29,504
找出损失函数的方程然后试着对a求导

60
00:03:29,504 --> 00:03:32,635
如果你不懂微积分也别担心

61
00:03:32,635 --> 00:03:35,491
现在已经算出了da的值

62
00:03:35,491 --> 00:03:38,825
是最终输出值对a的导数

63
00:03:38,825 --> 00:03:40,715
你可以继续往回

64
00:03:40,715 --> 00:03:45,525
可以算出dz

65
00:03:45,525 --> 00:03:47,648
是python例子中起的变量名

66
00:03:47,648 --> 00:03:51,200
是损失函数

67
00:03:51,200 --> 00:03:53,618
对z的导数

68
00:03:53,618 --> 00:03:59,850
你可以把损失函数显式地写成a和y的函数

69
00:03:59,850 --> 00:04:04,230
或者直接写L也行

70
00:04:04,230 --> 00:04:09,605
可以得到dz=a-y

71
00:04:09,605 --> 00:04:14,685
讲一点过程给熟悉微积分的同学

72
00:04:14,685 --> 00:04:16,795
如果你不熟悉也别担心

73
00:04:16,795 --> 00:04:20,320
这个dL/dz

74
00:04:20,320 --> 00:04:27,850
可以被表示成dL/da乘以da/dz

75
00:04:27,850 --> 00:04:29,940
da/dz可以算出

76
00:04:29,940 --> 00:04:33,755
是a(1-a) 注:仅对于sigmoid函数成立

77
00:04:33,755 --> 00:04:37,800
dl/da之前得到右边这个结果

78
00:04:37,800 --> 00:04:41,530
所以dl/da的表达式

79
00:04:41,530 --> 00:04:43,846
和da/dz的结合起来

80
00:04:43,846 --> 00:04:47,165
相乘

81
00:04:47,165 --> 00:04:51,915
可以得到结果是a-y

82
00:04:51,915 --> 00:04:53,220
这是求解过程

83
00:04:53,220 --> 00:04:57,390
这是链式法则

84
00:04:57,390 --> 00:05:02,770
如果你会微积分可以自己算算

85
00:05:02,770 --> 00:05:05,345
如果不会你只需要知道

86
00:05:05,345 --> 00:05:09,365
dz=a-y 我们已经替你做完了微积分

87
00:05:09,365 --> 00:05:13,010
反向传播的最后一步

88
00:05:13,010 --> 00:05:17,480
是反向算出你需要改变w和b多少

89
00:05:17,480 --> 00:05:24,610
特别地 你可以算出L对w1的导数

90
00:05:24,610 --> 00:05:31,810
通常记作dw1 它等于x1乘以dz

91
00:05:31,810 --> 00:05:36,485
同样 dw2 代表你要改变w2的值

92
00:05:36,485 --> 00:05:39,455
是x2乘以dz

93
00:05:39,455 --> 00:05:42,585
db等于dz

94
00:05:42,585 --> 00:05:47,375
所以如果你要对于一个例子进行梯度下降

95
00:05:47,375 --> 00:05:49,280
你需要做如下事情

96
00:05:49,280 --> 00:05:52,640
用公式算出dz

97
00:05:52,640 --> 00:05:56,707
然后算出dw1 dw2 和db

98
00:05:56,707 --> 00:06:01,170
然后进行更新

99
00:06:01,170 --> 00:06:04,538
w1=w1-α*dw1

100
00:06:04,538 --> 00:06:06,575
α代表学习速率

101
00:06:06,575 --> 00:06:09,245
w2也按一样的方式更新

102
00:06:09,245 --> 00:06:14,170
b=b-αdb

103
00:06:14,170 --> 00:06:18,860
这是一个简单例子 一步梯度的情况

104
00:06:18,860 --> 00:06:22,130
你看了 对于一个单一的训练样本
如何计算导数

105
00:06:22,130 --> 00:06:27,200
和执行逻辑回归的梯度下降

106
00:06:27,200 --> 00:06:28,987
但训练一个逻辑回归模型

107
00:06:28,987 --> 00:06:34,700
你不止有一个样本 而是有m个

108
00:06:34,700 --> 00:06:36,120
在下节课中

109
00:06:36,120 --> 00:06:39,350
我们将谈谈
如何应用这些想法

110
00:06:39,350 --> 00:06:40,760
到多个训练样本

111
00:06:40,760 --> 00:06:42,400
而不是单独的一个