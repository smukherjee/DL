이번 비디오에서는, deep l-larry 신경망이 무엇인지 다루었고, 또한, 이런 네트워크를 표현하는데 사용하는 표기방식에 대해 이야기 했습니다. 이번 비디오에서는 어떻게 깊은 네트워크에서
전 방향전파를 진행하는지 알아보겠습니다. 이전과 같이, 전 방향전파가 어떻게 생겼는지 1개의 트레이닝 샘플 x에 대하여 한번 보겠습니다. 그리고 나중에는, 전체의 트레이닝 세트에 한번에 전 방향전파를 진행할 수 있도록 벡터화된 버전을 이야기해보겠습니다. 먼저, 1개의 트레이닝 샘플 x의 경우, 첫번째 층의 activation을 계산하는 방법입니다. 이 첫번째 층에서는 z1=w1 곱하기 x 더하기 b1인데요, w1 과 b1의 파라미터 즉 첫번째 층의 activation에 영향을 주는 
파라미터인데요, 이것은 신경망의 첫번째 층인데요, 그리고 다음으로 그 층의 activation을 구하는데요, g(z1)입니다. 그리고 이 activation 함수는 그 층이 어디 있는지, 여기 지수 인덱스는 첫번째 층에 해당하는 1의 값을 나타내죠. 그렇게하면 첫번째 층의 activation 값을 구한 것인데요, 여기 이 두번째 층은 어떨까요? 그러면 z2=W 2 A 1 더하기 b2 그리고 2번째 층의 activation은 weight 매트릭스 곱하기 첫번째 층의 결과값입니다. 이 값이죠 더하기 2번째 층의 bias 벡터, 그리고 a2는 z2에 적용된 activation 함수입니다. 2번째 층은 이게 전부입니다. 이렇게 계속 결과값 층까지 반복해서 진행합니다. 4번째 층까지 말이죠, Z4는 그 층의 파라미터 곱하기 이전 층의 activation 더하기 벡터 와 bias 벡터 그 다음에는 a4=g의 z4입니다. 이런 과정을 통해 결과값 ŷ의 예상수치를 구랍니다. 한가지 염두할 것은 여기 x는 a0와 같습니다. 입력 특성 벡터 x는 또한 0번째의 층의 activation과도 같기 때문입니다. 그러므로 x를 지우고, 없애버리고, 0을 놓겠습니다. 그렇게하면 여기 모든 공식은 똑같이 생긴 공식이 되죠. 일반적인 규칙은 Zㅣ은 W ㅣ 곱하기 a 의 l-1 더하기 b 의 ㅣ입니다. 그리고 여기 이 층의 activation은 z값에 대해 적용된 activation 함수입니다. 그러면 이것이 일반적인 전 방향전파 공식이 됩니다. 1개의 트레이닝 샘플에 대해서 공식을 만들어 봤는데요, 그러면 전체 트레이닝 세트에 대해서 한번에 벡터화시키는 것을 어떨까요? 공식은 사실 이전과 비슷하게 보이는데요. 첫번째 층에서는, 대문자 Z1=w 1 곱하기 대문자 X 더하기 B1 그리고, A1은 g의 1 곱하기 z의 1입니다. 기억하실 것은, X가 0이라는 것입니다. 이것은 트레이닝 샘플들이 줄별로 쌓아진 것인데요, 이 부분은 빼서, X를 지울 수 있습니다. 그래서 값을 0으로 두면 되구요, 다음 층은 비슷하게 보일텐데요, z2=W 2 A 1 더하기 B 2, 그리고 a 2는 g의 z2입니다. 여기 이런 z와 a와 같은 벡터를 갖고 이런 벡터들의 값을 쌓아올 리는 것입니다. 첫번째 트레이닝 샘플은 z 벡터이고요, 이것은 2번째 트레이닝에 대한 z 벡터이고요, m번째 트레이닝 샘플까지 반복합니다. 그리하여 세로줄에 쌓아 올립니다. 대문자 Z로 표현하는데요, 대문자 A도, 대문자 X도 마찬가지로 이것은 세로 벡터를 왼쪽에서 오른쪽으로 쌓는 것입니다. 이런 절차의 마지막에는 ŷ값이 나오는데요 이 값이 g(z4) 이고 이 값은 a 4와 같습니다. 이 예측수치는 모든 트레이닝 샘플들을 가로로 쌓아올린 것입니다. 요약하자면, 여기 위에 변경해서 적을텐데요, 다른 표기 방식을 통해 여기 소문자 z와 a를 각각 상응하는 대문자 값으로 변경해주겠습니다. 이것은 이미 대문자 Z로 보이네요, 이렇게하면 전 방향전파의 벡터화된 버전이 나오는 것입니다. 한번에 전체 트레이닝 세트에대해 진행하는
경우 말이죠. 여기서 a0는 x입니다. 여기서 보는 벡터화의 도입을 보면, 여기에서 for lop이 있을 것처럼 보여집니다. for l=1에서 4까지이죠. for l 은 1에서 대문자 L까지 입니다. 그리고 첫번째 층에 대한 activation을 계산하고, 두번째 층, 그리고 세번째 층, 4번째 층 이런식으로 말이죠. 이와 같이 for loop 있는 것처럼 보입니다. 그리고 저는 네트워크를 도입하는 경우, explicit for loop를 제거하고 싶다는 것을 아는데요, 하지만 여기서는 아마 explicit for loop 없이 도입할 수 있는 방법이 없는 것 같습니다. 그러므로 전 방향전파을 도입할 때는 for loop이 있는 것이 괜찮습니다. 첫번째 층에 대한 activation을 계산해주고, 그 다음 2번째, 3번째 층, 4번째 층, 이렇게 말이죠, 아무도 모르고, 제 생각에는 for loop없이 하는 방법은 없는 것 같습니다. 여기 1에서 대문자 L까지 즉, 1에서 네트워크에 있는 모든 층까지 말이죠. 그렇기 때문에 이런 경우에는, explcit form을 가져도 괜찮습니다. 심층 네트워크에 대한 표기부분과 이런 네트워크에서 전 방향전파을 하는 방법에 대한 강의는 여기까지입니다. 여기서 본 내용들이 익숙해 보인다면, 그 이유는 저희가 공부한 내용이 1개의 숨겨진 레리어로 되어있는 신경망에서 본 것과 거의 유사하기 방식이기 때문인데요, 이전에서 본 내용에서 단순히 추가적으로 반복하는 것입니다. 저희가 신경망을 도입하는 경우에는, 버그가 없는 도입을 추진하기 위해서는 더욱 더 시스템적으로 생각하고 조심이 신중하게 그 시점에 작업하고 있는 매트릭스의 다이멘션에 대해 생각하는 것입니다. 제가 직접 개인적으로 코딩을 할 때는 종이를 꺼내서 현재 작업하고 있는 것의 매트릭스 다이멘션에 대해 신중하게 생각합니다. 이런 것을 어떻게 하는지 다음 비디오를 통해 알아보겠습니다.