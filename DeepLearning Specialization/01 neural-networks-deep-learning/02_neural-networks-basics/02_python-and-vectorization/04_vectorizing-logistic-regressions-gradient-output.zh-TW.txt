在先前的影片中, 您看到您如何使用向量化來計算他們的預估值 小寫的 a's, 在同一個時間內計算整個訓練集 在這段影片您看到如何使用也用向量化 執行在梯度計算所有的 m 訓練例子 同樣的都在同ㄧ個時間 在這段影片最後內 我們會將所有教過的放在一起, 告訴您如何可以推導 非常有效的羅吉斯迴歸分析建置 您也許記得在梯度計算 我們計算 dz1 在第一個例子 也就是 a1 減 y1 , 然後 dz2 等於 a2 減 y2 等等 等等對於所有 m 訓練例子 我們要做的是定義一個新的變數 dZ 就是 dz1, dz2, .. dzm 再次, 所有 d 小寫z 變數水平疊一起 這個是 1 乘 m 矩陣或者說一個 m 維度行向量 現在回想前面的投影片中 我們已經弄清楚了如何去計算大寫 A 就是 a1 到 am, 我們已經定義大寫 Y 就是 y1 到 ym 您知道, 水平疊起來 基於這些定義 也許您可以自己看 dz 可以計算為 就是 A 減 Y 因為這將是等於 a1 - y1 第一個元素, a2 - y2 在第二個元素等等 第一個元素 a1-y1 正是 dz1 的定義 第二個元素正是 dz2 等等 所以只用一行程式 您可以在一個時間內計算所有的元素 現在, 在前面的建置中 我們已經去掉一個for循環(for loop)但我們還 有這第二個for循環經過這些訓練例子 我們初始 dw 為 0 即 0 向量 但我們還必須迴過 20 個例子 dw 加等於 x1 乘 dz1 第一個訓練例子 dw 加等於 x2 dz2 等等 這樣總共做 m 次, 然後 dw 除以 m, 而 b 同樣作法 db 初始為 0, db 加等於 dz1 db 加等於 dz2 直到 dzm 然後 db 除以 m, 所以這是我們在前面做過的 我們已經去掉了一個循環。 至少現在 dw 是一個向量, 我們分開更新 dw1 dw2 等等 我們已經去掉那個迴圈, 但我們還是需要 有 for循環 在訓練集中的 m 個例子上。 讓我們用這個操作跟向量化它們 這是我們可以做的 db的向量化的建置,它基本上就是總和 所有這些 dz 然後除以 m db 基本上是 1 除以 m 總和dzi從 i 等於 1 到 m 所有 dz 是行向量, 在 python 您要建置的是 1 / m 乘 np. dz 的總和。 您可以拿這個變數然後呼叫 np. sum 函數在它就會給您 db dw 如何做呢? 我寫下 正確的公式就可以驗證是否在做正確的事 dw 原本就是 1 / m 乘上矩陣 X 乘上 dz 轉置 來看看為什麼是這樣 這個等於 1 / m, 然後是矩陣 X x1 到 xm 疊在欄位像這樣, dz 轉置就是 dz1 直到 dz(m) 像這樣 所以, 如果您瞭解這個矩陣乘上這個向量 它實際上是 1 / m 乘 x1 dz1 加 ... 加 xm dzm 所以這是一個 m 乘 1 向量, 而這是您最終得到的 dw 因為 dw 是從 xi dzi 總和起來所以這正是 這個矩陣向量乘積, 再一次 只用一行程式您可以計算 dw 所以向量化建置的導數計算就是 您用這行程式來建置 db 用 這行程式來建置 dw, 並注意,在訓練集上沒有 for 循環, 您可以計算您想更新的參數 現在, 讓我們把所有放在一起您實際上用來建置羅吉斯迴歸分析 這是原本的版本 非常沒有效率的非向量化建置 在前面頭影片中我們做的第一件事是去除這個迴圈 與其迴圈過 dw1, dw2 等等 我們取代這個為向量值 dw 為 dw += xi 是一個向量乘上 dz(i) 現在, 我們看到我們不只可以去掉 只是下面的一個for循環,
但也可以去掉這個for循環。 您可以這麼做 用前面的投影片 您會說, 大寫 Z Z 等於 w 轉置 X + B 您的程式寫成大寫 Z 等於 np.dot W轉置X + B 等於 S型函數 of 大寫 Z 您已經計算了所有這些 i 的值 接下來前一個投影片 我們說過你計算 dz 等於 A-Y. 現在您計算所有這些 i 值 最後 dw 等於 1 / m X dz 轉置而 db 等於 1/m 您知道 np. 和 dz 所以,你剛剛做了正向傳播和反向傳播, 真正計算了預估值跟導數在 所有 m 個訓練例子,而不使用 for循環。 所以梯度下降更新會是 w 更新為 w 減學習率乘上 dw 就是上面計算的跟 B 更新為 B 減學習率乘 db 有時用冒號來表示作為一個指定值, 但我想我對於符號並不一致 但有了這些, 您已經建置了 單次反覆運算的梯度下降,使用羅吉斯迴歸分析 我知道我說過我們應該去掉 明顯的迴圈盡我們可能, 但如果您 建置多重反覆運算 梯度下降, 那您仍然需要完整迴圈經過這遞迴 所以,如果你想做上千次的梯度下降反覆運算, 您還是需要迴圈來一個一個做 這是最外層的迴圈 我想沒辦法來去掉它 但我想已經是很酷的您可以建置 一次遞迴的梯度下降不用到迴圈 就這樣, 您現在有高度向量化跟 高效率的低度下降建置對於羅吉斯迴歸分析 在下一段影片我想談剩下一個細節 就是我們這裡簡單描述過的技術稱為廣播 廣播實際上是 python 跟 numpy 的一個技術 讓您用來使得一部份的程式變得更有效率 讓我們在下一段影片看更多廣播的細節