1
00:00:00,006 --> 00:00:04,743
在上一节课里你看到了
单隐层神经网络是什么样子的

2
00:00:04,743 --> 00:00:08,175
在这节课中 我们将详细了解一下

3
00:00:08,175 --> 00:00:10,361
神经网络计算它的输出的细节

4
00:00:10,361 --> 00:00:15,533
你将会发现 我们只是
将逻辑回归进行多次的重复

5
00:00:15,533 --> 00:00:16,423
让我们一起来看看

6
00:00:16,423 --> 00:00:19,364
这是一个双层的神经网络

7
00:00:19,364 --> 00:00:23,973
我们深入了解一下这个神经网络计算了什么

8
00:00:23,973 --> 00:00:26,653
我们在讲逻辑回归的时候曾经说过

9
00:00:26,653 --> 00:00:31,035
在逻辑回归中 一个圆代表了两步计算

10
00:00:31,035 --> 00:00:34,498
首先 你可以按照这种方式计算z 然后

11
00:00:34,498 --> 00:00:37,754
计算sigmoid(z)作为激活函数

12
00:00:37,754 --> 00:00:40,536
神经网络只是把这个过程做了多次

13
00:00:40,536 --> 00:00:43,953
首先我们注意隐藏层里面的一个节点

14
00:00:43,953 --> 00:00:46,320
我们先看隐藏层的第一个节点

15
00:00:46,320 --> 00:00:48,079
我把其他的节点标为灰色

16
00:00:48,079 --> 00:00:50,820
和左边的逻辑回归类似

17
00:00:50,820 --> 00:00:54,391
隐藏层中的节点进行了两步计算

18
00:00:54,391 --> 00:00:58,418
第一步 和左边的这个节点一样

19
00:00:58,418 --> 00:01:02,754
计算了了z=W^T·x+b 我们使用的符号是

20
00:01:02,754 --> 00:01:08,253
关联了第一隐藏层中的所有节点的

21
00:01:08,253 --> 00:01:13,458
这也是为什么这里有一堆方括号
这是隐藏层的第一个节点

22
00:01:13,458 --> 00:01:16,597
所以我们有一个上标1

23
00:01:16,597 --> 00:01:18,424
首先我们做了这个

24
00:01:18,424 --> 00:01:24,419
第二步就像这样计算
a_1^[1]=sigmoid(z_1^[1])

25
00:01:24,419 --> 00:01:29,013
对于z和a 在符号上的惯例为a^[l]_i

26
00:01:29,013 --> 00:01:33,770
这里的l指的是第l层

27
00:01:33,770 --> 00:01:37,720
这里的i指的则是
第l层中的第i个节点

28
00:01:37,720 --> 00:01:42,344
注意我们刚刚看的是
第一层(隐藏层)中的第一个节点

29
00:01:42,344 --> 00:01:45,878
所以它的上标和下标都是1

30
00:01:45,878 --> 00:01:49,965
这个小圆圈 表示神经网络中的第一个节点

31
00:01:49,965 --> 00:01:52,579
执行了这两步计算

32
00:01:52,579 --> 00:01:58,399
现在我们来看神经网络隐藏层中的第二个节点

33
00:01:58,399 --> 00:02:01,482
和左边的逻辑回归单元类似

34
00:02:01,482 --> 00:02:04,781
这个小圆表示了两步计算

35
00:02:04,781 --> 00:02:08,733
第一步计算z 这里还是第一层

36
00:02:08,733 --> 00:02:12,996
但是已经是第二个节点=W_2^[1]^T·x+b

37
00:02:12,996 --> 00:02:17,880
a_2^[1]=σ(z_2^[1])

38
00:02:17,880 --> 00:02:23,071
你可以暂停视频 再次检查上标和下标

39
00:02:23,071 --> 00:02:28,453
以确保它们遵循了
我们上面约定的书写规则

40
00:02:28,453 --> 00:02:32,831
我们探讨了神经网络中的前两个隐藏单元

41
00:02:32,831 --> 00:02:36,940
第三个和第四个隐藏单元表示了相似的计算

42
00:02:36,940 --> 00:02:39,778
现在 我把这组等式

43
00:02:39,778 --> 00:02:44,169
和这一组等式复制到下一页

44
00:02:44,169 --> 00:02:48,921
这就是我们的神经网络
这是第一个 这是第二个

45
00:02:48,921 --> 00:02:54,050
我们之前计算出的
第一隐藏单元和第二隐藏单元的等式

46
00:02:54,050 --> 00:02:59,022
然后继续写下第三隐藏单元
和第四隐藏单元对应的等式

47
00:02:59,022 --> 00:03:02,093
你就得到了下面的这些等式

48
00:03:02,093 --> 00:03:06,550
明确一下 这是向量W_1^[1]

49
00:03:06,550 --> 00:03:09,430
这是一个向量转置乘以X OK?

50
00:03:09,430 --> 00:03:13,460
所以这里有一个上标T 表示向量转置

51
00:03:13,460 --> 00:03:17,585
你可能猜到了 如果你实际实现一个神经网络

52
00:03:17,585 --> 00:03:20,209
使用for循环来实现它似乎效率很低

53
00:03:20,209 --> 00:03:25,174
所以我们下一步要做的
就是将这四个等式向量化

54
00:03:25,174 --> 00:03:29,348
我们将从如何用向量的方法计算z开始

55
00:03:29,348 --> 00:03:30,859
接下来你可以用这种方法来计算它

56
00:03:30,859 --> 00:03:34,843
将这些W叠放到一个矩阵中

57
00:03:34,843 --> 00:03:38,767
你就得到了w[1],1的转置
这就是一个行向量 或者说

58
00:03:38,767 --> 00:03:42,231
这是一个列向量经过转置所得到的行向量

59
00:03:42,231 --> 00:03:48,494
然后是w[1],2的转置
w[1],3的转置 w[1],4的转置

60
00:03:48,494 --> 00:03:54,499
如此 通过把那四个w向量堆叠在一起
你就得到了一个矩阵

61
00:03:54,499 --> 00:03:59,204
你也可以从另外一个角度来理解
我们现在有四个逻辑回归单元

62
00:03:59,204 --> 00:04:03,913
每一个逻辑回归单元
都有一个相对应的参数向量w

63
00:04:03,913 --> 00:04:06,535
通过堆叠这四个向量

64
00:04:06,535 --> 00:04:08,842
你就得到了这个(4,3)矩阵

65
00:04:08,842 --> 00:04:14,281
所以，如果你用这个矩阵去乘输入变量

66
00:04:14,281 --> 00:04:19,806
x1,x2,x3 通过矩阵乘法运算

67
00:04:19,806 --> 00:04:24,546
你就得到了w1[1]的转置乘以x
w2[1]的转置乘以x

68
00:04:24,546 --> 00:04:30,995
w3[1]的转置乘以x w4[1]的转置乘以x
同时别忘了那些b

69
00:04:30,995 --> 00:04:35,997
我们现在把这些b向量加上去
b[1]1,b[1]2

70
00:04:35,997 --> 00:04:40,811
b[1]3,b[1]4 也就是绿色的这些项

71
00:04:40,811 --> 00:04:45,654
那么这里就是b[1]1,b[1]2,b[1]3,b[1]4

72
00:04:45,654 --> 00:04:50,579
你就会看到结果中
每一行都是一一对应于

73
00:04:50,579 --> 00:04:55,772
上面这四行中的一行

74
00:04:55,772 --> 00:05:00,899
换句话说 我们刚刚展示了
我们得到的结果就等于z[1]1

75
00:05:00,899 --> 00:05:05,303
z[1]2,z[1]3,z[1]4，就像这里定义的一样。

76
00:05:05,303 --> 00:05:10,289
或许你已经猜到了
我们将把这一大坨东西叫做 向量Z[1]

77
00:05:10,289 --> 00:05:15,097
向量Z[1]就是把这些单独的Z
堆叠在一起而形成的列向量

78
00:05:15,097 --> 00:05:19,524
当我们进行向量化时
一个经验能帮到你

79
00:05:19,524 --> 00:05:23,966
那就是 在一层中有不同的神经元时
我们就把他们堆叠起来

80
00:05:23,966 --> 00:05:27,656
这就是为什么 当你有Z[1]1到Z[1]4这些

81
00:05:27,656 --> 00:05:31,852
在隐藏层中对应于不同神经元的时候，

82
00:05:31,852 --> 00:05:36,481
我们就把这四个
垂直堆叠起来而形成Z[1]向量

83
00:05:36,481 --> 00:05:40,457
与此同时 这边这个
通过堆叠w[1]1,w[1]2等

84
00:05:40,457 --> 00:05:45,233
而得到的(4,3)矩阵

85
00:05:45,233 --> 00:05:49,860
我们将把这个矩阵称为W[1]
类似的 这边这个向量

86
00:05:49,860 --> 00:05:54,623
我们将把它成为b[1] 这是一个(4,1)向量

87
00:05:54,623 --> 00:05:59,584
到目前为止 我们已经通过
这个向量矩阵计算了向量Z

88
00:05:59,584 --> 00:06:03,535
最后 我们要做的是去计算出这些a

89
00:06:03,535 --> 00:06:08,195
可能你又猜到了
我们将会通过堆叠来定义a

90
00:06:08,195 --> 00:06:13,019
我们把那些激活值a[1]1到a[1]4堆叠起来

91
00:06:13,019 --> 00:06:18,202
就是把这四个值堆叠在一起 得到向量a[1]

92
00:06:18,202 --> 00:06:21,122
这将会是Sigmoid函数作用在
z[1]上后得到的值

93
00:06:21,122 --> 00:06:25,794
这里这个Sigmoid函数

94
00:06:25,794 --> 00:06:30,761
将会接受z[1]中的每个元素
然后用Sigmoid函数来进行运算

95
00:06:30,761 --> 00:06:36,750
来复习一下 我们通过运算得出

96
00:06:36,750 --> 00:06:41,883
z[1]=W[1]x+b[1] 同时a[1]=Sigmoid*z[1]

97
00:06:41,883 --> 00:06:47,321
我们把这个拷贝到下一页 我们会看到

98
00:06:47,321 --> 00:06:52,156
对于第一层神经网络来说
如果输入是一组x (x向量)

99
00:06:52,156 --> 00:06:56,286
我们就有z[1]=W[1]x+b[1]，同时

100
00:06:56,286 --> 00:07:01,526
a[1]=σ(z[1])

101
00:07:01,526 --> 00:07:06,563
(4,1) = (4,3) x (3,1) + (4,1)

102
00:07:06,563 --> 00:07:11,297
这个的维度是(4,1) 和最后的维度是相同的

103
00:07:11,297 --> 00:07:16,793
还记得吧 我们说过x=a[0] y^=a[2]

104
00:07:16,793 --> 00:07:21,560
所以如果你想的话 你可以用a[0]来替代x

105
00:07:21,560 --> 00:07:25,417
因为a[0]就是x向量的别名。

106
00:07:25,417 --> 00:07:30,968
现在 通过类似的推导 你可以得出
下一层的表达式

107
00:07:30,968 --> 00:07:35,972
这个表达式和第一层的表达式十分类似

108
00:07:35,972 --> 00:07:40,770
输出层(就是z[2])的参数W[2]和b[2]

109
00:07:40,770 --> 00:07:44,549
这里的W[2]是一个(1,4)矩阵

110
00:07:44,549 --> 00:07:47,529
b[2]就是一个实数 你也可以说它是个(1,1)矩阵

111
00:07:47,529 --> 00:07:51,982
z[2]最后也是一个实数 也可以把它写成(1,1)矩阵

112
00:07:51,982 --> 00:07:57,267
这个W[2]是个(1,4)矩阵 用它去乘
a[1]这个(4,1)矩阵 然后加b[2]这个(1,1)矩阵

113
00:07:57,267 --> 00:08:02,397
所以 最后结果就是个实数
然后如果你把最后的输出项

114
00:08:02,397 --> 00:08:07,787
去和逻辑回归做一个类比
(逻辑回归有参数W和b)

115
00:08:07,787 --> 00:08:12,517
你会发现在逻辑回归运算里面的W其实就是起着
神经网络运算里面W[2]转置的作用 或者说

116
00:08:12,517 --> 00:08:16,675
(神经网络的)W[2]就是(逻辑回归的)W的转置，
同时(逻辑回归的)b就等于(神经网络的)b[2]

117
00:08:16,675 --> 00:08:21,665
现在 如果我们把左边这个部分都挡住不看

118
00:08:21,665 --> 00:08:26,434
那么 这最后的输出单元就与逻辑回归非常相似了

119
00:08:26,434 --> 00:08:30,010
有些不同的就是 我们现在不写(逻辑回归参数)W和b

120
00:08:30,010 --> 00:08:35,784
我们写(神经网络的)W[2]和b[2]
它们的维度分别是(1,4)和(1,1)

121
00:08:35,784 --> 00:08:39,765
来复习一下 对于逻辑回归来说 去实现一个输出

122
00:08:39,765 --> 00:08:44,620
或者说去预测一个结果
你会计算z=w^T*x+b

123
00:08:44,620 --> 00:08:51,143
y^等于a 等于 sigmoid of (z). 
(注：y^就等于a[2])

124
00:08:51,143 --> 00:08:55,499
当你有一个神经网络中
有一个隐藏层 你需要做的

125
00:08:55,499 --> 00:09:00,131
就是通过这四个运算去计算最终的输出

126
00:09:00,131 --> 00:09:04,902
你可以把这些运算想象成
第一步--通过向量化运算

127
00:09:04,902 --> 00:09:09,329
得出隐藏层a[1]里这四个逻辑回归的输出
(注：这四个输出是下一层a[2]的输入)

128
00:09:09,329 --> 00:09:13,867
第二步--用a[1]的四个输出作为a[2]层的输入
就像第三第四个运算做的一样

129
00:09:13,867 --> 00:09:18,401
希望你能理解我的表述
其实 你要掌握的就是

130
00:09:18,401 --> 00:09:22,001
你只需要那四行运算代码
去计算神经网络的输出

131
00:09:22,001 --> 00:09:25,706
现在你知道了 给你一个输入参量x

132
00:09:25,706 --> 00:09:30,278
你可以用四行代码就计算出
这个神经网络的输出。

133
00:09:30,278 --> 00:09:34,575
然后与逻辑回归类似 我们也想实现出一个

134
00:09:34,575 --> 00:09:39,002
能够同时在多个样本上
进行运算的向量化算法

135
00:09:39,002 --> 00:09:43,653
我们会看到 通过把训练样本堆叠在
矩阵的不同列中 只要通过很小的改变

136
00:09:43,653 --> 00:09:47,396
我们就能将逻辑回归中的
向量化实现照搬过来

137
00:09:47,396 --> 00:09:50,514
以使得神经网络不仅能
计算单个样本上的输出值

138
00:09:50,514 --> 00:09:55,114
而且能计算整个训练样本集上的输出值

139
00:09:55,114 --> 00:09:57,939
我们将在下一节课中讨论这些细节
GTC字幕组翻译