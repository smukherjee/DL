1
00:00:00,000 --> 00:00:02,705
从本周以及

2
00:00:02,705 --> 00:00:05,735
之前几周的视频里，

3
00:00:05,735 --> 00:00:08,075
你已经了解

4
00:00:08,075 --> 00:00:10,456
前向传播和后向传播的基本框架，

5
00:00:10,456 --> 00:00:14,090
以及构建深度神经网络须知的要点。

6
00:00:14,090 --> 00:00:18,303
我们来看怎样将这些要点合并起来构造深度神经网络。

7
00:00:18,303 --> 00:00:20,333
这是一个有一些层数的神经网络。

8
00:00:20,333 --> 00:00:27,290
让我们来选一层，并只关注该层中的计算。

9
00:00:27,290 --> 00:00:33,410
对于L层，你有参数WL和BL。

10
00:00:33,410 --> 00:00:35,645
对前向传播而言，

11
00:00:35,645 --> 00:00:42,180
你将从前一层输入激励向量AL-1

12
00:00:42,180 --> 00:00:48,895
然后输出AL。

13
00:00:48,895 --> 00:01:00,105
我之前是这样计算的：ZL=WLxAL-1+BL,

14
00:01:00,105 --> 00:01:08,025
和AL=G[ZL]，对吧？

15
00:01:08,025 --> 00:01:12,990
这就是怎样从输入AL-1计算输出AL.

16
00:01:12,990 --> 00:01:15,330
在后面的计算中你会发现，

17
00:01:15,330 --> 00:01:20,640
将ZL的值缓存是非常有用的。

18
00:01:20,640 --> 00:01:24,060
让我也将缓存也包含进来，因为

19
00:01:24,060 --> 00:01:28,170
把ZL的存起来对反向传播很有用

20
00:01:28,170 --> 00:01:31,290
把ZL的存起来对反向传播很有用

21
00:01:31,290 --> 00:01:32,745
对于反向传播

22
00:01:32,745 --> 00:01:34,785
对于反向传播

23
00:01:34,785 --> 00:01:37,390
我们只关注L层的计算，

24
00:01:37,390 --> 00:01:41,635
你将实现一个函数，它的输入为

25
00:01:41,635 --> 00:01:51,840
DAL和输出DAL-1。

26
00:01:51,840 --> 00:01:53,650
为了细节完整，我们说，

27
00:01:53,650 --> 00:01:56,050
输入实际上包括L层的DA

28
00:01:56,050 --> 00:01:59,130
以及缓存

29
00:01:59,130 --> 00:02:04,040
(所以你后面可以使用已经计算好的ZL值)，

30
00:02:04,040 --> 00:02:06,755
接着输出DAL-1，

31
00:02:06,755 --> 00:02:09,940
你也可以输出梯度，如果你想

32
00:02:09,940 --> 00:02:14,025
应用梯度下降学习算法。

33
00:02:14,025 --> 00:02:19,772
这就是基本框架：怎么实现前向传播

34
00:02:19,772 --> 00:02:22,680
(或称前向函数)以及实现反向传播

35
00:02:22,680 --> 00:02:24,970
(我们应该称为反向函数)。

36
00:02:24,970 --> 00:02:27,535
总结一下，在L层，

37
00:02:27,535 --> 00:02:32,650
你要有前向传播或前向函数。

38
00:02:32,650 --> 00:02:38,986
输入AL-1和输出AL，

39
00:02:38,986 --> 00:02:42,070
以及为了计算，

40
00:02:42,070 --> 00:02:45,650
需要使用WL和BL。以及，

41
00:02:45,650 --> 00:02:54,185
我要设置缓存为0，和反向函数

42
00:02:54,185 --> 00:02:56,357
(在反向传播被使用)

43
00:02:56,357 --> 00:03:01,650
将是另外一个函数，输出为

44
00:03:01,650 --> 00:03:08,065
DA[L]和输出DA[L-1]。

45
00:03:08,065 --> 00:03:13,820
这就告诉你，知道激励向量的导数

46
00:03:13,820 --> 00:03:16,950
即DA[L],其他导数是怎样的？

47
00:03:16,950 --> 00:03:20,400
我怎么知道A[L-1]的变化？

48
00:03:20,400 --> 00:03:24,590
这要计算上一层激励向量的导数。

49
00:03:24,590 --> 00:03:29,870
这个方框里，你要使用WL和BL，

50
00:03:29,870 --> 00:03:31,200
你会发现沿着这个方向，

51
00:03:31,200 --> 00:03:34,144
你要计算DZL。

52
00:03:34,144 --> 00:03:36,400
接着这个方框里，

53
00:03:36,400 --> 00:03:43,515
后向函数也输出DWL和DBL，

54
00:03:43,515 --> 00:03:47,740
但是我现在有时用红色箭头表示反向，

55
00:03:47,740 --> 00:03:51,668
要是乐意，你可以将箭头填成红色。

56
00:03:51,668 --> 00:03:55,305
如何你掌握了这两个函数，

57
00:03:55,305 --> 00:03:59,390
那么神经网络的基本计算将是如下这样的：

58
00:03:59,390 --> 00:04:02,453
你将用特征向量A0

59
00:04:02,453 --> 00:04:07,710
输入进来，接着算出第一层的激励向量，

60
00:04:07,710 --> 00:04:09,624
我们称之A1.

61
00:04:09,624 --> 00:04:14,535
算的时候，你需要W1和B1，

62
00:04:14,535 --> 00:04:21,090
同时也要缓存Z1.

63
00:04:21,090 --> 00:04:22,475
好，做完这些之后，

64
00:04:22,475 --> 00:04:25,290
我们把它们输入第二层，

65
00:04:25,290 --> 00:04:28,026
接着使用W2和B2，

66
00:04:28,026 --> 00:04:32,355
计算第二层的激励向量A2，

67
00:04:32,355 --> 00:04:36,475
不断重复，直到最终，

68
00:04:36,475 --> 00:04:40,065
输出A_capsule_L

69
00:04:40,065 --> 00:04:42,655
(等于Y_hat).

70
00:04:42,655 --> 00:04:51,595
沿着这个方向，我们缓存了所有的Z值，

71
00:04:51,595 --> 00:04:55,161
这就是前向传播步。

72
00:04:55,161 --> 00:04:57,585
现在来看后向传播步，

73
00:04:57,585 --> 00:05:03,220
我们将要做的是一系列的后向

74
00:05:03,220 --> 00:05:12,200
迭代，从中后向传播和计算梯度。

75
00:05:12,200 --> 00:05:17,350
我们这么输入DA[L],

76
00:05:17,350 --> 00:05:21,674
接着这个方框里算出DA[L-1]

77
00:05:21,674 --> 00:05:30,424
不断重复，直到得出DA2，DA1.

78
00:05:30,424 --> 00:05:35,500
你还会得到DA0，

79
00:05:35,500 --> 00:05:40,250
但是这个输入特征向量的导数没有什么用，

80
00:05:40,250 --> 00:05:46,237
至少对训练有监督的神经网络的权重是这样的，

81
00:05:46,237 --> 00:05:48,160
所以你可以就此停止。

82
00:05:48,160 --> 00:05:54,365
沿着这个方向，后向传播也输出DWL，DBL。

83
00:05:54,365 --> 00:05:58,800
使用参数WL和BL，

84
00:05:58,800 --> 00:06:04,810
这将输出DW3，

85
00:06:04,810 --> 00:06:09,238
DB3等等。

86
00:06:09,238 --> 00:06:16,660
最后你计算出你需要所有的导数。

87
00:06:16,660 --> 00:06:21,235
也许这个结构还要填入更多参数，

88
00:06:21,235 --> 00:06:28,500
这个方框还需要使用参数WL，

89
00:06:28,500 --> 00:06:34,560
BL。最后等会我们会发现这些方框里面，

90
00:06:34,560 --> 00:06:37,460
我最后还要计算DZ。

91
00:06:37,460 --> 00:06:43,305
训练神经网络的一步迭代从A0 也就是X开始，

92
00:06:43,305 --> 00:06:46,725
完成接下来的前向传播，

93
00:06:46,725 --> 00:06:50,715
计算Y^并用之计算这个，

94
00:06:50,715 --> 00:06:55,680
接着后向传播，这样做[后向]。

95
00:06:55,680 --> 00:06:59,880
好了，你得到了所有导数项，

96
00:06:59,880 --> 00:07:08,830
接着你可以用W减去学习速率乘以每层的Dw来更新W，

97
00:07:08,830 --> 00:07:12,570
B也是类似的。

98
00:07:12,570 --> 00:07:17,757
现在，计算了后向传播和其所有的导数。

99
00:07:17,757 --> 00:07:21,845
这就是神经网络的梯度下降的一步迭代。

100
00:07:21,845 --> 00:07:25,125
在继续之前，还有一个细节。

101
00:07:25,125 --> 00:07:29,665
从概念上讲，将这里的缓存理解为存储

102
00:07:29,665 --> 00:07:34,361
反向函数Z的值是很有用的，

103
00:07:34,361 --> 00:07:35,635
但是当你使用的时候，

104
00:07:35,635 --> 00:07:38,496
你在之前的练习中会发现，当你使用的时候，

105
00:07:38,496 --> 00:07:40,840
你发现使用缓存也许是

106
00:07:40,840 --> 00:07:43,730
获得参数W1，B1

107
00:07:43,730 --> 00:07:46,648
在反向传播函数的数值一种方便的方法。

108
00:07:46,648 --> 00:07:47,868
在之前练习中，

109
00:07:47,868 --> 00:07:49,795
你实际上存储了缓存Z，

110
00:07:49,795 --> 00:07:52,239
以及W和B，

111
00:07:52,239 --> 00:07:57,715
一样也存储Z2，W2，B2.

112
00:07:57,715 --> 00:07:59,860
但是从操作性来说，

113
00:07:59,860 --> 00:08:04,120
我就发现当你计算反向传播时就复制参数到

114
00:08:04,120 --> 00:08:08,975
后面你需要使用的地方是很方便的。

115
00:08:08,975 --> 00:08:15,446
这只是一些实现细节，你一会做练习时会用到它们。

116
00:08:15,446 --> 00:08:20,061
你现在知道了实现深度神经网络的基本框架。

117
00:08:20,061 --> 00:08:21,640
每一层中，有前向传播

118
00:08:21,640 --> 00:08:24,260
和反向传播，

119
00:08:24,260 --> 00:08:27,675
还有在他们之间传递值的缓存。

120
00:08:27,675 --> 00:08:28,930
下一个视频中，

121
00:08:28,930 --> 00:08:32,250
我将讨论怎么实际地使用这些基本框架。

122
00:08:32,250 --> 00:08:33,500
让我们来看下一个视频。
GTC字幕组翻译