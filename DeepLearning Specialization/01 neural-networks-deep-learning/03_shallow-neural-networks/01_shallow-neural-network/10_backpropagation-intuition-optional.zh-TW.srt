1
00:00:00,000 --> 00:00:01,230
在上一段影片中

2
00:00:01,230 --> 00:00:03,720
您看過反向傳播的方程式

3
00:00:03,720 --> 00:00:06,900
在這段影片, 讓我們用直觀的方式

4
00:00:06,900 --> 00:00:10,515
使用計算圖形來看這些方程式的推導

5
00:00:10,515 --> 00:00:12,385
這段影片是可選

6
00:00:12,385 --> 00:00:14,106
隨您意看或不看

7
00:00:14,106 --> 00:00:16,360
不管如何您都可以做作業

8
00:00:16,360 --> 00:00:19,410
記得當我們談論羅吉斯迴歸分析時

9
00:00:19,410 --> 00:00:23,685
我們用這個正向方向來計算 z

10
00:00:23,685 --> 00:00:26,145
然後 a 然後損失

11
00:00:26,145 --> 00:00:27,445
然後求導數時

12
00:00:27,445 --> 00:00:32,520
我們用反向方向來首先計算 da

13
00:00:32,520 --> 00:00:35,400
然後計算 dz

14
00:00:35,400 --> 00:00:40,720
然後計算 dw 跟 db

15
00:00:40,720 --> 00:00:46,970
損失的定義是  L of a,

16
00:00:46,970 --> 00:00:52,655
y 等於負 y log a - 1

17
00:00:52,655 --> 00:00:57,440
- y 乘 log 1 - a

18
00:00:57,440 --> 00:00:59,750
如果您熟悉微積分

19
00:00:59,750 --> 00:01:03,600
您可以求這項目相對於 a 的導數

20
00:01:03,600 --> 00:01:06,156
這會給您 da 的公式

21
00:01:06,156 --> 00:01:09,060
所以 da 等於這個

22
00:01:09,060 --> 00:01:12,750
從微積分可以導出這是

23
00:01:12,750 --> 00:01:18,808
負 y 除以 a 加 1 - y 除以 1 - a

24
00:01:18,808 --> 00:01:23,040
您只需用微積分導出來這個導數

25
00:01:23,040 --> 00:01:26,680
實際上當您反向下一步計算 dz

26
00:01:26,680 --> 00:01:32,430
我們計算出 dz 等於 a - y, 我在前面解釋過

27
00:01:32,430 --> 00:01:37,920
從微積分的連鎖律 dz 是等於

28
00:01:37,920 --> 00:01:45,425
da 乘 g prime of z

29
00:01:45,425 --> 00:01:50,535
這裡 g of z 等於S型函數 of z

30
00:01:50,535 --> 00:01:56,245
是我們的啟動函數對於輸出單元在羅吉斯迴歸分析中,對吧?

31
00:01:56,245 --> 00:02:00,570
只是請記得這還是羅吉斯迴歸分析而我們有 x1, x2

32
00:02:00,570 --> 00:02:05,757
x3 跟一個S型函數單元來給我們 a

33
00:02:05,757 --> 00:02:07,400
給我們 y-hat

34
00:02:07,400 --> 00:02:11,400
這裡的啟動函數是S型函數

35
00:02:11,400 --> 00:02:12,960
並順便說一句

36
00:02:12,960 --> 00:02:17,205
只對那些熟悉微積分連鎖律的您

37
00:02:17,205 --> 00:02:22,520
這裡的原因是因為 a 等於S型函數 of z

38
00:02:22,520 --> 00:02:29,310
所以 偏微 of L 相對於 z 等於 偏微 of

39
00:02:29,310 --> 00:02:36,800
L 相對於 a 乘上 da/dz

40
00:02:36,800 --> 00:02:39,611
這裡的 a 是S型函數 of z

41
00:02:39,611 --> 00:02:42,970
這等於是 d/dz

42
00:02:42,970 --> 00:02:49,080
g(z) 也就是 g prime of z

43
00:02:49,080 --> 00:02:54,060
這是為什麼這個算式 dz 在我們程式中是等於

44
00:02:54,060 --> 00:02:59,484
這個算式也就是 da 在我們的程式乘上 g prime of z

45
00:02:59,484 --> 00:03:05,860
所以這個就是那個

46
00:03:05,860 --> 00:03:09,172
最後這個推導會是否合理在於

47
00:03:09,172 --> 00:03:13,510
您是否熟悉微積分特別是微積分的連鎖律

48
00:03:13,510 --> 00:03:15,325
但如果您不熟悉也不用擔心

49
00:03:15,325 --> 00:03:18,853
我會試著解釋這些直觀當需要時

50
00:03:18,853 --> 00:03:22,315
最後計算完 dz 對於這個迴歸分析

51
00:03:22,315 --> 00:03:26,335
我們來計算 dw , 這會是

52
00:03:26,335 --> 00:03:31,470
dz 乘 x 而 db 就是 dz 當您只有一個訓練例子

53
00:03:31,470 --> 00:03:33,822
所以這是羅吉斯迴歸分析

54
00:03:33,822 --> 00:03:36,700
我們將要做的是當計算

55
00:03:36,700 --> 00:03:40,090
神經網路的反向傳播時是很像這樣的計算

56
00:03:40,090 --> 00:03:46,995
只是要做兩次因為我們沒有 x 進入輸出單元

57
00:03:46,995 --> 00:03:50,930
x 進入隱藏層然後到輸出單元

58
00:03:50,930 --> 00:03:58,405
與其像這樣的計算只用一個步驟

59
00:03:58,405 --> 00:04:04,483
我們會有兩個步驟在這種兩層的神經網路

60
00:04:04,483 --> 00:04:08,586
在這樣兩層神經網路我們有輸入層

61
00:04:08,586 --> 00:04:10,138
一個隱藏層然後是輸出層

62
00:04:10,138 --> 00:04:12,070
記得計算的步驟

63
00:04:12,070 --> 00:04:17,210
首先我們計算 z1 用這個方程式

64
00:04:17,210 --> 00:04:22,177
然後計算 a1 然後計算 z2

65
00:04:22,177 --> 00:04:25,505
注意到 z2 也取決於參數 w2 跟 b2

66
00:04:25,505 --> 00:04:27,530
然後基於 z2

67
00:04:27,530 --> 00:04:32,815
計算 a2, 最後計算損失

68
00:04:32,815 --> 00:04:41,560
而反向傳播做的是反方向計算 da2 然後 dz2

69
00:04:41,560 --> 00:04:48,805
然後反向計算 dw2 跟 db2

70
00:04:48,805 --> 00:04:53,232
反向計算 da1

71
00:04:53,232 --> 00:04:57,278
dz1 等等

72
00:04:57,278 --> 00:05:00,290
我們不用求導數相對於

73
00:05:00,290 --> 00:05:03,745
輸入 x 因為輸入 x 對於監督式學習是固定的(不是變數)

74
00:05:03,745 --> 00:05:07,845
我們不會試著優化 x 所以我們不用對其求導數

75
00:05:07,845 --> 00:05:09,655
至少對於監督式學習

76
00:05:09,655 --> 00:05:15,605
相對於 x, 我將跳過 da2 的一步一步計算

77
00:05:15,605 --> 00:05:18,110
如果您要的話您可以計算

78
00:05:18,110 --> 00:05:20,750
da2 然後用來計算 dz2, 但實用上

79
00:05:20,750 --> 00:05:25,760
您可以折疊這兩個步驟變成一個步驟您會

80
00:05:25,760 --> 00:05:31,715
得到 dz2 = a2 - y 跟以前一樣

81
00:05:31,715 --> 00:05:33,620
您也會有

82
00:05:33,620 --> 00:05:38,615
我將 dw2 跟 db2 寫在下面

83
00:05:38,615 --> 00:05:46,700
您有 dw2 = dz2 * a1轉置

84
00:05:46,700 --> 00:05:52,040
db2 = dz2

85
00:05:52,040 --> 00:05:55,990
這一步驟類似於羅吉斯迴歸分析, 我們有

86
00:05:55,990 --> 00:06:03,550
dw = dz 乘 x 除了現在

87
00:06:03,550 --> 00:06:08,770
a1 扮演了 x 的角色且有一個額外的轉置在這裡因為

88
00:06:08,770 --> 00:06:14,125
介於大寫矩陣 W 跟我們的個別參數 w 的關係

89
00:06:14,125 --> 00:06:16,660
有一個轉置在那裡, 對吧?

90
00:06:16,660 --> 00:06:24,370
因為 W=[---] 行向量在羅吉斯迴歸分析中使用單一個輸出

91
00:06:24,370 --> 00:06:26,980
dw2 像這樣而

92
00:06:26,980 --> 00:06:32,440
這裡的 w 是列向量, 所以這是為什麼有一個額外的轉置在 a1

93
00:06:32,440 --> 00:06:36,980
而在羅吉斯迴歸分析中我們的 x 不用這樣

94
00:06:36,980 --> 00:06:40,335
這樣完成了一半的反向傳播

95
00:06:40,335 --> 00:06:44,045
然後, 同樣的, 您可以計算 da1 如果您要的話

96
00:06:44,045 --> 00:06:49,440
雖然在實用上, 計算 da1 跟

97
00:06:49,440 --> 00:06:52,330
dz1 通常折疊為一步, 所以

98
00:06:52,330 --> 00:06:57,130
您真正建置的是 dz1 = w2

99
00:06:57,130 --> 00:07:03,480
轉置乘 dz2 然後乘上逐元素

100
00:07:03,480 --> 00:07:10,383
乘積對於 g1 prime of z1

101
00:07:10,383 --> 00:07:13,960
我們來檢查一下維度

102
00:07:13,960 --> 00:07:19,510
如果您有一個神經網路像這樣

103
00:07:19,510 --> 00:07:23,000
輸出 y

104
00:07:23,000 --> 00:07:28,265
如果您有 n0, nx=n0 個輸入特徵

105
00:07:28,265 --> 00:07:30,230
n1 隱藏單元

106
00:07:30,230 --> 00:07:34,275
跟 n2

107
00:07:34,275 --> 00:07:36,740
n2 在我們的例子

108
00:07:36,740 --> 00:07:38,565
只是一個輸出單元

109
00:07:38,565 --> 00:07:48,795
那這個矩陣 w2 是 (n2, n1) 維度

110
00:07:48,795 --> 00:07:57,490
z2 跟 dz2 將會是 (n2, 1)  維度

111
00:07:57,490 --> 00:07:59,850
這實際上是 1 乘 1 維度當我們做二元分類時

112
00:07:59,850 --> 00:08:04,750
z1 跟

113
00:08:04,750 --> 00:08:10,045
dz1 將是 n1 乘 1 維度, 對吧?

114
00:08:10,045 --> 00:08:16,115
注意對於任何變數 foo 跟 d foo 永遠是一樣的維度

115
00:08:16,115 --> 00:08:20,850
這是為什麼 w 跟 dw 有相同的維度, 同樣的

116
00:08:20,850 --> 00:08:23,680
b 跟 db, z 跟 dz 等等

117
00:08:23,680 --> 00:08:26,895
為了確定這些維度都配對了

118
00:08:26,895 --> 00:08:35,430
我們有 dz1 = w2 轉置乘 dz2

119
00:08:35,430 --> 00:08:44,490
然後這是逐元素乘積 乘 g1 prime of z1

120
00:08:44,490 --> 00:08:47,040
對應上面的維度

121
00:08:47,040 --> 00:08:52,575
這是 n1 乘  1 = w2 轉置

122
00:08:52,575 --> 00:08:57,945
我們轉置這個所以這會是 n1 乘 n2 維度

123
00:08:57,945 --> 00:09:05,790
dz2 是 n2 乘 1 維度, 而這個是

124
00:09:05,790 --> 00:09:07,230
這是跟 z1 同維度

125
00:09:07,230 --> 00:09:11,820
這也是 n1 乘 1 維度在逐元素乘積中

126
00:09:11,820 --> 00:09:14,350
這個維度是合理的,對吧?

127
00:09:14,350 --> 00:09:18,330
n1 乘 1 維度向量可以從

128
00:09:18,330 --> 00:09:23,520
n1 乘 n2 維度矩陣乘 n2 乘 n1 因為

129
00:09:23,520 --> 00:09:28,890
這兩項的乘積會是 n1 乘 1 維度矩陣所以這變成

130
00:09:28,890 --> 00:09:34,618
逐元素乘積對於兩個 n1 乘 1 維度向量

131
00:09:34,618 --> 00:09:36,060
所以維度相符合

132
00:09:36,060 --> 00:09:40,620
一個建置反向傳播的技巧

133
00:09:40,620 --> 00:09:44,790
如果您確認了您的矩陣的維度都配對了

134
00:09:44,790 --> 00:09:47,190
所以您確認過了這些維度在

135
00:09:47,190 --> 00:09:50,430
這些矩陣包含 w1, w2, z1

136
00:09:50,430 --> 00:09:54,180
z2, a1, a2 等等確認了

137
00:09:54,180 --> 00:09:58,642
這些矩陣運算的維度配對了

138
00:09:58,642 --> 00:10:03,390
有時候這已經去除了很多臭蟲在反向傳播中

139
00:10:03,390 --> 00:10:06,960
這會給我們 dz1 最後

140
00:10:06,960 --> 00:10:12,160
只要搞定 dw1 跟 db1

141
00:10:12,160 --> 00:10:13,965
我們應該寫在這裡

142
00:10:13,965 --> 00:10:17,200
但這裡沒有位置了, 我把它寫在投影片右邊

143
00:10:17,200 --> 00:10:21,965
dw1 跟 db1 給予了下面的公式

144
00:10:21,965 --> 00:10:25,950
這個是等於 dz1 乘 x轉置

145
00:10:25,950 --> 00:10:28,905
這個將是等於 dz

146
00:10:28,905 --> 00:10:34,045
您也許注意到這個方程式跟這個方程式的相似性

147
00:10:34,045 --> 00:10:37,095
這真的不是巧合, 因為 x

148
00:10:37,095 --> 00:10:41,660
扮演 a0 的角色, 所以 x轉置就是 a0 轉置

149
00:10:41,660 --> 00:10:45,484
這些方程式實際上非常相似的

150
00:10:45,484 --> 00:10:50,260
這給了一些感覺反向傳播如何推導

151
00:10:50,260 --> 00:10:54,530
我們有六個方程式, dz2, dw2,

152
00:10:54,530 --> 00:11:00,190
db2, dz1, dw1 跟 db1

153
00:11:00,190 --> 00:11:05,767
讓我複製這六個方程式到下一張投影片

154
00:11:05,767 --> 00:11:08,950
我們已經推導了反向傳播

155
00:11:08,950 --> 00:11:13,959
對於如果您訓練單一訓練例子

156
00:11:13,959 --> 00:11:21,530
但這不意外的是, 與其一次作用一個訓練例子

157
00:11:21,530 --> 00:11:27,810
我們想向量化同時做整個訓練例子

158
00:11:27,810 --> 00:11:30,971
我們記得在正向傳播

159
00:11:30,971 --> 00:11:33,545
當我們運作於一次一個例子時

160
00:11:33,545 --> 00:11:41,665
我們有方程式像這樣 a1 = g1 of z1

161
00:11:41,665 --> 00:11:43,655
為了向量化

162
00:11:43,655 --> 00:11:51,260
我們拿 z 在列疊起來

163
00:11:51,260 --> 00:12:00,775
像這樣 z[1]1, ... z[1]m 稱這個為大寫 Z

164
00:12:00,775 --> 00:12:04,960
然後我們發現把它們用列疊起來

165
00:12:04,960 --> 00:12:10,240
然後定義大寫版本

166
00:12:10,240 --> 00:12:17,093
我們就有 Z1 = W1X + b

167
00:12:17,093 --> 00:12:24,700
跟 A1 = g1 of Z1, 對吧?

168
00:12:24,700 --> 00:12:28,645
我們在這個課程很小心定義這些符號為了確定

169
00:12:28,645 --> 00:12:35,550
將這些例子疊在矩陣中不同的列使得這一切可行

170
00:12:35,550 --> 00:12:40,105
實際上, 如果您仔細透過數學

171
00:12:40,105 --> 00:12:46,645
同樣的技巧在反向傳播也是可行, 所以向量化方程式如下

172
00:12:46,645 --> 00:12:52,250
首先, 如果您拿這些 dz 對於不同的訓練例子，將它們

173
00:12:52,250 --> 00:12:58,339
疊在矩陣中不同列， 這個也一樣，這個也一樣

174
00:12:58,339 --> 00:13:03,070
那這就是向量化的建置, 這個是這樣定義

175
00:13:03,070 --> 00:13:05,569
您如何來計算 dw2

176
00:13:05,569 --> 00:13:11,130
有一個額外的 1/m 因為成本函數 J 是

177
00:13:11,130 --> 00:13:18,410
1/m 和對於 i 從 1 到 m of 損失

178
00:13:18,410 --> 00:13:20,615
當計算導數時

179
00:13:20,615 --> 00:13:23,885
我們有這個額外的 1/m 項目, 就像我們

180
00:13:23,885 --> 00:13:27,982
計算羅吉斯迴歸分析的加權平均

181
00:13:27,982 --> 00:13:31,790
這是您得到 db2 的更新

182
00:13:31,790 --> 00:13:40,640
對於 dz 的和然後 1/m 然後 dz1 如此計算

183
00:13:40,640 --> 00:13:49,109
再一次, 這是逐元素乘積只是之前

184
00:13:49,109 --> 00:13:56,595
我們前面投影片上是 n1 乘 1 維度向量

185
00:13:56,595 --> 00:14:03,185
現在則是 n1 乘 m 維度矩陣

186
00:14:03,185 --> 00:14:09,045
而這些也是 n1 乘 m 維度

187
00:14:09,045 --> 00:14:19,310
這是為什麼這個星號是逐元素乘積, 最後

188
00:14:19,310 --> 00:14:21,454
剩下兩個更新

189
00:14:21,454 --> 00:14:25,836
也許這不應該太意外

190
00:14:25,836 --> 00:14:29,510
我希望給您一些直觀有關於反向傳播如何推導出來

191
00:14:29,510 --> 00:14:32,205
在所有機器學習中

192
00:14:32,205 --> 00:14:34,820
我想反向傳播演算法的推導

193
00:14:34,820 --> 00:14:38,465
真的是我看過最複雜的數學之一

194
00:14:38,465 --> 00:14:42,470
而它需要線性代數跟

195
00:14:42,470 --> 00:14:46,830
矩陣導數從第一個定理開始推導

196
00:14:46,830 --> 00:14:50,165
如果您是矩陣微積分的專家

197
00:14:50,165 --> 00:14:54,255
使用這個方法, 您或許可以自己導出這些演算法

198
00:14:54,255 --> 00:14:57,500
但我想很多深度學習從業者

199
00:14:57,500 --> 00:15:01,060
看過這樣的推導過程的水準像您

200
00:15:01,060 --> 00:15:04,100
看到這段影片, 已經

201
00:15:04,100 --> 00:15:08,580
有所有這些直觀能夠很有效地建置這個演算法

202
00:15:08,580 --> 00:15:10,070
如果您是微積分的專家

203
00:15:10,070 --> 00:15:13,395
看看是否能推導這些從零開始

204
00:15:13,395 --> 00:15:15,665
這是很難的數學

205
00:15:15,665 --> 00:15:20,010
我看過所有機器學習中很難的導數之一

206
00:15:20,010 --> 00:15:22,861
不管如何, 如果您建置這個

207
00:15:22,861 --> 00:15:27,260
這個可行而我相信您有足夠直觀來調整它讓它可行

208
00:15:27,260 --> 00:15:30,830
最後一個細節我想要

209
00:15:30,830 --> 00:15:34,190
跟您分享在您建置您的神經網路之前

210
00:15:34,190 --> 00:15:37,720
也就是如何初始您神經網路的權值

211
00:15:37,720 --> 00:15:40,600
實際上初始化您的參數

212
00:15:40,600 --> 00:15:42,560
不是0而是隨機值

213
00:15:42,560 --> 00:15:45,515
變成很重要在您訓練您的神經網路

214
00:15:45,515 --> 00:15:48,000
在下一段影片, 您會看到為什麼