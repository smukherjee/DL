1
00:00:00,650 --> 00:00:04,360
為什麼神經網路需要非線性啟動函數呢？

2
00:00:04,360 --> 00:00:07,796
實際上,為了使您的神經網路能計算有趣的函數

3
00:00:07,796 --> 00:00:11,325
你確實需要選擇一個非線性
啟動函數,讓我們來看看為什麼。

4
00:00:11,325 --> 00:00:15,920
所以,這裡是神經網路的
四個正向傳播的方程式。

5
00:00:15,920 --> 00:00:17,700
我們為什麼不去掉這個?

6
00:00:17,700 --> 00:00:19,450
去掉函數 g?

7
00:00:19,450 --> 00:00:21,550
將 a1 設置為等於 z1。

8
00:00:21,550 --> 00:00:27,630
或者,你可以說 g(z) 等於 z,對嗎?

9
00:00:27,630 --> 00:00:32,035
有時這被稱為
線性啟動函數。

10
00:00:32,035 --> 00:00:35,070
或者我們稱它為恆等啟動函數.

11
00:00:35,070 --> 00:00:38,170
因為它的輸出會是永遠等於輸入

12
00:00:38,170 --> 00:00:42,576
為此,如果 a2 也正好等於 z2?

13
00:00:42,576 --> 00:00:48,759
事實證明,如果你這樣做,
那麼這個模型只是計算 y 或

14
00:00:48,759 --> 00:00:53,010
y-hat 只作為線性函數,對於輸入 x,

15
00:00:53,010 --> 00:00:55,990
拿前面兩個方程式

16
00:00:55,990 --> 00:01:01,504
如果你有那個a1

17
00:01:01,504 --> 00:01:08,395
=z1=w1x + b1,以及

18
00:01:08,395 --> 00:01:14,459
然後 a2 = z 2 =

19
00:01:14,459 --> 00:01:19,710
w2a1 + b2

20
00:01:19,710 --> 00:01:24,720
然後,如果您採取此 a1 定義,和

21
00:01:24,720 --> 00:01:30,040
取代它,你會發現 a2 =

22
00:01:30,040 --> 00:01:36,880
w2(w1x + b1)

23
00:01:36,880 --> 00:01:42,830
對嗎？所以這是a1,加 b2,所以

24
00:01:42,830 --> 00:01:44,960
這可以簡化成:

25
00:01:45,550 --> 00:01:49,960
(w2w1)x +

26
00:01:49,960 --> 00:01:56,800
(w2b1 + b2)。

27
00:01:58,030 --> 00:02:01,300
所以這只是

28
00:02:01,300 --> 00:02:06,500
讓我們稱這項為 w', b'

29
00:02:06,640 --> 00:02:11,600
因此,這僅等於 w' x + b'。

30
00:02:11,600 --> 00:02:14,440
如果您使用線性啟動函數

31
00:02:14,440 --> 00:02:17,810
或者我們稱它為恆等啟動函數

32
00:02:17,810 --> 00:02:22,660
那這神經網路只是將輸入用線性函數輸出

33
00:02:23,890 --> 00:02:28,580
稍後我們將討論深層網路,
神經網路具有許多,許多層,

34
00:02:28,580 --> 00:02:31,300
許多隱藏的層。事實證明

35
00:02:31,300 --> 00:02:34,760
如果使用線性啟動函數
或者

36
00:02:34,760 --> 00:02:38,860
如果您沒有啟動功能,
然後,不管你的神經網路有多少層

37
00:02:38,860 --> 00:02:43,460
它只會計算線性啟動函數

38
00:02:43,460 --> 00:02:45,590
所以您有隱藏層跟沒有一樣

39
00:02:47,030 --> 00:02:51,270
提一些簡要的案例
,實際上,如果你有

40
00:02:51,270 --> 00:02:57,050
線性啟動函數在這裡和
sigmoid 函數在這裡,然後此模型是

41
00:02:57,050 --> 00:03:02,960
沒有比任何沒有隱藏層的
標準羅吉斯迴歸分析更有用。

42
00:03:02,960 --> 00:03:06,550
所以,我不會費心證明這一點,但
如果你想的話,你可以嘗試這樣做。

43
00:03:06,550 --> 00:03:11,700
一個線性隱藏層幾乎是沒有用處的

44
00:03:11,700 --> 00:03:16,570
因為兩個線性函數的組合還是線性函數

45
00:03:17,570 --> 00:03:21,910
所以除非你用一個非線性的作用
在那裡,那麼你不會計算更多

46
00:03:21,910 --> 00:03:25,320
更有趣的函數,即便您用了更深層網路.

47
00:03:25,320 --> 00:03:29,890
只有一個地方您或許會用到線性啟動函數

48
00:03:29,890 --> 00:03:32,610
g(z) = z.

49
00:03:32,610 --> 00:03:37,170
這是如果你正在做機器
學習迴歸分析問題。

50
00:03:37,170 --> 00:03:39,850
如果 y 是實數

51
00:03:39,850 --> 00:03:43,150
舉個例子, 如果您試著預估房價

52
00:03:43,150 --> 00:03:50,080
所以y不是0,1,而是一個真正的
實數,任何地方 - 我不知道 -

53
00:03:50,080 --> 00:03:55,330
$0, 是房子的價格,然而
昂貴,對,房子,我猜。

54
00:03:55,330 --> 00:04:00,520
也許房子可能是
數百萬美元,所以

55
00:04:00,520 --> 00:04:04,940
不管如何, 有很多房價的資料在您的資料集

56
00:04:04,940 --> 00:04:09,590
如果 y 用這些實數

57
00:04:10,620 --> 00:04:14,640
也許這種情況下可以用線性啟動函數

58
00:04:14,640 --> 00:04:19,490
您的輸出 y-hat

59
00:04:19,490 --> 00:04:22,970
也是實數從負的無窮大到正的無窮大.

60
00:04:24,000 --> 00:04:29,070
但這些隱藏單元,不應該用線性啟動函數

61
00:04:29,070 --> 00:04:34,790
他們可以使用 ReLU 或 tanh 或
Leaky ReLU 或者其他函數。

62
00:04:34,790 --> 00:04:38,200
只有一個地方您或許會用到線性啟動函數

63
00:04:38,200 --> 00:04:40,500
通常在輸出層。

64
00:04:40,500 --> 00:04:47,500
但除此之外,在隱藏層中使用線性的啟動函數,

65
00:04:47,500 --> 00:04:52,000
除了一些非常特殊的情況
與壓縮有關,我們

66
00:04:52,000 --> 00:04:56,360
要談論,使用線性
啟動功能極為罕見。

67
00:04:56,360 --> 00:04:58,990
當然,如果我們
實際預測房價,

68
00:04:58,990 --> 00:05:02,730
正如你看到在第一周的影片,
因為房價都是非負值的,

69
00:05:02,730 --> 00:05:06,760
也許即使這樣,你還是可以使用
ReLU啟動函數,因此

70
00:05:06,760 --> 00:05:10,870
您的輸出 y-hat 會大於,等於 0

71
00:05:10,870 --> 00:05:15,380
所以,我希望這給你一些感覺
為什麼有非線性啟動

72
00:05:15,380 --> 00:05:19,250
啟動函數在神經網路很重要.

73
00:05:19,250 --> 00:05:23,180
接下來,我們將開始
談論梯度下降和

74
00:05:23,180 --> 00:05:26,400
做到這一點,設置
我們關於梯度下降的討論,

75
00:05:26,400 --> 00:05:30,800
在下一個影片中,我想告訴你如何
估計-如何計算-斜率,或

76
00:05:30,800 --> 00:05:34,050
每種啟動函數的導數.

77
00:05:34,050 --> 00:05:35,320
讓我們進入下一段影片