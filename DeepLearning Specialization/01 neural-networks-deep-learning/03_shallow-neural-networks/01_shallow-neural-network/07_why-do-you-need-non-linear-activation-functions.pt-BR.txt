Por que uma rede neural precisa de<br />uma função de ativação não linear? Isso acontece porque para computar<br />funções interessantes, você precisa mesmo de uma função<br />de ativação não-linear. Vamos ver o porquê. Aqui estão as equações de 
propagação para frente, de uma rede neural. Por que apenas não nos livramos disto? Nos livramos da função g<br />e definir a1 igual a z1. Ou de maneira alternativa, você pode dizer<br />que g de z é igual a z, certo? Às vezes, isto é chamado de função de ativação linear. Talvez um nome melhor para isso fosse função identidade de ativação, porque<br />ela apenas retorna a própria entrada. Com esse propósito, e se a'²' fosse apenas igual a z'²'? Acontece que se você fizer isso, então este modelo estará apenas computando<br />y ou ŷ como uma função linear das características x de entrada. Analisando as duas primeiras equações, se você tiver a'¹' = z'¹' = (w'¹'. x + b'¹') z'¹' = (w'¹'. x + b'¹') e só depois disso,
 a'²' = z'²' = (w'²'. a'¹' + b'²') z'²' = (w'²'. a'¹' + b'²'), então se você pegar<br />a definição de a'¹' e substituir aqui, você descobre que a'²' = w'²'.(w'¹'.x + b'¹') + b'²' a'²' = w'²'.(w'¹'.x + b'¹') + b'²', certo? Então, isto é a'¹' + b'²' e então podemos simplificar para (w'²'.w'¹').x + (w'²'.b'¹' + b'²') (w'²'.w'¹').x + (w'²'.b'¹' + b'²') Então isto é apenas, vamos chamar disso de
w-linha e b-linha. Que é igual a
w'. x + b' Se você usar funções de ativação lineares também chamadas de funções identidade de ativação, então, a rede neural está apenas<br />retornando uma função linear da entrada. E falaremos sobre redes profundas mais tarde. Redes neurais de muitas, e muitas camadas, muitas camadas ocultas. E acontece que, se você utilizar uma<br />função de ativação linear ou, de modo alternativo, se você não tiver uma função de ativação, então não importa quantas camadas sua rede neural tenha, tudo que ela está fazendo é apenas<br />computar uma função de ativação linear. Então você pode apenas não ter nenhuma camada oculta. Em alguns dos casos, brevemente mencionados, se você tiver apenas uma função linear de ativação aqui e<br />uma função sigmoide aqui, então este modelo não é mais expressivo do que uma regressão logística padrão sem nenhuma camada oculta. Não vou tentar provar isso,<br />mas você pode tentar, se quiser. Então, a mensagem para guardar é: uma camada linear oculta
é mais ou menos inútil porque a composição de duas funções lineares<br />é uma função linear. Então, a menos que você aplique<br />não-linearidade aqui, você não está computando funções mais interessantes, mesmo que<br />você aprofunde sua rede. Há apenas um lugar que você pode usar funções lineares de ativação, g(z) = z, e isso se você estiver fazendo<br />aprendizado de máquina nos problemas de regressão. Então, se y é um número real. Por exemplo, se você estiver tentando prever preços de casas, y não é 0, nem é 1,
 mas um número real, e você está na faixa de 0 dólares até quão<br />cara uma casa possa custar. Eu acho que casas podem custar, potencialmente,<br />milhões de dólares. Porém, não importa o quanto as casas<br />custem no seu conjunto de dados, mas se y se basear nesses valores
em números reais, então, tudo bem ter uma função linear 
de ativação aqui para que seu resultado, ŷ seja também um número real indo<br />de menos infinito até mais infinito. Mas então, as unidades ocultas 
não devem usar funções lineares de ativação. Elas podem usar ReLU, ou Tanh, ou Leaky ReLU<br />ou talvez outra função diferente. Então, o único lugar que você pode usar uma 
função de ativação linear é, geralmente, na camada de saída. Mas, fora isso, usar uma função linear de ativação em uma camada oculta exceto em<br />circunstâncias muito especiais, relativas à compressão que nós falamos a respeito, usar uma função de ativação linear <br />é extremamente raro (em camadas ocultas). E, é claro, se você estiver realmente prevendo<br />preços de residências, como vimos no vídeo da primeira semana, dado que os preços de casas<br />são valores não negativos, talvez nestes casos você possa usar ReLU <br />como função de ativação para que seu resultado ŷ
 seja maior ou igual a zero. Espero que isso 
dê uma noção do motivo por que ter uma função não-linear de ativação é uma
parte crítica das redes neurais. A seguir, começaremos a falar sobre<br />descidas do gradiente, e para termos uma discussão sobre gradiente decrescente, <br />no próximo video eu quero mostrar como estimar, como computar a inclinação ou as derivadas<br />das funções de ativação. Então vamos para o próximo vídeo.
[Tradução: Plínio Almeida de Oliveira | Revisão: Carlos Lage]