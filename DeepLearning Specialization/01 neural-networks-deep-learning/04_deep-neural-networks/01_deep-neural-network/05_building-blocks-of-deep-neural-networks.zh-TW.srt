1
00:00:00,000 --> 00:00:02,705
在這個禮拜前面的影片及

2
00:00:02,705 --> 00:00:05,735
前面幾個禮拜的影片

3
00:00:05,735 --> 00:00:08,075
您已經看過基本建構礎石

4
00:00:08,075 --> 00:00:10,456
的正向傳播及反向傳播

5
00:00:10,456 --> 00:00:14,090
建置深度神經網路時您需要的關鍵元件

6
00:00:14,090 --> 00:00:18,303
我們來看您如何將這些元件
放在一起來建立深度網路

7
00:00:18,303 --> 00:00:20,333
這是一個網路有幾個層

8
00:00:20,333 --> 00:00:27,290
讓我們選一層只集中注意
到這一層的計算

9
00:00:27,290 --> 00:00:33,410
對於 l 層您有一些參數 wl 跟 bl

10
00:00:33,410 --> 00:00:35,645
對於正向傳播

11
00:00:35,645 --> 00:00:42,180
您會輸入啟動值 a[l-1]

12
00:00:42,180 --> 00:00:48,895
從前面一層來的, 然後輸出 a[l]

13
00:00:48,895 --> 00:01:00,105
我們之前做的方式是您計算 z[l] = w[l] 乘 a[l-1] + b[l]

14
00:01:00,105 --> 00:01:08,025
然後 a[l] = g of z[l], 對吧?

15
00:01:08,025 --> 00:01:12,990
這是您如何從輸入 a[l-1] 到輸出 a[l]

16
00:01:12,990 --> 00:01:15,330
實際上為了以後用到

17
00:01:15,330 --> 00:01:20,640
將 z[l] 值快取(cache)起來會很有用

18
00:01:20,640 --> 00:01:24,060
讓我一起將這個快取寫下來因為

19
00:01:24,060 --> 00:01:28,170
儲存 z[l] 的值對反向有用

20
00:01:28,170 --> 00:01:31,290
對於等一下的反向傳播步驟, 然後

21
00:01:31,290 --> 00:01:32,745
反向傳播步驟

22
00:01:32,745 --> 00:01:34,785
對於反向傳播步驟, 再一次

23
00:01:34,785 --> 00:01:37,390
注意力集中在計算這個 l 層

24
00:01:37,390 --> 00:01:41,635
您將建置這個函數輸入

25
00:01:41,635 --> 00:01:51,840
da[l] 輸出 da[l-1]

26
00:01:51,840 --> 00:01:53,650
來充實一下細節

27
00:01:53,650 --> 00:01:56,050
輸入實際上是 da of l

28
00:01:56,050 --> 00:01:59,130
跟這個 快取

29
00:01:59,130 --> 00:02:04,040
裡面存放您計算過的 z[l] 值

30
00:02:04,040 --> 00:02:06,755
然後除了輸出 da[l-1]

31
00:02:06,755 --> 00:02:09,940
您可以輸出梯度

32
00:02:09,940 --> 00:02:14,025
為了建置梯度下降法來學習

33
00:02:14,025 --> 00:02:19,772
這是基本的架構您如何建置這個正向步驟

34
00:02:19,772 --> 00:02:22,680
我們稱為正向函數以及反向步驟

35
00:02:22,680 --> 00:02:24,970
我們應該稱它為反向函數

36
00:02:24,970 --> 00:02:27,535
總結一下, 在 l 層

37
00:02:27,535 --> 00:02:32,650
您有正向步驟或者說正向傳播
或者說正向函數

38
00:02:32,650 --> 00:02:38,986
輸入 a[l-1] 輸出 a[l]

39
00:02:38,986 --> 00:02:42,070
為了做這個計算

40
00:02:42,070 --> 00:02:45,650
您會用到 w[l] 跟 b[l] 也

41
00:02:45,650 --> 00:02:54,185
輸出快取包含了 z[l], 然後反向函數

42
00:02:54,185 --> 00:02:56,357
使用反向傳播步驟

43
00:02:56,357 --> 00:03:01,650
會是另一個函數輸入

44
00:03:01,650 --> 00:03:08,065
da of l 輸出 da[l-1]

45
00:03:08,065 --> 00:03:13,820
這個告訴您, 給予相對於這個啟動值的導數

46
00:03:13,820 --> 00:03:16,950
也就是 da of l, 導數是什麼?

47
00:03:16,950 --> 00:03:20,400
我希望 a[l-1] 改變多少?

48
00:03:20,400 --> 00:03:24,590
所以來計算相對於前面一層的啟動值的導數

49
00:03:24,590 --> 00:03:29,870
在這個箱子裡您會需要 w[l] 跟 b[l]

50
00:03:29,870 --> 00:03:31,200
實際上一路下來

51
00:03:31,200 --> 00:03:34,144
您也會計算 dz[l]

52
00:03:34,144 --> 00:03:36,400
然後這個箱子

53
00:03:36,400 --> 00:03:43,515
這個反向函數也會輸出 dw[l] 跟 db[l]

54
00:03:43,515 --> 00:03:47,740
但有時候我會用紅色箭頭
來表示反向評測

55
00:03:47,740 --> 00:03:51,668
如果您要的話, 您也可以用紅色表示這些箭頭

56
00:03:51,668 --> 00:03:55,305
如果您可以建置這兩個函數

57
00:03:55,305 --> 00:03:59,390
那基本的神經網路計算如下

58
00:03:59,390 --> 00:04:02,453
您拿這個輸入特徵 a[0]

59
00:04:02,453 --> 00:04:07,710
餵這個進去, 然後計算第一層的啟動值

60
00:04:07,710 --> 00:04:09,624
我們稱之為 a[1]

61
00:04:09,624 --> 00:04:14,535
為了做這個, 您需要 w[1] 跟 b[1]

62
00:04:14,535 --> 00:04:21,090
我們同時將 z[1] 快取起來

63
00:04:21,090 --> 00:04:22,475
做完這個

64
00:04:22,475 --> 00:04:25,290
您將它餵到第二層

65
00:04:25,290 --> 00:04:28,026
然後使用 w[2] 跟 b[2]

66
00:04:28,026 --> 00:04:32,355
您將計算出下一層啟動值 a[2]

67
00:04:32,355 --> 00:04:36,475
等等直到

68
00:04:36,475 --> 00:04:40,065
您最終輸出 a 大寫 L

69
00:04:40,065 --> 00:04:42,655
也就等於 y-hat

70
00:04:42,655 --> 00:04:51,595
一路下來我們快取了這些 z 值

71
00:04:51,595 --> 00:04:55,161
所以這是正向傳播步驟

72
00:04:55,161 --> 00:04:57,585
現在反向傳播步驟

73
00:04:57,585 --> 00:05:03,220
我們要做的是反向一系列的

74
00:05:03,220 --> 00:05:12,200
迭代, 我們反向的計算梯度像這樣

75
00:05:12,200 --> 00:05:17,350
我們從這裡餵 da of l

76
00:05:17,350 --> 00:05:21,674
而這個箱子會給我們 da of l - 1

77
00:05:21,674 --> 00:05:30,424
等等，直到我們到 da[2], da[1]

78
00:05:30,424 --> 00:05:35,500
您實際上可以得到額外一個的輸出 da[0]

79
00:05:35,500 --> 00:05:40,250
但這是相對於您的輸入特徵的導數是沒用的

80
00:05:40,250 --> 00:05:46,237
至少對於訓練這些
監督式神經網路的權重而言

81
00:05:46,237 --> 00:05:48,160
所以您可以停在這裡

82
00:05:48,160 --> 00:05:54,365
一路下來, 反向傳播也輸出了 dw[l], db[l]

83
00:05:54,365 --> 00:05:58,800
用到了 w[l] 跟 b[l] 參數

84
00:05:58,800 --> 00:06:04,810
這個會輸出 dw[3]

85
00:06:04,810 --> 00:06:09,238
db[3] 等等

86
00:06:09,238 --> 00:06:16,660
您最終會計算到所有您所需要的導數

87
00:06:16,660 --> 00:06:21,235
也許多填一點東西到這個架構上

88
00:06:21,235 --> 00:06:28,500
這些箱子也會用到這些參數 w[l],

89
00:06:28,500 --> 00:06:34,560
b[l], 實際上我們以後會看到在這個箱子裡

90
00:06:34,560 --> 00:06:37,460
我們也會計算 dz

91
00:06:37,460 --> 00:06:43,305
一次迭代的神經網路訓練包含了從 a[0] 也就是 x 開始

92
00:06:43,305 --> 00:06:46,725
經歷正向傳播像這樣

93
00:06:46,725 --> 00:06:50,715
計算 y-hat, 然後用它來計算這個

94
00:06:50,715 --> 00:06:55,680
然後反向傳播這樣做

95
00:06:55,680 --> 00:06:59,880
這樣您會得到所有導數

96
00:06:59,880 --> 00:07:08,830
所以 w 會更新為 w 減學習率乘上 dw 對於每一層

97
00:07:08,830 --> 00:07:12,570
b 也一樣

98
00:07:12,570 --> 00:07:17,757
計算反向傳播來得到這些所有導數

99
00:07:17,757 --> 00:07:21,845
這是一個迭代的梯度下降
對於您的神經網路

100
00:07:21,845 --> 00:07:25,125
在進入下一步之前, 有一項資訊細節

101
00:07:25,125 --> 00:07:29,665
概念上，可以將快取想像成儲存

102
00:07:29,665 --> 00:07:34,361
z 的值用來反向函數時用到

103
00:07:34,361 --> 00:07:35,635
但當您建置這個

104
00:07:35,635 --> 00:07:38,496
您看過之前的練習，我們建置這個時

105
00:07:38,496 --> 00:07:40,840
您發現快取也是一個方便的方式來

106
00:07:40,840 --> 00:07:43,730
儲存這些參數 w1,

107
00:07:43,730 --> 00:07:46,648
b1 的值用在反向函數上

108
00:07:46,648 --> 00:07:47,868
在前面的練習中

109
00:07:47,868 --> 00:07:49,795
您實際上在快取上儲存了 z

110
00:07:49,795 --> 00:07:52,239
跟 w, b

111
00:07:52,239 --> 00:07:57,715
所以儲存了 z2, w2, b2

112
00:07:57,715 --> 00:07:59,860
從建置的角度看

113
00:07:59,860 --> 00:08:04,120
我只是發現到這是一個方便的方式
將這些參數複製

114
00:08:04,120 --> 00:08:08,975
等您以後在計算反向傳播時會用到它們

115
00:08:08,975 --> 00:08:15,446
這只是一個建置上的細節
您會在您的程式練習中看到

116
00:08:15,446 --> 00:08:20,061
您已經看到了一個基本的建構基石
在建置深度神經網路上

117
00:08:20,061 --> 00:08:21,640
每一層, 有一個正向傳播步驟

118
00:08:21,640 --> 00:08:24,260
跟一個相對的反向傳播步驟

119
00:08:24,260 --> 00:08:27,675
然後用快取來傳送資訊從一個箱子到另一個箱子

120
00:08:27,675 --> 00:08:28,930
在下一個影片中

121
00:08:28,930 --> 00:08:32,250
我們會談到您如何實際上來建置這些建構基石

122
00:08:32,250 --> 00:08:33,500
讓我們進入下一段影片