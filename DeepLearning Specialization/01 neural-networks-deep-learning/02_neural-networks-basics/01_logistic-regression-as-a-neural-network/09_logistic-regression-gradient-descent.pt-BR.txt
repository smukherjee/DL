Bem-vindo de volta. Neste vídeo, vamos falar sobre como calcular derivadas para você implementar um gradiente descendente para uma regressão logística. Os pontos-chave serão o que você precisa implementar. Ou seja, as principais equações que você precisa para implementar um gradiente descendente para uma regressão logística. Neste vídeo, eu quero fazer estes cálculos usando computação gráfica. Tenho que admitir que usar computação gráfica é um pouco de exagero para se derivar o gradiente descendente para uma regressão logística Mas eu quero começar explicando as coisas desta maneira para que você se familiarize com estas ideias, com sorte, isto fará mais sentido quando falarmos sobre redes neurais plenas. Para isso, vamos conhecer o gradiente descendente para regressão logística. Recapitulando, tínhamos montado uma regressão logística como esta, suas previsões, onde ŷ (y circunflexo), está definido assim, onde Z é isso, e se focarmos em apenas um exemplo agora, então a perda, ou em relação àquele exemplo, estará definido da seguinte forma, onde "a" é o resultado da regressão logística, e y é o rótulo verdadeiro. Vamos escrever isto como uma computação gráfica e para este exemplo, consideraremos que temos apenas dois elementos, x₁ e x₂. Para calcular Z, Vamos precisar informar w₁ w₂ e b, além dos valores das características x₁ e x₂. Estes elementos, em computação gráfica, são usados para calcular z=w₁x₁ + w₂x₂ + b vou marcar com um retângulo. Então, podemos calcular ŷ, ou a = Sigmoide de z, Esse é o próximo passo no gráfico de cálculo e então, finalmente, calculamos ʆ(a,y) e eu não vou copiar a fórmula novamente. Em regressão logística, o que teremos de fazer é modificar os parâmetros 'w' e 'b', a fim de reduzir esta perda. Nós descrevemos passos da propagação para frente, de como realmente calcular a perda de um exemplo de um treinamento único, agora falaremos sobre como você pode retornar para calcular as derivadas. Temos aqui uma versão limpa do diagrama. Pois o que desejamos fazer é calcular derivadas em relação a esta perda, a primeira coisa que desejamos fazer quando retornamos é calcular a derivada desta perda em relação, o script lá, em relação a esta variável 'a'. Assim, no código, você apenas utiliza dₐ
 para denotar esta variável. Acontece que se você tiver familiaridade com Cálculo, você demonstraria que isso acaba sendo
 -y/a + (1-y)/(1-a). E a forma de fazer isso é tomar a fórmula para a perda e, se você tiver familiaridade com Cálculo, você pode calcular a derivada em relação à variável 'a' minúsculo, e você consegue esta fórmula. Mas se você não for familiarizado com Cálculo, não se preocupe. Nós forneceremos a forma derivada, o que mais você precisar, ao longo deste curso. Se você é um expert em Cálculo, eu o encorajo a checar a fórmula para a perda da apresentação passada e tentar obter 
a derivada em relação a 'a' usando Cálculo, mas se você não domina Cálculo bem, não se preocupe com isso. Agora, tendo calculado esta quantidade de dₐ e a derivada ou sua variável final alfa em relação a 'a', você pode então retornar. Acontece que você pode demonstrar dZ que, esta é a parte chamada nome da variável, esta vai ser a derivada da perda (dʆ), sobre a derivada de Z, 
sendo que para ʆ, você poderia, de fato, escrever a perda incluindo 'a' e 'y' assim: dʆ (a,y),
explicitamente como parâmetros ou não, certo? Qualquer tipo de notação é igualmente aceitável. Podemos demonstrar que isto é igual a 
a-y. Só alguns detalhes, apenas para aqueles experts em Cálculo, se você não é expert em Cálculo, não se preocupe com isso. Acontece que este, dʆ sobre dZ, pode ser expresso como dʆ / dₐ vezes dₐ / dZ, e que dₐ sobre dz, vem a ser a vezes (1-a), e dʆ sobre dₐ, 
nós já trabalhamos com ele antes aqui, se você tomar estas duas quantidades, dʆ sobre dₐ, que é este termo, junto com dₐ sobre dz, que é este termo, e apenas pegar estas duas coisas e multiplicá-las, você pode demonstrar que a equação pode ser simplificada por a-y. Então é assim que você deriva isso, e que esta é realmente a regra de cadeia que eludiu brevemente ao formulário. Então sinta-se à vontade para experimentar esse cálculo você mesmo,
se você possui conhecimento de Cálculo, porém se você não tiver, tudo que você precisa saber é que você pode calcular dz como a-y 
e nós já fizemos esse cálculo para você. Então, o passo final na retropropagação é o quanto você precisa alterar 'w' e 'b'. Particularmente, você pode demonstrar que a derivada
em relação a w₁ e entre aspas, chamamo-na de dw₁, que isto é igual a 
x₁ vezes dz. E então, da mesma forma, dw₂, que é o quanto você deseja alterar w₂, é x₂ vezes dz
e db é igual a dz. Logo, se você deseja fazer o gradiente descendente em relação a apenas este exemplo, o que você deve fazer é o seguinte: você deve usar esta fórmula para calcular dz, e então usar estas fórmulas para calcular dw₁, dw₂, e db, e então você executa estas atualizações. w₁ torna-se w₁ menos taxa de aprendizagem alfa vezes dw₁. w₂ se transforma de maneira parecida, e 'b' fica como b menos a taxa de aprendizagem vezes db. E assim, este será um passo de grau em relação a um único exemplo. Assim, você vê em como calcular derivadas e implementar gradiente descendente para regressão logística
 em relação a um exemplo único de treinamento. Mas para praticar modelo de regressão logística, não temos só um exemplo de treinamento,
mas conjuntos inteiros com 'm' exemplos de treinamento. No próximo vídeo, veremos como você pode usar estas ideias,
aplicando-as ao aprendizado, não apenas para um exemplo, mas para um conjunto inteiro de treinamento.
Tradução: Diogo dos Santos Farias
Revisão: Carlos Lage