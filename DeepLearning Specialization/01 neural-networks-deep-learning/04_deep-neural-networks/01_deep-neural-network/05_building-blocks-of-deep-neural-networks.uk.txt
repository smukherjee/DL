В попередніх відео цього тижня, як і у відео кількох попередніх тижнів, ти вже бачив/ла основні структурні блоки прямого і зворотного поширення - ключові компоненти, які потрібні 
для реалізації глибокої нейронної мережі (НМ). Давай подивимось як об'єднати ці блоки, щоб побудувати глибоку НМ. Ось НМ з кількома шарами. Давай виокремимо 1 шар і сфокусуємось лише на 
обчисленнях, що в ному проходять. Тож для шару l ми маємо деякі параметри w[l] і b[l]. І для прямого поширення ми введемо активатори a[l-1] з попереднього шару і виведемо a[l]. Ось як ми це раніше обчислювали: z[l]=w[l]a[l-1]+b[l], потім a[l]=g(z[l]). Правильно? Тож отак ми переходимо від вхідного a[l-1] до вихідного a[l]. Виявляється, що для подальшого використання буде корисно кешувати значення z[l]. [cache z[l]] Тож давай я включу також і кеш, бо зберігання значення z[l] буде корисним для кроку зворотного поширення пізніше. І далі для зворотного кроку (для кроку зворотного поширення), знову ж, фокусуємось на обчисленнях l-ого шару, ми збираємось реалізувати функцію, яка отримує da[l] і видає da[l-1]. Уточню, що що на вході маємо як da[l], так і кеш. Тож нам доступно значення z[l], яке ми обчислили. І далі разом з виведенням da[l-1] ми можемо вивести і градієнти, щоб реалізувати Градієнтний спуск для навчання. Тож це - базова структура реалізації як прямого кроку, який ми називаємо прямою функцією, так і зворотного кроку, яку ми мали б називати зворотною функцією. Тож резюмуємо. В l-ому шарі ми матимемо прямий крок або пряме поширення, 
або пряму функцію, що отримує a[l-1] і видає a[l]. А щоб здійснити обчислення, нам потрібно використати w[l] і b[l] і також вивести і кеш, який містить z[l]. А далі зворотна функція, крок зворотного поширення, буде іншою функцією, яка отримує da[l] і видає da[l-1]. Тобто каже нам, маючи похідні від активаторів (тобто da[l]), які ці похідні. Тобто наскільки ми хочемо щоб змінилось a[l-1]. Тобто обчислює похідні від активаторів з попереднього шару. І в цьому квадраті ми повинні використати w[l] і b[l]. І, виявляється, що по дорозі ми обчислили dz[l]. І потім цей квадрат, ця зворотна функція, може також вивести dw[l] і db[l]. Інколи я використовую червоний колір, 
щоб позначити зворотні обчислення. Тож якщо хочеш, ми можемо їх намалювати червоним. Тож якщо ми можемо реалізувати обидві ці функції, то базові обчислення НМ будуть наступними. Ми беремо вхідні ознаки a[0] і даємо їх на вхід. А це обчислить активатори 1-ого шару. Позначимо їх a[1]. Щоб це зробити нам потрібно w[1] і b[1]. Потім ми також кешуємо z[1]. Зробивши це, ми даємо це на вхід 2-ого шару. Використовуємо w[2] і b[2]. Обчислюємо активатори наступного шару a[2] і т.д. аж поки не дійдемо до a[велике L], що =ŷ. І по дорозі кешуємо всі значення z. Тож це крок прямого поширення. Тепер для кроку зворотного поширення ми побудуємо зворотну послідовність ітерацій, в яких ми рухаємось назад 
і обчислюємо градієнти. Ось так. Тож тут на вході ми маємо da[L]. І цей квадрат дає нам da[L-1] і т.д. доки ми не отримаємо da[2], da[1]. Ми можемо, насправді, піти далі і обчислити da[0], але це похідна від вхідних ознак, яка нам непотрібна. Принаймі для тренування коефіцієнтів цієї контрольованої НМ. Тож ми можемо просто зупинитись тут. По дорозі зворотне поширення теж виводить dw[L], db[L]. Тут використовуються параметри w[l] і b[l]. Тут виведуться dw[3], db[3]. І так далі. І ми обчислимо всі похідні, які нам потрібні. Тепер просто трохи заповнимо ці блоки. Ці квадрати будуть також використовувати ці параметри. І, виявляється, ми ще побачимо пізніше, 
що всередині цих квадратів обчислюються також і dz. Тож ось з чого складається одна ітерація тренування НМ: 
починається з a[0], що є x, проходить через пряме поширення ось так, обчислює ŷ, а далі використовує його, щоб обчислити оце, а потім - зворотне поширення. Тепер ми маємо всі ці рівняння похідних. w буде скориговано як w мінус навчальний темп 
помножений на dw в кожному шарі. І аналогічно для b. І тепер обчислюється зворотне поширення і 
обчислюються всі ці похідні. Тож це - одна ітерація Градієнтного спуску для НМ. Перед тим як піти далі - одна маленька деталь. Згідно концепції, буде дуже корисно 
розуміти під кешем - зберігання значення z для зворотної функції, але при реалізації (ти побачиш це в програмувальній вправі) можна побачити, що кеш може бути зручним способом отримати також і значення параметрів w[1] і b[1] всередині зворотної функції. В програмувальній вправі ти зберігатимеш як кеш z, так і w, і b. Зберігатимуться z[2], w[2], b[2]. А з реалізаційної точки зору, я бачу в цьому ще й зручний спосіб отримання копій параметрів там, де вони використовуватимуться пізніше 
при реалізації зворотного поширення. Тож це просто підказка для реалізації, яку ти побачиш, 
коли виконуватимеш програмувальне завдання. Тож тепер ти ознайомився/лась з основними 
структурними блоками для реалізації глибокої НМ. В кожному шарі є крок прямого поширення і відповідний крок зворотного поширення, і кеш для передачі інформації від одного до іншого. У наступному відео ми поговоримо як практично реалізувати ці структурні блоки. Давай перейдемо до наступного відео.