Коли ти тренуєш свою нейронну мережу (НМ) дуже важливо ініціалізувати коефіцієнти в випадкові значення. Для логістичної регресії (ЛР) підходила й ініціалізація в 0. Але для НМ ініціалізація всіх коефіцієнтів або параметрів в 0 із наступним застосуванням Градієнтного спуску не буде працювати. Давай подивимось чому. Тож ми маємо 2 вхідні ознаки - n[0]=2 - і 2 прихованих вузли - n[1]=2. Тож матриця, пов'язана з прихованим шаром, w[1] матиме розмір 2*2. Скажімо, ми ініціалізуємо її в 0, тобто 0, 0, 0, 0 - матрицю 2*2. І, скажімо, b[1] теж буде 0, 0. Виявляється, ініціалізація в 0 зсувів b є прийнятною, але ініціалізація w в 0 буде проблемою. А проблема такої ініціалізації заключається в тому, що всі зразки - a[1]₁ і a[1]₂ - будуть однаковими. Правильно? Тобто ця активація і оця активація будуть однаковими, бо обидва приховані вузли будуть обчислювати ідентичні функції. А потім, коли буде обчислюватись зворотне поширення, виявиться, що dz[1]₁ і dz[1]₂ теж будуть однаковими або симетричними. Правильно? Обидва приховані вузли ініціалізуватимуться однаковим способом. Я хочу сказати, що технічно я припускаю, що вихідні коефіцієнти теж будуть ідентичними. Тож w[2] буде 0, 0. Тож, якщо ініціалізувати таким чином НМ, то оцей і оцей приховані вузли будуть повністю ідентичні. Інколи ще кажуть, що вони повністю симетричні, що означає, що вони просто обчислюють одну і ту ж функцію. Індуктивне доведення показує, що під час кожної ітерації тренування обидва приховані вузли будуть обчислювати ту саму функцію. Можна показати, що матриця dw виглядатиме отак. Кожний її рядок міститиме ідентичні величини. Тож застосуємо коригування коефіцієнтів. Коли ми застосуємо коригування коефіцієнтів - w[1]:=w[1]-αdw - то побачимо, що після кожного ітерації 1-ий рядок w[1] = 2-ому. Тож можна індукційно довести, що ініціалізація всіх w в 0 (через те, що обидва приховані вузли 
обчислюють ту саму функцію і обидва приховані вузли мають однаковий 
вплив на вихідний вузол), то після однієї ітерації наше твердження справджуватиметься - обидва приховані вузли залишаться симетричними. І, отже, індукційно, після 2-ох ітерацій, після 3-ох ітерацій і т.д. - незалежно від тривалості тренувань НМ - обидва приховані вузли все ще 
обчислюватимуть ту саму функцію. Тож в цьому випадку, насправді, 
нема потреби мати більше одного прихованого вузла, тому що всі вони обчислюватимуть те саме. І, звичайно, для більших НМ, скажімо, з 3-ома ознаками і, можливо, з дуже великою кількістю прихованих вузлів нічого не зміниться. Можна показати, що якщо в отакої НМ (не буду малювати всіх зв'язків) коефіцієнти будуть ініціалізовані в 0, то всі приховані вузли будуть симетричними і не матиме значення скільки часу працюватиме Градієнтний спуск. Всі вузли продовжуватимуть обчислювати одну й ту ж функцію. Тож це не приносить користі, бо ми хочемо, щоб приховані вузли обчислювали різні функції. 
Виходом в цьому випадку є ініціалізація в випадкові значення. 
Тож ось що ми робимо. Ми встановлюємо w[1]= np.random.randn - це генерує Гаусівські випадкові значення - (2, 2). І далі, зазвичай, ми множимо це на маленьке число - типу 0,01. Тож генеруються маленькі випадкові значення. А тепер b. Виявляється, що b не має проблеми симетрії, так званої проблеми порушення рівноваги [symmetry breaking]. Тож b можна ініціалізувати в 0. Тому що через те, що w ініціалізується в випадкові значення, приховані вузли вже будуть обчислювати різні функції. Тож проблеми порушення рівноваги вже не буде. Далі, аналогічно w[2] ініціалізуємо в випадкові значення. А b[2] можемо теж ініціалізувати в 0. Можливо, трохи дивно звідки взялась константа і чому вона =0,01. Чому не 100 чи 1000? Що ж, зазвичай, ми ініціалізуємо коефіцієнти в дуже маленькі випадкові значення. Тому що, якщо використовується, скажімо, функція th або сигмоїда, або сигмоїда є лише в вихідному вузлі, то, якщо коефіцієнти надто великі, при обчисленні значень активаторів (пам'ятаєш, що z[1]=w[1]x+b[1], а потім a[1] - функція активації - обчислюється на основі z[1])... Тож, якщо w дуже велике, z теж буде дуже велике або, принаймі, якісь значення z будуть дуже великими, а інші - дуже малими, то в цьому випадку є більшою ймовірність попасти в оцю ділянку функції th або сигмоїди, де нахил або градієнт - дуже малі. А це значить, що Градієнтний спуск буде дуже повільним. А, отже, і навчання буде дуже повільним. Тож, резюмуємо. Якщо w надто велике, то збільшується ймовірність що, навіть на початку тренування, z буде дуже великим. А це призводить до перенасичення функція th або сигмоїди, що сповільнює навчання. Якщо сигмоїди чи th немає серед активаційних функцій твоєї НМ, то цієї проблеми не буде. Але, якщо ти робиш двійкову класифікацію і на виході маєш сигмоїду, то просто не потрібно ініціалізувати параметри в надто великі значення. Тож ось чому варто застосувати множення на 0,01 або інше маленьке число. І те саме для w[2]. Правильно? Це буде random.randn... повинно бути 1*2 в цьому випадку... помножити на 0,01. Пропустив тут s. Отже, на останок. Виявляється, 
що інколи можна підібрати кращу константу ніж 0,01. Коли ми тренуємо НМ з лише одним прихованим шаром, тобто неглибоку НМ, 
не з багатьма прихованими шарами, то використовувати 0,01 може підійти. Але коли ми тренуємо дуже-дуже глибоку НМ, то, можливо, треба буде підібрати константу відмінну від 0,01. Тож на наступному тижні ми поговоримо трохи про те, як і коли може бути потрібно вибрати константу відмінну від 0,01. Але, в будь-якому випадку, вона завжди буде відносно малим числом. Тож це все на цей тиждень. Тепер ти знаєш як налаштовувати приховані шари НМ, як ініціалізувати параметри, 
як обчислювати передбачення, використовуючи пряме поширення і як обчислювати похідні і реалізовувати Градієнтний спуск, 
використовуючи зворотне поширення. Тож, з цими знаннями, ти повинен/на пройти тестування і виконати цьоготижневі програмувальні вправи. Успіхів тобі у цьому! Я надіюсь тобі сподобаються програмувальні вправи і чекаю тебе на наступному тижні.