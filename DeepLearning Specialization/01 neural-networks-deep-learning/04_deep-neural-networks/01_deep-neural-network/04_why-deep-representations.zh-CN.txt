我们都听说过 深度神经网络
对于很多问题确实很有效 而它们不仅仅需要是 大型的 神经网络 具体而言 它们需要是 深度的
或者 有很多隐藏层 为什么会这样？ 让我们来看几个例子
并试图获得一些直观的感受 为什么深度网络可能会很有效 那么首先 什么是深度网络计算? 如果我们搭建一个系统用于 面部识别 或者 面部检测
那么神经网络就可以在此运用 如果我们输入一张面部图片
那么 神经网络的第一层 可以被我们认为是一个
特征检测器 或 边缘检测器 在这个例子中 我正在绘制一个具有
20个隐藏神经元的 神经网络 可能是 在这个图像上的 某种算法 且这20个隐藏神经元通过这些小方块可视化 所以例如 这个 这个微型可视化图
表示一个 隐藏神经元 正在试图找出 在DMH (Depth-MHI-HOG) 中该方向的边缘位置 也许 这个隐藏神经元可能试图 找出这幅图像中的 水平边缘 在哪里 并且当我们在稍后的课程中讨论 卷积网络 时 这个特殊的可视化可能会更有意义 但是 形式上
我们可以认为神经网络的第一层就好比 看一张图片 并尝试找出这张图片的边缘 现在 让我们来找出这张图片的边缘 通过 将像素分组 来形成边缘的方法 然后 可以取消检测边缘
并将边缘组合在一起 以形成面部的一部分 所以例如 我们可能有一个低神经元
试图看看它是否发现了一个眼睛 或者一个不同的神经元
试图找到鼻子的一部分 所以通过把 大量的边缘 放置在一起 我们可以开始检测面部的不同部位 然后 最后 通过将面部的不同部位 如眼睛 鼻子 耳朵或下巴组合在一起 然后 可以尝试识别或检测不同类型的面部 所以直观地 我们可以将神经网络的浅层 看作是简单的检测函数 如 (检测) 边缘 然后 在神经网络的后一层 将它们组合在一起 以便它可以学习 更多和更复杂 的功能 当我们谈论 卷积网络时 这些可视化将更有意义 (值得注意的是) 这种可视化的一个技术细节 边缘检测器检测图像中相对较小的区域 也许是像这样的非常小的区域 然后 面部检测器可能会看到更大的图像区域 但是 我们从中获取的进一步的关键信息
仅仅是找到像边缘这样的简单事物 然后构建它们 将它们组合在一起<br />以检测更复杂的事物 例如 然后 (再次) 将它们组合在一起<br />以找到更复杂的事物 这种由简单到复杂的 分层表示 或 组合表示 不仅适用于图像和面部识别<br />也适用于其它类型的数据 例如 如果我们尝试建立语音识别系统 则很难难形象化演说 但如果我们输入一段音频剪辑<br />那么 神经网络的第一层 可能学会检测低级的音频波形特征<br />比如这个音调升高了吗? 亦或是降低了? 这是白噪声还是滑动声 如 [声音]? 什么是音调? 当涉及到这一点时
如上所述 检测低级的波形特征 然后通过组合低级波形 (特征) 我们就可以学到检测基本的声音单位 在语言学中 他们称之为音素 但是 例如 在猫这个词中 C是一个音素 A是一个音素 T又是另一个音素 但是学着找到可能的语音基本单元 然后把它们组合在一起 可能用来识别语音中的单词 然后 再把它们组合在一起 用以识别整个短语或句子 因此 拥有众多隐藏层的深度神经网络可能能够让早期的 神经层学习这些较低级别的简单特征 然后让后面更深的神经层
汇聚前面所检测到的简单信息 以便检测更复杂的事物
例如 识别特定的单词 甚至是短语或句子 说出来才能进行语音识别 我们看到的是 尽管其它神经层正在计算着 类似边缘这类 似乎相对简单的输入函数 而当我们深入研究 (神经) 网络的时候
我们实际上可以做一些令人惊讶复杂的事情 例如 检测面部 或是 检测单词 短语 句子 某些人喜欢在深度神经网络与人脑之间做类比 我们认为 或神经科学家认为 人类的大脑开始也检测简单的东西
如你所看到的边缘 然后 (将这些信息) 建立起来
以检测更复杂的事物 如你所看到的面孔 我认为将深度神经网络 与人脑做类比是有点危险的 但这对我们认知人脑的工作存在着很多启迪 那就是 人脑可能检测像边缘这类简单的信息 然后将它们汇聚在一起 来形成愈加复杂的物体 从而成为 人们学习 的松散灵感形式 本周 我们将在视频领域中
看到更多有关人类大脑或生物大脑的内容 本周 我们将在视频领域中
看到更多有关人类大脑或生物大脑的内容 关于深度网络似乎有效的另一条直觉如下 关于深度网络似乎有效的另一条直觉如下 所以这个结果来自电路理论 它涉及到什么类型的函数可以用<br />不同的 (与 或 非) 逻辑情况来计算 所以非正式地 我们用相对较小但具有深度的神经网络来计算这些功能 而这里的小 是说隐藏神经元的数量相对较小 但是 如果我们尝试使用浅网络计算相同的功能 它没有足够多的隐藏层 然后 我们可能需要更多的<br />指数级的隐藏神经元来进行计算 所以让我们来看一个例子<br />来非正式的说明这一点 假设我们尝试计算所有输入特征的异或 或者奇偶性 所以我们尝试计算 X1异或X2异或X3异或... 直到Xn 如果我们有n或者nX特征的话 如果我们像这样自由建立异或树<br />则首先计算X1和X2的异或 然后是X3和X4的异或 而在技术上 如果我们只是使用 与 或 非  门 我们可能需要几层来计算异或函数 而不只是一层 但是使用相对较小的电路<br />我们可以计算异或 等等 然后 你可以建立一个真正的异或树 直到最终 你有一个电路输出<br />好吧 让我们称之为Y 输出向量Y等于Y 也就是 所有这些输入位的异或 或 奇偶性 因此 为了计算异或<br />左侧确定的网络将有logN阶的复杂度 我们就有了一个异或树 所以在此网络中的节点数或电路组件数 或门级数并不是很多 我们并不需要很多门去计算异或 但现在 如果我们不能使用具有多层隐藏层的神经网络  如本例 对数阶的隐藏层 如果我们强制采用一层隐藏层来计算该函数 就像这样 那么 通过隐藏层 将输出Y 那么 为了计算出奇偶性或异或函数 这些隐藏层将会呈指数级的扩大 因为必然地 我们需要穷举2到N种可能性 也就是从2到N 异或为1或0的输入位的所有可能性 所以我们最终需要一个位数是指数级的隐藏层 所以我们最终需要一个位数是指数级的隐藏层 我认为技术上你可以用2到N-1个隐藏单元来做到这一点 但2的指数阶显然比常数阶大得多 所以我希望这个例子告诉我们 对一些数学函数 使用深层网络进行计算比使用浅层网络更容易 实际上 我个人发现电路理论很难理解 但它是人们在解释 深度学习网络的价值时<br />经常引用的例子之一 但它是人们在解释 深度学习网络的价值时<br />经常引用的例子之一 除了这个原因之外 我认为深度学习这个词 除了这个原因之外 我认为深度学习这个词 已经被滥用 被炒作了 神经网络原指有很多的隐藏层的网络 但深度学习已经是一个很大的概念了<br />它包罗万象 所以我认为 我们应该给原来的神经网络 就是有很多隐藏层的神经网络<br />再取一个名字 还给他本来的面目 大家都形成了这样一个概念<br />那就是深度网络确实很有效 有时候人们滥用神经网络 弄太多隐藏层了 但是当我开始解决一个新的问题时<br />我经常会首先采用甚至是逻辑回归 然后尝试一个或两个的隐藏层 把它当作超参数 将其用作我们调整的参数或超参数 来为我们的神经网络找到合适的深度 但是在过去的几年中 人们发现 对于某些应用来说<br />超深的深度神经网络 有时是(解决)问题的最佳模型 这就是为什么深度学习看起来管用的原因 现在让我们看看如何实现正向传播机制 以及反向传播机制