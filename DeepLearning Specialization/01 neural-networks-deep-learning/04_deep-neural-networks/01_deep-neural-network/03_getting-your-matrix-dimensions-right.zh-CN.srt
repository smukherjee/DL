1
00:00:00,028 --> 00:00:04,605
当采用深度神经网络时<br />我常用的一个用于检查代码正确性的调试工具就是

2
00:00:04,605 --> 00:00:08,118
拿出一张纸

3
00:00:08,118 --> 00:00:11,727
然后检查我使用的矩阵的维度

4
00:00:11,727 --> 00:00:15,895
我将告诉你如何做<br />因为我希望这能够帮你更容易的使用深度网络

5
00:00:15,895 --> 00:00:18,275
我将告诉你如何做<br />因为我希望这能够帮你更加容易的使用深度网络

6
00:00:18,275 --> 00:00:23,174
大写L等于5 快点数 不要数输入层

7
00:00:23,174 --> 00:00:27,390
这儿有5层网络 其中 四层是隐藏层以及一层输出层

8
00:00:27,390 --> 00:00:34,878
如果你采用前向传播

9
00:00:34,878 --> 00:00:41,408
那么 第一步将是z1 = w1 * x + b1

10
00:00:41,408 --> 00:00:48,144
现在我们忽略偏置b 只关注参数w

11
00:00:48,144 --> 00:00:54,501
现在第一级隐藏层有3个隐藏单元

12
00:00:54,501 --> 00:00:59,517
这是0层 1层 2层 3层 4层和5层 

13
00:00:59,517 --> 00:01:05,741
所以利用前期视频中的注释

14
00:01:05,741 --> 00:01:11,265
我们有n1 也就是第一层中隐藏单元数 等于3

15
00:01:11,265 --> 00:01:16,202
然后n2=5 n3=4 n4=2以及n5=1

16
00:01:16,202 --> 00:01:23,018
然后n2=5 n3=4 n4=2以及n5=1

17
00:01:23,018 --> 00:01:27,715
到现在为止 我们只看具有一个输出单元的神经网络

18
00:01:27,715 --> 00:01:32,497
在后续课程中 我们也会谈到具有多个输出单元的神经网络

19
00:01:32,497 --> 00:01:36,989
最后 对于输入层

20
00:01:36,989 --> 00:01:40,443
我们有n0=nx=2

21
00:01:40,443 --> 00:01:45,860
现在 让我们思考z w以及x的维度

22
00:01:45,860 --> 00:01:49,120
z是第一级隐藏层的激活矢量

23
00:01:49,120 --> 00:01:54,244
所以z的维度是3×1

24
00:01:54,244 --> 00:01:58,675
是一个3维矢量

25
00:01:58,675 --> 00:02:03,093
我将写作n1个一维矢量

26
00:02:03,093 --> 00:02:08,546
也就是n1×1矩阵 此处为3×1矩阵

27
00:02:08,546 --> 00:02:12,319
那么关于输入特征x呢? 我们有两个输入

28
00:02:12,319 --> 00:02:18,622
所以x也就是2×1矩阵 更一般地 写作n0×1矩阵

29
00:02:18,622 --> 00:02:24,082
所以我们需要的是<br />对于矩阵w

30
00:02:24,082 --> 00:02:30,181
应该满足乘以n0×1矢量得到n1×1矢量 是这样吧?

31
00:02:30,181 --> 00:02:34,747
所以我们有一个3维矢量等于

32
00:02:34,747 --> 00:02:38,600
某个矩阵乘以一个2维矢量

33
00:02:38,600 --> 00:02:42,993
运用矩阵乘法定理

34
00:02:42,993 --> 00:02:46,041
得到一个3×2矩阵

35
00:02:46,041 --> 00:02:51,138
好的 因为3×2矩阵乘以2×1矩阵

36
00:02:51,138 --> 00:02:56,249
或者2×1矢量 等于一个3×1矢量

37
00:02:56,249 --> 00:03:02,771
更一般地说 会得到一个n1×n0的矩阵

38
00:03:02,771 --> 00:03:07,167
所以结果是 w1的维度为n1×n0

39
00:03:07,167 --> 00:03:12,665
所以结果是 w1的维度为n1×n0

40
00:03:12,665 --> 00:03:20,191
更一般地说 wL的维度为nL×n(L-1)

41
00:03:20,191 --> 00:03:26,021
举个例子 w2的维度为 在这里

42
00:03:26,021 --> 00:03:31,508
是5×3

43
00:03:31,508 --> 00:03:35,119
或者是n2×n1

44
00:03:35,119 --> 00:03:40,036
我们将计算z2

45
00:03:40,036 --> 00:03:45,132
等于w2 乘以 a1

46
00:03:45,132 --> 00:03:50,059
这里再一次忽略偏置b

47
00:03:50,059 --> 00:03:54,584
因为a1的维度为3×1

48
00:03:54,584 --> 00:03:59,432
我们需要z2的维度为5×1

49
00:03:59,432 --> 00:04:03,169
所以 w2的维度为5×3

50
00:04:03,169 --> 00:04:10,273
相似地 w3的维度就是下一层的维度

51
00:04:10,273 --> 00:04:15,501
乘以上一层的维度

52
00:04:15,501 --> 00:04:19,266
所以就是4×5

53
00:04:22,055 --> 00:04:27,489
w4的维度就是2×4

54
00:04:27,489 --> 00:04:34,405
w5的维度就是1×2 对吧?

55
00:04:34,405 --> 00:04:38,730
所以一般检测公式是

56
00:04:38,730 --> 00:04:43,416
L层的矩阵维度是nL×n(L-1)

57
00:04:43,416 --> 00:04:48,475
L层的矩阵维度是nL×n(L-1)

58
00:04:48,475 --> 00:04:55,362
现在 让我们思考矢量b的维度

59
00:04:55,362 --> 00:05:01,017
它的维度为3×1 那样你才可以加上另一个3×1矢量

60
00:05:01,017 --> 00:05:06,008
从而获得3×1的输出矢量

61
00:05:06,008 --> 00:05:11,287
或者在这里 我们需要加上这个 它的维度为5×1

62
00:05:11,287 --> 00:05:14,823
那么这需要另一个5×1矢量

63
00:05:14,823 --> 00:05:19,122
为了让被框出的这两个矩阵的和的维度

64
00:05:19,122 --> 00:05:22,767
和它本身一样是5×1

65
00:05:22,767 --> 00:05:30,090
所以更一般的规则是 如左侧的例子

66
00:05:30,090 --> 00:05:35,470
b1的维度是n1×1 也就是3×1

67
00:05:35,470 --> 00:05:41,156
在第二个例子中 维度为n2×1

68
00:05:41,156 --> 00:05:45,891
那么 更一般的情况是

69
00:05:45,891 --> 00:05:50,637
bL的维度应该为nL×1

70
00:05:50,637 --> 00:05:56,402
希望这两个等式能帮助你们再次检查矩阵w

71
00:05:56,402 --> 00:06:02,091
以及矢量b的维度 确保都是正确的维度

72
00:06:02,091 --> 00:06:06,206
当然 如果你用反向传播

73
00:06:06,206 --> 00:06:10,657
那么dw的维度应该和w的维度一致

74
00:06:10,657 --> 00:06:16,373
所以dw和w的维度一样

75
00:06:16,373 --> 00:06:22,276
且db和b的维度一样

76
00:06:22,276 --> 00:06:28,399
现在需要检查维度的其他关键数组是z

77
00:06:28,399 --> 00:06:33,658
z x以及aL 这里我们不会过多谈及

78
00:06:33,658 --> 00:06:39,856
因为zL等于gL作用于aL

79
00:06:39,856 --> 00:06:46,914
那么z和a在这类网络中有着相同的维度

80
00:06:46,914 --> 00:06:51,582
现在让我们看看当我们一次性对多个例子采用矢量化会发生什么

81
00:06:51,582 --> 00:06:53,258
现在让我们看看当我们一次性对多个例子采用矢量化会发生什么

82
00:06:53,258 --> 00:06:56,092
即使是矢量化的实现

83
00:06:56,092 --> 00:07:00,687
显然易见 wb dw以及db的维度保持一致

84
00:07:00,687 --> 00:07:04,929
但是z a以及x的维度

85
00:07:04,929 --> 00:07:09,771
在矢量化过程中 将会发生一点变化

86
00:07:09,771 --> 00:07:13,420
如上所述

87
00:07:13,420 --> 00:07:18,372
z1 = w1 * x + b1

88
00:07:18,372 --> 00:07:23,845
z1 = w1 * x + b1 z1的维度为n1×1

89
00:07:23,845 --> 00:07:28,276
w1的维度为n1×n0

90
00:07:28,276 --> 00:07:35,846
x1的维度为n0×1

91
00:07:35,846 --> 00:07:40,979
b1的维度为n1×1

92
00:07:40,979 --> 00:07:46,398
现在 在矢量化过程中

93
00:07:46,398 --> 00:07:53,536
我们有Z1 = W1 * X + b1

94
00:07:53,536 --> 00:07:58,023
Z1通过提取一系列的z1获得

95
00:07:58,023 --> 00:08:03,575
在此例中 是z11 z12 直到z1m

96
00:08:03,575 --> 00:08:10,207
直到z1m 把它们堆叠在一起从而获得Z1

97
00:08:10,207 --> 00:08:15,042
所以Z1的维度不是n1×1

98
00:08:15,042 --> 00:08:20,285
而是n1×m 且m的大小由我们训练集决定

99
00:08:20,285 --> 00:08:26,140
W1的维度保持一致 仍然是n1×n0

100
00:08:26,140 --> 00:08:29,201
对于X 其维度并不是n0×1

101
00:08:29,201 --> 00:08:33,431
现在是所有的训练样本水平堆叠

102
00:08:33,431 --> 00:08:38,565
所以维度是n0×m 我们注意到当我们

103
00:08:38,565 --> 00:08:43,833
用一个n1×n0矩阵乘以一个n0×m矩阵

104
00:08:43,833 --> 00:08:50,160
实际上会得到一个维度为n1×m的矩阵

105
00:08:50,160 --> 00:08:55,030
现在 最后的细节是b1的维度仍然是n1×1

106
00:08:55,030 --> 00:09:01,147
但是如果你用这个b 那么通过Python自适应特性

107
00:09:01,147 --> 00:09:08,218
它将会被重复以扩展成n1×m矩阵 然后添加此扩展元素

108
00:09:08,218 --> 00:09:14,977
在上一张幻灯片中 我们谈到w b dw以及db的维度

109
00:09:14,977 --> 00:09:21,143
这里 我们看到zL以及aL的维度都是nL×1

110
00:09:21,143 --> 00:09:26,922
zL以及aL的维度都是nL×1

111
00:09:26,922 --> 00:09:34,650
在此 大写ZL和AL的维度都是nL×m

112
00:09:34,650 --> 00:09:40,410
大写ZL和AL的维度都是nL×m

113
00:09:40,410 --> 00:09:45,188
特别地 当L为0时 此时A0

114
00:09:45,188 --> 00:09:49,543
它的维度等于训练集输入特征X

115
00:09:49,543 --> 00:09:54,616
所以为n0×m

116
00:09:54,616 --> 00:10:01,259
当然 当我们在反向传播中应用它时

117
00:10:01,259 --> 00:10:06,749
在后续中将会提到 我们最终会计算dZ和dA

118
00:10:06,749 --> 00:10:11,327
显而易见 它们的维度和Z以及A保持一致

119
00:10:11,327 --> 00:10:15,736
它们的维度和Z以及A保持一致

120
00:10:15,736 --> 00:10:19,467
所以我希望这些小练习能够帮助区分

121
00:10:19,467 --> 00:10:21,685
实际使用中不同矩阵的维度

122
00:10:21,685 --> 00:10:25,947
当我们在深度神经网络中采用反向传播时

123
00:10:25,947 --> 00:10:30,350
请检查一遍你的代码
确保所有的矩阵维度保持一致

124
00:10:30,350 --> 00:10:31,825
这会减少你代码中的错误

125
00:10:31,825 --> 00:10:35,908
这会减少你代码中的错误

126
00:10:35,908 --> 00:10:40,325
我希望这些检查矩阵维度的练习对你有用

127
00:10:40,325 --> 00:10:41,979
我希望这些检查矩阵维度的练习对你有用

128
00:10:41,979 --> 00:10:44,788
当我们用到深度神经网络时

129
00:10:44,788 --> 00:10:48,241
如果你能一直保持这些不同矩阵以及向量维度的一致性

130
00:10:48,241 --> 00:10:52,162
这将会帮助你减少bug

131
00:10:52,162 --> 00:10:54,467
这对我确实很有效

132
00:10:54,467 --> 00:10:58,882
所以接下来 我们都已经知道<br />神经网络中的前向传播的机制

133
00:10:58,882 --> 00:11:01,227
所以接下来 我们都已经知道<br />神经网络中的前向传播的机制

134
00:11:01,227 --> 00:11:04,163
但是为什么深度神经网络如此有效?

135
00:11:04,163 --> 00:11:07,243
为什么它们比浅层网络效果要好?

136
00:11:07,243 --> 00:11:09,939
让我们花点时间在下部视频中探讨它
GTC字幕组翻译