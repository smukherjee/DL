In diesem Video wollen wir die Logistische Regression behandeln. Die Logistische Regression ist ein Lernalgorithmus, welcher benutzt werden kann falls beim überwachten Lernen (= supervised learning) alle Ausgabebezeichnungen stets "0" (= falsch) oder "1" (= wahr) sind; also für binäre Klassifikationsprobleme. Gegeben sei ein Eigenschaftsvektor X als Eingabe, z.B. entsprechend einem Bild, das Sie entweder als Katzenbild oder als kein Katzenbild erkennen möchten. Sie benötigen also einen Algorithmus, der eine Prognose stellen kann, welche - nennen wir sie "Y Dach" - man als eine Schätzung von Y auffassen kann. Formaler: Man möchte, dass "Y Dach" die Wahrscheinlichkeit ausdrückt inwiefern Y gleich "1" ist, bei einem als Eingabe gegebenen Eigenschaftsvektor X. Oder mit anderen Worten: Wenn X ein Bild ist, wie wir es im vorhergehenden Video gesehen haben, möchten Sie von "Y Dach" wissen wie groß die Chance ist, dass es sich um ein Katzenbild handelt. Also X, wie schon im vorhergehenden Video gesagt, ist ein nx-dimensionaler Vektor mit den gegebenen Parametern der Logistischen Regression W, welcher ebenfalls ein nx-dimensionaler Vektor ist, zusammen mit b, welches einfach eine reelle Zahl ist. Folglich, gegeben die Eingabe X und die Parameter W und b: Wie können wir die Ausgabe "Y Dach" generieren? Nun, eine Möglichkeit wie man es versuchen könnte, die aber nicht funktioniert, wäre: "Y Dach" ist gleich W transponiert mal X plus b in der Art einer linearen Funktion auf die Eingabe X. Und in der Tat, das ist was Sie anwenden würden, falls Sie eine Lineare Regression durchführten. Doch dies ist kein sehr guter Algorithmus für die binäre Klassifikation, weil Sie durch "Y Dach" die Wahrscheinlichkeit ausdrücken möchten, dass Y gleich 1 ist. Daher sollte der Wert von "Y Dach" wirklich zwischen 0 und 1 liegen; und es ist schwierig dies zu erzwingen, weil "W transponiert mal X plus b" viel größer als "1" oder sogar negativ sein kann, was jedoch für eine Wahrscheinlichkeit keinen Sinn ergibt; man möchte ja, dass die Wahrscheinlichkeit zwischen 0 und 1 ausgedrückt wird. Bei der logischen Regression wird daher auf unsere Ausgabegröße die Sigmoidfunktion angewendet, was nun "Y Dach" ergibt. Die Sigmoidfunktion sieht so aus: Wenn ich auf der horizontalen Achse "Z "darstelle, dann sieht die Funktion "Sigmoid" auf "Z" etwa so aus. Sie geht kontinuierlich von "0" auf "1". Ich möchte die Achsen hier beschriften. Hier ist "0" ... und sie kreuzt die vertikale Achse bei 0.5 So, das wäre also wie Sigmoid auf "Z" aussieht und wir wollen "Z" benutzen für die Größe "W transponiert mal X plus b". Nun also die Formel für die Sigmoidfunktion: Sigmoid von Z - wobei Z eine reelle Zahl ist - ist gleich '1 dividiert durch (1 plus "e" hoch minus Z)'. Beachten Sie dabei einige Dinge: Wenn Z sehr groß ist, wird "e" hoch minus Z nahezu "0". So, dass Sigmoid von Z ungefähr "eins geteilt durch (eins plus etwas, was sehr nahe bei Null ist)" wird, da "e" hoch minus eine sehr große Zahl praktisch "0" sein wird und so ist dies nahezu "1". Und in der Tat, betrachtet man den Graphen links wird Sigmoid - falls Z sehr groß ist - nahezu "1". Umgekehrt, falls Z sehr klein oder eine sehr große negative Zahl, dann wird Sigmoid von "Z" 'eins über (eins plus "e" hoch die negative Zahl Z)' und das wird eine riesige Nummer, so das dies - denken Sie dabei an "eins plus eine Nummer, die sehr, sehr groß ist" und damit dies nahezu zu Null wird. Und, in der Tat können Sie sehen, dass wenn Z eine sehr große negative Zahl wird, Sigmoid von Z gegen null strebt. Wenn Sie also eine Logistische Regression implementieren, besteht Ihre Aufgabe darin zu versuchen, dass die Parameter "W" und "b" so gelernt werden, dass "Y Dach" eine gute Schätzung für 'Y gleich 1' wird. Bevor wir weiterfahren, eine weiterer Hinweis zur Notation: Wenn wir Neuronale Netzwerke programmieren, behandeln wir den Parameter W und den Parameter b separat, wobei hier "b" einem Abfänger (= interceptor) resp. einem Offset entspricht. In einigen anderen Kursen, haben Sie möglicherweise eine Notation gesehen, welche dies unterschiedlich handhabt. In einigen Konventionen definiert mein eine extra Eigenschaft, man nennt sie X[0], und setzt sie auf "1"; so dass X nun in R ist mit 'nx plus 1'. And dann definiert man "Y Dach" gleich Sigma von Theta transponiert mal X. In dieser alternativen, konventionellen Notation hat man einen Vektorparameter Theta mit Theta[0], Theta[1], Theta[2], bis runter nach Theta [nx] und somit übernimmt Theta[0] die Rolle von b, welche einfach einer reellen Zahl entspricht. und Theta[1] bis runter nach Theta[nx] spielen dann die Rolle von W. Es stellt sich heraus: Wenn man ein Neuronales Netzwerk implementiert, wird es einfacher, wenn man "b" und "W" als eigenständige Parameter beibehaltet. And daher werden wir in dieser Klasse die Konvention der Notation, welche ich soeben in rot geschrieben habe, NICHT verwenden. Falls Sie diese Notation in anderen Kursen noch nicht gesehen haben - kümmern Sie sich einfach nicht darum. Es ist einfach für diejenigen unter Ihnen, die diese Notation bereits gesehen haben und ich wollte einfach nur explizit erwähnen, dass wir diese Notation in diesem Kurs NICHT verwenden. Doch falls sie dies zuvor noch nicht gesehen haben, ist es nicht wichtig und Sie brauchen sich keine Gedanken zu darum zu machen. Alles in allem, haben Sie nun gesehen wie das Modell der Logistischen Regression aussieht. Um als nächstes die Parameter W und b zu trainieren, benötigen Sie eine Kostenfunktion (= cost function). Das machen wir im nächsten Video.