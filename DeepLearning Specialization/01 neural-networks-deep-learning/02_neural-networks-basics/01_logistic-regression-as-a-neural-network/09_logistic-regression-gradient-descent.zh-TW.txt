歡迎回來.歡迎回來. 這個影片中 我們將談到有關如何計算導數讓您 建置羅吉斯迴歸分析梯度下降 重點是您需要建立什麼? 也就是您需要的關鍵方程式來 建立羅吉斯迴歸分析梯度下降法 在這段影片中, 我希望使用計算圖形來做計算 我必須承認, 使用計算圖形是有點 殺雞焉用牛刀對於推論羅吉斯迴歸分析的梯度下降法 但我想開始如此解釋 讓您熟悉這種觀念 希望, 當我們談到正式的神經網路時會比較有感覺 讓我們開始進入羅吉斯迴歸分析的梯度下降法 回顧一下，我們已經建立羅吉斯迴歸分析如下 您的預估 y-hat 定義為 而 z 是這項 如果我們只注意在一個例子上, 那這個損失 相對於那個例子是 定義如下 a 是羅吉斯迴歸分析的輸出 y 是真正的標籤值 讓我們為這個例子寫下計算圖形 假設我們只有兩個特徵 x1 跟 x2 為了計算 Z 我們需要輸入 w1 w2 跟 b, 除了特徵 x1, x2 外 這些東西, 在計算圖形中 用來計算 z , 也就是 w1 x1+w2x2+b 用ㄧ個長方形框住 然後我們計算 y-hat 或者說 a = Sigma of z 這是在這計算圖形地下一步, 最後 我們計算 L(a,y) 我不再複製這個公式 在羅吉斯迴歸分析, 我們要做的是修改這些參數 w 跟 b 為了減少損失 我們已經描述了正向傳播的步驟如何來實際 計算單一訓練例子的損失 現在讓我們談談如何使用反向傳播來計算導數 這個是乾淨的版本 因為我們要計算的是相對於這個損失的導數 第一件事我們要做的是反向來 計算這個損失的導數相對於 花體的 L, 對於變數 a 在程式裡 您只用 da 來記這個變數 如果您熟悉微積分 您可以知道這是 - y/a  + 1-y/1-a 而您要做的是您拿損失函數的公式 如果您熟悉微積分 您可以計算相對於這個變數的導數 小寫 a, 您就會得到這個公式 但如果你不熟悉微積分，別擔心 我們會提供導數公式 您所需要的, 在整個課程裡 如果您是微積分的專家 我鼓勵您看這前面投影片的損失函數的公式 使用微積分來算相對於 a 的導數 但如果您不熟悉微積分, 也不用擔心 現在已經計算了這個項目 da 您最後輸出變數相對於 a 的導數 您可以反向 事實上您可以證明 dz 這是 python 用的變數名稱 這將是損失函數的導數 相對於 z, 對於 L 函數 您也可以這樣寫包含 a 跟 y 明確的作為參數, 是吧? 兩種方式的寫法都可接受 我們可以證明這個是 a - y 對於微積分的專家們這裡有一些註解 如果您不熟悉微積分, 不用擔心 但實際上這個 dL/dz 這可以表示成 dL/da 乘 da/dz 而這個 da/dz 實際上是 a 乘 (1-a) 而 dL/da 前面我們計算過 如果您拿這兩個項目 dL/da 也就是這個項目, 跟 da/dz 一起 也就是這個項目, 拿這兩項相乘 您可以證明這個方程式變成 a - y 這就是您如何推導出這個項目 而這實際上是應用連鎖律來導出 如果您有足夠的微積分知識, 歡迎用微積分自己導出 但如果您不是, 您只需知道您可以計算 dz 是 a - y, 我們為您用微積分算出 最後計算的步驟是返回 計算您需要改變多少 w 跟 b 尤其是, 您可以證明對於 w1 的微分, 在程式中 稱為 dw1, 這是等於 x1 乘 dz 同樣地，dw2, 也就是您想改變多少 w2 是 x2 乘 dz, 而  b 是 db  等於 dz 如果您要做梯度下降相對於這個例子 您要做的是如下 您會用這個公式計算 dz 然後用這個公式計算 dw1, dw2 跟 db, 然後進行更新 w1 更新為 w1 減 學習率 alpha 乘 dw1 w2 也用同樣方式更新 而 b 更新為 b 減學習率乘 db 所以這是梯度下降法的一個遞迴相對於單一例子 您看到了如何計算導數及建立 羅吉斯迴歸分析梯度下降法對於單一訓練例子 但訓練羅吉斯迴歸分析模型 您不只用一的訓練例子, 您的訓練集有 m 個例子 在下一個影片中 讓我們看您如何用這個觀念應用到 不只一個例子 但是在整個訓練集