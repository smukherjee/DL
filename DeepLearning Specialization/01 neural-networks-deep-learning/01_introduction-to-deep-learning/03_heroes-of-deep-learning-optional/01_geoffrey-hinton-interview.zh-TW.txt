部分的deeplearning.ai 課程 希望不只是教您在
深度學習的技術思維, 同時 介紹一些人
一些在深度學習的英雄們 邀請這些人是 因為您在這個專業或這課堂上您所學到的觀念 在這段影片中, 我希望
問問這些在深度學習的領袖 給於您一些職涯上的建議
您如何進入深度學習 您如何做研究
或者在深度學習找到工作 在第一個面談系列 我非常高興邀請到
 Geoffrey Hinton (傑佛里.辛頓) 歡迎 Geoff, 謝謝您
加入 deeplearing.ai 的面談 >>謝謝您的邀請 >>我想在這一時間點您比
這星球上任何一個人 發現了
這麼多深度學習的觀念 而許多人都稱您為
深度學習的教父 雖然直到不久前
我才發現 您覺得我是第一位這樣稱呼您的
我很高興是我先說的 但我想問的是
很多人都知道您是傳奇 我想要問您
在這傳奇背後個人的故事 所以您如何涉入
回到從前， 您如何進入 AI 跟 機器學習跟神經網路? >>當我在中學時
我有一個同學一直 比我行
他是個聰明的數學家 有一天他到學校說
您知道頭腦使用全息影像 我想那是大約在 1966 年
我說什麼是全息影像 他解釋說一個全息影像
您可以砍對半 您還是可以看到整個影像 而這在腦中的記憶也許
分散在整個腦中 我猜他可能讀到
有關 Lashley 實驗 您切掉一部分
老鼠的頭腦 發現很難去找到一點
它儲存一段特別的記憶 所以這是我第一次對於
頭腦儲存記憶有興趣 當我進入大學時 我學習生理學跟
物理學 我想是在劍橋大學 我是唯一的大學生
學習生理學跟物理學 然後我放棄了 試著念哲學, 因為我
覺得也許可以給我一些洞察力 但似乎對我而言實際上 缺乏方法去分辨
當他們說某些東西是假的 所以我轉向心理學 在心理學他們有
很簡單的定理那似乎對我而言 是一種絕望的不足的方式
來解釋頭腦的運作 所以我休學
變成一個木匠 然後我決定試試 AI 
 去了愛丁堡 跟 Langer Higgins 學 AI 他曾經做了很棒的
神經網路工作 他放棄了神經網路
但留下 Winograd's 論文令世人留下深刻印象 所以當我到那裡時他想我
應該做一些舊東西 我應該從符號AI 開始 我們有很大的爭論關於這點, 但
我還是做我相信的 >>然後呢? >>我最終拿到 AI 的博士
但沒法在英國找到工作 但我看到了一個很棒的廣告在 加州的史隆獎學金
我拿到這個獎學金 我到了加州
所有的東西有很大的不同 在英國
神經網路有一點蠢 在加州, Don Norman 跟 David Rumelhart 對於
神經網路的觀念非常開放 這是第一次我在想
頭腦如何運作 想著如何跟
心理學的關係 被視為正向的想法 而有很多的樂趣在裡面 特別是跟
 David Rumelhart 一起工作非常棒 >>懂了
所以這是您在 UCSD (加州聖地牙哥分校) 時 您跟Rumelhart 大約是在 1982年 最後寫了
反向傳播的論文, 對吧? >> 實際上，比這更複雜。 >> 發生了什麼事？ >>我想, 在 1982年初 David Rumlhart 跟我跟 Ron Williams 我們開發這個
反向傳播演算法 主要是 David Rumelhart 的想法 我們發現其他很多人
已經發現它 David Parker 已經發現也許
比我們晚，但比我們早發表 Paul Werbos 已經發表
幾年了 但沒有人注意到 而有其他人也
發展類似的演算法 當時並不清楚反向傳播的意義 但使用連結規則來得到
導數並不是新奇的想法 >>了解, 但您想為什麼
是您們的論文幫助 這個社群連起了反向傳播 似乎是您們的論文被標註為
接受 這演算法的影響, 不管誰接受它 >>我們能夠讓
自然雜誌(Nature)接受這篇論文 我做了許多政治上
的工作讓這份論文被接受 我發現其中一個裁判
將會是 Stuart Sutherland 是一位有名的
英國心理學家 我去跟他
談了很久 跟他解釋
到底這是怎麼回事 而他對此印象深刻 我們展示了反向傳播可以
學習字元表示 您可以看這些
表示法, 是一些向量 您可以理解
每一個特徵的意義 我們實際上訓練了
三個字有關於家族樹 像是瑪莉的母親是維多莉雅 您給他前面兩個字
它會預測最後的字 而在訓練後 您可以見到所有的特徵
表示在每一個字元 像是這個人的國籍 是第幾代, 是
哪一枝旁枝在家族樹中等等 這使得 Stuart Sutherland 
印象深刻 我想這是為什麼論文被接受 >>非常早期的字元嵌入
而您已經見到了 語意的學習特徵
從訓練演算法發起 >>是的, 從心理學的角度
有趣的是它結合了 兩股完全不同的
觀念有關於知識的樣子 有舊式心理學的
觀點, 亦即觀念是一大堆 的特徵
有很多的證據展現這個觀點 也有 AI 的觀點
就是正式的結構觀點 亦即一個觀點
如何跟另一個觀點相關 而為了捕捉觀點
您必須用圖形結構或者 也許語意網路 而反向傳播
的例子展現的是, 您可以給與 一些資訊進入這個圖形
結構, 在這個例子是家族樹 而它可以轉化這些資訊到
特徵藉此用 這些特徵來導出新的
一致性的資訊, 像是第幾世代 但重要的是在
圖形表示中, 或者 樹狀結構表示的家族樹中跟 代表人的巨大特徵向量中來回 而實際上圖形表現
您可以得到特徵 跟向量 而從特徵向量中您可以
得到圖形表現 >>所以這是在 1986? 是在 1990 早期, Bengio 展示了
您可以使用真正的資料 您可以拿英文句子
使用同樣技巧來 得到英文句子的嵌入字
這真的令人印象深刻 >>我猜我們最近使用
很多快速電腦像 GPU 跟 超級電腦也
推動了深度學習 我並不知道在 1986
到90年代早期, 似乎 您跟 Benjio 已經
開始了這個趨勢 >>是的, 那是很大的進步 在 1986我用的機器
是小於 十分之ㄧ的百萬浮點運算 而在大約 1993 
人們已經使用 10 倍的百萬浮點運算 >>是的
>> 所以相差 100 倍 而就是這個觀點
讓它容易使用 因為電腦
越來越快 >>過去幾個世紀
您發明了 這麼多的神經網路跟
深度學習的片段 我很好奇
在所有您發明中 哪一個在今天您還是
最令您興奮 >>我想最漂亮的是
我跟 Terry Sejnowski 一起做的 Boltzmann Machine (玻爾茲曼機) 我們所發現的是真的 很簡單的學習演算法
應用在大型的 密度連結網路, 而您
只能見到一些節點 所以它會學習做隱藏層
它是很簡單的演算法 而它就像
您從人腦得到的東西般因為 每個突觸只需要知道
兩個它連結 的神經元的動作 而這個資訊的
傳導是一樣的 有兩種不同的情況
我們稱之為醒來跟睡著 在不同的情況下 您傳導資訊的
方法是一樣的 而就像反向
傳播一樣, 有正向跟 反向
但作用不同 它們傳達
不同的訊號 所以我想那是
最美麗的事 這幾年來它似乎
只剩好奇心 因為看來它
跑得太慢了 但後來, 我去掉了一些
漂亮的東西, 它開始讓 我安心下來, 只使用一次遞迴
一種簡單的網路 這就是受限制
玻爾茲曼機 實際上
已經可以實作 舉個例子
像在 Netflix 競爭中 受限玻爾茲曼機是
得獎者使用的其中一種成分 >>實際上, 很多最近
再次興起的神經網路跟 深度學習, 從 2007 開始
是受限波爾茲曼機 而受限波爾茲曼機
就是您跟您的實驗室做的 >>是的所以那是另一段
我喜歡做的事 這個觀念是您可以訓練您的
受限波爾茲曼機, 亦即 一層的隱藏特徵
您可以學習一層的特徵 然後您可以視這些
特徵為資料再做一次 然後您可以視這些新學習的特徵
為資料再做一次 喜歡做多少次就做多少次 所以這很棒, 也實際可行 然後 UY Tay 發現這整個
事件可以視為單一模型 只是是很奇怪的模型 這個模型是在上面有
受限波爾茲曼機 下面是 S狀信念
網路是 很多年以前發明的 這是一個直接模型 我們已經設法
訓練這些受限波爾茲曼機 在S狀信念網路能有效的做
推演 所以在那時 也有人做神經網路
他們使用緊密連結網路, 但 沒有好的方法做
機率印記 而您讓人們使用圖形模型
不像我孩子們 可以做好推衍但
侷限在稀疏連結網路上 我們設法做到的是
使用這些深度 信念網路設法接近
很快的推衍的形式 它只是使用單一正向
是一個很漂亮的結果 您可以保證每一次
您學習額外一層的特徵 這裡有一群帶, 每次您學習一個新的層, 您得到一個新的帶, 而 新的帶永遠
比舊的帶好 >>這個變化帶
展示了您加入新的層 是的, 我記得這段影片 >>所以這是我第二件事
我非常興奮 我想第三件事是
我做的變異方法 事實上人們在統計學上
早期已經有類似的 但我們並不知道 所以我們設法讓 EN 作用得更好來顯示
您並不需要用完美的 E 步驟 您可以使用趨近E步驟 而EN是一個大的統計演算法 我們顯示了一個大型的
一般化 特別是在 1993年
我想, 跟 Van Camp 我做了一篇論文
第一篇貝式變異論文 我們展示了您可以
作貝式學習的版本 這是遠遠可以追蹤
趨近於統計後驗 您可以在神經網路做到 而我一直非常興奮 >>我懂了
哇 是的, 我想我記得
這些論文 您 Hinton, 趨近論文
花了許多的時間讀它 而我想一些
您今天使用的演算法 一些演算法許多
人每天使用的, 像是丟棄法(dropout)
我想是來自您團隊的激發 >>是跟不是 所以一些人想到
線性整流函數 我們確實做一些事
用受限波爾茲曼機展示 線性整流函數幾乎相當於
一大疊的羅吉斯單元 而那是其中之一
我們幫助線性整流函數的運用 >>我對此相當好奇 這篇論文有一大堆
數學顯示這個函數 可以趨近於這個
相當複雜的方程式 您做這些數學所以您的論文
可以被學院接受 或者做這些數學真的影響了
取0跟x最大值的開發 >>這是一個例子說明了
數學很重要 去發展一個觀念 很明顯我知道有關線性整流函數 我也知道羅吉斯單元 而因為做
波爾茲曼機 所有基本工作
使用羅吉斯單元 所以問題是 這個學習演算法可以
用線性整流函數? 而顯示了線性整流函數
幾乎相等於一疊 羅吉斯單元, 我們展示了
使用所需的數學來證明 >>了解 而今天它提供了許多啟示
很多人使用 ReLU 它就是可行, 不需要...>>喔 >>不需要必須去
了解同樣的動機 >>是的, 我注意到一件事
在我去谷歌之後 我記得是2014年, 我做一個演講
在谷歌有關使用ReLU 跟使用單位矩陣來做初始化 因為有關ReLU很棒的是
如果您一直複製隱藏 層然後
您用單位矩陣來做初始化 它就只是複製這個
在這一層底下的模式 所以我可以顯示您可以訓練
300隱藏層的網路 您可以有效地訓練他們
如果您用單位矩陣來做初始化 但我並沒有繼續追下去
而我很後悔沒有追下去 我們公布一篇論文來展示
您可以初始 展示您可以初始
重複像那樣 但我應該繼續追下去
因為往後在剩餘 網路實際上就是這樣的東西 >>過去幾年我聽到
您談論有關人腦 我聽到您談論有關於
反向傳播與人腦的關係 您現在的想法如何? >>我實際上正在寫
一篇論文 我想我主要的想法是 如果實際上反向傳播是真的
好的學習演算法 那確定的是進化會
發現到如何去建立它 我的意思是細胞可以
變成眼睛或牙齒 如果細胞可以這樣做, 他們可以
確定可以建立反向傳播 假設有一股強大的
壓力在上面 所以我想神經學家的觀念是
覺得不可行有點蠢 有一些障礙
阻止建立它 但我想人腦也許
不完全是 反向傳播但
也相去不遠 而過去幾年, 我想出
一些想法也關於它如何作用 所以在1987年,跟 Jay McClelland 一起工作 我想出了
再循環演算法 而這個想法是您送
資訊進入一個迴圈 而您試著讓 這些東西保持不變當
資訊在這個迴圈時 所以最簡單的版本是您
有輸入單元跟隱藏單元 您送資訊從輸入
到隱藏然後回到輸入 再回到隱藏跟
回到輸入等等 而您想要
想要訓練一個自動編碼機 但您希望訓練它不
使用反向傳播 您只是訓練它試著除掉
所有不同的動作 這個觀念是學習規則是對於 突觸改變權重
相當於突觸前輸入及 相當於突觸後的
改變率輸入 但在再循環, 您試著
讓突觸後輸入 您試著讓舊的
變好的而新的變壞的 您改變了方向 我們發明了這個演算法在
神經學家提出 尖峰時段依賴可塑性之前 尖峰時段依賴可塑性實際是
一樣的演算法但是 是相反方向, 新的是好的而
舊的是不好的在學習規則中 您改變權重
相對於預設的動作 乘上新的動作減去舊的 後來在2007我理解了
如果您用一疊 受限玻爾茲曼機而
您訓練它們 當訓練完, 您有
完全對的情況來 建立反向傳播
只要重新建構 如果您看這重構
這重構會 實際上告訴您
差別表現的導數 而在第一次深度學習研討會
在2007年, 我演講過這個主題 但完全被忽略了 後來, Joshua Benjo,
重拾起這個觀念 然後實際上做了相當
多的工作 我自己也做了
更多的工作 我想這個觀念是如果您有
一疊的自動編碼機, 您可以 用發送動作
反向跟找出重構者來得到導數 這真是有趣的觀念
也許人腦就是這樣作用 >>我知道您還有其他的主題
是我聽說您還在 做如何處理
深度學習的多重時間技巧 您可以分享您的想法嗎? >>是的, 實際上, 要回到
我在當研究生的第一年 我第一次的演講是有關
使用我稱為快速權重 也就是權重會快速用上
也迅速衰退 所以可以用來保有短期記憶 我顯示一個很簡單的例子
在1973, 您可以 真的使用這些權重做循環 而我所謂真的循環
是神經元用來 重現事情是回收
那些表現事情在循環核心 而權重是用來 回收真的知識
在循環核心 所以這導出一個問題
當您倒出循環核心時 您如何記得
這過程中的事情 記憶放在哪裡? 因為您使用神經元來
做循環核心 而這個答案是您放
記憶在快速權重 您可以重現這些活動
神經元從這些快速權重 而最近跟 Jimmy Ba 一起工作 我們實際上得到一篇論文使用
循環的快速權重 >>了解 >>所以那距離很久 第一個模型是
在1973沒出版 而 Jimmy Ba 模型是在2015
我想, 或者2016 所以是在40年後 >>我想
另一個觀念也是很久以前 超過5年, 我想的是膠囊
現在狀況如何 >>這個要回到
當年我在美國時 我有這個想法, 我真的
相信但是沒有其他人相信 我提出有關的論文而
被拒絕了 但我真的相信這個想法
我持續的推動它 它取決於
幾個關鍵的想法 一個是有關您如何去表示
多重維度的實體, 您 可以用小小的捨棄法來表現多重維度實體 只要您知道
其中任何一個 這個想法是每個影像區域
您假設最多 一個特別的特徵 而您使用一堆神經元
然後 他們的活動會表示
對於這特徵不同的面向 像是在區域裡確實
它的x和y座標 它的方向? 多快速的移動? 什麼顏色? 它多亮? 這些東西 您可以用一大堆的神經元
來表示不同的維度 對於同一件東西 前提是只有一件東西 有很多不同的方式
可以來表示 從我們通常
使用的神經網路 通常在神經網路
我們有一個大層 所有的單元
做他們做的事 但您不會想說將他們綁一起
變成一些小群來表示 不同的座標對於同一件事 所以我想我應該用
這個額外的架構 所以有了另一種想法
來實現它 >>這個意思是在整個的
的表示 您切割這個表示 >>是的
>>到不同的子集 >>是的
>>來表示, 而不是... >>我稱這些子集為膠囊 >>了解 >>而膠囊的想法是能夠
表示一個特徵的實例, 但 只有一個 而它代表了這個特徵所有不同的
屬性 它是一個特徵有很多的
屬性相對於 一個平常的神經元跟一個神經網路
只有一個屬性 >> 我明白了，沒錯 >>而您可以做的是, 如果您了解了
您可以做一些平常 神經網路做不好的, 您
可以做我稱為協議程序 假設您想要
做分割 您有一些也許是嘴巴
一些也許是鼻子 您想知道是否您應該
把他們放在一起做一件事 觀念應該是有一個膠囊相對到 嘴巴, 有一些
嘴巴的參數 您有一個鼻子膠囊
有鼻子參數 然後解碼是否
將他們放在一起 您讓他們每一個投票對於
什麼參數會成為一張臉 如果嘴巴跟鼻子是
正確的空間關係 他們會同意 當您讓兩個膠囊在同一水準
投票相同的參數集在 下一個水準上
您可以假設他們是對的 因為協議在高
維度空間是不太可能的 而這是非常不同
方式做篩選 比起我們通常使用在神經網路 我想這是協議程序
將會是非常關鍵對於 讓神經網路一般化
比起有限資料好很多 我想這是很棒的在
改變看事情的角度 很棒的去做分割 我希望它會更
有統計效率比起我們 目前在神經網路 如果您想要
處理改變慨事情的角度 您就給它一堆改變
視角然後全部訓練他們 >>了解, 與其用
先進先出學習, 監督式學習 您可以用不同方式學習這個 >>我還是想用它
在監督式學習 但正向的機制
是非常不同的 它並非是純粹正向
感覺上有一點做遞迴 您想您發現
一張嘴跟您想您發現一個鼻子 用一些
遞迴來決定 是否他們應該真的
在一起變成一張臉 您可以做反向傳播
從這個遞迴 您可以試著
做一點差異 我們多倫多的團隊正在做這件事 現在我有一個小的谷歌團隊
在多倫多是大腦團隊的一部份 這是我現在很興奮的事 >>了解了，沒錯 非常期待這篇論文 >>是啊, 如果出得來的話[笑] >>您在深度學習
幾十年 我實在很好奇
您如何想的 您的了解AI
在這幾年的改變? >>我想很多我的思考
一直圍繞著反向傳播 如何使用反向傳播
如何使用它的力量 所以從80年中開始
我們使用它來 差異學習
做得很好 然後在90年初我決定 實際上大部分人類學習是
在非監督式學習 我非常有興趣在
非監督式學習 當時我做了
像Wegstein演算法 >>而當時您的意見
真的影響我的想法 當我領導谷歌大腦團隊
我們第一個專案花了很多 工作在非監督式學習
因為您的影響 >>是啊, 我也許誤導您了 因為長期來看 我想非監督式學習
將會是絕對關鍵 但您要面對現實 最近10年真正作用的
是監督式學習 差異訓練
您有標籤 您試著去預測
序列的下一個東西, 那也是標籤 而那作用得非常良好 我還是相信非監督式學習
將會是關鍵，而 會做得比現在好很多
當我們正確得做時 但我們還沒到那個地步 >>是的, 我想很多資深
深度學習的人 包括我在內
仍然保有興奮的期待 就只是我們沒有人
有任何觀念來做它 也許您有, 我不覺得我有 >>差異改變編碼是
您可用來做重新參數化的技巧 在我看來是一個很棒的主意 生成對抗網路
對我而言似乎也是個好主意 我想生成
對抗網路將是一個 很大的觀念在
深度學習中且很新 我希望我可以讓
膠囊那樣成功但 現在而言生成對抗網路
我想有大的突破 >>稀疏跟緩慢的特徵呢? 這是另外兩個原則來
建立非監督性模型 我對稀疏
不像您研究這麼深 但緩慢特徵,我想,是個錯誤 您不應該用緩慢這個字眼 基本的觀念是對的但您不應該
針對沒有改變的特徵 您應該針對
那些可預期改變的特徵 這是基本原則
有關於您如何建模 您拿您的測量
您用在非線性 轉換對於您的
測量直到您得到 一種狀態向量
而您可以用在線性方式 您不要假裝它是線性
像您用一般的篩選器 但您可以發現一種轉換
從觀測到 底下的變數
是線性操作 像是矩陣相乘到底下
變數,會做到您要的 舉例來說
如果您想改變觀點 如果您想要產生一個影像
從另一個觀點 您應該做的是從
像素到座標 一旦您進入
座標展示 這是一種
我希望找到的 您可以做矩陣相乘
來改變觀點 然後您可以回到像素 >>是啊,那是為什麼您要做這些 >>我想這是
很一般的原則 >>這是為什麼您做了所有
臉孔合成的事, 對嗎? 您拿一張臉孔壓縮它
到很低維度向量 您可以擺弄它
得到另一張臉孔 >>我有個學生做這個
我並沒有在上面做太多 >>我相信您還是
常常被問到這個 如果有人想進入深度
學習,他們該怎麼做? 您可以給一點建議? 我相信您給予很多的建議
在一對一的情況下 對於全球的觀眾
觀看這段影片的人 對於進入深度學習
您有何建議? >>好的, 我的建議是讀一些
論文, 但不要讀太多 所以這是我的指導老師的建議，
非常不同于大多數人所說的忠告。 大多數人說，你應該花數
年時間，閱讀論文和 然後你應該開始
致力於自己的想法。 對於某些研究人員也許是對的
但對於有創意的研究人員 您要的是讀
一些論文 而注意到一些您
想每個人都做錯的事 我用相反的方式 您看它
覺得不太對勁 然後想辦法把它弄對 當有人告訴您
這不好, 要用原來的 而我有ㄧ個很好的原則
來幫人們保有它 要麼是你的直覺很好，要麼不是。 如果你的直覺是好的你應該跟隨它們和 你最終會成功 如果你的直覺不是好的
你做什麼都無所謂 >>我明白了[笑] 真是有啟發的建議，我也許會照著做 >> 你也要相信你的直覺。 沒有理由不信任他們 >> 我明白了，沒錯 我通常建議人們不要只讀論文
要複製發表的論文 也許會自然地
侷限您可以做多少 因為複製結果是
相當耗時的 是的的確，當您
想要複製出版論文 你會發現所有必要的小
技巧來使它可行 另一個建議是，
永遠都不要停止程式設計 因為如果您讓一個學生做
一些事，如果他們搞砸了 他們會回來跟您說, 它不可行 它不可行的原因，是他們
所做的一些小的決定 他們沒有意識到那是至關重要的 如果您給它一個好學生
舉個例子 UY Tay 你能給他任何東西，
他會回來，並且說，它可行 我記得有一次，
我說, 稍等一下 UY 自從我們上次談完之後 您說我意識到這並不可行
因為以下原因 然後UY說，我立刻意識到說
我以為你不是那個意思 >>[笑]了解了，沒錯 有沒有其他建議對於 人們想要進入AI或者
深度學習 >>我想基本上, 讀足夠後
開始從直覺開發 然後，信任你的直覺，
就去做 不要太擔心，如果別人說
這是一派胡言 >>我猜沒有其他方法
去知道別人是對 或錯當他們說一派胡言, 但您
就是去做然後,再來發現是對或錯 >> 是吧，但是有這麼回事，就是，
如果你認為它是個很好的主意 而其他人告訴您
這完全是一派胡言 那您知道您
真的在一件很重要的事情上 一個例子是當
我第一次想到差異方法 我送電子郵件去一位以前
學生叫 Peter Brown 他知道很多EN這方面的知識 他拿給跟他
一起工作的人看 稱他為兄弟
他們是雙胞胎 然後他後來告訴我
他們說 這傢伙不是喝醉了
就是太傻了 他們真的
覺得是一派胡言 也許部分的原因
是我的解釋它的方式 因為我用直覺的方式來解釋 但是當你有你
認為是一個好主意時 其他人覺得是垃圾的時候
那是一個很好的跡象 >>我明白了, 而研究主題 新研究生應該
研究膠囊 或是非監督式學習, 還是其他? >>對於新研究生的
一個忠告 看是否能找到指導老師
跟您相信的一致 因為如果您做的東西
您指導教授深深覺得可行 您會得到很好的建議跟
時間從您的指導教授 如果您做的東西您的
指導老師沒有興趣 您得到的會是一些建議
但幾乎不會有所幫助 >>了解,
最後給一些建議給學生 您覺得一個人
去拿博士學位? 或者是去大公司
或是去大的研究單位? >>這個很複雜
我想現在,正在發生的是 沒有足夠的深度學習的學院訓練在
來教育所有人們 在大學所需的教育 就是沒有足夠的
師資 但我想這是短期的問題 我想
大部分的科系已經慢慢的 理解這種
革命正在發生 我有點同意您, 這不像
二次工業革命, 但 是是差不多的規模 這是一個海洋般巨大的改變 基本上因為我們與電腦的關係
發生了變化。 與其用程式設計他們，
我們現在展示給它們，讓它們自己弄明白 這是完全不同的
方式使用電腦，而 電腦科學系都是圍繞
電腦程式設計的想法 他們不明白的是 這樣展示給電腦將會跟
電腦程式設計一樣大 除了他們不了解一半科系的人
應該是 讓電腦做
那些顯示給電腦事情的人 所以我的部門拒絕承認
要很多很多 人做這件事 他們想一些
也許少數，但不要太多 而在這種狀況下 您必須提醒大公司
要做很多的培訓 谷歌現在訓練人
我們稱為大腦居民 我懷疑大學
終究會趕上來 >>了解, 實際上, 也許很多
學生已經發現了 很多頂尖50大學
超過一半確實是 希望工作於展示
而不是程式設計 是的很酷，是的事實上，
把功勞歸到哪 應該是深度學習AI創造了
深度學習的專業化 就我所知，他們第一次深度
學習公開課是實際上你 在Coursera的課, 在2012年 而奇怪的是 那剛好是您第一次出版RMS
演算法, 也是相當的挑戰 >>是的, 如您所知
那是因為您邀我做公開課 然後很懷疑這樣做的時候
，你推我做到這一點，所以 它是很棒的
雖然它是很多的工作 >>是的, 謝謝您的幫忙
我記得您向我抱怨過 它是多麼多的工作 讓您很多次熬夜工作
但我想很多很多學生 從您第一個公開課獲益
所以我很感謝您 >>，很好，是 >> 是的多年來 我看到您被牽連到
有關人工智慧的典範爭論中 是否已經是人工智慧的典範轉移 您可以分享您的想法嗎? >>是的很高興,我想在
早期, 回到50年代 人們像 von Neumann跟Turing
並不相信符號人工智慧 他們遠從人腦啟發 不幸的是，他們都太早逝，
聽不到他們的聲音。 在早期的 AI， 人們完全相信
您需要 智慧的表現是符號
運算式 一種清理過的邏輯，您可以
做非單調邏輯, 並不是 邏輯, 但像是邏輯但那是
推理智慧的本質 現在發生的是
發生完全不同的觀點 這就是所謂的思考, 亦即
一個大的神經活動向量 對比於思考
是符號運算 我想人們想
思考是符號運算是 一個巨大的錯誤 從一個字串進來, 然後
出去一個字串 因為這樣,字串
變成明顯的方式還代表事情 所以他們想一定是
中間也是字串 或者像字串一般 而我想的是中間
一點都不像是字串 我想的是思考應該是
一種語言笨得像 用這種觀念來了解
空間場景 應該是像素, 像素進來 而如果我們可以, 如果有點
矩陣印表機連接 那像素會印出來, 但
中間並不是像素 所以我想思考是
這些巨大的向量, 而 這個巨大的向量有因果能力 他們能導致其他巨大的向量 這是完全不像標準AI的看法
思考是符號運算 >> 明白，太好了 我猜AI應該會走到
這樣的觀點最近的日子 >>有一些 我想很多AI的人還是
想著符號雲算 >> 非常感謝你做這個採訪 聽到這麼迷人的深度
學習如何發展這幾年來 同時聽到我們如何朝向未來
謝謝您, Geoff >> 好的，謝謝你給我這個機會。 謝謝