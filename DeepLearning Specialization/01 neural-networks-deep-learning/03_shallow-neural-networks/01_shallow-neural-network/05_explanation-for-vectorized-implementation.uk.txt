У попередньому відео ми бачили як для тренувальних зразків, 
складених по горизонталі в матрицю X, можна вивести векторизовану реалізацію 
прямого поширення для нейронної мережі (НМ). Давай дамо дещо ширше обґрунтування чому рівняння, 
які ми записали, є правильною реалізацією векторизації для багатьох прикладів. Тож давай розглянемо частину обчислень прямого поширення 
на кількох прикладах. Скажімо, що для першого тренувального зразка ми вивели ось це: z¹⁽¹⁾=w¹x⁽¹⁾+b¹; для другого тренувального зразка - ось це: z¹⁽²⁾=w¹x⁽²⁾+b¹; потім для третього тренувального зразка - ось це: z¹⁽³⁾=w¹x⁽³⁾+b¹. Тож, щоб трохи спростити пояснення, 
я проігнорую b на цьому слайді. Тож, скажімо (щоб трохи спростити обґрунтування), b=0. Проте, аргумент, що залишається буде працювати практично так само як і коли b ненульове. Ми просто трохи спростимо описання на цьому слайді. Тож, w[1] буде деякою матрицею. Ця матриця має деяку кількість рядків. Тож, якщо глянути на ці вирази, ми маємо добуток w[1]x⁽¹⁾, що дає в результаті вектор-стовпець. 
Намалюємо його ось так. Аналогічно, отут ми маємо добуток w[1]x⁽²⁾, що також дає в результаті вектор-стовпець. Це буде z[1]⁽²⁾. І, нарешті, отут ми маємо добуток w[1]x⁽³⁾, що дає в результаті третій вектор-стовпець, оце z[1]⁽³⁾. Тепер, якщо взяти тренувальний набір велике X, який ми формуємо, складаючи докупи наші тренувальні зразки. Тож матриця велике X формується вектором x⁽¹⁾ складеним по вертикалі з x⁽²⁾, а потім з x⁽³⁾. Це у випадку якщо ми маємо лише 3 тренувальні зразки. Коли їх більше, то їх можна просто продовжити складати по горизонталі. І тепер, якщо помножити цю матрицю X на W, то, якщо згадати як множити матриці, ми побачимо, що 1-ий стовпець буде складатись з тих же величин, які я намалював отут фіолетовим, 2-ий стовпець міститиме оці 4 величини, а 3-ій міститиме оці оранжеві величини. Ось як, виявляється, це буде виглядати. І, звичайно, це = z[1]⁽¹⁾, представленому як вектор-стовпець, z[1]⁽²⁾, представленому як вектор-стовпець, z[1]⁽³⁾, також представленому як вектор-стовпець. Це коли ми маємо 3 тренувальні зразки. Якщо ми маємо більше зразків, то тут буде більше стовпців. І це лише матриця велике Z[1]. Тож, надіюсь, це дає обґрунтування чому до цього ми мали w[1]x⁽ⁱ⁾=z[1]⁽ⁱ⁾ коли розглядали окремий тренувальний зразок. А коли складаємо окремі тренувальні зразки в стовпці, то в результаті, який ми отримуємо, z, відповідно, теж складені стовпцями. І я вже не буду показувати, але ти можеш переконатись 
з допомогою посіву [broadcasting] Python, що якщо ми додамо оці величини b, то результат залишиться правильним. І, справді, ми отримаємо посів Python, b⁽ⁱ⁾ додасться до кожного стовпця матриці. Тож на цьому слайді я лише обґрунтував, що Z[1]=W[1]X+b[1] є правильною векторизацією першого кроку з 4-ох, які ми мали на попередньому слайді. І, виявляється, що подібним чином можна показати, що інші кроки можна обчислити, користуючись схожою логікою: якщо скласти вхідні дані в стовпці, 
то, розв'язавши результуюче рівняння, ми отримаємо вихідні дані відповідно складені в стовпці. І, наостанок, давай резюмуємо про що ми говорили в цьому відео. Якщо ми маємо таку НМ, ми сказали, що нам потрібно обчислити ось це, 
щоб реалізувати пряме поширення по черзі для кожного тренувального зразка при i=1 до m. 
Також ми сказали, що потрібно скласти тренувальні зразки в стовпці, ось так, 
для кожної величини Z[1], A[1], Z[2], A[2]. Ось так потрібно скласти у відповідні стовпці. Тут приклад для A[1], проте, він правильний і для Z[1], A[1], Z[2] і A[2]. Потім на попередньому слайді ми показали, що оцей рядок дозволяє 
векторизувати обчислення для всіх m зразків одночасно. І, виявляється, що, опираючись на це, можна показати, що інші рядки є правильною векторизацією всіх 4-ох рядків коду. І просто нагадаю, що через те що X також =A[0], тому що, пам'ятаємо, вхідний вектор ознак x дорівнював a[0]. Тож x⁽ⁱ⁾=a[0]⁽ⁱ⁾. І, справді, є певна симетрія між цими рівняннями. Це рівняння може бути записане як Z[1]=W[1]A[0]+b[1]. Тож, як бачиш, оця пара рівнянь і оця пара рівнянь дійсно виглядають дуже схоже, 
їхні індекси відрізняються лише на 1. Тож це ніби показує, що різні шари НМ роблять практично одні і ті ж обчислення знову і знову. Тут ми маємо 2-шарову НМ. А коли ми перейдемо до набагато глибших НМ на наступному тижні, ти побачиш, що навіть глибші НМ, по суті, містять оці 2 кроки і просто обчислюють їх набагато більше разів 
ніж ти бачиш тут. Тож отак ти можеш векторизувати НМ 
для багатьох тренувальних зразків. Далі. До цього часу ми використовували 
сигмоїдальну функцію для всієї НМ. Але, виявляється, це не найкращий підхід. В наступному відео давай дещо глибше розглянемо як можна використовувати, так звані, функції активації, 
для яких сигмоїда - лише один з можливих варіантів.