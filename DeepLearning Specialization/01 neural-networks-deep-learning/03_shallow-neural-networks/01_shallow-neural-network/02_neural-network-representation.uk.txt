Ви бачили схеми нейронних мереж (НМ), що я малював. У цьому відео поговоримо по суті
 про значення тих малюнків. Інакше кажучи, що саме ті замальовки НМ відображають. А почнемо ми з НМ, що зветься одношаровою. Ось схема НМ. Давай назвемо окремі частини цієї схеми. Ми маємо вхідні ознаки x₁, x₂, x₃ складені по вертикалі. Це називається вхідним шаром НМ. Напевно не буде сюрпризом факт, що він містить вхідні дані НМ. А ось інший шар, з колами. Він називається прихованим шаром НМ. Через хвильку я до цього повернусь і поясню, 
що мається на увазі під прихованим. Останній шар, в цьому випадку, сформований одним вузлом. Цей одновузловий шар називається вихідним шаром. 
Він відповідальний за генерацію передбачень ŷ. В НМ, що тренується контрольованим навчанням, тренувальний набір містить значення вхідних x і цільових вихідних y. Тож термін "прихований шар" відсилає до факту, 
що в тренувальному наборі дійсні значення вказаних вузлів не відстежуються. Тобто їх немає у тренувальному наборі. Ми бачимо вхідні дані. Ми знаємо, якими повинні бути вихідні дані. Проте, інформація про прихований шар 
відсутня в тренувальному наборі. Це трохи пояснює назву "прихований". Ми просто не бачимо його в тренувальному наборі. 
Давай введемо ще одне позначення. До цього часу ми використовували вектор x для позначення вхідних даних. Інше позначення значень вхідних даних буде a з верхнім індексом 0 в квадратних дужках. Термін a також означає "активатори" і відповідає значенням, що передаються іншим шарам НМ, які також передаються наступним шарам. Тож вхідний шар передає значення x прихованому шару. Ми називаємо це активаторами вхідного шару a з верхнім індексом [0]. Наступний (прихований) шар, в свою чергу, генерує певні активатори, які я запишу як a з верхнім індексом [1]. Зокрема, оцей перший вузол шару генеруватиме значення a[1] з нижнім індексом 1. Наступний вузол цього ж шару генеруватиме значення a[1] з нижнім індексом 2 і так далі. Тож a з верхнім індексом в квадратних дужках [1] є чотиримірним вектором або, в Python, матриця 4*1, або 4-елементний вектор-стовпчик, що виглядає ось так. 
І він є 4-елементним, бо в цьому випадку ми маємо 4 вузли, або 4 одиниці, чи вірніше, 4 прихованих одиниці в цьому прихованому шарі. І, нарешті, вихідний шар генеруватиме деякі значення a[2], що є дійсним числом, а ŷ прийме значення a[2]. Тож це схоже на регресію, де ми маємо ŷ=a, і в моделі ми маємо лише один, вихідний шар, тож, не використовуємо там верхній індекс в квадратних дужках. Але тепер в НМ будемо використовувати 
верхній індекс в квадратних дужках, щоб наочно вказувати, до якого шару це відноситься. Дещо дивна річ конвенції позначень НМ. НМ, що ти зараз бачиш, називається двошаровою НМ. Це тому, що рахуючи шари НМ, ми не враховуємо вхідний шар. Тож прихований шар - це шар 1, вихідний шар - це шар 2. 
В нашій конвенції позначень вхідний шар називається шаром 0. Тож технічно тут 3 шари, бо тут є вхідний шар, прихований шар і вихідний шар. Але, в загальноприйнятому вживанні, 
у дослідницьких статтях, матеріалах курсу, таку НМ всі називатимуть двошаровою. Ми не вважаємо вхідний шар за справжній. 
Зрештою, пізніше ми будемо говорити, що прихований шар і вихідні шари матимуть параметри пов'язані з ними. Тож прихований шар матиме пов'язані з ним параметри w та b і я буду надписувати верхні індекси 1 в квадратних дужках, щоб позначити, що ці параметри пов'язані з шаром 1 (з прихованим шаром). Ми побачимо пізніше, що w - це матриця 4*3, а b - вектор 4*1 в цьому випадку. Де перша розмірність "4" відображає, що ми маємо 4 вузли (або приховані одиниці) в шарі, а трійка відображає, що ми маємо 3 вхідні ознаки. Ми поговоримо пізніше про розмірність цих матриць. Тоді для цього буде більш вдалий час. Аналогічно вихідний шар теж має пов'язані з ним параметри w з верхнім індексом 2 в квадратних дужках і 
b з верхнім індексом 2 в квадратних дужках. І, виявляється, що їхні розміри - 1*4 і 1*1. 1*4 тому що прихований шар має 4 прихованих вузли, а вихідний шар має лише 1 вузол. Але, повторю, 
ми поговоримо про розмірність цих матриць у подальших відео. Тож щойно ти бачив/ла як виглядає двошарова НМ. Ця мережа має один прихований шар. У наступному відео ми детальніше розберемо обчислення НМ. Тобто як НМ приймає на вхід x і весь шлях до обчислення на виході ŷ.