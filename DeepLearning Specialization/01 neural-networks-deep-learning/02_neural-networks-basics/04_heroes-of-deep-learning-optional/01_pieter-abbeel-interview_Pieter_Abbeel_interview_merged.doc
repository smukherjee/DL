<html xmlns:o="urn:schemas-microsoft-com:office:office" xmlns:w10="urn:schemas-microsoft-com:office:word" xmlns:m="http://schemas.microsoft.com/office/omml/2004/12/core" xmlns:ve="http://schemas.openxmlformats.org/markup-compatibility/2006" xmlns:o12="http://schemas.microsoft.com/office/2004/7/core" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:v="urn:schemas-microsoft-com:vml" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/3/wordprocessingDrawing" xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/3/main" xmlns="http://www.w3.org/TR/REC-html40">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <head>
    <style>style</style>
  </head>
  <body>
    <div class="Section1">
      <p>
        <b>
          <u>
            <span style="font-size:20.0pt">Pieter Abbeel interview</span>
          </u>
        </b>
      </p>
      <p>
(DESCRIPTION)        <br/>
Logo, Deep Learning.ai. Pieter Abbeel        <br/>
      </p>
      <p>
(SPEECH)        <br/>
So, thanks a lot, Pieter, for joining me today.        <br/>
      </p>
      <p>
I think a lot of people know you as a well-known machine learning and deep learning and robotics researcher.        <br/>
      </p>
      <p>
I'd like to have people hear a bit about your story.        <br/>
      </p>
      <p>
How did you end up doing the work that you        <br/>
      </p>
      <p>
(DESCRIPTION)        <br/>
Andrew Ng sits across from Pieter Abbeeel.        <br/>
      </p>
      <p>
(SPEECH)        <br/>
do?        <br/>
      </p>
      <p>
That's a good question and actually if you would have asked me as a 14-year-old, what I was aspiring to do, it probably would not have been this.        <br/>
      </p>
      <p>
In fact, at the time, I thought being a professional basketball player would be the right way to go.        <br/>
      </p>
      <p>
I don't think I was able to achieve it.        <br/>
      </p>
      <p>
I feel the machine learning lucked out, that the basketball thing didn't work out.        <br/>
      </p>
      <p>
Yes, that didn't work out.        <br/>
      </p>
      <p>
It was a lot of fun playing basketball but it didn't work out to try to make it into a career.        <br/>
      </p>
      <p>
So, what I really liked in school was physics and math.        <br/>
      </p>
      <p>
And so, from there, it seemed pretty natural to study engineering which is applying physics and math in the real world.        <br/>
      </p>
      <p>
And actually then, after my undergrad in electrical engineering, I actually wasn't so sure what to do because, literally, anything engineering seemed interesting to me.        <br/>
      </p>
      <p>
Understanding how anything works seems interesting.        <br/>
      </p>
      <p>
Trying to build anything is interesting.        <br/>
      </p>
      <p>
And in some sense, artificial intelligence won out because it seemed like it could somehow help all disciplines in some way.        <br/>
      </p>
      <p>
And also, it seemed somehow a little more at the core of everything.        <br/>
      </p>
      <p>
You think about how a machine can think, then maybe that's more the core of everything else than picking any specific discipline.        <br/>
      </p>
      <p>
I've been saying AI is the new electricity, sounds like the 14-year-old version of you; had an earlier version of that even.        <br/>
      </p>
      <p>
You know, in the past few years you've done a lot of work in deep reinforcement learning.        <br/>
      </p>
      <p>
What's happening? Why is deep reinforcement learning suddenly taking off?        <br/>
      </p>
      <p>
Before I worked in deep reinforcement learning, I worked a lot in reinforcement learning; actually with you and Durant at Stanford, of course.        <br/>
      </p>
      <p>
And so, we worked on autonomous helicopter flight, then later at Berkeley with some of my students who worked on getting a robot to learn to fold laundry.        <br/>
      </p>
      <p>
And kind of what characterized the work was a combination of learning that enabled things that would not be possible without learning, but also a lot of domain expertise in combination with the learning to get this to work.        <br/>
      </p>
      <p>
And it was very interesting because you needed domain expertise which was fun to acquire but, at the same time, was very time-consuming for every new application you wanted to succeed of; you needed domain expertise plus machine learning expertise.        <br/>
      </p>
      <p>
And for me it was in 2012 with the ImageNet breakthrough results from Geoff Hinton's group in Toronto, AlexNet showing that supervised learning, all of a sudden, could be done with far less engineering for the domain at hand.        <br/>
      </p>
      <p>
There was very little engineering by vision in AlexNet.        <br/>
      </p>
      <p>
It made me think we really should revisit reinforcement learning under the same kind of viewpoint and see if we can get the diversion of reinforcement learning to work and do equally interesting things as had just happened in the supervised learning.        <br/>
      </p>
      <p>
It sounds like you saw earlier than most people the potential of deep reinforcement learning.        <br/>
      </p>
      <p>
So now looking in to the future, what do you see next?        <br/>
      </p>
      <p>
What are your predictions for the next several ways to come in deep reinforcement learning?        <br/>
      </p>
      <p>
So, I think what's interesting about deep reinforcement learning is that, in some sense, there is many more questions than in supervised learning.        <br/>
      </p>
      <p>
In supervised learning, it's about learning an input output mapping.        <br/>
      </p>
      <p>
In reinforcement learning there is the notion of: Where does the data even come from?        <br/>
      </p>
      <p>
So that's the exploration problem.        <br/>
      </p>
      <p>
When you have data, how do you do credit assignment?        <br/>
      </p>
      <p>
How do you understand what actions you took early on got you the reward later?        <br/>
      </p>
      <p>
And then, there is issues of safety.        <br/>
      </p>
      <p>
When you have a system autonomously collecting data, it's actually rather dangerous in most situations.        <br/>
      </p>
      <p>
Imagine a self-driving car company that says, we're just going to run deep reinforcement learning.        <br/>
      </p>
      <p>
It's pretty likely that car would get into a lot of accidents before it does anything useful.        <br/>
      </p>
      <p>
You needed negative examples of that, right?        <br/>
      </p>
      <p>
You do need some negative examples somehow, yes; and positive ones, hopefully.        <br/>
      </p>
      <p>
So, I think there is still a lot of challenges in deep reinforcement learning in terms of working out some of the specifics of how to get these things to work.        <br/>
      </p>
      <p>
So, the deep part is the representation, but then the reinforcement learning itself still has a lot of questions.        <br/>
      </p>
      <p>
And what I feel is that, with the advances in deep learning, somehow one part of the puzzle in reinforcement learning has been largely addressed, which is the representation part.        <br/>
      </p>
      <p>
So, if there is a pattern we can probably represent it with a deep network and capture that pattern.        <br/>
      </p>
      <p>
And how to tease apart the pattern is still a big challenge in reinforcement learning.        <br/>
      </p>
      <p>
So I think big challenges are, how to get systems to reason over long time horizons.        <br/>
      </p>
      <p>
So right now, a lot of the successes in deep reinforcement learning are a very short horizon.        <br/>
      </p>
      <p>
There are problems where, if you act well over a five second horizon, you act well over the entire problem.        <br/>
      </p>
      <p>
And so a five second scale is something very different from a day long scale, or the ability to live a life as a robot or some software agent.        <br/>
      </p>
      <p>
So, I think there's a lot of challenges there.        <br/>
      </p>
      <p>
I think safety has a lot of challenges in terms of, how do you learn safely and also how do you keep learning once you're already pretty good?        <br/>
      </p>
      <p>
So, to give an example again that a lot of people would be familiar with, self-driving cars, for a self-driving car to be better than a human driver, should human drivers maybe get into bad accidents every three million miles or something.        <br/>
      </p>
      <p>
And so, that takes a long time to see the negative data; once you're as good as a human driver.        <br/>
      </p>
      <p>
But you want your self-driving car to be better than a human driver.        <br/>
      </p>
      <p>
And so, at that point the data collection becomes really really difficult to get that interesting data that makes your system improve.        <br/>
      </p>
      <p>
So, it's a lot of challenges related to exploration, that tie into that.        <br/>
      </p>
      <p>
But one of the things I'm actually most excited about right now is seeing if we can actually take a step back and also learn the reinforcement learning algorithm.        <br/>
      </p>
      <p>
So, reinforcement is very complex, credit assignment is very complex, exploration is very complex.        <br/>
      </p>
      <p>
And so maybe, just like how deep learning for supervised learning was able to replace a lot of domain expertise, maybe we can have programs that are learned, that are reinforcement learning programs that do all this, instead of us designing the details.        <br/>
      </p>
      <p>
During the reward function or during the whole program?        <br/>
      </p>
      <p>
So, this would be learning the entire reinforcement learning program.        <br/>
      </p>
      <p>
So, it would be, imagine, you have a reinforcement learning program, whatever it is, and you throw it out some problem and then you see how long it takes to learn.        <br/>
      </p>
      <p>
And then you say, well, that took a while.        <br/>
      </p>
      <p>
Now, let another program modify this reinforcement learning program.        <br/>
      </p>
      <p>
After the modification, see how fast it learns.        <br/>
      </p>
      <p>
If it learns more quickly, that was a good modification and maybe keep it and improve from there.        <br/>
      </p>
      <p>
Well, I see, right. Yes, and pace the direction.        <br/>
      </p>
      <p>
I think it has a lot to do with, maybe, the amount of compute that's becoming available.        <br/>
      </p>
      <p>
So, this would be running reinforcement learning in the inner loop.        <br/>
      </p>
      <p>
For us right now, we run reinforcement learning as the final thing.        <br/>
      </p>
      <p>
And so, the more compute we get, the more it becomes possible to maybe run something like reinforcement learning in the inner loop of a bigger algorithm.        <br/>
      </p>
      <p>
Starting from the 14-year-old, you've worked in AI for some 20 plus years now.        <br/>
      </p>
      <p>
So, tell me a bit about how your understanding of AI has evolved over this time.        <br/>
      </p>
      <p>
When I started looking at AI, it's very interesting because it really coincided with coming to Stanford to do my master's degree there, and there were some icons there like John McCarthy who I got to talk with, but who had a very different approach to, and in the year 2000, for what most people were doing at the time.        <br/>
      </p>
      <p>
And also talking with Daphne Koller.        <br/>
      </p>
      <p>
And I think a lot of my initial thinking of AI was shaped by Daphne's thinking.        <br/>
      </p>
      <p>
Her AI class, her probabilistic graphical models class, and kind of really being intrigued by how simply a distribution of her many random variables and then being able to condition on some subsets variables and draw on conclusions about others could actually give you so much if you can somehow make it computationally attractable, which was definitely the challenge to make it computable.        <br/>
      </p>
      <p>
And then from there, when I started my Ph.D. And you arrived at Stanford, and I think you give me a really good reality check, that that's not the right metric to evaluate your work by, and to really try to see the connection from what you're working on to what impact they can really have, what change it can make rather than what's the math that happened to be in your work.        <br/>
      </p>
      <p>
Right.        <br/>
      </p>
      <p>
(DESCRIPTION)        <br/>
Holds both hands up, palms facing each other, moves them from directly in front of him to his right, then swipes them quickly to his left.        <br/>
      </p>
      <p>
(SPEECH)        <br/>
That's amazing.        <br/>
      </p>
      <p>
I did not realize, I've forgotten that.        <br/>
      </p>
      <p>
Yes, it's actually one of the things, aside most often that people asking, if you going to cite only one thing that has stuck with you from Andrew's advice, it's making sure you can see the connection to where it's actually going to do something.        <br/>
      </p>
      <p>
You've had and you're continuing to have an amazing career in AI.        <br/>
      </p>
      <p>
So, for some of the people listening to you on video now, if they want to also enter or pursue a career in AI, what advice do you have for them?        <br/>
      </p>
      <p>
I think it's a really good time to get into artificial intelligence.        <br/>
      </p>
      <p>
If you look at the demand for people, it's so high, there is so many job opportunities, so many things you can do, researchwise, build new companies and so forth.        <br/>
      </p>
      <p>
So, I'd say yes, it's definitely a smart decision in terms of actually getting going.        <br/>
      </p>
      <p>
A lot of it, you can self-study, whether you're in school or not.        <br/>
      </p>
      <p>
There is a lot of online courses, for instance, your machine learning course, there is also, for example, Andrej Karpathy's deep learning course which has videos online, which is a great way to get started, Berkeley who has a deep reinforcement learning course which has all of the lectures online.        <br/>
      </p>
      <p>
So, those are all good places to get started.        <br/>
      </p>
      <p>
I think a big part of what's important is to make sure you try things yourself.        <br/>
      </p>
      <p>
So, not just read things or watch videos but try things out.        <br/>
      </p>
      <p>
With frameworks like TensorFlow, Chainer, Theano, PyTorch and so forth, I mean whatever is your favorite, it's very easy to get going and get something up and running very quickly.        <br/>
      </p>
      <p>
To get to practice yourself, right?        <br/>
      </p>
      <p>
With implementing and seeing what does and seeing what doesn't work.        <br/>
      </p>
      <p>
So, this past week there was an article in Mashable about a 16-year-old in United Kingdom, who is one of the leaders on Kaggle competitions.        <br/>
      </p>
      <p>
And it just said, he just went out and learned things, found things online, learned everything himself and never actually took any formal course per se.        <br/>
      </p>
      <p>
And there is a 16-year-old just being very competitive in Kaggle competition, so it's definitely possible.        <br/>
      </p>
      <p>
We live in good times.        <br/>
      </p>
      <p>
If people want to learn.        <br/>
      </p>
      <p>
Absolutely.        <br/>
      </p>
      <p>
One question I bet you get all sometimes is if someone wants to enter AI machine learning and deep learning, should they apply for a Ph.D. program or should they get the job with a big company?        <br/>
      </p>
      <p>
I think a lot of it has to do with maybe how much mentoring you can get.        <br/>
      </p>
      <p>
So, in a Ph.D. program, you're such a guaranteed, the job of the professor, who is your adviser, is to look out for you.        <br/>
      </p>
      <p>
Try to do everything they can to, kind of, shape you, help you become stronger at whatever you want to do, for example, AI.        <br/>
      </p>
      <p>
And so, there is a very clear dedicated person, sometimes you have two advisers.        <br/>
      </p>
      <p>
And that's literally their job and that's why they are professors, most of what they like about being professors often is helping shape students to become more capable at things.        <br/>
      </p>
      <p>
Now, it doesn't mean it's not possible at companies, and many companies have really good mentors and have people who love to help educate people who come in and strengthen them, and so forth.        <br/>
      </p>
      <p>
It's just, it might not be as much of a guarantee and a given, compared to actually enrolling in a Ph.D. program or that's the crooks of the program is that you're going to learn and somebody is there to help you learn.        <br/>
      </p>
      <p>
So it really depends on the company and depends on the Ph.D. program.        <br/>
      </p>
      <p>
Absolutely, yes. But I think it is key that you can learn a lot on your own.        <br/>
      </p>
      <p>
But I think you can learn a lot faster if you have somebody who's more experienced, who is actually taking it up as their responsibility to spend time with you and help accelerate your progress.        <br/>
      </p>
      <p>
So, you've been one of the most visible leaders in deep reinforcement learning.        <br/>
      </p>
      <p>
So, what are the things that deep reinforcement learning is already working really well at?        <br/>
      </p>
      <p>
I think, if you look at some deep reinforcement learning successes, it's very, very intriguing.        <br/>
      </p>
      <p>
For example, learning to play Atari games from pixels, processing this pixels which is just numbers that are being processed somehow and turned into joystick actions.        <br/>
      </p>
      <p>
Then, for example, some of the work we did at Berkeley was, we have a simulated robot inventing walking and the reward that it's given is as simple as the further you go north the better and the less hard you impact with the ground the better.        <br/>
      </p>
      <p>
And somehow it decides that walking slash running is the thing to invent whereas, nobody showed it, what walking is or running is.        <br/>
      </p>
      <p>
Or robot playing with children's stories and learn to kind of put them together, put a block into matching opening, and so forth.        <br/>
      </p>
      <p>
And so, I think it's really interesting that in all of these it's possible to learn from raw sensory inputs all the way to raw controls, for example, torques at the motors.        <br/>
      </p>
      <p>
But at the same time.        <br/>
      </p>
      <p>
So it is very interesting that you can have a single algorithm.        <br/>
      </p>
      <p>
(DESCRIPTION)        <br/>
Throws hands in the air.        <br/>
      </p>
      <p>
(SPEECH)        <br/>
For example, you know thrust is impulsive and you can learn, can have a robot learn to run, can have a robot learn to stand up, can have instead of a two legged robot, now you're swapping a four legged robot.        <br/>
      </p>
      <p>
You run the same reinforcement algorithm and it still learns to run.        <br/>
      </p>
      <p>
And so, there is no change in the reinforcement algorithm.        <br/>
      </p>
      <p>
It's very, very general. Same for the Atari games.        <br/>
      </p>
      <p>
DQN was the same DQN for every one of the games.        <br/>
      </p>
      <p>
But then, when it actually starts hitting the frontiers of what's not yet possible as well, it's nice it learns from scratch for each one of these tasks but would be even nicer if it could reuse things it's learned in the past; to learn even more quickly for the next task.        <br/>
      </p>
      <p>
And that's something that's still on the frontier and not yet possible.        <br/>
      </p>
      <p>
It always starts from scratch, essentially.        <br/>
      </p>
      <p>
How quickly, do you think, you see deep reinforcement learning get deployed in the robots around us, the robots they're getting deployed in the world today.        <br/>
      </p>
      <p>
I think in practice the realistic scenario is one where it starts with supervised learning, behavioral cloning; humans do the work.        <br/>
      </p>
      <p>
And I think a lot of businesses will be built that way where it's a human behind the scenes doing a lot of the work.        <br/>
      </p>
      <p>
Imagine Facebook Messenger assistant.        <br/>
      </p>
      <p>
Assistant like that could be built with a human behind the curtains doing a lot of the work; machine learning, matches up with what the human does and starts making suggestions to human so the humans has a small number of options that we can just click and select.        <br/>
      </p>
      <p>
And then over time, as it gets pretty good, you're starting fusing some reinforcement learning where you give it actual objectives, not just matching the human behind the curtains but giving objectives of achievement like, maybe, how fast were these two people able to plan their meeting?        <br/>
      </p>
      <p>
Or how fast were they able to book their flight?        <br/>
      </p>
      <p>
Or things like that. How long did it take?        <br/>
      </p>
      <p>
How happy were they with it?        <br/>
      </p>
      <p>
But it would probably have to be bootstrap of a lot of behavioral cloning of humans showing how this could be done.        <br/>
      </p>
      <p>
So it sounds behavioral cloning just supervise learning to mimic whatever the person is doing and then gradually later on, the reinforcement learning to have it think about longer time horizons?        <br/>
      </p>
      <p>
Is that a fair summary?        <br/>
      </p>
      <p>
I'd say so, yes.        <br/>
      </p>
      <p>
Just because straight up reinforcement learning from scratch is really fun to watch.        <br/>
      </p>
      <p>
It's super intriguing and very few things more fun to watch than a reinforcement learning robot starting from nothing and inventing things.        <br/>
      </p>
      <p>
But it's just time consuming and it's not always safe.        <br/>
      </p>
      <p>
Thank you very much. That was fascinating.        <br/>
      </p>
      <p>
I'm really glad we had the chance to chat.        <br/>
      </p>
      <p>
Well, Andrew thank you for having me. Very much appreciate it.        <br/>
      </p>
      <p>
(DESCRIPTION)        <br/>
Both smile, Pieter shakes his head        <br/>
      </p>
    </div>
  </body>
</html>
