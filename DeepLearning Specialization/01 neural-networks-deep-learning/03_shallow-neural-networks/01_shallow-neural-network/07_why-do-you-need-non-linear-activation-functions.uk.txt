Чому нейронна мережа (НМ) потребує нелінійної функції активації? Виявляється, що, для того щоб твоя НМ обчислила цікаву функцію, тобі потрібно використати нелінійну функцію активації. Давай подивимось чому. 
Ось рівняння прямого поширення для НМ. Чому б просто не позбутись цього, просто позбутись функції g і прирівняти a до z? Або чому б не сказати, що g(z)=z? Інколи це називають лінійною функцією активації. Можливо, її краще називати тотожною функцією активації, 
тому що вона видає те, що приймає на вхід. І, так само, що якщо a[2] просто =z[2]? Виявляється, якщо ми це зробимо, то ця модель буде обчислювати y або ŷ 
як лінійну функцію від вхідних ознак x. Візьмемо перших 2 рівняння. Тут матимемо a[1]=z[1]=w[1]+b[1]. Далі матимемо a[2]=z[2]=w[2]+b[2]. Тепер, якщо візьмемо нове визначення a[1] і підставимо сюди, то отримаємо, що a[2]=w[2](w[1]x+b[1])+b[2]. Правильно? Тобто оце - a[1]. І це спрощується до (w[2]w[1])x+ +(w[2]b[1]+b[2]) А це буде просто... Давай назвемо оце [головним] w' і [головним] b'. То це буде просто w'x+b'. Якби ми використовували лінійні функції активації або (ми їх також називаємо) тотожні функції активації, то НМ обчислювала б просто лінійну функцію від вхідних даних. Ми поговоримо пізніше про глибокі НМ, НМ з багатьма- багатьма шарами, багатьма-багатьма прихованими шарами. То, виявиться, що, при використанні лінійних функцій активації або взагалі без функцій активації, немає значення скільки шарів має НМ, все що вона робитиме - це обчислювати лінійну функцію активації. Тож можна взагалі обійтись без прихованих шарів, Як деякі мимохіть згадані випадки. Виявляється, якщо ми маємо лінійну функцію активації отут і сигмоїду отут то така модель не більш виразна ніж стандартна логістична регресія (ЛР) без прихованих шарів. Я не набридатиму доведенням, 
але якщо хочеш, то можеш перевірити сам/а. Варто запам'ятати, що лінійний прихований шар практично зайвий, тому що сукупність 2-ох лінійних функцій 
залишається лінійною функцією. Тож, якщо не додати нелінійності, то неможливо буде обчислити більш цікаві функції, 
навіть якщо обчислення проводяться глибоко в НМ. Є лише один випадок, де підходять лінійні функції активації (g(z)=z), і це - машинне навчання (МН) для задачі регресії. Тобто, якщо y - дійсне число. Наприклад, для передбачення цін нерухомості, де y - не 0 або 1, а дійсне число в межах від $0 і до ціни найдорожчого будинку, скажімо, потенційно мільйони доларів (в будь-якому випадку, найдорожчий в твоєму наборі даних). Тож якщо y лежить у цих межах, то, напевно, підійде і лінійна функція активації. А твій результат ŷ теж буде дійсним числом в межах від -нескінченності до +нескінченності. Проте, в цьому випадку, приховані вузли 
не повинні використовувати лінійні функції активації. Вони можуть використовувати випрямлений лінійний вузол (ВЛВ), 
гіперболічний тангенс (th), нещільний ВЛВ чи щось інше. Тож єдине місце, де може бути використана лінійна 
функція активації, - це, зазвичай, вихідний шар. В іншому місці використання лінійної функції активації - в прихованому шарі, окрім деяких дуже специфічних умов, пов'язаних зі стисненням, яке ми не будемо обговорювати - зустрічається надзвичайно рідко. І, звичайно, якщо ми й справді передбачаємо ціну нерухомості, яку розглядали минулого тижня, то через те, 
що вартість невід'ємна, і в цьому випадку можна використовувати ВЛВ функцію активації, щоб результат ŷ був більшим або рівним 0. Отже, надіюсь, я дав тобі розуміння того, що нелінійна функція активації - критичний пункт в НМ. Далі ми почнемо говорити про Градієнтний спуск. А перед цим, щоб підготуватись до обговорення Градієнтного спуску, 
в наступному відео я хочу показати як оцінювати, як обчислювати нахил або похідну окремої функції активації. Тож давай перейдемо до наступного відео.