1
00:00:00,000 --> 00:00:01,230
前回の動画では

2
00:00:01,230 --> 00:00:03,720
逆伝播の式を見てきました

3
00:00:03,720 --> 00:00:06,900
今回の動画では
計算グラフを使って

4
00:00:06,900 --> 00:00:10,515
その式がどうやって導けるのか
直感をつかみましょう

5
00:00:10,515 --> 00:00:12,385
この動画は完全に任意のものなので

6
00:00:12,385 --> 00:00:14,106
見るかどうかは
ご自由にしてくださいね

7
00:00:14,106 --> 00:00:16,360
どちらにしても作業は
ちゃんとできるはずです

8
00:00:16,360 --> 00:00:19,410
ロジスティック回帰についてお話ししたとき

9
00:00:19,410 --> 00:00:23,685
zやaそして損失を計算するために

10
00:00:23,685 --> 00:00:26,145
前方へ進んでいきましたよね

11
00:00:26,145 --> 00:00:27,445
そして微分を求めるために

12
00:00:27,445 --> 00:00:32,520
このように後方へ進んでいって
まずdaを計算し

13
00:00:32,520 --> 00:00:35,400
それからdzを計算し

14
00:00:35,400 --> 00:00:40,720
更にdwとdbを計算しました

15
00:00:40,720 --> 00:00:46,970
損失 L(a,y)の定義は

16
00:00:46,970 --> 00:00:52,655
-yloga-(1-y)log(1-a)

17
00:00:52,655 --> 00:00:57,440
でした

18
00:00:57,440 --> 00:00:59,750
もし微積分について詳しくて

19
00:00:59,750 --> 00:01:03,600
これのaについての微分を求めれば

20
00:01:03,600 --> 00:01:06,156
それはdaの公式となります

21
00:01:06,156 --> 00:01:09,060
daとはこれのことです

22
00:01:09,060 --> 00:01:12,750
そして本当に微分をしてみると

23
00:01:12,750 --> 00:01:18,808
これは-y/a+(1-y/1-a)だとわかります

24
00:01:18,808 --> 00:01:23,040
これの微分を取ったらこうなると
微積分学からわかるのです

25
00:01:23,040 --> 00:01:26,680
後方にもう一歩進んでdzを計算すると

26
00:01:26,680 --> 00:01:32,430
dzはa-yだとわかります

27
00:01:32,430 --> 00:01:37,920
わけは前に説明しましたが

28
00:01:37,920 --> 00:01:45,425
連鎖律によるとdz=da×g'(z)になります

29
00:01:45,425 --> 00:01:50,535
ここでg(z)とは zに
シグモイド関数を適用したものです

30
00:01:50,535 --> 00:01:56,245
ロジスティック回帰での
出力ユニットの活性化関数ですね

31
00:01:56,245 --> 00:02:00,570
これもまだロジスティック回帰だと
思い出してください

32
00:02:00,570 --> 00:02:05,757
x1、x2、x3と
一つのシグモイドユニットがあって

33
00:02:05,757 --> 00:02:07,400
そこから出てきたaから
yハットを求められるものです

34
00:02:07,400 --> 00:02:11,400
ここの活性化関数はシグモイド関数です

35
00:02:11,400 --> 00:02:12,960
余談ですが

36
00:02:12,960 --> 00:02:17,205
微積分学の連鎖律について
詳しい人に向けてです

37
00:02:17,205 --> 00:02:22,520
これの理由なのですが aはσ(z)ですよね

38
00:02:22,520 --> 00:02:29,310
なのでzについてのLの微分は

39
00:02:29,310 --> 00:02:36,800
aについてのLの微分×da/dzです

40
00:02:36,800 --> 00:02:39,611
このaはσ(z)に等しく

41
00:02:39,611 --> 00:02:42,970
これはd/dz×g(z)に等しく

42
00:02:42,970 --> 00:02:49,080
つまりg'(z)です

43
00:02:49,080 --> 00:02:54,060
そのためこの表現 コード内で
dzと書いてあるものは

44
00:02:54,060 --> 00:02:59,484
コードでdaと表されているもの
×g'(z)なのです

45
00:02:59,484 --> 00:03:05,860
これはただ
この式を表しているだけです

46
00:03:05,860 --> 00:03:09,172
最後の式の導出は微積分学
特に連鎖律に慣れていないと

47
00:03:09,172 --> 00:03:13,510
意味がわからないと思います

48
00:03:13,510 --> 00:03:15,325
わからなくても心配要りません

49
00:03:15,325 --> 00:03:18,853
必要なときには直感的な説明を
するようにしますね

50
00:03:18,853 --> 00:03:22,315
ロジスティック回帰のために
dzを計算したので

51
00:03:22,315 --> 00:03:26,335
dwとdbを計算します
dwはdz・xで

52
00:03:26,335 --> 00:03:31,470
dbは教師サンプルが一つのときは
ただのdzです

53
00:03:31,470 --> 00:03:33,822
これがロジスティック回帰でした

54
00:03:33,822 --> 00:03:36,700
ニューラルネットワークで
逆伝播を計算するときには

55
00:03:36,700 --> 00:03:40,090
これに似た計算をしますが
計算を二回行うという点で異なります

56
00:03:40,090 --> 00:03:46,995
xは直接出力ユニットに行くのではなく

57
00:03:46,995 --> 00:03:50,930
まず隠れ層を通ってから
出力ユニットに行くからです

58
00:03:50,930 --> 00:03:58,405
ここでやった計算は
１回のステップのようなもので

59
00:03:58,405 --> 00:04:04,483
このような二層のニューラル
ネットワークでは二回行います

60
00:04:04,483 --> 00:04:08,586
このように入力層隠れ層と出力層のある

61
00:04:08,586 --> 00:04:10,138
二層のニューラルネットワークを
考えます

62
00:04:10,138 --> 00:04:12,070
計算の進め方を思い出してください

63
00:04:12,070 --> 00:04:17,210
まずこの式を使ってz[1]を計算し

64
00:04:17,210 --> 00:04:22,177
それからa[1]とz[2]を計算します

65
00:04:22,177 --> 00:04:25,505
z[2]はパラメータW[2]と
b[2]の影響も受けます

66
00:04:25,505 --> 00:04:27,530
次はz[2]をもとにして

67
00:04:27,530 --> 00:04:32,815
a[2]が求まって
最後に損失を計算できます

68
00:04:32,815 --> 00:04:41,560
誤差逆伝播法では反対方向に進みます
まずda[2] そしてdz[2]

69
00:04:41,560 --> 00:04:48,805
また戻ってdW[2] db[2]を計算します

70
00:04:48,805 --> 00:04:53,232
更に戻ってda[1]

71
00:04:53,232 --> 00:04:57,278
dz[1]…というふうに続きます

72
00:04:57,278 --> 00:05:00,290
入力xに対する微分を計算する
必要はありません

73
00:05:00,290 --> 00:05:03,745
教師あり学習ではxは固定されていて

74
00:05:03,745 --> 00:05:07,845
最適化する必要はないので

75
00:05:07,845 --> 00:05:09,655
少なくとも教師あり学習では
xについての微分は計算しません

76
00:05:09,655 --> 00:05:15,605
da[2]の明示的な計算は飛ばします

77
00:05:15,605 --> 00:05:18,110
実際にda[2]を計算して

78
00:05:18,110 --> 00:05:20,750
それを使って
dz[2]を求めることもできますが

79
00:05:20,750 --> 00:05:25,760
実際にはこれらの二つを
一つの手順にまとめられます

80
00:05:25,760 --> 00:05:31,715
すると前と同じように
dz[2]=a[2]-y

81
00:05:31,715 --> 00:05:33,620
それからdW[2]とdb[2]もあります

82
00:05:33,620 --> 00:05:38,615
これらは下に書きますね

83
00:05:38,615 --> 00:05:46,700
dW[2]=dz[2]a[1]Tで

84
00:05:46,700 --> 00:05:52,040
db[2]=dz[2]です

85
00:05:52,040 --> 00:05:55,990
このあたりはロジスティック回帰と
よく似ていますね

86
00:05:55,990 --> 00:06:03,550
ロジスティック回帰では
dw=dz・xというのがありましたが

87
00:06:03,550 --> 00:06:08,770
今回はa[1]がxの役割を担っています

88
00:06:08,770 --> 00:06:14,125
大文字の行列Wと個別のパラメータwとの
関係を考えれば

89
00:06:14,125 --> 00:06:16,660
ここに転置がある意味もわかりますよね

90
00:06:16,660 --> 00:06:24,370
出力が一つのロジスティック回帰では
wは行ベクトルだからです

91
00:06:24,370 --> 00:06:26,980
dW[2]も似たようなものですが

92
00:06:26,980 --> 00:06:32,440
列ベクトルなので

93
00:06:32,440 --> 00:06:36,980
ロジスティック回帰のxとは違って
a[1]を転置するのです

94
00:06:36,980 --> 00:06:40,335
これで逆伝播の半分が終わりました

95
00:06:40,335 --> 00:06:44,045
ここでもda[1]を計算して構いませんが

96
00:06:44,045 --> 00:06:49,440
実際はda[1]とdz[1]の計算は

97
00:06:49,440 --> 00:06:52,330
一つにまとめてしまうことが多いです

98
00:06:52,330 --> 00:06:57,130
実装するときには dz[1]は

99
00:06:57,130 --> 00:07:03,480
W[2]T×dz[2]とg[1]'(z[1])の

100
00:07:03,480 --> 00:07:10,383
要素ごとの掛け算です

101
00:07:10,383 --> 00:07:13,960
ここで次元の確認をしてみましょう

102
00:07:13,960 --> 00:07:19,510
このようなニューラルネットワークがあって

103
00:07:19,510 --> 00:07:23,000
yハットがここにありますね

104
00:07:23,000 --> 00:07:28,265
n[0] これはnxに等しいですが
n[0]個の入力特徴量があって

105
00:07:28,265 --> 00:07:30,230
n[1]個の隠れユニット

106
00:07:30,230 --> 00:07:34,275
そしてn[2]があります

107
00:07:34,275 --> 00:07:36,740
この場合出力ユニットは一つなので

108
00:07:36,740 --> 00:07:38,565
n[2]は１です

109
00:07:38,565 --> 00:07:48,795
W[2]は(n[2],n[1])次元なので

110
00:07:48,795 --> 00:07:57,490
z[2] そしてdz[2]は(n[2],1)次元です

111
00:07:57,490 --> 00:07:59,850
二項分類をするときには
(1,1)になりますね

112
00:07:59,850 --> 00:08:04,750
z[1]とdz[1]は

113
00:08:04,750 --> 00:08:10,045
(n[1],1)次元となります

114
00:08:10,045 --> 00:08:16,115
任意の変数fooとdfooは
いつでも同じ次元を持ちます

115
00:08:16,115 --> 00:08:20,850
だからWとdWやbとdb、zとdzなども

116
00:08:20,850 --> 00:08:23,680
いつも同じ次元を持つのです

117
00:08:23,680 --> 00:08:26,895
これら全ての次元が合うかを
確かめてみましょう

118
00:08:26,895 --> 00:08:35,430
dz[1]=W[2]T×dz[2]がありますね

119
00:08:35,430 --> 00:08:44,490
これは要素同士の掛け算で
g[1]'(z[1])がかかっています

120
00:08:44,490 --> 00:08:47,040
上記から dz[1]の次元は

121
00:08:47,040 --> 00:08:52,575
(n[1],1)次元になり

122
00:08:52,575 --> 00:08:57,945
W[2]は転置されているので
(n[1],n[2])次元となります

123
00:08:57,945 --> 00:09:05,790
dz[2]は(n[2],1)次元になって

124
00:09:05,790 --> 00:09:07,230
これはz[1]と同じ次元です

125
00:09:07,230 --> 00:09:11,820
なのでこれも(n[1],1)となって
要素同士の掛け算ですね

126
00:09:11,820 --> 00:09:14,350
これで次元はわかったでしょう

127
00:09:14,350 --> 00:09:18,330
(n[1],1)次元のベクトルが

128
00:09:18,330 --> 00:09:23,520
(n[1],n[2])次元の行列
×(n[2],1)から得られます

129
00:09:23,520 --> 00:09:28,890
なぜならこれら二つの掛け算で
(n[1],1)次元の行列ができて

130
00:09:28,890 --> 00:09:34,618
二つの(n[1],1)次元のベクトル同士の
要素ごとの掛け算だからです

131
00:09:34,618 --> 00:09:36,060
そのため次元はぴったり合います

132
00:09:36,060 --> 00:09:40,620
逆伝播を実装するときのアドバイスですが

133
00:09:40,620 --> 00:09:44,790
行列の次元数が一致することを
確認したら

134
00:09:44,790 --> 00:09:47,190
つまりW[1]やW[2]、Z[1]、

135
00:09:47,190 --> 00:09:50,430
Z[2]、A[1]、A[2]などの行列の次元数を

136
00:09:50,430 --> 00:09:54,180
しっかり考えて

137
00:09:54,180 --> 00:09:58,642
それらの次元数が合うことを
確認しておけば

138
00:09:58,642 --> 00:10:03,390
そのおかげで逆伝播のバグを
たくさん取り除けることがあります

139
00:10:03,390 --> 00:10:06,960
いいですね
これでdz[1]が求まりました

140
00:10:06,960 --> 00:10:12,160
最後にdW[1]とdb[1]です

141
00:10:12,160 --> 00:10:13,965
ここに書いた方がいいのですが

142
00:10:13,965 --> 00:10:17,200
スペースが不足しているので
右側に書きますね

143
00:10:17,200 --> 00:10:21,965
dW[1]とdb[1]は次の式で求められます

144
00:10:21,965 --> 00:10:25,950
これはdz[1]・xTで

145
00:10:25,950 --> 00:10:28,905
こちらはdz[1]です

146
00:10:28,905 --> 00:10:34,045
この式とこの式はよく似ていますね

147
00:10:34,045 --> 00:10:37,095
これはただの偶然ではありません

148
00:10:37,095 --> 00:10:41,660
xがa[0]の役割を担っていて
xの転置はa[0]の転置なので

149
00:10:41,660 --> 00:10:45,484
実際これらの式はとてもよく似ているのです

150
00:10:45,484 --> 00:10:50,260
これで逆伝播がどのように
得られるかわかったでしょうか

151
00:10:50,260 --> 00:10:54,530
dz[2]、dW[2]、db[2]、dz[1]、dW[1]
そしてdb[1]という

152
00:10:54,530 --> 00:11:00,190
６つの大切な式があります

153
00:11:00,190 --> 00:11:05,767
これら６つの式を取って
次のスライドにコピーしますね

154
00:11:05,767 --> 00:11:08,950
ここまで 一つの教師サンプルが
あるときのための

155
00:11:08,950 --> 00:11:13,959
逆伝播の方法を求めてきました

156
00:11:13,959 --> 00:11:21,530
もちろん 一度に一つの
教師サンプルを処理するより

157
00:11:21,530 --> 00:11:27,810
全ての教師サンプルをベクトル化して
処理したいですよね

158
00:11:27,810 --> 00:11:30,971
順伝播のときには

159
00:11:30,971 --> 00:11:33,545
一つの教師サンプルを処理しているとき

160
00:11:33,545 --> 00:11:41,665
このような式がありました

161
00:11:41,665 --> 00:11:43,655
ベクトル化するためには

162
00:11:43,655 --> 00:11:51,260
全てのzを取ってきて
このように列として並べ

163
00:11:51,260 --> 00:12:00,775
z[1](m)までありますね
これを大文字のZと呼びました

164
00:12:00,775 --> 00:12:04,960
列として並べたものを

165
00:12:04,960 --> 00:12:10,240
大文字バージョンとして扱うことで

166
00:12:10,240 --> 00:12:17,093
Z[1]=W[1]X+b[1]と

167
00:12:17,093 --> 00:12:24,700
A[1]=g[1](Z[1])であることが
わかりました

168
00:12:24,700 --> 00:12:28,645
このコースのために表記法を
注意深く定義したので

169
00:12:28,645 --> 00:12:35,550
サンプルを行列の列として並べることで
うまくいくようになっています

170
00:12:35,550 --> 00:12:40,105
慎重に計算していけば
逆伝播のときも同じようにできるはずです

171
00:12:40,105 --> 00:12:46,645
ベクトル化した式は次の通りです

172
00:12:46,645 --> 00:12:52,250
まず全ての教師サンプルから
dzを取ってきて

173
00:12:52,250 --> 00:12:58,339
行列のそれぞれの列として並べます
これらも同じようにすると

174
00:12:58,339 --> 00:13:03,070
これがベクトル化した実装になります

175
00:13:03,070 --> 00:13:05,569
これがdW[2]の計算方法です

176
00:13:05,569 --> 00:13:11,130
ここには1/mをつけました
なぜならコスト関数Jは

177
00:13:11,130 --> 00:13:18,410
iが１からmまでの損失を足して
mで割ったものだからです

178
00:13:18,410 --> 00:13:20,615
微分を計算するときには

179
00:13:20,615 --> 00:13:23,885
ロジスティック回帰で重みを
更新したときと同じように

180
00:13:23,885 --> 00:13:27,982
1/mをつけるのです

181
00:13:27,982 --> 00:13:31,790
db[2]はこのようになります

182
00:13:31,790 --> 00:13:40,640
これもdZの合計を1/mしたものです
dZ[1]は次のように計算できます

183
00:13:40,640 --> 00:13:49,109
繰り返しますがこれは要素ごとの掛け算です

184
00:13:49,109 --> 00:13:56,595
前のスライドで見たように
これは(n[1],1)次元のですが

185
00:13:56,595 --> 00:14:03,185
今回は(n[1],m)になっています

186
00:14:03,185 --> 00:14:09,045
これらはどちらも(n[1],m)次元です

187
00:14:09,045 --> 00:14:19,310
そのためこのアスタリスクが
要素ごとの掛け算になるのです

188
00:14:19,310 --> 00:14:21,454
残りの二つはこのようになります

189
00:14:21,454 --> 00:14:25,836
これらはそんなに
衝撃的な変化ではないですよね

190
00:14:25,836 --> 00:14:29,510
これで逆伝播アルゴリズムの導き方を
直感でわかってもらえていたら嬉しいです

191
00:14:29,510 --> 00:14:32,205
機械学習のすべての中で

192
00:14:32,205 --> 00:14:34,820
逆伝播のアルゴリズムの導き方が

193
00:14:34,820 --> 00:14:38,465
私が見た中で最も複雑な
数学の一つだと思います

194
00:14:38,465 --> 00:14:42,470
線形代数の知識が必要とされる上に

195
00:14:42,470 --> 00:14:46,830
行列の微分を第一原理よりゼロから
求めなければなりません

196
00:14:46,830 --> 00:14:50,165
行列の微積分学の専門家の方なら

197
00:14:50,165 --> 00:14:54,255
この過程を使って 自分で微分アルゴリズムを
証明できるかもしれませんが

198
00:14:54,255 --> 00:14:57,500
深層学習を実践している人たちの中には

199
00:14:57,500 --> 00:15:01,060
この動画と同じレベルの
式の導出方法を見て

200
00:15:01,060 --> 00:15:04,100
直感を持ってこのアルゴリズムを

201
00:15:04,100 --> 00:15:08,580
効果的に実装できている人が
たくさんいると思います

202
00:15:08,580 --> 00:15:10,070
微積分学のエキスパートだったら

203
00:15:10,070 --> 00:15:13,395
ゼロからこれらを求められるか
試してみるといいと思います

204
00:15:13,395 --> 00:15:15,665
これは機械学習で私が見てきた中で

205
00:15:15,665 --> 00:15:20,010
最も難しい導出の一つです

206
00:15:20,010 --> 00:15:22,861
どちらにせよ これを実装すれば
うまくいくでしょうし

207
00:15:22,861 --> 00:15:27,260
これを調整するのに
十分な直感は持っていると思います

208
00:15:27,260 --> 00:15:30,830
ニューラルネットワークを実装する前に

209
00:15:30,830 --> 00:15:34,190
最後にもう一つ
細かいことをお教えしようと思います

210
00:15:34,190 --> 00:15:37,720
それはニューラルネットワークの重みを
どうやって初期化するかです

211
00:15:37,720 --> 00:15:40,600
パラメータをゼロではなく

212
00:15:40,600 --> 00:15:42,560
ランダムに初期化することは

213
00:15:42,560 --> 00:15:45,515
ニューラルネットワークを訓練するうえで
大事であることがわかっています

214
00:15:45,515 --> 00:15:48,000
次のビデオでそのわけを見てみましょう