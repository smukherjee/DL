1
00:00:00,000 --> 00:00:01,619
여러분이 신경망을 변경할때는, 

2
00:00:01,619 --> 00:00:03,955
weight를 임의로 초기화하는 것이 중요합니다.

3
00:00:03,955 --> 00:00:08,426
로지스틱 회귀분석에서는 
weight를 0으로 초기화하는 것이 괜찮았습니다.

4
00:00:08,426 --> 00:00:12,258
하지만 신경망에서는 모든 weight를 0으로하고

5
00:00:12,258 --> 00:00:14,706
기울기 강하를 적용하면
작동하지 않을 것입니다.

6
00:00:14,706 --> 00:00:15,289
왜 그런 것인지 보겠습니다.

7
00:00:15,289 --> 00:00:20,173
여기는 2개의 입력값이 있는데요

8
00:00:20,173 --> 00:00:25,953
n0=2, 와 2개의 숨겨진 유닛 n1=2입니다.

9
00:00:25,953 --> 00:00:31,547
숨겨진 레이어와 연관된 매트릭스는 

10
00:00:31,547 --> 00:00:35,373
w1이고 이것은 2 x 2가 됩니다.

11
00:00:35,373 --> 00:00:41,230
이제 이것은 모두 0으로 초기화한다고 해보겠습니다. 
0 0 0 0으로 말이죠. 2 x 2 매트릭스에서요. 

12
00:00:41,230 --> 00:00:45,531
그리고 B1도 마찬가지로 0 0이라고 하겠습니다.

13
00:00:45,531 --> 00:00:50,788
알고보면 bias term 은 0으로 초기화하여도
무방합니다.

14
00:00:50,788 --> 00:00:54,240
하지만 w를 모두 0으로 초기화하는 것이 문제가 됩니다.

15
00:00:54,240 --> 00:00:59,625
여기 초기화의 문제는

16
00:00:59,625 --> 00:01:05,522
어떤 예제의 경우라고 하면, 
a11, 그리고 

17
00:01:05,522 --> 00:01:09,253
a12 가 동일하겠죠. 맞죠?

18
00:01:09,253 --> 00:01:12,613
여기 activation과 여기 activation이 동일할 것입니다.

19
00:01:12,613 --> 00:01:17,170
그 이유는 여기 숨겨진 유닛이 모두
똑같은 함수를 산출하기 때문입니다.

20
00:01:17,170 --> 00:01:21,810
그리고 후 방향전파을 산출하면, 

21
00:01:21,810 --> 00:01:24,478
dz11과 dz12가

22
00:01:24,478 --> 00:01:30,165
대칭에 의해서 말이죠. 맞죠?

23
00:01:30,165 --> 00:01:33,720
여기 2개의 숨겨진 유닛은 모두 
같은 방식으로 초기화될 것입니다.

24
00:01:33,720 --> 00:01:36,080
엄밀히 이야기하면, 제가 말하는 것은

25
00:01:36,080 --> 00:01:39,851
outgoing weight가 동일하다는 가정하에 이루어집니다.

26
00:01:39,851 --> 00:01:45,122
즉 w2 가 0 0 인 것이죠.

27
00:01:45,122 --> 00:01:48,691
하지만 신경망을 이런식으로 초기화하는 경우, 

28
00:01:48,691 --> 00:01:53,590
여기 숨겨진 유닛그리고 여기 숨겨진 유닛은 
완전히 동일합니다.

29
00:01:53,590 --> 00:01:57,011
가끔은 이것들이 완전히 대칭을 이룬다고 합니다.

30
00:01:57,011 --> 00:02:01,687
그 뜻은 이것들이 동일한 함수를 산출한다는 뜻입니다.

31
00:02:01,687 --> 00:02:03,765
그럼 일종의 유도 증명으로 보면

32
00:02:03,765 --> 00:02:08,064
iteration마다 이 2개의 숨겨진 유닛은

33
00:02:08,064 --> 00:02:11,272
계속해서 똑같은 함수를 산출합니다.

34
00:02:11,272 --> 00:02:17,521
dw는 이렇게 보이는 매트릭스가 될 텐데요, 

35
00:02:17,521 --> 00:02:20,681
줄마다 똑같은 값을 갖습니다.

36
00:02:20,681 --> 00:02:23,318
그러면 weight 업데이트를 진행합니다.

37
00:02:23,318 --> 00:02:30,163
여러분이 weight 업데이트를 진행하면, 
w1은 w1 - 알파 곱하기 dw로 업데이트 됩니다.

38
00:02:30,163 --> 00:02:33,740
그러면 w1은 반복업무마다, 

39
00:02:33,740 --> 00:02:37,616
첫번째 줄이 두번째 줄가 일치합니다.

40
00:02:37,616 --> 00:02:41,487
그러면 유도증명을 할 수 있는데요, 

41
00:02:41,487 --> 00:02:44,688
모든 w값을 0으로 한다면, 

42
00:02:44,688 --> 00:02:49,164
2개의 숨겨진 유닛 모두 
똑같은 함수를 산출하고, 

43
00:02:49,164 --> 00:02:53,541
2개의 hidden unit이 
결과값 유닛에 똑같은 영향을 주기 때문에, 

44
00:02:53,541 --> 00:02:57,542
iteration 후에 똑같은 statement가 참일 것입니다.

45
00:02:57,542 --> 00:03:00,273
이 2개의 숨겨진 유닛이 대칭이라는 것 말이죠.

46
00:03:00,273 --> 00:03:04,507
그러므로 유도증명을 통해 2번 또는
3번 등등의 반복 ieteration을 통해

47
00:03:04,507 --> 00:03:07,013
여러분이 신경망을 얼마나 오래 트레이닝 해도

48
00:03:07,013 --> 00:03:10,373
2개 숨겨진 유닛 모두
완전히 똑같은 함수를 산출합니다.

49
00:03:10,373 --> 00:03:15,212
그럼 이런 경우, 1개 이상의 숨겨진 유닛을 
갖는 것이 무의미해집니다.

50
00:03:15,212 --> 00:03:17,692
서로 똑같은 것을 산출하기 때문입니다.

51
00:03:17,692 --> 00:03:22,378
신경망이 크면 클수록 또한 
예를 들어, 3개의 특성

52
00:03:22,378 --> 00:03:24,972
그리고 많은 수의 숨겨진 유닛이 있다고 해보겠습니다.

53
00:03:24,972 --> 00:03:29,239
이런 신경망에서도 비슷하게 적용되는데요, 

54
00:03:29,239 --> 00:03:34,107
모서리를 그리구요, 
모든 weight를 0으로 만들면

55
00:03:34,107 --> 00:03:37,103
모든 숨겨진 유닛이 대칭이 됩니다.

56
00:03:37,103 --> 00:03:40,603
그렇기 때문에 기울기 강하가 얼마나 길게 하더라도

57
00:03:40,603 --> 00:03:44,037
모두 똑같은 함수를 산출합니다.

58
00:03:44,037 --> 00:03:48,785
그러면 다른 숨겨진 유닛이 

59
00:03:48,785 --> 00:03:52,835
다른 함수를 계산하고 싶기 때문에 
도움이 되질 않겠죠. 

60
00:03:52,835 --> 00:03:57,748
이겻에 대한 해결책은 
파라미터를 임의로 초기화하는 것입니다.

61
00:03:57,748 --> 00:03:58,677
이렇게 할 수 있습니다.

62
00:03:58,677 --> 00:04:04,053
w1=np.random.randn으로 설정합니다.

63
00:04:04,053 --> 00:04:07,037
이렇게 하면 a gaussian
random variable이 생성되는데요, 

64
00:04:07,037 --> 00:04:12,358
그리고 주로 이 값을 아주 작은 숫자로 곱합니다.
0.01이 예가 되겠습니다.

65
00:04:12,358 --> 00:04:14,951
그렇게하여 아주 작은 값으로 
초기화시키는 것이죠.

66
00:04:14,951 --> 00:04:20,590
그리고 b는 대칭 문제가 없습니다.

67
00:04:20,590 --> 00:04:24,735
symmetry breaking 문제라고 불리는 것이 
문제되지 않습니다.

68
00:04:24,735 --> 00:04:29,370
그렇기 때문에 b는 0으로 초기화해도 됩니다.

69
00:04:29,370 --> 00:04:32,166
w가 임의로 초기화된 이상, 

70
00:04:32,166 --> 00:04:36,769
숨겨진 유닛들이 다른 값을 산출하도록 만들어 줍니다.

71
00:04:36,769 --> 00:04:40,912
그러면 이제 더 이산 symmetry breaking 문제가 
없게 됩니다.

72
00:04:40,912 --> 00:04:43,795
w2도 비슷하게 임의로 초기화를 진행할 것입니다.

73
00:04:43,795 --> 00:04:48,858
b2는 마찬가지로 0으로 초기화시킵니다.

74
00:04:48,858 --> 00:04:55,321
그러면 여러분은 의문을 가질 수 있는데요, 
여기 상수는 어디에서 왔으며 왜 0.01일까? 말이죠.

75
00:04:55,321 --> 00:04:58,478
왜 100 또는 1000은 안될까? 

76
00:04:58,478 --> 00:05:02,313
알고보면 저희는 보통

77
00:05:02,313 --> 00:05:05,763
weight를 초기화하는데 임의의 작은 값을
선호합니다.

78
00:05:05,763 --> 00:05:10,443
그 이유는 tan h나 
시그모이드 activation 함수

79
00:05:10,443 --> 00:05:14,047
또는 다른 시그모이드 함수가 결과값 층에 있는 경우

80
00:05:14,047 --> 00:05:17,922
weight가 만약에 너무 크면

81
00:05:17,922 --> 00:05:23,967
activation 값을 산출할 때, 

82
00:05:23,967 --> 00:05:28,621
z[1]=w1 x + b라는 것을 기억하실 것입니다.

83
00:05:28,621 --> 00:05:34,094
그러면 a1은 z1에 적용된
activation 함수입니다.

84
00:05:34,094 --> 00:05:39,097
그러므로 만약 w가 매우 크면
z도, 또는 적어도

85
00:05:39,097 --> 00:05:44,235
일부분의z 값은 매우 크거나 
매우 작을 것입니다.

86
00:05:44,235 --> 00:05:49,789
이런 경우, 여기 두꺼운 부분에 남을 확률이 높습니다.
tan h 함수에서 말이죠.

87
00:05:49,789 --> 00:05:55,699
또는 시그모이드 함수에서도 그렇습니다.
기울기의 값이 아주 작은 부분입니다.

88
00:05:55,699 --> 00:05:58,302
이 뜻은, 기울기 강하가 매우 느릴 것이라는 것을 
뜻합니다.

89
00:05:58,302 --> 00:05:59,730
그리하려 러닝도 매우 느린 것이죠.

90
00:05:59,730 --> 00:06:04,133
복습하자면, 만약 w의 값이 매우 큰 경우, 

91
00:06:04,133 --> 00:06:08,633
트레이닝 초기에도 z의 값이 클 확률이 높습니다

92
00:06:08,633 --> 00:06:13,525
그러면 tan h나 시그모이드 함수가 포화되게 하죠. 

93
00:06:13,525 --> 00:06:15,418
그리하여 러닝의 속도를 늦추 구요.

94
00:06:15,418 --> 00:06:17,231
만약 시그모이드난 

95
00:06:17,231 --> 00:06:22,149
tan h 함수가 신경망에서 없는 경우, 
이것은 문제가 덜 됩니다.

96
00:06:22,149 --> 00:06:26,506
하지만 이진분류법의 경우, 
결과값 유닛이 시그모이드 함수인 경우, 

97
00:06:26,506 --> 00:06:30,806
초기 parameter가 너무 크지 않는 게 좋습니다.

98
00:06:30,806 --> 00:06:35,435
그렇기 때문에 처음에 0.01을 곱하는 것이 
합리적인 방법입니다.

99
00:06:35,435 --> 00:06:36,872
또는 아무 작은 숫자를 말이죠.

100
00:06:36,872 --> 00:06:38,536
w2도 마찬가지입니다. 맞죠?

101
00:06:38,536 --> 00:06:44,295
이것은 random.ranom이 되고요

102
00:06:44,295 --> 00:06:49,545
이것은 1 x 2 예제이겠죠. 곱하기 0.01 입니다.

103
00:06:49,545 --> 00:06:51,404
여기 s가 빠졌군요

104
00:06:51,404 --> 00:07:00,085
마지막으로, 상수의 값이 0.01보다 
더 좋은 값이 있을 수 있는데요, 

105
00:07:00,085 --> 00:07:04,304
만약 한개의 숨겨진 유닛에서 신경망을 트레이닝 하는 경우, 

106
00:07:04,304 --> 00:07:09,129
꽤 얕은 신경망이고 숨겨진 유닛의 수가 많지 않기 때문에, 

107
00:07:09,129 --> 00:07:12,392
0.01로 설정하면 적당히 작동할 것입니다.

108
00:07:12,392 --> 00:07:15,705
하지만 아주 깊은 심층신경망의 경우, 

109
00:07:15,705 --> 00:07:19,294
0.01 과는 다른 상수를 고르고 싶을 것입니다.

110
00:07:19,294 --> 00:07:23,642
다음주에는 0.01과는 다른

111
00:07:23,642 --> 00:07:27,925
상수의 값을 선택하는 것에 대한 내용을 
다루겠습니다.

112
00:07:27,925 --> 00:07:32,008
어떻던 간에, 꽤 작은 숫자일 것입니다.

113
00:07:32,008 --> 00:07:34,584
이번주 비디오는 이것이 전부인데요, 

114
00:07:34,584 --> 00:07:38,348
이제 여러분은 숨겨진 유닛의 신경망을 설정하실 줄
알게 되었습니다.

115
00:07:38,348 --> 00:07:42,430
그리고 파라미터를 초기화하고 
전 방향전파을 이용해서 예측을 하고, 

116
00:07:42,430 --> 00:07:45,445
dervative를 산출하는 방법과, 기울기 강하를
도입하는 방법을 배웠습니다.

117
00:07:45,445 --> 00:07:46,275
후 방향전파을 이용해서 말이죠.

118
00:07:46,275 --> 00:07:48,654
이제 여러분은 퀴즈를 푸실 수 있을 것입니다.

119
00:07:48,654 --> 00:07:51,166
이번 주 연습 문제도 말이죠. 

120
00:07:51,166 --> 00:07:52,143
행운을 빌겠습니다.

121
00:07:52,143 --> 00:07:54,802
연습문제를 즐기시길 바라며, 

122
00:07:54,802 --> 00:07:57,728
다음주 4주차에서 보길 희망합니다.