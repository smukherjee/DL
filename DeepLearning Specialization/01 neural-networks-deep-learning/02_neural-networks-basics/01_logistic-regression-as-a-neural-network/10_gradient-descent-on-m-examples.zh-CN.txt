在之前的课程中，你们学习了怎样 计算导数以及怎样实现 梯度下降 在只有 一个训练样例的逻辑回归情况下 现在 我们来讨论 m个训练样例的情况 我们先回顾一下 代价函数J(w,b) 我们关心的J(w,b)是个平均数 1/m乘上求和从i取1到m时的 这个损失函数L 当你的算法 输出关于样本y的a^i 你知道a^i是训练样本的预测值 也就是𝜎(z^i) 等于𝜎函数作用于w的转置乘上x^i 加上b 所以我们在 前几页幻灯片中展示的是对任意单个 训练样本如何计算导数 当你只有一个 训练样本时 dw1 dw2和 db 加上上标i 表示你求得的相应值 如果你现在在做我们在 之前的幻灯片中演示的情况 但只使用一个训练样本(x^i,y^i) 抱歉 我这里少了个i 现在你发现了带求和的全局代价函数 实际上是从第1项到第m项 各项损失的平均 所以这表明全局代价函数 对w_1的导数 也同样是各项损失 对w_1导数的平均值 但之前我们 已经演示了如何计算 这项 也就是我所写的 即之前幻灯片中演示的如何 对单个训练样本进行计算 所以你真正需要做的是计算 这些导数 正如我们在 之前的训练例子中演示的 并且求平均 这会给你 全局梯度值 你能够 直接把它实现 到梯度下降法中 所以这里有 很多细节 但让我们把这些 装进一个具体的算法 你需要实现的就是 使逻辑回归和其中的 梯度下降法生效 我们可以初始化J等于0 dw_1等于0 dw_2等于0 db等于0 我们将要做的是使用一个 for循环遍历训练集 同时计算相应的每个 训练样本的微分并把它们加起来 好的 如我们所做的让i 取1到m m是训练样本数 我们计算z^i就等于 w的转置乘上x^i加上b a^i的预测值等于σ(z^i) 然后我们要累加J J加等于(y^i)log(a^i)加上 (1-y^i)log(1-a^i) 然后加一个 负号在整个公式的前面 然后如我们早前所见 我们有dz^i或者它等于a^i减去y^i dw加等于(x_1)^i乘上dz^i dw_2加 等于(x_2)^i乘上dz^i 我做这个计算 已经假设你只有 两个特征 所以n等于2 否则你需要对dw_1 dw_2 dw_3等等 做同样的计算 同时db加等于dz^i 我觉得这个for循环就到此结束 最终对所有的m个训练样本 都进行这个计算后 你还需要 除以m 因为我们是在 计算平均值 因此dw_1除等于m dw_2除等于m db除等于m 全都以平均的形式 做完所有这些计算后 你已经计算了 代价函数J 对各个参数w_1 w_2 和b的导数 回顾我们正在做的细节 我们使用dw_1 dw_2和db 作为累加器 所以算完这个以后 dw_1等于你全局代价函数 对w_1的导数 对dw_2和db也是一样 注意dw_1和dw_2没有上标i 这是因为我们在这代码中把它们作为 累加器去求取整个 训练集上的和 然而dz^i是对应于 单个训练样本的dz 这也就是为什么这里会有个上标i 指代对应的第i个训练样本 所以完成所有 这些计算后实现一步 梯度下降来更新w_1 即w_1减去学习率 乘上dw_1 而w_2更新为w_2减去 学习率乘上dw_2 同时b更新为b减去学习率乘上db 这里dw_1 dw_2和db都是如之前所说那样计算的 最终这里的J也会 是你代价函数的正确值 所以幻灯片上的所有东西 只实现了一步 梯度下降 因此你需要 重复以上内容很多次 以完成多次梯度下降 这些细节看起来 似乎很复杂 但目前不要担心太多 所有这些将会变的更加清楚 当你继续尝试并在编程作业中 实现这些方法的时候 但它表明计算中有两个缺点 如果按照这里的方法实现的话 那就是这样实现 逻辑回归时 你需要两个for循环 第一个for循环 是一个小循环用于遍历m个训练样本 第二个for循环是一个 遍历所有特征的for循环 这个例子中我们只有2个特征 所以n等于2并且n_x等于2 但如果你有更多特征 你需要编写dw_1 dw_2 以及类似地计算dw_3等等 直到dw_n 所以看来你 需要一个for循环遍历 所有n个特征 当你实现深度学习算法时 你会发现在代码中显式地 使用for循环会使你的 算法不够高效 在深度学习时代会有 越来越大的数据集 所以不使用显式的for循环来 实现你的算法 是非常重要的而且会帮你 适用于更大的数据集 所以这里有一些技术 叫做矢量化技术 它可以帮助你的代码摆脱 这些显式的for循环 我想在前深度学习的时代 也就是深度学习兴起之前 矢量化是有着两面性的 有时候能加速你的代码 有时候也未必能加速 但在深度学习时代 矢量化 也就是像这样摆脱for循环 已经变得相当重要 因为我们越来越多地训练 非常大的数据集 因此你需要代码变得非常高效 所以在接下来的几个视频中 我们将讨论矢量化以及如何实现这些 同时连一个for循环都不使用 所以到这里 我希望你明白 如何实现逻辑回归 或者逻辑回归中的梯度下降法 这一切都会变得更加清晰 当你进行编程练习以后 但在真正做编程练习前 让我们先谈谈矢量化 所以到时候你可以实现 全部这些东西 实现一个 梯度下降的迭代 而不使用任何for循环
GTC字幕组翻译