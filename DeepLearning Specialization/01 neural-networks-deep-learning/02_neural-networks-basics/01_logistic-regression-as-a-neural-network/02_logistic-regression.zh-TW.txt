在這段影片, 我們將談論羅吉斯迴歸分析 這是種當您使用學習演算法時輸出標籤 y 在監督式學習問題中不是 0 就是 1 也就是二元分類問題 給予一個輸入特徵向量 x 也許相當於 一張影像您想辨識是貓的照片或不是貓的照片 您要一個演算法能夠輸出預測 我們稱為 y-hat 就是您的預估 y 更正式一點來說, 您希望 y-hat 是一個機率當 輸入 x, y 等於 1 的機會 換句話說, 如果 x 是一張照片 像我們在上一段影片中看到的 您要 y-hat 告訴您 這是一張貓的照片的機會是多少 X, 就像我們上一段影片中說的 是一個 nx 維度向量 羅吉斯迴歸分析的參數會是 w 也是一個 nx 維度向量 跟 b 是一個實數 給予一個輸入 x 跟參數 w, b 我們如何產生輸出 y-hat 一種方式您可以試, 不過行不通 是讓 y-hat 為 w轉置x 加 b 類似是輸入 x 的線性函數 實際上, 這是如果您使用線迴歸分析的方式 但這並不是一個很好的演算法在二元分類 因為您希望 y-hat 是 y 等於 1 的機會 所以 y-hat 應該是介於 0 跟 1 很難做到, 因為 w 轉置 x 加 b 可能遠大於 1 或者甚至是負數 對於機率而言是沒意義的 您想要的是它介於 0 跟 1 所以在羅吉斯迴歸分析我們的輸出會讓 y-hat  等於S型函數應用到這個量 S型函數的樣子像這樣 如果在橫軸我畫 z 那S型函數 of z 像這樣 它平緩的從 0 到 1 讓我在軸上做標籤 這是 0 而它跟縱軸的交叉是 0.5 這就是S型函數 of z 的樣子而我們將使用 z 來代表這個量 w轉置x 加 b 這是S型函數的公式 S型函數 of z, z 是一個實數 是 1 / 1 加 e 的 負z 請注意幾件事 如果 z 很大那 e 的負z 會趨近於 0 所以S型函數 of z 會是 趨近於 1 除以 1 加這個趨近於0 的東西 因為 e 的負很大的值會趨近於 0 所以這整個趨近於 1 實際上, 如果您看圖的左邊 如果 z 很大這S型函數會趨近於 1 相反的, 如果 z 很小 或者是很大的負數 那S型函數 of z 變成 1 除以 1 加 e 的負 z 這個會變成很大的數 這個想成是 1 除以 1 加很大很大 的數 這整個會趨近於 0 真的, 您看當 z 變成很大的負數 S型函數會趨近於 0 所以當您建置羅吉斯迴歸分析 您的工作是學習參數 w 跟 b 使得 y-hat 變成一個好的預測對於 y 等於 1 的機會 在繼續往前之前, 關於符號的一個註解 當我們做神經網路的程式時 我們通常會將參數 w 跟參數 b 分開 這裡, b 相當於截距 在其他一些課程中 您也許看到一些不同的符號 在某些約定中您定義ㄧ個額外的特徵 x0 而這等於 1 所以 X 是 R 的 nx + 1 而您定義 y-hat 等於 Sigma of theta 轉置 X 在這另一種符號約定中 您有向量參數 theta theta0, theta1, theat2.. 直到 theta nx theta0 放在 b 的位置 一樣只是個實數 theta1 直到 theta nx 則跟 W 一樣, 實際上 當您建置您的神經網路時 保持 B 跟 W 分開會比較容易 所以, 在這個課程 我們不會使用這個紅色的符號約定 如果您在本課程前不曾見過這些符號, 不用擔心 只是要讓那些見過這樣符號的人我要 明確的提醒我們在這課程不使用這樣的符號 但如果您不曾見過這些 這不重要您不需要擔心 您已經看到羅吉斯迴歸分析模型的樣子 接下來要改變參數 w 跟 b 您需要定義成本函數 讓我們下一段影片談這個部分