1
00:00:00,000 --> 00:00:01,530
在上一段影片中

2
00:00:01,530 --> 00:00:04,227
您看到羅吉斯迴歸分析模型

3
00:00:04,227 --> 00:00:07,526
為了訓練參數W跟B對於羅吉斯迴歸分析模型

4
00:00:07,526 --> 00:00:10,570
您需要定義成本函數

5
00:00:10,570 --> 00:00:14,430
讓我們看看您可以用來訓練羅吉斯迴歸分析的成本函數

6
00:00:14,430 --> 00:00:18,195
回顧一下, 這是我們在上一節課談過的

7
00:00:18,195 --> 00:00:20,792
您的輸出y-hat是S型函數對於w

8
00:00:20,792 --> 00:00:24,690
轉置x加上b而這裡的S型函數of z 定義在這裡

9
00:00:24,690 --> 00:00:27,600
所以為了學習模型的參數,您給予

10
00:00:27,600 --> 00:00:31,200
一個訓練集有m個訓練例子而它

11
00:00:31,200 --> 00:00:34,060
似乎自然而然的您會想找到參數W

12
00:00:34,060 --> 00:00:37,781
跟B至少在訓練集裡, 您得到的輸出

13
00:00:37,781 --> 00:00:40,225
從您在訓練集的預測

14
00:00:40,225 --> 00:00:43,260
我們寫成y-hat(i)會趨近於

15
00:00:43,260 --> 00:00:47,720
真正的標籤值y_i 從您訓練及的資料得到的

16
00:00:47,720 --> 00:00:52,110
所以再進一步的細節利用上面方程式

17
00:00:52,110 --> 00:00:56,205
我們說y-hat定義在上面的

18
00:00:56,205 --> 00:01:00,930
訓練例子x, 當然對於每一個例子

19
00:01:00,930 --> 00:01:03,240
我們使用這個上標用

20
00:01:03,240 --> 00:01:07,710
圓括弧內用索引來區分不同的例子

21
00:01:07,710 --> 00:01:12,870
您的預測在訓練例子(i)會是y-hat(i)將是

22
00:01:12,870 --> 00:01:18,835
通過S型函數用於W轉置X

23
00:01:18,835 --> 00:01:25,905
(i) 訓練例子的輸入加上B, 而您可以定義Z(i) 為

24
00:01:25,905 --> 00:01:30,110
Z(i) 等於W轉置x(i)加b

25
00:01:30,110 --> 00:01:31,350
所以這整個課程裡

26
00:01:31,350 --> 00:01:33,966
我們將用這個符號約定

27
00:01:33,966 --> 00:01:41,605
也就是上標括號i代表資料

28
00:01:41,605 --> 00:01:47,615
X或Y或Z是一些相關於第i個訓練例子

29
00:01:47,615 --> 00:01:50,885
相關於第i個例子

30
00:01:50,885 --> 00:01:54,840
這就是上標i在括號中的意義

31
00:01:54,840 --> 00:01:57,630
現在讓我們看看什麼是損失函數或者錯誤

32
00:01:57,630 --> 00:02:01,315
函數我們可以用來衡量我們的演算法有多好

33
00:02:01,315 --> 00:02:06,015
一種方式是您可以定義損失為當您的演算法輸出

34
00:02:06,015 --> 00:02:12,320
y-hat而真正的標籤是y也許平方誤差或者是平方誤差的一半

35
00:02:12,320 --> 00:02:14,975
事實上您可以這樣做

36
00:02:14,975 --> 00:02:17,670
但在羅吉斯迴歸分析, 人們並不這樣做

37
00:02:17,670 --> 00:02:21,000
因為當您用來學習參數時

38
00:02:21,000 --> 00:02:25,682
您會發現當我們以後會談論到優化問題時會變成非凸

39
00:02:25,682 --> 00:02:30,105
所以這樣會在優化問題時有很多的局部最佳

40
00:02:30,105 --> 00:02:33,285
所以梯度下降可能無法找到全域最佳解

41
00:02:33,285 --> 00:02:35,580
如果你不明白最後幾點意見

42
00:02:35,580 --> 00:02:38,320
別擔心，我們會在以後的影片談到

43
00:02:38,320 --> 00:02:40,990
但直覺上要記住的是

44
00:02:40,990 --> 00:02:44,620
這個函數L稱為損失函數是一個函數您會

45
00:02:44,620 --> 00:02:51,265
需要去定義衡量我們的輸出y-hat有多好當這真正的標籤是y

46
00:02:51,265 --> 00:02:54,345
誤差平方看起來像它可能是一個合理的選擇

47
00:02:54,345 --> 00:02:58,160
除了它會讓梯度下降不可行

48
00:02:58,160 --> 00:03:00,500
所以在羅吉斯迴歸分析, 我們實際上定義

49
00:03:00,500 --> 00:03:05,695
不一樣的損失函數扮演類似平方誤差的角色

50
00:03:05,695 --> 00:03:08,910
會給我們在優化時是

51
00:03:08,910 --> 00:03:13,530
凸形而我們將在往後的影片中變得比較容易做優化

52
00:03:13,530 --> 00:03:17,310
因此，我們在羅吉斯迴歸分析中實際上是使用

53
00:03:17,310 --> 00:03:21,795
底下的損失函數就像上面這裡

54
00:03:21,795 --> 00:03:31,740
是負y log y-hat 加 1 減 y log

55
00:03:31,740 --> 00:03:34,600
1 減 y-hat

56
00:03:34,600 --> 00:03:38,785
這裡是一些直覺為什麼這個損失函數是合理的

57
00:03:38,785 --> 00:03:41,285
請記住，如果我們使用

58
00:03:41,285 --> 00:03:45,820
平方誤差，那麼你想要的平方誤差要盡可能小

59
00:03:45,820 --> 00:03:48,680
同樣地用這個羅吉斯迴歸分析損失函數

60
00:03:48,680 --> 00:03:51,495
我們也會想這要它盡可能小

61
00:03:51,495 --> 00:03:53,508
了解它為什麼是合理的

62
00:03:53,508 --> 00:03:55,260
讓我們看兩種情況

63
00:03:55,260 --> 00:03:56,570
第一種情況

64
00:03:56,570 --> 00:03:59,430
假設Y等於 1 那損失

65
00:03:59,430 --> 00:04:05,415
函數y-hat 逗點就只是第一項也就是

66
00:04:05,415 --> 00:04:08,735
負的 log y-hat

67
00:04:08,735 --> 00:04:10,770
如果 y 等於 1, 因為如果 y 等於

68
00:04:10,770 --> 00:04:14,070
1 那第二項 1 減 y 會等於 0

69
00:04:14,070 --> 00:04:19,880
所以這是說如果 y 等於 1 您希望 負的 log y-hat 越小越好

70
00:04:19,880 --> 00:04:26,040
也就是說 log y-hat 越大越好

71
00:04:26,040 --> 00:04:32,935
越大表示您希望 y-hat 越大

72
00:04:32,935 --> 00:04:35,170
但因為您知道

73
00:04:35,170 --> 00:04:38,440
在S型函數中,它不會大於1

74
00:04:38,440 --> 00:04:41,850
所以這也就是說如果 y 等於 1, 您

75
00:04:41,850 --> 00:04:44,050
希望 y-hat 越大越好

76
00:04:44,050 --> 00:04:48,220
但它不會大於 1 所以這說明了您希望 y-hat 盡量接近 1

77
00:04:48,220 --> 00:04:50,740
另一種情況是如果 y 等於 0

78
00:04:50,740 --> 00:04:55,375
如果 y 等於 0 那損失函數第一項會等於 0 因為

79
00:04:55,375 --> 00:05:01,290
y 是 0, 然後第二項定義了損失函數

80
00:05:01,290 --> 00:05:07,210
所以損失變成負的 log 1 減 y-hat

81
00:05:07,210 --> 00:05:11,480
所以如果在學習過程中您試著讓損失函數最小

82
00:05:11,480 --> 00:05:19,450
也就是您希望 log 1 減 y-hat 最大

83
00:05:19,450 --> 00:05:22,050
因為這裡有一個負號而

84
00:05:22,050 --> 00:05:24,660
經過類似的原因您可以得到結論

85
00:05:24,660 --> 00:05:30,870
說這個損失函數試著讓 y-hat 越小越好

86
00:05:30,870 --> 00:05:34,320
再一次因為 y-hat 是介於 0 跟 1

87
00:05:34,320 --> 00:05:38,155
這說明了如果 y 等於 0 那

88
00:05:38,155 --> 00:05:43,790
您的損失函數會推向那些使得 y-hat 接近於 0 的參數

89
00:05:43,790 --> 00:05:48,305
現在有很多的函數具備 Rafidah 效果, 也就是 y 等於

90
00:05:48,305 --> 00:05:52,950
1 我們試著讓 y-hat 越大, y 如果等於 0 我們試著讓 y-hat 越小

91
00:05:52,950 --> 00:05:55,150
我們在綠色部分

92
00:05:55,150 --> 00:05:59,920
是一個非正式的理解對於這個損失函數, 會在以後提供

93
00:05:59,920 --> 00:06:03,970
一個額外的影片來給與正式的證明

94
00:06:03,970 --> 00:06:08,500
為什麼在羅吉斯迴歸分析我們喜歡用這個損失函數用這個特別樣式

95
00:06:08,500 --> 00:06:13,630
最後這損失函數是定義相對於單一的訓練例子

96
00:06:13,630 --> 00:06:16,760
它衡量了您作用於單一訓練例子好不好

97
00:06:16,760 --> 00:06:21,148
我現在將定義稱為成本函數

98
00:06:21,148 --> 00:06:24,690
來衡量您在整個訓練集作用得好不好

99
00:06:24,690 --> 00:06:28,660
所以成本函數 J 是適用於

100
00:06:28,660 --> 00:06:33,130
您的參數W 跟 B 將是平均於

101
00:06:33,130 --> 00:06:43,270
1/m 的損失函數的和應用到每一個訓練例子

102
00:06:43,270 --> 00:06:45,435
而這裡的 y-hat 當然是

103
00:06:45,435 --> 00:06:49,570
應用您的羅吉斯迴歸分析演算法的預測輸出, 使用了

104
00:06:49,570 --> 00:06:52,430
一組特別的參數 W 跟 B

105
00:06:52,430 --> 00:06:54,480
而將這個展開

106
00:06:54,480 --> 00:06:58,010
就是等於負的 1 / m

107
00:06:58,010 --> 00:07:03,550
取和從 i 等於 1 到 m 對於這個損失函數的定義

108
00:07:03,550 --> 00:07:07,530
所以這是 y(i) log y-hat(i)

109
00:07:07,530 --> 00:07:14,530
加 1 減 y(i) log 1 減 y-hat (i)

110
00:07:14,530 --> 00:07:17,880
我想我可以放一個方括號在這裡

111
00:07:17,880 --> 00:07:20,945
所以負號在外面

112
00:07:20,945 --> 00:07:23,665
所以我將使用這些專有名詞

113
00:07:23,665 --> 00:07:29,120
損失函數是應用在單一的訓練例子

114
00:07:29,120 --> 00:07:33,010
而成本函數是您參數的成本

115
00:07:33,010 --> 00:07:36,115
所以在訓練您的羅吉斯迴歸分析模型時

116
00:07:36,115 --> 00:07:38,980
我們將試著找尋參數 W 跟 B 使得

117
00:07:38,980 --> 00:07:43,475
讓所有成本最小對於 在最底下的 J 函數

118
00:07:43,475 --> 00:07:48,040
所以, 您剛剛看到對羅吉斯迴歸分析演算法的設定

119
00:07:48,040 --> 00:07:50,770
訓練例子的損失函數跟

120
00:07:50,770 --> 00:07:54,190
整個成本函數對於您演算法的參數

121
00:07:54,190 --> 00:07:59,485
實際上羅吉斯迴歸分析可以視為非常非常小的神經網路

122
00:07:59,485 --> 00:08:01,905
在下一段影片我們會談論到所以您可以開始

123
00:08:01,905 --> 00:08:04,965
得到一些直觀對於神經網路

124
00:08:04,965 --> 00:08:08,230
所以讓我們進入下一段影片對於如何

125
00:08:08,230 --> 00:08:11,630
視羅吉斯迴歸分析為一個小的神經網路