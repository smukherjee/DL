好的，我認為這段影片會很令人激動 在這個影片中，您將看到如何 對於有一隱藏層的神經網路建置梯度下降法 於此影片，我只給你需要的方程式 來建置反向傳播，使得梯度下降法起效果 並且，在下一段影片中 我會給出直覺，關於為什麼 這些特定的方程式是精確的、 正確的方程當你需為你的神經網路計算梯度時 所以，你的神經網路 現在只有一個隱藏層的話 會有參數W[1] B[1],W[2]與B[2] 所以，作為提醒， 如果您有N_x或換句話說N[0]個輸入特徵的話 和N[1]個隱藏單元 跟N[2]個輸出單元在我們的例子中 到目前為止，我只有N[2]=1 那麼矩陣W[1]=N[1] 乘 N[0] B[1]會時N[1]維度向量 所以我們將他寫作N[1]乘以1維矩陣 實際上是一個列向量 W[2]的維度是(N[2],N[1]) B[2]的維度是(N[2],1) 好的，目前我們只看到N[2]=1的例子 其中你只有一個隱藏單位 所以，對於一個神經網路，你也有一個成本函數 從現在開始，我將假設你在做二元分類 所以，這樣子的話 您的參數的成本將是 1/M的成本函數求平均值 所以，在這裡的L是成本，當你的神經網路預測 y-hat 實際上這裡是a[2]當梯度標籤=y時 如果您要做二元分類， 損失函數會恰好等於之前用的羅吉斯迴歸分析的損失函數 所以，為了訓練您的演算法的參數 你需要執行梯度下降 當訓練神經網路時 隨機初始化這些權重而不是全部設為0
是很重要的 我們之後就會看見為什麼會這樣 但是經過初始參數為某些值的話 每個迴圈或梯度下降計算出的預測 所以，你基本上計算y-hat_i i=1,...,m，然後 你需計算導數 所以你需要計算dw1 這是成本函數對於w1參數的導數 你可以計算另外一個變數 我把它稱作db[1] 它等於你的成本函數的導數或者說斜率 相對於變數b[1]... 對於w[2]和b[2]的計算很相似 那麼最後，梯度下降更新會更新w[1]為w[1]-alpha 學習率*dw[1] b[1]更新為b[1]-學習率 (alpha) 乘以db[1]，同樣對w[2]和b[2]計算相似的過程 有時，我使用 :=，有時 = 兩者都可使用 所以，這會是梯度下降的一次迭代 並且你重複這個迭代幾次 直到您的參數看到來收斂了 所以，在上一段影片中 我們討論如何計算預測 如何計算輸出之類的 並且我們看到在向量形式的表示法下<br />如何做這些事 所以重點是如何計算這些偏導數 dw[1]，db[1]和導數dw[2],db[2] 所以，我想做的只是給您 您需要的方程式來計算導數 請不要忘記看下段影片，下段是選看影片 更深入的了解我們如何
導出了這些公式。 所以,讓我再總結一遍
傳播的公式。 所以,你有Z[1]等於 W[1] 乘 X 加 b[1], 然後 A[1] 等於啟動函數
應用在該層中逐元素 - Z[1], 然後Z[2]等於W[2], A[1] 加 b[2],最後, 所有的訓練集都一樣的向量化,對嗎? A[2] 等於 g[2] Z[2] 再次,現在,如果我們假設我們
在做二元分類分類, 這裡的啟動函數應該是S型函數(sigmoid) 只是作為結束。 所以,這是向前傳播或左向右 給你的神經網路做的運算 接下來,讓我們計算導數。 所以,這是反向傳播步驟。 然後,我計算 dZ[2] 等於 A[2]
減去基本真值 Y, 只是要提醒一下, 所有這些例子都已向量化。 因此,矩陣 Y 是這個 1 by m 的矩陣,所有您的例子水平的堆放 原來實際上 dw[2] 等於這個, 事實上,這前面三個方程 非常類似于羅吉斯迴歸分析的梯度下降。 axis 等於 1, 逗號, keepdims等於True。 只是一點點細節,這個np.sum是 個 Python NumPy 的指令
用於求和於所有的一維矩陣。 在這個例子是水平的總和 而 keepdims 做的是避免Python 產生奇怪的
等級 1 的數列,對不對? 維度是 (n,)。 所以, 通過保持 keepdims 等於 True。 Python 輸出 db[2] 是一個 n 乘 1 向量 事實上,從技術上講,我想這將是 n2 乘 1。 在這種情況下,它只是一個數字, 所以也許沒關係. 但稍後,我們將看到它何時變成真正重要。 所以,到目前為止,我們所做的是非常
類似于羅吉斯迴歸分析 但現在,當你繼續反向傳播, 您需要計算這個 (dZ[1] = ...)dZ[2] 乘以g[1] prime of z[1]。 所以,這個數量 g[1] prime是 用於隱藏層的啟動函數的導數, 對於輸出層, 我假設你在進行二元分類
使用 S型函數 sigmoid 。 這已經包括到dZ[2]的公式中 提醒一下, 這個 乘 是逐元素乘積 所以,這裡將是一個 (n1, m) 矩陣,在這裡, 這個逐元素的導數也將是 (n1, m) 矩陣, 所以這個乘那個是兩個矩陣逐元素相乘的積。 最後, dW[1] 等於這個, 和 dB[1] 等於這個, 和dp.sum dZ[1], axis 等於 1, keepdims等於True。 所以,而以前如果 n[2] 等於 1,
 keepdims 可能就不那麼重要了。 結果只是 (1,1) 的項目,只是一個實數。 在這裡,db1 將是一個 (n1, 1) 的向量, 所以你想要Python,你想要np.sum 功能正常。 我會放個維度keppdims的東西,
而不產生一個奇怪的等級 1 的數列 同樣的維度,可能最終
搞砸了一些資料的計算。 另一種方法是,不用 keepsims 參數, 但明確的用reshape 參數,將 np.sum 的輸出到此維度, 您希望 db 的維度. 所以,這是向前傳播
我想,有四個方程式, 和反向傳播,
我想,有六個方程式。 我知道我只是寫下了這些方程式, 但在下一個選修的影片中, 讓我們用一些直覺的方式去 導出反向傳播演算法的六個方程式. 隨您意看或不看. 不管如何, 如果您建置這些演算法, 您要知道如何正確的建置
正向傳播跟反向傳播 您要能夠計算你需要的導數
來應用梯度下降, 來學習神經網路的參數。 您可以運用這個演算法, 有成果,而沒有深入理解微積分。 很多成功的深度學習從業者是這樣做。 但是,如果你想, 您也可以觀看下一段影片 會得到多一點這些方程式推導的直覺。