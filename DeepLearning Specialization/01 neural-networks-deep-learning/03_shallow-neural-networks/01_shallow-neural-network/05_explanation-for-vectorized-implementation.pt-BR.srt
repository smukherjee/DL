1
00:00:00,000 --> 00:00:01,530
No vídeo anterior,

2
00:00:01,530 --> 00:00:06,885
vimos que, com os exemplos de treinamento 
empilhados horizontalmente na matriz x,

3
00:00:06,885 --> 00:00:11,158
você pode derivar a implementação vetorizada 
 da propagação (para frente), através de sua rede neural.

4
00:00:11,158 --> 00:00:14,760
Vamos explicar um pouco mais 
o porquê as equações que escrevemos,

5
00:00:14,760 --> 00:00:19,775
é uma implementação correta de 
vetorização em múltiplos exemplos.

6
00:00:19,775 --> 00:00:25,590
Então, vamos repassar parte do cálculo 
de propagação de alguns exemplos.

7
00:00:25,590 --> 00:00:27,645
Digamos para o primeiro 
exemplo de treinamento,

8
00:00:27,645 --> 00:00:29,130
você acabou calculando
z'¹'⁽¹⁾ =

9
00:00:29,130 --> 00:00:38,970
w'¹'. x⁽¹⁾ + b'¹', depois no 
segundo exemplo de treinamento

10
00:00:38,970 --> 00:00:49,310
você calculou 
z'¹'⁽²⁾ = w'¹' x⁽²⁾ + b'¹'

11
00:00:49,310 --> 00:00:50,900
e, no terceiro exemplo,

12
00:00:50,900 --> 00:00:56,064
você calculou z'¹'⁽³⁾ = w'¹' x⁽³⁾ + b'¹'

13
00:00:56,064 --> 00:01:00,930
Então, para simplificar a 
explicação neste slide, vou ignorar b.

14
00:01:00,930 --> 00:01:08,395
Digamos que, para simplificar um pouco 
esta explicação, b é igual a zero.

15
00:01:08,395 --> 00:01:11,140
Mas o argumento que vamos 
esquematizar funcionará com

16
00:01:11,140 --> 00:01:14,320
uma pequena mudança mesmo 
quando b é diferente de zero.

17
00:01:14,320 --> 00:01:17,610
Isso apenas simplifica 
um pouco o slide.

18
00:01:17,610 --> 00:01:21,110
w'¹' será uma matriz,
 certo?

19
00:01:21,110 --> 00:01:25,625
Então, eu tenho um 
número de linhas nesta matriz.

20
00:01:25,625 --> 00:01:28,296
Então, se olhar o cálculo de x⁽¹⁾,

21
00:01:28,296 --> 00:01:30,070
o que você tem é,

22
00:01:30,070 --> 00:01:40,021
que w'¹' vezes x⁽¹⁾ resulta em um vetor 
coluna que você deve desenhar assim.

23
00:01:40,021 --> 00:01:47,420
E da mesma forma, se você 
olha para este vetor x⁽²⁾,

24
00:01:47,420 --> 00:01:54,730
você tem que w'¹' vezes

25
00:01:54,730 --> 00:02:00,460
x⁽²⁾ dá um outro vetor coluna, certo?

26
00:02:00,460 --> 00:02:03,250
E resulta este z'¹'⁽²⁾.

27
00:02:03,250 --> 00:02:06,730
E finalmente, se você olha para o x⁽³⁾,

28
00:02:06,730 --> 00:02:12,315
você tem w'¹' vezes x⁽³⁾,

29
00:02:12,315 --> 00:02:19,530
dá um terceiro 
vetor coluna, isso é z'¹'⁽³⁾.

30
00:02:19,530 --> 00:02:25,250
Então, se você considera 
o conjunto de treinamento X,

31
00:02:25,250 --> 00:02:31,475
que formamos empilhando todos 
os nossos exemplos de treinamento.

32
00:02:31,475 --> 00:02:37,010
Então, a matriz X maiúscula, é formada 
pegando o vetor x⁽¹⁾ e

33
00:02:37,010 --> 00:02:43,430
empilhá-lo verticalmente 
com x⁽²⁾ e x⁽³⁾ também.

34
00:02:43,430 --> 00:02:46,250
Isso é se tivermos apenas 
três exemplos de treinamento.

35
00:02:46,250 --> 00:02:50,371
Se você tem mais, sabe, continuará 
empilhando horizontalmente desta forma.

36
00:02:50,371 --> 00:02:57,790
Mas agora, se você tem esta 
matriz X e a multiplica por w'¹', e resulta,

37
00:02:57,790 --> 00:03:00,190
se você pensar como funciona 
a multiplicação de matrizes,

38
00:03:00,190 --> 00:03:02,680
resulta com a primeira coluna sendos

39
00:03:02,680 --> 00:03:06,313
estes mesmos valores que 
eu desenhei aqui em roxo.

40
00:03:06,313 --> 00:03:10,930
A segunda coluna será 
aqueles mesmos quatro valores.

41
00:03:10,930 --> 00:03:16,612
E a terceira será aqueles valores em laranja,

42
00:03:16,612 --> 00:03:19,480
o que eles resultarão.

43
00:03:19,480 --> 00:03:27,740
Mas claro, isso é igual 
ao z'¹'⁽¹⁾ expressado como

44
00:03:27,740 --> 00:03:37,185
um vetor coluna, seguido por z'¹'⁽²⁾, representado 
por um vetor coluna, seguido por z'¹'⁽³⁾,

45
00:03:37,185 --> 00:03:39,273
também representado como um vetor coluna.

46
00:03:39,273 --> 00:03:41,100
E isso é se você tem três exemplos de treinamento.

47
00:03:41,100 --> 00:03:44,255
Se você tem mais exemplos, 
então haveria mais colunas.

48
00:03:44,255 --> 00:03:51,220
E então, essa é nossa matriz Z'¹'.

49
00:03:51,220 --> 00:03:55,230
Espero que isso dê a você uma 
explicação do porquê você antes

50
00:03:55,230 --> 00:04:02,830
teve w'¹' vezes x⁽ⁱ⁾ igual

51
00:04:02,830 --> 00:04:08,310
z'¹'⁽ⁱ⁾ quando nós estamos olhando para um 
único exemplo de treinamento no momento.

52
00:04:08,310 --> 00:04:12,565
Quando você pegou os distintos exemplos de 
treinamento e empilhá-los em diferentes colunas,

53
00:04:12,565 --> 00:04:15,250
então o resultado 
correspondente é que você acaba

54
00:04:15,250 --> 00:04:18,725
com os z's também 
empilhados em colunas.

55
00:04:18,725 --> 00:04:24,565
E eu não mostrarei, mas você pode se convencer, 
se usar Python broadcasting

56
00:04:24,565 --> 00:04:26,245
se você adicionasse isso,

57
00:04:26,245 --> 00:04:30,534
estes valores de b
os valores são ainda corretos.

58
00:04:30,534 --> 00:04:34,540
E o que realmente acaba acontecendo 
é que quando você utiliza broadcasting no Python,

59
00:04:34,540 --> 00:04:41,790
você acaba tendo b⁽ⁱ⁾ individualmente 
para cada coluna desta matriz.

60
00:04:41,790 --> 00:04:48,220
Então, neste slide, eu só 
expliquei que z'¹' igual

61
00:04:48,220 --> 00:04:51,980
w'¹' X + b'¹' é

62
00:04:51,980 --> 00:04:54,020
uma vetorização correta do

63
00:04:54,020 --> 00:04:57,493
primeiro passo dos quatro passos 
que tivemos no slide anterior,

64
00:04:57,493 --> 00:04:59,990
mas resulta que uma análise 
similar permite que você

65
00:04:59,990 --> 00:05:02,660
mostre que outras etapas 
também funcionam utilizando

66
00:05:02,660 --> 00:05:08,105
uma lógica similar onde se você empilha as 
entradas nas colunas, então após a equação,

67
00:05:08,105 --> 00:05:11,510
você obtém as saídas correspondentes, 
também empilhadas em colunas.

68
00:05:11,510 --> 00:05:14,970
Finalmente, vamos recapitular 
tudo o que falamos neste vídeo.

69
00:05:14,970 --> 00:05:16,520
Se esta é sua rede neural,

70
00:05:16,520 --> 00:05:21,693
dissemos que isto é o que você precisa fazer, se 
você estiver implementando propagação para frente,

71
00:05:21,693 --> 00:05:27,693
um exemplo de treinamento por vez, 
indo de i igual a 1 até m. E então dizemos,

72
00:05:27,693 --> 00:05:34,100
vamos empilhar os exemplos de treinamento 
em colunas desta forma, e para cada valor z'¹',

73
00:05:34,100 --> 00:05:38,265
a'¹', z'²', a'²', vamos empilhar as 
colunas correspondentes como segue.

74
00:05:38,265 --> 00:05:43,820
Então, este é um exemplo 
para A'¹', mas é válido para Z'¹',

75
00:05:43,820 --> 00:05:46,975
A'¹', Z'²', e A'²'.

76
00:05:46,975 --> 00:05:51,090
Então, o que mostramos 
no slide anterior foi que

77
00:05:51,090 --> 00:05:58,785
esta linha permite que você vetorize 
todos os m exemplos ao mesmo tempo.

78
00:05:58,785 --> 00:06:00,555
E com um raciocínio semelhante,

79
00:06:00,555 --> 00:06:03,880
você pode mostrar que 
todas as outras linhas são

80
00:06:03,880 --> 00:06:08,811
vetorizações corretas de 
todas as quatro linhas de código.

81
00:06:08,811 --> 00:06:10,675
E somente como um lembrete,

82
00:06:10,675 --> 00:06:18,960
porque X é também igual a matriz A'⁰', porque lembre-se que

83
00:06:18,960 --> 00:06:27,980
o vetor das características de entrada x = a'⁰', então x⁽ⁱ⁾ = a'⁰'⁽ⁱ⁾.

84
00:06:27,980 --> 00:06:30,870
Então, há na verdade uma certa simetria para

85
00:06:30,870 --> 00:06:34,110
essas equações onde esta 
primeira equação pode também ser

86
00:06:34,110 --> 00:06:41,790
escrito Z'¹' = W'¹' A'⁰' + b'¹'.

87
00:06:41,790 --> 00:06:45,680
E assim, você vê que este par 
de equações e este par de

88
00:06:45,680 --> 00:06:51,805
equações na verdade parece muito similar 
mas apenas todos os índices avançam por um.

89
00:06:51,805 --> 00:06:55,880
Então, isso mostra que as diferentes 
camadas de uma rede neural estão

90
00:06:55,880 --> 00:07:00,585
fazendo mais ou menos a mesma coisa 
ou fazendo os mesmos cálculos repetidamente.

91
00:07:00,585 --> 00:07:04,220
E aqui nós temos uma rede 
neural de duas camadas onde vamos

92
00:07:04,220 --> 00:07:08,475
à uma rede neural mais profunda 
nos vídeos da próxima semana.

93
00:07:08,475 --> 00:07:11,670
Veja que, mesmo redes neurais mais 
profundas, estão basicamente tomando

94
00:07:11,670 --> 00:07:16,215
estes dois passos, apenas fazendo 
isso mais vezes do que vimos aqui.

95
00:07:16,215 --> 00:07:21,255
Então, assim é como você pode vetorizar sua 
rede neural em múltiplos exemplos de treinamento.

96
00:07:21,255 --> 00:07:25,590
A seguir, nós viemos usando até agora as 
funções sigmoide ao longo de nossas redes neurais.

97
00:07:25,590 --> 00:07:27,925
Resulta que esta, na verdade, 
não é a melhor escolha.

98
00:07:27,925 --> 00:07:29,675
No próximo vídeo, vamos 
mergulhar um pouco mais

99
00:07:29,675 --> 00:07:32,450
em como você pode usar 
diferentes, o que são chamadas,

100
00:07:32,450 --> 00:07:37,190
funções de ativação, onde a função sigmoide 
é apenas uma das escolhas possíveis. 
[Tradução: Renato Barata Gomes | Revisão: Carlos Lage.]