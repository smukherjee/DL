1
00:00:00,000 --> 00:00:03,339
سمعنا جميعًا بأن
الشبكات العصبية العميقة تعمل بشكل جيد

2
00:00:03,339 --> 00:00:07,073
فيما يتعلق بكثير من المشاكل، والأمر لا يتعلق
بهذا فحسب فهي يجب أن تكون شبكات عصبية كبيرة

3
00:00:07,073 --> 00:00:10,718
على وجه التحديد، يجب أن تكون عميقة
أو أن تحتوي على الكثير من الطبقات المخفية.

4
00:00:10,718 --> 00:00:12,208
إذًا لماذا هذا؟

5
00:00:12,208 --> 00:00:15,833
دعونا نستعرض بعض الأمثلة
ونحاول تخمين

6
00:00:15,833 --> 00:00:17,720
الأسباب التي تجعل الشبكات
العميقة قد تعمل جيدًا.

7
00:00:17,720 --> 00:00:22,181
أولاً، ما المقصود
بالحوسبة العميقة للشبكة؟

8
00:00:22,181 --> 00:00:25,393
إذا كنتم تقومون بإنشاء نظام
للتعرف على الوجه أو

9
00:00:25,393 --> 00:00:29,631
اكتشاف الوجه، فإليكم
ما قد تفعله الشبكة العصبية العميقة.

10
00:00:29,631 --> 00:00:35,059
ربما تقومون بإدخال صورة لوجه، وبعد ذلك
يمكنكم اعتبار الطبقة الأولى من الشبكة العصبية

11
00:00:35,059 --> 00:00:40,000
بمثابة مكتشفة
للميزات أو كاشفة للحواف.

12
00:00:40,000 --> 00:00:45,519
في هذا المثال، سأرسم ما المقصود بالشبكة
العصبية من خلال ربما 20 وحدة مخفية،

13
00:00:45,519 --> 00:00:48,017
وهي ربما محاولة للحساب على هذه الصورة.

14
00:00:48,017 --> 00:00:52,357
وبالتالي تم تمثيل الـ 20 وحدة مخفية مرئيًا
من خلال هذه المربعات الصغيرة.

15
00:00:52,357 --> 00:00:57,325
فعلى سبيل المثال، يمثل هذا
الرسم المرئي الصغير وحدة مخفية

16
00:00:57,325 --> 00:01:01,978
تحاول معرفة أين توجد
حواف هذا التوجه في الصورة.

17
00:01:01,978 --> 00:01:06,110
وربما تحاول هذه
الوحدة المخفية معرفة

18
00:01:06,110 --> 00:01:09,955
أين توجد الحواف الأفقية
في هذه الصورة.

19
00:01:09,955 --> 00:01:13,184
وعندما نتحدث حول الشبكات الالتفافية
في دورة تدريبية لاحقة،

20
00:01:13,184 --> 00:01:16,129
فسيكون هذا التمثيل المرئي
منطقيًا أكثر.

21
00:01:16,129 --> 00:01:19,562
ولكن يمكنكم التفكير في الطبقة الأولى
للشبكة العصبية من خلال النظر إلى

22
00:01:19,562 --> 00:01:22,690
الصورة ومحاولة معرفة
أين توجد الحواف في هذه الصورة.

23
00:01:22,690 --> 00:01:27,050
والآن دعونا نفكر في مكان الحواف
في هذه الصورة من خلال تجميع

24
00:01:27,050 --> 00:01:28,730
وحدات البكسل معًا لتشكيل الحواف.

25
00:01:28,730 --> 00:01:34,670
ويمكن بعد ذلك الكشف عن الحواف
وتجميعها معًا لتشكيل أجزاء الوجه.

26
00:01:34,670 --> 00:01:40,289
فعلى سبيل المثال، قد تكون لديكم خلية عصبية
صغيرة تحاول معرفة ما إذا كان يمكنها اكتشاف عين،

27
00:01:40,289 --> 00:01:44,480
أو خلية عصبية مختلفة تحاول
اكتشاف هذا الجزء من الأنف.

28
00:01:44,480 --> 00:01:47,463
ومن خلال تجميع الكثير من الحواف معًا،

29
00:01:47,463 --> 00:01:50,970
يمكن البدء في اكتشاف
أجزاء مختلفة من الوجوه.

30
00:01:50,970 --> 00:01:56,035
وأخيرًا من خلال تجميع
أجزاء مختلفة من الوجوه،

31
00:01:56,035 --> 00:02:01,006
مثل عين أو أنف أو أذن أو ذقن،
يمكن بعدها أن تحاول معرفة أو

32
00:02:01,006 --> 00:02:03,564
اكتشاف أنواع مختلفة من الوجوه.

33
00:02:03,564 --> 00:02:07,755
لذا بشكل بديهي، يمكنكم التفكير في
الطبقات السابقة للشبكة العصبية باعتبارها

34
00:02:07,755 --> 00:02:10,190
تكتشف وظائف بسيطة، مثل الحواف.

35
00:02:10,190 --> 00:02:14,573
وبعد ذلك تعمل على تجميعها
في الطبقات اللاحقة للشبكة العصبية

36
00:02:14,573 --> 00:02:17,625
حتى يمكنها معرفة
المزيد من الوظائف المعقدة.

37
00:02:17,625 --> 00:02:23,640
ستبدو هذه المرئيات منطقية أكثر
عندما نتحدث حول الشبكات الالتفافية.

38
00:02:23,640 --> 00:02:26,203
ومن التفاصيل الفنية
لهذا التمثيل المرئي،

39
00:02:26,203 --> 00:02:29,802
فإن كاشفات الحواف تبحث في
مناطق صغيرة نسبيًا للصورة،

40
00:02:29,802 --> 00:02:31,703
والتي قد تكون مناطق صغيرة جدًا كهذه.

41
00:02:31,703 --> 00:02:37,950
وبعد ذلك قد تكون أجهزة كشف الوجه التي
يمكنكم النظر إليها تمثل مناطق أكبر من الصورة.

42
00:02:37,950 --> 00:02:41,308
ولكن الحدس الرئيسي الذي تنتزعه من هذا
هو اكتشاف الأشياء البسيطة

43
00:02:41,308 --> 00:02:43,675
مثل الحواف ثم تطويرها.

44
00:02:43,675 --> 00:02:47,216
وتجميعها معًا لاكتشاف أشياء
أكثر تعقيدًا مثل عين أو أنف

45
00:02:47,216 --> 00:02:50,530
وبعد ذلك تجميعها معًا
لاكتشاف المزيد من الأشياء المعقدة.

46
00:02:50,530 --> 00:02:55,665
وهذا النوع من التمثيل الهرمي
البسيط إلى المعقد،

47
00:02:55,665 --> 00:02:58,508
أو التمثيل التركيبي،

48
00:02:58,508 --> 00:03:04,114
يتم تطبيقه في أنواع بيانات أخرى
بخلاف الصور والتعرف على الوجه.

49
00:03:04,114 --> 00:03:07,100
على سبيل المثال، إذا كنتم تحاولون
إنشاء نظام تعرّف على الكلام،

50
00:03:07,100 --> 00:03:09,000
فمن الصعب إعادة تمثيل الكلام ولكن

51
00:03:09,000 --> 00:03:14,550
إذا قمتم بإدخال مقطع صوتي، فربما
يكون المستوى الأول من الشبكة العصبية قد

52
00:03:14,550 --> 00:03:20,863
تعلم اكتشاف ميزات الأشكال الموجية للصوت
منخفضة المستوى، مثل هل هذه النبرة ترتفع؟

53
00:03:20,863 --> 00:03:21,703
هل هي تنخفض؟

54
00:03:21,703 --> 00:03:26,869
هل هذا ضجيج أبيض أم
صوت شهيق مثل [صوت].

55
00:03:26,869 --> 00:03:27,903
وما حدة الصوت هذه؟

56
00:03:27,903 --> 00:03:31,124
وعندما يتعلق الأمر بهذا، يتم اكتشاف
ميزات الأشكال الموجية منخفضة المستوى مثل هذه.

57
00:03:31,124 --> 00:03:34,233
وبعد ذلك من خلال تجميع
الأشكال الموجية منخفضة المستوى،

58
00:03:34,233 --> 00:03:37,937
ربما تتعلمون اكتشاف
وحدات الصوت الأساسية.

59
00:03:37,937 --> 00:03:40,297
ويطلق على هذا في اللغويات الوحدات الصوتية.

60
00:03:40,297 --> 00:03:45,098
ولكن، على سبيل المثال، في الكلمة cat، الحرف
C هو وحدة صوتية والحرف A هو وحدة صوتية،

61
00:03:45,098 --> 00:03:46,787
والحرف T هو وحدة صوتية أخرى.

62
00:03:46,787 --> 00:03:49,987
ولكن تعلم اكتشاف
وحدات الصوت الأساسية،

63
00:03:49,987 --> 00:03:54,688
ثم تجميع ذلك معًا قد
يؤدي إلى معرفة الكلمات في الصوت.

64
00:03:54,688 --> 00:03:58,270
وبعد ذلك ربما يتم تجميعها معًا،

65
00:03:58,270 --> 00:04:02,912
للتعرف على العبارات
أو الجمل الكاملة.

66
00:04:02,912 --> 00:04:07,572
لذا فإن الشبكة العصبية العميقة
مع الطبقات المخفية المتعددة، ربما يدفع

67
00:04:07,572 --> 00:04:10,477
الطبقات السابقة لتعلم هذه
الميزات البسيطة منخفضة المستوى،

68
00:04:10,477 --> 00:04:15,339
مما يؤدي بعد ذلك إلى دفع الطبقات الأعمق
اللاحقة إلى تجميع الأشياء الأبسط التي اكتشفها معًا

69
00:04:15,339 --> 00:04:19,392
لاكتشاف الأشياء الأكثر تعقيدًا
مثل تمييز الكلمات المحددة أو

70
00:04:19,392 --> 00:04:21,040
حتى العبارات أو الجمل.

71
00:04:21,040 --> 00:04:24,745
النطق من أجل تنفيذ
التعرف على الكلام.

72
00:04:24,745 --> 00:04:30,168
وما نراه هو أنه بينما
تحسبون الطبقات الأخرى، ما يبدو مثل

73
00:04:30,168 --> 00:04:35,673
الوظائف البسيطة نسبيًا للإدخال
مثل أين توجد الحافة،

74
00:04:35,673 --> 00:04:41,046
عندما تتعمقون في الشبكة يمكنكم
القيام بأشياء معقدة على نحو مدهش.

75
00:04:41,046 --> 00:04:44,876
مثل اكتشاف الوجوه أو
اكتشاف الكلمات أو العبارات أو الجمل.

76
00:04:44,876 --> 00:04:48,767
يحب بعض الأشخاص المقارنة
بين الشبكات العصبية العميقة

77
00:04:48,767 --> 00:04:52,656
والعقل البشري، حيث نعتقد،
أو يعتقد علماء الأعصاب،

78
00:04:52,656 --> 00:04:57,162
أن العقل البشري يبدأ أيضًا في اكتشاف
الأشياء البسيطة مثل الحواف فيما تراه العيون،

79
00:04:57,162 --> 00:05:00,370
وبعد ذلك يعمل على تطويرها
لاكتشاف أشياء أكثر تعقيدًا

80
00:05:00,370 --> 00:05:02,440
مثل الوجوه التي ترونها.

81
00:05:02,440 --> 00:05:05,038
أعتقد أن المقارنات بين
التعلم العميق

82
00:05:05,038 --> 00:05:08,276
والعقل البشري أحيانًا
ما تكون خطيرة قليلاً.

83
00:05:08,276 --> 00:05:13,301
ولكن هناك الكثير من الحقيقة في هذا الأمر،
فيما يتعلق بكيفية تفكيرنا في عمل العقل البشري

84
00:05:13,301 --> 00:05:18,102
وأن العقل البشري ربما
يكتشف الأشياء البسيطة مثل الحواف أولاً

85
00:05:18,102 --> 00:05:22,598
وبعد ذلك يعمل على تجميعها معًا لتشكيل
أشياء أكثر تعقيدًا

86
00:05:22,598 --> 00:05:27,430
ويمكن اعتبار هذا بمثابة شكل غير محكم من
أشكال الإلهام لبعض أجزاء التعلم العميق أيضًا.

87
00:05:27,430 --> 00:05:29,850
سنعرف أكثر حول
العقل البشري أو

88
00:05:29,850 --> 00:05:33,065
حول العقل البيولوجي في
فيديو لاحق هذا الأسبوع.

89
00:05:35,534 --> 00:05:40,407
الجزء الآخر المتعلق بالحدس
حول الأسباب التي تجعل الشبكات العميقة تبدو

90
00:05:40,407 --> 00:05:42,756
عاملة بشكل جيد هو ما يلي.

91
00:05:42,756 --> 00:05:47,868
لذا فهذه النتيجة تأتي من نظرية
الدوائر والتي تتعلق بالتفكير

92
00:05:47,868 --> 00:05:50,260
في أنواع الوظائف التي يمكنكم
حسابها من خلال

93
00:05:50,260 --> 00:05:53,760
بوابات AND وبوابات OR
وبوابات NOT، بوابات منطقية أساسية.

94
00:05:53,760 --> 00:05:58,860
لذا بصورة غير رسمية، يتم حساب وظائفها
من خلال شبكة عصبية صغيرة نسبيًا ولكنها عميقة،

95
00:05:58,860 --> 00:06:03,595
وأعني بكلمة صغيرة أن عدد
الوحدات المخفية صغير نسبيًا.

96
00:06:03,595 --> 00:06:07,553
ولكن إذا حاولتم حساب نفس
الوظيفة باستخدام شبكة غير عميقة،

97
00:06:07,553 --> 00:06:09,178
لذلك إذا لم تكن هناك طبقات مخفية كافية،

98
00:06:09,178 --> 00:06:13,296
فحينها قد تتطلبون وحدات إضافية أكثر بصورة كبيرة للحساب.

99
00:06:13,296 --> 00:06:18,109
لذا، دعوني أعطِكم مثالاً واحدًا
وكذلك أوضح هذا بصورة غير رسمية.

100
00:06:18,109 --> 00:06:21,423
ولكن دعونا نفترض أنكم تحاولون
حساب OR الحصري أو

101
00:06:21,423 --> 00:06:23,349
تعادل جميع ميزات الإدخال.

102
00:06:23,349 --> 00:06:26,200
لذا فأنتم تحاولون حساب X1
وXOR وX2 وXOR

103
00:06:26,200 --> 00:06:33,064
وX3 وXOR حتى Xn
إذا كانت لديكم ميزات n أو n X.

104
00:06:33,064 --> 00:06:39,924
لذا إذا قمتم بدمج شجرة XOR كهذه،
لذا فبالنسبة لنا تحسب XOR الخاص بـ X1

105
00:06:39,924 --> 00:06:44,586
وX2، وبعد ذلك تأخذ X3
وX4 وتحسب XOR الخاص بها.

106
00:06:44,586 --> 00:06:48,650
وبصورة تقنية، إذا كنتم تستخدمون
بوابة AND أو NOT، فقد تحتاجون إلى

107
00:06:48,650 --> 00:06:54,196
بعض الطبقات لحساب دالة
XOR بدلاً من طبقة واحدة، ولكن

108
00:06:54,196 --> 00:06:58,791
من خلال دائرة صغيرة نسبيًا،
يمكنكم حساب XOR وهكذا.

109
00:06:58,791 --> 00:07:03,987
وبعد ذلك يمكنكم بناء
شجرة XOR هكذا،

110
00:07:03,987 --> 00:07:12,090
حتى تكون لديكم في النهاية دائرة هنا
وتكون نتائجها، دعونا نطلق عليها Y.

111
00:07:12,090 --> 00:07:15,236
نتائج Y تساوي Y.

112
00:07:15,236 --> 00:07:18,398
OR الحصري
تعادل جميع أجزاء الإدخال هذه.

113
00:07:18,398 --> 00:07:24,790
لذلك لحساب XOR، فإن عمق
الشبكة سيكون في حدود log N.

114
00:07:24,790 --> 00:07:27,410
ستكون لدينا شجرة XOR فحسب.

115
00:07:27,410 --> 00:07:30,836
لذلك، فإن عدد العقد أو
عدد مكونات الدائرة أو

116
00:07:30,836 --> 00:07:33,929
عدد البوابات في هذه الشبكة
ليس كبيرًا للغاية.

117
00:07:33,929 --> 00:07:38,452
فلستم بحاجة إلى الكثير من البوابات
لحساب OR الحصري.

118
00:07:38,452 --> 00:07:43,458
ولكن الآن، إذا لم يكن مسموحًا لكم
باستخدام شبكة عصبية من خلال العديد من

119
00:07:43,458 --> 00:07:48,203
الطبقات المخفية، في هذه الحالة،
سجل الأوامر والطبقات المخفية،

120
00:07:48,203 --> 00:07:53,382
إذا فُرض عليكم حساب هذه الدالة
من خلال طبقة مخفية واحدة،

121
00:07:53,382 --> 00:07:57,912
فإن لديكم كل هذه الأشياء ذاهبة إلى
الوحدات المخفية.

122
00:07:57,912 --> 00:08:02,116
وتكون نتيجة هذه الأشياء Y.

123
00:08:02,116 --> 00:08:07,120
ولحساب دالة XOR هذه،
فإن هذه الطبقة المخفية

124
00:08:07,120 --> 00:08:12,124
ستحتاج إلى أن تكون كبيرة جدًا،
لأنه في الأساس،

125
00:08:12,124 --> 00:08:18,397
تحتاجون إلى تعداد تكوينات
2 إلى N الممكنة باستفاضة.

126
00:08:18,397 --> 00:08:23,139
لذا في ترتيب 2 إلى N،
فإن التكوينات الممكنة

127
00:08:23,139 --> 00:08:27,898
لأجزاء الإدخال التي تؤدي إلى
OR الحصري 1 أو 0.

128
00:08:27,898 --> 00:08:32,213
وبالتالي فإنكم تحتاجون في النهاية إلى طبقة مخفية
كبيرة للغاية

129
00:08:32,213 --> 00:08:33,554
في عدد الأجزاء.

130
00:08:33,554 --> 00:08:38,229
أعتقد من الناحية الفنية، يمكنكم تنفيذ ذلك
من خلال 2 إلى N ناقص 1 من الوحدات المخفية.

131
00:08:38,229 --> 00:08:43,948
ولكن هذه عمليات 2 إلى N القديمة،
والتي ستكون كبيرة جدًا في عدد الأجزاء.

132
00:08:43,948 --> 00:08:49,149
لذا آمل أن يعطي هذا صورة
عن وجود دوال رياضية،

133
00:08:49,149 --> 00:08:55,275
يمكن حسابها بصورة أسهل باستخدام الشبكات
العميقة عما هو الحال مع الشبكات غير العميقة.

134
00:08:55,275 --> 00:09:01,028
بالفعل لقد وجدت بصورة شخصية أن النتيجة
من نظرية الدوائر أقل فائدة

135
00:09:01,028 --> 00:09:05,985
للتمكّن من التخمين، ولكن
هذه واحدة من النتائج التي كثيرًا

136
00:09:05,985 --> 00:09:11,223
ما يستشهد بها الناس عند توضيح قيمة
امتلاك تمثيلات عميقة للغاية.

137
00:09:11,223 --> 00:09:13,600
الآن وبالإضافة إلى هذه الأسباب

138
00:09:13,600 --> 00:09:17,400
لتفضيل الشبكات العصبية العميقة،
لكي أكون أمينًا،

139
00:09:17,400 --> 00:09:22,204
أعتقد أن الأسباب الأخرى التي جعلت مصطلح
التعلم العميق بارزًا، هي العلامة التجارية فحسب.

140
00:09:22,204 --> 00:09:26,776
هذه الأشياء التي اعتدنا أن نطلق عليها الشبكات
العصبية والتي تحتوي على الكثير من الطبقات المخفية،

141
00:09:26,776 --> 00:09:31,198
ولكن العبارة "التعلم العميق"، كما تعلمون،
تعد علامة تجارية رائعة، فهي عميقة للغاية.

142
00:09:31,198 --> 00:09:36,284
لذا أعتقد أنه بمجرد اعتماد المصطلح
فإنه تمت إعادة تسمية الشبكات العصبية أو

143
00:09:36,284 --> 00:09:39,622
تمت إعادة الشبكات العصبية التي تحتوي
على العديد من الطبقات المخفية،

144
00:09:39,622 --> 00:09:42,970
على أمل جذب
خيال الناس أيضًا.

145
00:09:42,970 --> 00:09:47,479
ولكن بصرف النظر عن العلامة التجارية للعلاقات العامة
فإن الشبكات العميقة تعمل بشكل جيد.

146
00:09:47,479 --> 00:09:51,342
أحيانًا يذهب الناس بعيدًا
ويصرون على استخدام الكثير من الطبقات المخفية.

147
00:09:51,342 --> 00:09:55,500
ولكن عندما أبدأ مشكلة جديدة،
فإنني أبدأ كثيرًا

148
00:09:55,500 --> 00:09:58,803
بالانحدار اللوجستي
ثم أحاول بعد ذلك مع

149
00:09:58,803 --> 00:10:01,722
طبقة مخفية واحدة أو طبقتين مخفيتين
وأستخدم ذلك كمعلمة مفرطة.

150
00:10:01,722 --> 00:10:05,731
استخدام ذلك كمعلمة أو معلمة مفرطة
تعملون على مواءمتها، من أجل محاولة إيجاد

151
00:10:05,731 --> 00:10:07,935
العمق المناسب لشبكتكم العصبية.

152
00:10:07,935 --> 00:10:12,800
ولكن على مدار السنوات العديدة الأخيرة،
كان هناك ميل تجاه الأشخاص الذين يجدون أنه

153
00:10:12,800 --> 00:10:17,590
بالنسبة لبعض التطبيقات، فإن
الشبكات العصبية العميقة جدًا هنا التي قد تحتوي على

154
00:10:17,590 --> 00:10:22,264
الكثير من الطبقات،
قد تكون أحيانًا النموذج الأفضل لمشكلة.

155
00:10:22,264 --> 00:10:27,605
إذًا هذا كل شيء بالنسبة للأسباب
التي تجعل الفرضيات تبدو وأنها تعمل جيدًا للتعلم العميق.

156
00:10:27,605 --> 00:10:31,411
دعونا الآن نلقِ نظرة على آليات
كيفية تنفيذ

157
00:10:31,411 --> 00:10:33,769
ليس الانتشار الأمامي فحسب ولكن الانتشار الخلفي أيضًا.