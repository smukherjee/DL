Son videoda, geri yayılım denklemlerini gördünüz. Bu videoda hesaplama grafiği kullanarak bu denklemlerin nasıl elde edildiğini inceleyelim. Bu videoyu izlemek tamamen isteğinize bağlı. Yani, izleseniz de izlemeseniz de ödevleri eksiksiz yapabileceksiniz. Hatırlarsınız ki, lojistik regresyondan bahsederken önce z'yi, sonrasında ise A'yı ve kayıp fonksiyonunu<br /> hesapladığımız ileri yayılımdan bahsetmiştik. Sonrasında ise türev alarak öncelikle da, sonrasında dz, ve son olarak da dw ve db'yi hesapladığımız<br /> geri yayılımdan bahsetmiştik. Kayıp(loss) fonksiyonunun tanımı şuydu: L(a,y) = -ylog(a) - (1-y)log(1-a) L(a,y) = -ylog(a) - (1-y)log(1-a) Eğer calculusle aranız iyiyse ve bunun a'ya göre türevini alırsanız da'i elde edersiniz. Yani da buna eşit. Ve eğer calculus kısmını tam olarak kavrarsak -(y/a) + (1-y)/(1-a) Türevin sonucu bu. Bir adım daha geri gidip dz'yi hesaplarsanız dZ'nin a - y'ye eşit olduğunu görürsünüz. Calculusteki zincir kuralına göre dz da . g'(z)'ye eşittir. g(z) = sigmoid(z), burada lojistik regresyondaki sonuç ünitemizin <br />aktivasyon fonksiyonudur Unutmayalım ki bu hala x1, x2, <br />x3'ün olduğu lojistik regresyon bir sigmoid ünitesi ve bu bize a'yı yani y-şapka'yı verir. Yani burada aktivasyon fonksiyonu bir sigmoid fonksiyondu. Ek olarak, calculusü gerçekten iyi olanlar için söylüyorum, bunun sebebi a'nın z'nin sigmoidine eşit olması ve bu yüzden L'nin Z'ye göre kısmi türevi L'nin a'ya göre kısmi türevi <br />çarpı a'nın z'ye göre türevine eşit Burada a = sigmoid(z) ve bu (d/dz)(g(z))'ye eşit ve o da g'nin türevi z'ye [ g'(z) ] eşit. Bu yüzden kodumuzda 'dz' olarak adlandırdığımız bu değer kodumuzda da olarak adlandırdığımız<br /> 'da' ile g'(z)'nin çarpımına eşit. Yani bu sadece şu. Aldığımız son türev calculus ile, özellikle zincir kuralıyla, aranız iyiyse mantıklı gelecektir. Fakat aranız iyi değilse, endişelenmeyin. Gerektiğinde bunları açıklamaya çalışacağım. Lojistik regresyon için dz'yi hesapladıktan sonra tek öğretme örneğimiz olduğunda <br />dz çarpı x şeklinde hesaplayacağımız dw'yi ve direkt dz diyeceğimiz db'yi hesaplayacağız. İşte bu, lojistik regresyon. Bir nöral ağ için geri yayılım hesaplarken buna çok benzer işlemler yapacağız fakat bunu iki kere yapacağız, çünkü x bu sefer direkt sonuç ünitesine değil önce saklı katmana, sonrasında sonuç ünitesine gidecek. Bu yüzden, işlemlerimiz burada olduğu gibi tek aşama değil, iki katmanlı bir nöral ağda iki aşamada olacak. Bu iki katmanlı nöral ağda: girdi katmanı, saklı katman ve çıktı katmanı var. İşlem adımlarını hatırlayın. Öncelikle bu denklemi kullanarak z1'i hesaplıyoruz, sonrasında a1'i, ardından da z2'yi hesaplıyoruz. Dikkat edin, z2 aynı zamanda w2 ve b2 parametrelerine de bağlı. z2'yi hesapladıktan sonra z2'yi kullanarak a2'yi hesaplıyoruz ve bu da bize kayıp fonksiyonumuzu veriyor. Geri yayılımın yaptığı şey geri giderek önce da2'yi sonrasında dz2'yi daha sonra dw2 ve db2'yi, geriye gitmeye devam ederek da1'i dz1'i ve daha geridekileri hesaplamak. Girdi x'e göre türev almamıza gerek yok, çünkü gözetimli öğrenmede x bir sabit. Bu yüzden x'i optimize etmek istemiyoruz <br />dolayısıyla da türev almamıza gerek yok En azından gözetimli öğrenme için. da2'yı ayriyeten hesaplamayı atlayacağım. Eğer isterseniz öncelikle da2'yi hesaplayıp sonrasında onu kullanarak dz2'yi hesaplayabilirsiniz. Fakat uygulamada, bu iki adımı önceden <br />gördüğümüz gibi dz2 = a - y diyerek tek adımda birleştirebilirsiniz. dw2 ve db2'yi buraya yazacağım. dw2 eşittir dz2 çarpı a1'in devriği(transpozu) ve db2 eşittir dz2. Bu aşama lojistik regresyonda dw eşittir dz çarpı x dediğimiz aşamaya çok benziyor. Fakat burada x'in rolünü a1 üstleniyor. Devriğini almamızın sebebi de W ile w'lar arasında geçiş yaparken de devriğini alıyor olmamız. Çünkü tek çıktılı lojistik regresyonda W bir satır vektörü. dw2 da öyle fakat W sütun vektörü olduğu için a1'in devriğini alıyoruz, lojistik regresyon yaparken x'te böyle bir işleme ihtiyaç duymamıştık. Geri yayılımın yarısını tamamladık. Tekrar belirteyim, eğer isterseniz da1'i hesaplayabilirsiniz. Fakat uygulamada genelde da1'ın ve dz1'in hesabı tek aşamada birleştirilir ve dz1 eşittir w2'nin transpozu çarpı dz2 çarpı (element bazında) g1'(z1) Boyutları kontrol edelim. Eğer böyle görünen bir nöral ağınız varsa y-şapka çıktısı versin. nx = n0 tane girdi özniteliğiniz, n1 saklı üniteniz ve n2 varsa. bu durumda n2 sadece bir çıktı ünitesi. W2 matrisi (n2, n1) boyutundadır. z2 ve dolayısıyla dz2 (n2, 1) boyutunda olacaktır. İkili sınıflandırma yaparken bu (1,1) olacaktır. z1 ile dz1 ise (n1, 1) boyutunda olacaklardır. foo ve dfoo değişkenleri her zaman aynı boyutta olacaklardır. Bu yüzden w ve dw her zaman aynı boyuttadır, aynı şekilde b ve db, z ve dz, vb. Tüm boyutların eşit olduğundan emin olmak için dz1 = w2.T * dz2 *(element bazında çarpım) g1'(z1) dz1 = w2.T * dz2 *(element bazında çarpım) g1'(z1) Yukarıdaki boyutları kullanınca dz1'in boyutu (n1, 1) eşittir w2'nin transpozunun boyutu (n1, n2) çarpı dz2'nin boyutu (n2, 1) çarpı bu fonksiyondur. Bu fonksiyonun sonucunun boyutu z1'in boyutuyla aynıdır, yani bu da (n1, 1) boyutundadır. Boyutların birbiriyle ilişkisi mantıklı, değil mi? (n1,1) boyutlu vektör (n1,n2) boyutlu matris ve (n2,1) boyutlu matrisin çarpımından elde edilebilir çünkü bu ikisinin çarpımı (n1,1) boyutlu matrisi verir. Yani dz1, iki (n1,1) boyutunda iki vektörün element bazında çarpımına eşit oluyor ve boyutlar sonucun boyutuyla eşleşmiş oluyor. Size geri yayılım hesabı için bir ipucu vereyim, eğer matrislerinizin boyutlarının birbirleriyle eşleştiğinden emin olursanız, yani matrislerinizin W1,W2,Z1, Z2,A1,A2 gibi ve daha farklı matrislerin boyutlarının ne olduğunu düşünürseniz sadece matrislerin boyutunun operasyonlar sırasında eşleştiğinden emin olun, bazen bu yöntem sayesinde geri yayılımda yapabileceğiniz çoğu hatanın önüne geçmiş olursunuz. Tamamdır, bu bize dz1'ı veriyor ve sadece dw1'ı ve db1'ı toparlamak amacıyla, onları da buraya yazayım çünkü burada yazabileceğim alan azaldı. dw1 ve db1 size birazdan vereceğim formüller ile hesaplanıyor: dw1 dz1 ile x'in devriğinin çarpımına ve db1 ise dz1'e eşit oluyor. Az önce açıkladığım eşitlikler ile bu eşitlikler arasında bir benzerlik farkedebilirsiniz ve bu rastlantı değil çünkü X a0 rolü oynuyor, yani a0'nun devriği X'in devriğine eşit. Bu eşitlikler aslında birbirlerine baya benzer. Açıkladıklarım bize geri yayılımın nasıl bulunduğuna dair bir fikir veriyor. Elimizde 6 önemli denklem var: dz2,dw2, db2,dz1,dw1 ve db1. Bu 6 eşitliği diğer slayta kopyalayayım. İşte buradalar: Buraya kadar, geri yayılım denklemini eğer bir seferde tek örnek üzerinden eğittiysek çıkarttık. Ama bariz nedenlerden dolayı bir <br /> seferde tek örnek üzerinden eğitmek yerine, farklı farklı eğitim örnek vektörleri kullanarak eğitmek istiyoruz. Hatırladığınız üzere, ileri yayılım için tek seferde bir örnek üzerinden eğittiğimizde Bunun gibi eşitliklerimiz vardı, a1 = g1(z1) gibi. Vektörleştirmek için, z'leri alıp kümeliyoruz, bu şekilde sütunlarda z1m'e kadar istifliyoruz ve buna büyük Z diyoruz. Sonrasında z'leri sütunlarda kümeleyerek ve bunun büyük harfli versiyonunu tanımlayarak denklemi Z1 eşittir W1 çarpı X artı B'ye ve A1 eşittir g1(Z1)'e indirgemiş oluyoruz, değil mi? Bu kursta gösterimlere özellikle özen gösteriyoruz çünkü örnekleri istifleme işlemi <br />bunların hepsinin çalışmasını sağlıyor. Matematiğin üzerinden dikkatlice geçerseniz, göreceksiniz ki aynı işlemler geri yayılım için de kullanılabiliyor, <br />vektörize edilmiş denklemler ise şöyle: öncelikle farklı örnekler için hesaplanan dz'leri bir matrisin farklı sütunlarına denk gelecek <br />şekilde kümelersek ve bunu diğerleri için de yaparsak burada gördüğünüz vektörize edilmiş <br /> uygulama ve bu da dW2'yi hesaplamanız için gereken denklem. Ekstra olarak bir m'e bölme işlemi var çünkü maliyet fonksiyonumuz 1'den m'e kadar olan kayıp fonksiyonlarının toplamı. Türevini alırken ise yine (1/m) çarpanını görüyoruz, lojistik regresyonda ağırlık güncellemelerini gördüğümüzdeki gibi. Bu da db2 için güncelleme fonksiyonumuz. DZ'lerin toplamı ve 1/m'ye bölümü sonrasında DZ1 hesaplanır. Önceden de söylediğim gibi bu element bazında bir çarpım işlemi önceki slaytta da gördüğümüz gibi bu (n1, 1) boyutunda bir vektör. Şimdi bu (n1, m) boyutlarında bir matris ve bu ikisi de (n1, m) boyutunda matrisler. Bu yüzden asterisk burada element bazında çarpımı temsil ediyor. Ve son olarak kalan iki güncelleme, belki de o kadar şaşırtıcı görünmemeliler. Umarım bu size geri yayılım algoritmasının <br />çıkarılması konusunda biraz sezi kazandırmıştır. Bence, makine öğrenmesi konusunda geri yayılım algoritmasının çıkartılması benim gördüğüm en karışık matematik işlemlerinden bir tanesi ve sıfırdan oluşturmak hem lineer cebir bilmeyi hem de matrislerin türevini almayı iyi bilmeyi gerektiriyor. Eğer matris calculusunde gerçekten iyiyseniz, bu süreci takip ederek, algoritmayı kendiniz çıkartabilirsiniz ama bence derin öğrenme üzerine çalışan ve bu videoda gördüğünüz seviyede türev görmüş birçok insan şimdiden bu algoritmayı etkili bir şekilde uygulayabiliyorlar. Calculus'ta uzman iseniz, her şeyi sıfırdan çıkartıp çıkartamayacağınıza bakın. Bu matematiğin en zor parçalarından biri. Makine öğrenmesinde gördüğüm en zor türevlerden biri. Sıfırdan çıkartamasanız da, çıkarttıklarımızı uygularsanız çalışacaktır ve bence şu an gerektiğinde <br />değişiklik yapacak kadar seziye sahipsiniz. Nöral ağınızı oluşturmaya başlamadan önce sizlerle paylaşmak istediğim son bir detay var, bu da nöral ağınızın ilk ağırlıklarını nasıl atayacağınız. Parametrelerinizin ilk değerlerini sıfır yerine rastgele şekilde atamak nöral ağınızı eğitmek için çok önemli. Sonraki videoda nedenini göreceksiniz.