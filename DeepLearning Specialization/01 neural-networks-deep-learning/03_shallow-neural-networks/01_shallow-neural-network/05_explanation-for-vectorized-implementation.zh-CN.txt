在上节课里 我们看到通过将你的训练样本水平地堆叠进矩阵x 能为神经网络中的传播过程推导出一个向量化的实现 接下来我们将给出更多的解释说明 确保我们所写下的公式是对多个样本向量化的正确实现 让我们看一下对于几个样本(前向)传播计算过程的一部分 对于第一个训练样本 你最后会计算得到 (w1)x1加上b1 然后对于第二个训练样本 最终计算得到(w1)x2加上b1 接下来对于第三个训练样本 最终计算得到(w1)x3加上b1 为了简要说明问题，接下来在幻灯片上我将省略b 也就是说 通过令b等于0来做一点简化 但只需对我们将要给出的讨论做一些小的修改 就能在b是非0的情况下适用 这些改动只是为了简化在幻灯片上的描述 那么 w1会是一个矩阵 所以在这个矩阵中会有一定数量的行 观察这个对于x1的运算 你会得到的是 w1乘以x1得到的某个列向量 就像画出来的这样 相似地 如果你看x2这个向量 你会有w1乘以 x2得到另一个列向量 也就是得到这个z12 最后 如果你看x3 你会得到w1乘以x3 得到第三个列向量 也就是这个z13 现在 如果你将训练集记成大写的X 它是通过将所有的训练样本堆叠在一起得到的 矩阵X的构成是在向量x1之上 水平堆叠x2然后再水平堆叠x3 这是只有三个训练样本的情况 如果有更多的样本 会像这样一直水平堆叠下去 将w与这个矩阵x相乘 如果考虑下矩阵乘法的工作原理 你最终得到的第一列会是 这些我画在这里的紫色的数值 第二列会是这四个数值 第三列会是这些橙色的数值 它们最终会是这样 当然这等于是表示为列向量的z11 后面跟着表示为列向量的z12 后面跟着同样被表示为列向量z13 这是当你有三个训练样本的情况 当有更多的样本时矩阵中会有更多的列 因此 这个就是我们的矩阵大写的Z1 我希望这解释了为什么我们先前使用 w1乘以xi等于z1i 这是当我们只考虑一个训练样本的时候 当使用不同训练样本 并把它们堆叠到不同列的时候 对应的结果也变成了 向量z的按列堆叠 我将不再展示 但是你可以利用Python广播机制自己验证 如果把b加进来 这些b的数值仍然是正确的 实际上会发生的是通过Python的广播机制 bi被分别地加到这个矩阵的每一列上 所以在这个幻灯片上 我只证明了Z1等于 w1X加b1 前面幻灯片中四步中的第一步 是一个正确的向量化 但实际上相似的分析方法可以让你 证明其他步骤同样正确 使用非常相似的逻辑
如果你将输入按列进行堆叠 经过计算后 你会得到对应的同样按列堆叠的结果 最后总结一下我们在这个视频中所谈到的内容 如果这是你的神经网络 如果你想实现前向传播 那么你将要做的事情是 从i等于1到m 每次输入一个训练样本 然后我们像这样把训练样本按列堆叠 对于这些数值 z1 a1 z2 a2 像下面这样进行对应列的堆叠 这是一个对于A1的例子 但是对于Z1 A1 Z2和A2都是一样的 我们之前幻灯片所展示的是 这一行向量化操作 让你能同时处理m个样本 用类似的方法 你会发现所有其他行 都是所有这四行代码的正确向量化形式 再提醒一下 因为X也等于A0 因为输入的特征向量x等于a0 所以xi等于a0i 这些公式实际上具有一定的对称性 其中第一个公式也可以被写作 Z1等于w1A0加上b1 可以看到这一对公式和这一对公式 实际上看起来十分相似 只有上标之间相差1 这实际上说明了一个神经网络中的不同层 大致上都在做着相同的事情
或者说一遍又一遍地做着相同的计算 这里我们有两层神经网络 下一周的课程中我们会接触更深的神经网络 你会看到即使是更深的神经网络本质上来说 也只是在进行这两个步骤
仅仅比在这里看到的进行了更多次 这就是基于多个训练样本向量化你的神经网络的方法 到目前为止 我们的神经网络中一直在使用sigmoid函数 这实际上并不是最好的选择 在下个视频中 让我们进一步研究 你该如何使用不同的 所谓的激活函数 sigmoid函数只是其中的一个可能选择