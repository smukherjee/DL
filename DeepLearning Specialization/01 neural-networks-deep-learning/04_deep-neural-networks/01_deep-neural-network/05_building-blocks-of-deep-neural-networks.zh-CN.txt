从本周以及 之前几周的视频里， 你已经了解 前向传播和后向传播的基本框架， 以及构建深度神经网络须知的要点。 我们来看怎样将这些要点合并起来构造深度神经网络。 这是一个有一些层数的神经网络。 让我们来选一层，并只关注该层中的计算。 对于L层，你有参数WL和BL。 对前向传播而言， 你将从前一层输入激励向量AL-1 然后输出AL。 我之前是这样计算的：ZL=WLxAL-1+BL, 和AL=G[ZL]，对吧？ 这就是怎样从输入AL-1计算输出AL. 在后面的计算中你会发现， 将ZL的值缓存是非常有用的。 让我也将缓存也包含进来，因为 把ZL的存起来对反向传播很有用 把ZL的存起来对反向传播很有用 对于反向传播 对于反向传播 我们只关注L层的计算， 你将实现一个函数，它的输入为 DAL和输出DAL-1。 为了细节完整，我们说， 输入实际上包括L层的DA 以及缓存 (所以你后面可以使用已经计算好的ZL值)， 接着输出DAL-1， 你也可以输出梯度，如果你想 应用梯度下降学习算法。 这就是基本框架：怎么实现前向传播 (或称前向函数)以及实现反向传播 (我们应该称为反向函数)。 总结一下，在L层， 你要有前向传播或前向函数。 输入AL-1和输出AL， 以及为了计算， 需要使用WL和BL。以及， 我要设置缓存为0，和反向函数 (在反向传播被使用) 将是另外一个函数，输出为 DA[L]和输出DA[L-1]。 这就告诉你，知道激励向量的导数 即DA[L],其他导数是怎样的？ 我怎么知道A[L-1]的变化？ 这要计算上一层激励向量的导数。 这个方框里，你要使用WL和BL， 你会发现沿着这个方向， 你要计算DZL。 接着这个方框里， 后向函数也输出DWL和DBL， 但是我现在有时用红色箭头表示反向， 要是乐意，你可以将箭头填成红色。 如何你掌握了这两个函数， 那么神经网络的基本计算将是如下这样的： 你将用特征向量A0 输入进来，接着算出第一层的激励向量， 我们称之A1. 算的时候，你需要W1和B1， 同时也要缓存Z1. 好，做完这些之后， 我们把它们输入第二层， 接着使用W2和B2， 计算第二层的激励向量A2， 不断重复，直到最终， 输出A_capsule_L (等于Y_hat). 沿着这个方向，我们缓存了所有的Z值， 这就是前向传播步。 现在来看后向传播步， 我们将要做的是一系列的后向 迭代，从中后向传播和计算梯度。 我们这么输入DA[L], 接着这个方框里算出DA[L-1] 不断重复，直到得出DA2，DA1. 你还会得到DA0， 但是这个输入特征向量的导数没有什么用， 至少对训练有监督的神经网络的权重是这样的， 所以你可以就此停止。 沿着这个方向，后向传播也输出DWL，DBL。 使用参数WL和BL， 这将输出DW3， DB3等等。 最后你计算出你需要所有的导数。 也许这个结构还要填入更多参数， 这个方框还需要使用参数WL， BL。最后等会我们会发现这些方框里面， 我最后还要计算DZ。 训练神经网络的一步迭代从A0 也就是X开始， 完成接下来的前向传播， 计算Y^并用之计算这个， 接着后向传播，这样做[后向]。 好了，你得到了所有导数项， 接着你可以用W减去学习速率乘以每层的Dw来更新W， B也是类似的。 现在，计算了后向传播和其所有的导数。 这就是神经网络的梯度下降的一步迭代。 在继续之前，还有一个细节。 从概念上讲，将这里的缓存理解为存储 反向函数Z的值是很有用的， 但是当你使用的时候， 你在之前的练习中会发现，当你使用的时候， 你发现使用缓存也许是 获得参数W1，B1 在反向传播函数的数值一种方便的方法。 在之前练习中， 你实际上存储了缓存Z， 以及W和B， 一样也存储Z2，W2，B2. 但是从操作性来说， 我就发现当你计算反向传播时就复制参数到 后面你需要使用的地方是很方便的。 这只是一些实现细节，你一会做练习时会用到它们。 你现在知道了实现深度神经网络的基本框架。 每一层中，有前向传播 和反向传播， 还有在他们之间传递值的缓存。 下一个视频中， 我将讨论怎么实际地使用这些基本框架。 让我们来看下一个视频。
GTC字幕组翻译