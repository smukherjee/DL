歡迎回來, 向量化基本上是 一種去掉明顯迴圈的技術在您的程式中 在深度學習時代, 安全的深度學習實踐時 您通常發現您自己訓練到一個相對大的資料集 因為這是使用深度學習演算法發光發亮的時候 所以很重要的是您的程式很快速, 要不然 如果您跑在一個大的資料集 您的程式會花很久的時間來跑而您 會等很長的時間來獲得結果 所以在深度學習時代 我想能夠進行向量化變成一個重要的技術 讓我們先從一個例子開始 所以, 什麼是向量化 在羅吉斯迴歸分析您需要計算 z 等於 w 轉置 x 加 b 而 w 是這個欄向量而 x 是這個向量 也許這是一個很大的向量如果您有很多特徵 所以, w 跟 x 都是 R n 或者 R nx 維度向量 所以要計算 w轉置x 如果您使用非向量化建置 您會用像類似這樣, z 等於 0 然後 對於 i in range of nx 所以, 對於 i 等於 1, 2 到 nx z 加等於 w i 乘 x i 然後也許您做 z 加等於 b 在最後 所以這是非向量化的做法 然後您會發現這樣做會很慢 相對的, 向量化建置會只直接計算 w轉置x 在 Python 或者說 numpy 您用的指令是 z 等於 np.dot(w, x), 所以這是計算 w轉置x 而您可以直接加 b 您會發現這快多了 讓我們實際上演練一下 這個是我的 jupyter notebook, 我將使用他來寫 python 程式 首先我們先導入 numpy 函式庫 np 舉個例子 我可以建立一個矩陣 A 像這樣 讓我們印 a 現在有了這一段程式 如果我按 shift enter 它會執行這對程式 所以它會建立一個矩陣 a 然後印出 現在讓我們做向量化的展示 我將導入 time 函式庫 因為我們即將使用到 為了要算不同的運算的時間 我們先建立一個矩陣 a 等於 random.rand 這會用亂數建立一個百萬維度的矩陣 b = np.random.rand 另一個百萬維度的矩陣 現在 tic = time.time(), 所以這是用來測量現在的時刻 c = np.dot(a, b) toc = time.time() 然後印出 這個是向量化版本 是向量化版本 我們將它印出來 讓我看這是後面的時間 所以是 toc - tic 乘 1000 所以我們可以表示成 毫秒 所以 ms 是毫秒 我將按 shift Enter 所以這個程式花了 3 毫秒, 也許這次 1.5 毫秒 也許 1.5 或者 3.5 毫秒 它每次跑都差一點點 但似乎也許平均大約是 1.5 毫秒 也許是 2 毫秒當我跑這次 好吧。好吧。 讓我們繼續加入一些程式 讓我們建置一個非向量化的版本 讓我用, c = 0 然後 tic = time.time() 現在讓我們建立一個迴圈 for i in range of 一百萬 我要把零的數目弄對 c += a i 乘 b i, 然後 toc = time.time() 最後印出 for loop 總共花費時間是 1000 乘 toc - tic + "ms" 讓自己知道我們用毫秒 讓我們再做一件事 讓我們印出 計算出來 c 的值 確定它們都是一致的在兩種情況下 我將按 shift enter 跑這段程式跟檢查結果 在兩種情況下, 向量化版本 跟非向量化版本計算出同一個值 您看到 250286.99 等等 向量化版本花了 1.5 毫秒 而明顯的迴圈跟非向量化版本花了約 400 將近 500 毫秒 而非向量化版本花了約 300 倍 長比起向量化版本 在這個例子您看到如果您只記得向量化您的程式 您的程式跑大約 300 倍快速 讓我們再跑一次 再跑一次 向量化版本 1.5 毫秒而迴圈 花了 481 毫秒, 再次 約 300 倍慢使用明顯的迴圈 如果整個慢了下來 兩個程式的差別一個會是跑一分鐘 一個則會跑五個小時 而當您要建置深度學習算法時 您真的可以比較快得到結果 如果您向量化您的程式會快很多 您們一些人也許聽過很多 可擴展的深度學習建置在 GPU 或圖形處理單元 但所有我做的這些展示在 jupyter notebook 都實際上是在 CPU 上 實際上 GPU 跟 CPU 都有平行指令 有時候稱為 SIMD 指令 是 單一指令多資料 (Single Instruction Multiple Data) 的簡寫 但基本上這個意思是 如果您使用內建函數像這樣 np.function 或者其他函數不需要您明顯的建立一個迴圈 它使得 Python numpy 利用平行處理使得計算快很多 而這在 CPU 的計算跟 GPU 的計算都是相同的 就只是 GPU 是顯著擅長在 SIMD 計算但 CPU 實際上做得也不差 也許不像 GPU 那麼好 您見過了向量化如何顯著的加快您的程式 重要的法則是記得當可能時 盡量避免明顯的迴圈 讓我們繼續到下一段影片看更多的 向量化例子同時開始做向量化羅吉斯迴歸分析