여러분이 신경망을 변경할때는, weight를 임의로 초기화하는 것이 중요합니다. 로지스틱 회귀분석에서는 
weight를 0으로 초기화하는 것이 괜찮았습니다. 하지만 신경망에서는 모든 weight를 0으로하고 기울기 강하를 적용하면
작동하지 않을 것입니다. 왜 그런 것인지 보겠습니다. 여기는 2개의 입력값이 있는데요 n0=2, 와 2개의 숨겨진 유닛 n1=2입니다. 숨겨진 레이어와 연관된 매트릭스는 w1이고 이것은 2 x 2가 됩니다. 이제 이것은 모두 0으로 초기화한다고 해보겠습니다. 
0 0 0 0으로 말이죠. 2 x 2 매트릭스에서요. 그리고 B1도 마찬가지로 0 0이라고 하겠습니다. 알고보면 bias term 은 0으로 초기화하여도
무방합니다. 하지만 w를 모두 0으로 초기화하는 것이 문제가 됩니다. 여기 초기화의 문제는 어떤 예제의 경우라고 하면, 
a11, 그리고 a12 가 동일하겠죠. 맞죠? 여기 activation과 여기 activation이 동일할 것입니다. 그 이유는 여기 숨겨진 유닛이 모두
똑같은 함수를 산출하기 때문입니다. 그리고 후 방향전파을 산출하면, dz11과 dz12가 대칭에 의해서 말이죠. 맞죠? 여기 2개의 숨겨진 유닛은 모두 
같은 방식으로 초기화될 것입니다. 엄밀히 이야기하면, 제가 말하는 것은 outgoing weight가 동일하다는 가정하에 이루어집니다. 즉 w2 가 0 0 인 것이죠. 하지만 신경망을 이런식으로 초기화하는 경우, 여기 숨겨진 유닛그리고 여기 숨겨진 유닛은 
완전히 동일합니다. 가끔은 이것들이 완전히 대칭을 이룬다고 합니다. 그 뜻은 이것들이 동일한 함수를 산출한다는 뜻입니다. 그럼 일종의 유도 증명으로 보면 iteration마다 이 2개의 숨겨진 유닛은 계속해서 똑같은 함수를 산출합니다. dw는 이렇게 보이는 매트릭스가 될 텐데요, 줄마다 똑같은 값을 갖습니다. 그러면 weight 업데이트를 진행합니다. 여러분이 weight 업데이트를 진행하면, 
w1은 w1 - 알파 곱하기 dw로 업데이트 됩니다. 그러면 w1은 반복업무마다, 첫번째 줄이 두번째 줄가 일치합니다. 그러면 유도증명을 할 수 있는데요, 모든 w값을 0으로 한다면, 2개의 숨겨진 유닛 모두 
똑같은 함수를 산출하고, 2개의 hidden unit이 
결과값 유닛에 똑같은 영향을 주기 때문에, iteration 후에 똑같은 statement가 참일 것입니다. 이 2개의 숨겨진 유닛이 대칭이라는 것 말이죠. 그러므로 유도증명을 통해 2번 또는
3번 등등의 반복 ieteration을 통해 여러분이 신경망을 얼마나 오래 트레이닝 해도 2개 숨겨진 유닛 모두
완전히 똑같은 함수를 산출합니다. 그럼 이런 경우, 1개 이상의 숨겨진 유닛을 
갖는 것이 무의미해집니다. 서로 똑같은 것을 산출하기 때문입니다. 신경망이 크면 클수록 또한 
예를 들어, 3개의 특성 그리고 많은 수의 숨겨진 유닛이 있다고 해보겠습니다. 이런 신경망에서도 비슷하게 적용되는데요, 모서리를 그리구요, 
모든 weight를 0으로 만들면 모든 숨겨진 유닛이 대칭이 됩니다. 그렇기 때문에 기울기 강하가 얼마나 길게 하더라도 모두 똑같은 함수를 산출합니다. 그러면 다른 숨겨진 유닛이 다른 함수를 계산하고 싶기 때문에 
도움이 되질 않겠죠. 이겻에 대한 해결책은 
파라미터를 임의로 초기화하는 것입니다. 이렇게 할 수 있습니다. w1=np.random.randn으로 설정합니다. 이렇게 하면 a gaussian
random variable이 생성되는데요, 그리고 주로 이 값을 아주 작은 숫자로 곱합니다.
0.01이 예가 되겠습니다. 그렇게하여 아주 작은 값으로 
초기화시키는 것이죠. 그리고 b는 대칭 문제가 없습니다. symmetry breaking 문제라고 불리는 것이 
문제되지 않습니다. 그렇기 때문에 b는 0으로 초기화해도 됩니다. w가 임의로 초기화된 이상, 숨겨진 유닛들이 다른 값을 산출하도록 만들어 줍니다. 그러면 이제 더 이산 symmetry breaking 문제가 
없게 됩니다. w2도 비슷하게 임의로 초기화를 진행할 것입니다. b2는 마찬가지로 0으로 초기화시킵니다. 그러면 여러분은 의문을 가질 수 있는데요, 
여기 상수는 어디에서 왔으며 왜 0.01일까? 말이죠. 왜 100 또는 1000은 안될까? 알고보면 저희는 보통 weight를 초기화하는데 임의의 작은 값을
선호합니다. 그 이유는 tan h나 
시그모이드 activation 함수 또는 다른 시그모이드 함수가 결과값 층에 있는 경우 weight가 만약에 너무 크면 activation 값을 산출할 때, z[1]=w1 x + b라는 것을 기억하실 것입니다. 그러면 a1은 z1에 적용된
activation 함수입니다. 그러므로 만약 w가 매우 크면
z도, 또는 적어도 일부분의z 값은 매우 크거나 
매우 작을 것입니다. 이런 경우, 여기 두꺼운 부분에 남을 확률이 높습니다.
tan h 함수에서 말이죠. 또는 시그모이드 함수에서도 그렇습니다.
기울기의 값이 아주 작은 부분입니다. 이 뜻은, 기울기 강하가 매우 느릴 것이라는 것을 
뜻합니다. 그리하려 러닝도 매우 느린 것이죠. 복습하자면, 만약 w의 값이 매우 큰 경우, 트레이닝 초기에도 z의 값이 클 확률이 높습니다 그러면 tan h나 시그모이드 함수가 포화되게 하죠. 그리하여 러닝의 속도를 늦추 구요. 만약 시그모이드난 tan h 함수가 신경망에서 없는 경우, 
이것은 문제가 덜 됩니다. 하지만 이진분류법의 경우, 
결과값 유닛이 시그모이드 함수인 경우, 초기 parameter가 너무 크지 않는 게 좋습니다. 그렇기 때문에 처음에 0.01을 곱하는 것이 
합리적인 방법입니다. 또는 아무 작은 숫자를 말이죠. w2도 마찬가지입니다. 맞죠? 이것은 random.ranom이 되고요 이것은 1 x 2 예제이겠죠. 곱하기 0.01 입니다. 여기 s가 빠졌군요 마지막으로, 상수의 값이 0.01보다 
더 좋은 값이 있을 수 있는데요, 만약 한개의 숨겨진 유닛에서 신경망을 트레이닝 하는 경우, 꽤 얕은 신경망이고 숨겨진 유닛의 수가 많지 않기 때문에, 0.01로 설정하면 적당히 작동할 것입니다. 하지만 아주 깊은 심층신경망의 경우, 0.01 과는 다른 상수를 고르고 싶을 것입니다. 다음주에는 0.01과는 다른 상수의 값을 선택하는 것에 대한 내용을 
다루겠습니다. 어떻던 간에, 꽤 작은 숫자일 것입니다. 이번주 비디오는 이것이 전부인데요, 이제 여러분은 숨겨진 유닛의 신경망을 설정하실 줄
알게 되었습니다. 그리고 파라미터를 초기화하고 
전 방향전파을 이용해서 예측을 하고, dervative를 산출하는 방법과, 기울기 강하를
도입하는 방법을 배웠습니다. 후 방향전파을 이용해서 말이죠. 이제 여러분은 퀴즈를 푸실 수 있을 것입니다. 이번 주 연습 문제도 말이죠. 행운을 빌겠습니다. 연습문제를 즐기시길 바라며, 다음주 4주차에서 보길 희망합니다.