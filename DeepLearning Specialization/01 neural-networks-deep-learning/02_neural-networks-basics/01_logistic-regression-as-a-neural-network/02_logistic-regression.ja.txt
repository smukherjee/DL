この動画では、ロジスティック回帰について
学びます。 これは、教師あり学習で出力のラベルyが 0か1のどちらかのとき、
つまり二項分類の問題で 使われるアルゴリズムです。 猫の画像かそうでないかを判断したい
画像に対応する 特徴ベクトルxの入力があるとしましょう。 予測を出力できる
アルゴリズムが欲しいですね。 そのyの予測のことを、 y hatと表します。 より形式的に言うと、y hatとは、
特徴ベクトルxが与えられたときに yが1である可能性であってほしいです。 前の動画で見たように xが画像だとすると、 その画像が猫である可能性を y hatに教えてもらいたいわけです。 前の動画で言ったように、 xはnx次元のベクトルです。 すると、ロジスティック回帰のパラメータは xと同じようにnx次元のベクトルであるwと、 ただの実数であるbということになります。 入力xとパラメータw,bがあるとき、 どうやってy hatを求めるのでしょうか。 うまくいかないですが
やってみることができることとして、 y hatを
wの転置かけるxプラスbにしてみること、 つまり入力xの線形の
関数にしてみることです。 実際、線形回帰を行う場合には
これを使います。 しかし、二項分類においては
このアルゴリズムはあまり良くありません。 なぜなら、
y hatはy=1である可能性でなければならず、 それは0と1の間でなければ
ならないからです。 この式ではそうなりそうにありません、 なぜならwの転置かけるxはすごく大きくなるか
負になることすらありえます。 そのような値では、0と1の間になってほしい 可能性として意味を成しません。 なので、ロジスティック回帰では y hatはこの値にシグモイド関数を
適用した値となります。 シグモイド関数とはこのようなものです。 水平方向の軸をzとすると、
zについてのシグモイド関数はこうなります。 0から1までなめらかに進みます。 軸にラベルを書きますね。 ここが0で、0.5の位置で
垂直方向の軸と交わります。 これがzについてのシグモイド関数で、
zをこの値を表すのに使います。 wの転置かけるxプラスbです。 シグモイド関数の公式はこれです。 実数であるzについてのシグモイド関数は、 1+e^(-z)分の1です。 ここから、いくつかのことがわかります。 もしzがとても大きければ、
e^(-z)は0に近くなります。 なので、zについてのシグモイドは (1+0に限りなく近いもの)分の1となります。 なぜなら、eの(マイナスに大きい数字)乗というのは
0に近くなるからです。 これは1に近くなります。 実際、左のグラフを見ると、 zがとても大きいと
zについてのシグモイドは1に近くなります。 反対に、zがとても小さいと、 もしくはとても大きな負の数字だと、 zについてのシグモイドは
1+e^(-z)分の1で、 これが巨大な数字になります。 (1+巨大な数字)分の1と考えると、 これは、 0に近くなります。 実際、zがとても大きな負の数字になると、 zについてのシグモイドは0に近くなります。 ロジスティック回帰を実装するとき、 あなたがすることは、
y hatがy=1である可能性の良い予測になるような パラメータwとbを学習することです。 次に進む前に、表記法について少し説明します。 ニューラルネットワークを
プログラムするとき、 大抵パラメータwとパラメータbは
別々にしておきます。 ここでは、bは切片項に対応します。 他のコースでは、 これらを別の方法で扱っている
表記を見たことがあるかもしれません。 ある慣例では、x0と呼ばれる余分な特徴を
定義し、1とすることがあります。 その場合、xはnx+1次元です。 すると、y hatはσ(θの転置)です。 この表記の慣例では、 θというベクトルパラメータがあります。 θ0、θ1、θ2…と続き、 θnxまであります。 θ0はbのことで、 これはただの実数です。 θ1からθnxがwの働きをします。 ニューラルネットワークを
実装するときには、 bとwを別々のパラメータとして
扱った方が楽です。 なので、このクラスでは、 さっき赤色で書いた表記法は
使わないことにします。 この表記法を他のクラスで見たことがなければ、
気にしないでください。 この表記法を見たことがある人に
向けての話なので。 ただ、このコースではこれらを使わない
ということを示しておきたかったのです。 見たことがなければ、 全然大切なことではないので
心配しないでください。 さて、ロジスティック回帰のモデルが
どのようなものかを見てきました。 次は、パラメータwとbを変えるために
コスト関数を定義しましょう。 次のビデオでやってみましょう。