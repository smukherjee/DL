1
00:00:00,000 --> 00:00:03,475
在這段影片, 我們將談論羅吉斯迴歸分析

2
00:00:03,475 --> 00:00:07,080
這是種當您使用學習演算法時輸出標籤 y

3
00:00:07,080 --> 00:00:10,690
在監督式學習問題中不是 0 就是 1 

4
00:00:10,690 --> 00:00:13,600
也就是二元分類問題

5
00:00:13,600 --> 00:00:18,350
給予一個輸入特徵向量 x 也許相當於

6
00:00:18,350 --> 00:00:23,150
一張影像您想辨識是貓的照片或不是貓的照片

7
00:00:23,150 --> 00:00:26,525
您要一個演算法能夠輸出預測

8
00:00:26,525 --> 00:00:28,254
我們稱為 y-hat

9
00:00:28,254 --> 00:00:31,130
就是您的預估 y

10
00:00:31,130 --> 00:00:35,896
更正式一點來說, 您希望 y-hat 是一個機率當

11
00:00:35,896 --> 00:00:40,630
輸入 x, y 等於 1 的機會

12
00:00:40,630 --> 00:00:43,880
換句話說, 如果 x 是一張照片

13
00:00:43,880 --> 00:00:45,530
像我們在上一段影片中看到的

14
00:00:45,530 --> 00:00:47,300
您要 y-hat 告訴您

15
00:00:47,300 --> 00:00:49,820
這是一張貓的照片的機會是多少

16
00:00:49,820 --> 00:00:53,420
X, 就像我們上一段影片中說的

17
00:00:53,420 --> 00:00:56,960
是一個 nx 維度向量

18
00:00:56,960 --> 00:01:02,000
羅吉斯迴歸分析的參數會是

19
00:01:02,000 --> 00:01:07,745
w 也是一個 nx 維度向量

20
00:01:07,745 --> 00:01:11,670
跟 b 是一個實數

21
00:01:11,670 --> 00:01:16,055
給予一個輸入 x 跟參數 w, b

22
00:01:16,055 --> 00:01:20,595
我們如何產生輸出 y-hat

23
00:01:20,595 --> 00:01:22,970
一種方式您可以試, 不過行不通

24
00:01:22,970 --> 00:01:27,590
是讓 y-hat 為 w轉置x 加 b

25
00:01:27,590 --> 00:01:33,045
類似是輸入 x 的線性函數

26
00:01:33,045 --> 00:01:37,145
實際上, 這是如果您使用線迴歸分析的方式

27
00:01:37,145 --> 00:01:41,345
但這並不是一個很好的演算法在二元分類

28
00:01:41,345 --> 00:01:45,575
因為您希望 y-hat 是 y 等於 1 的機會

29
00:01:45,575 --> 00:01:50,480
所以 y-hat 應該是介於 0 跟 1

30
00:01:50,480 --> 00:01:54,697
很難做到, 因為 w 轉置 x

31
00:01:54,697 --> 00:01:58,475
加 b 可能遠大於 1 或者甚至是負數

32
00:01:58,475 --> 00:02:00,905
對於機率而言是沒意義的

33
00:02:00,905 --> 00:02:03,620
您想要的是它介於 0 跟 1

34
00:02:03,620 --> 00:02:07,670
所以在羅吉斯迴歸分析我們的輸出會讓 y-hat

35
00:02:07,670 --> 00:02:12,050
 等於S型函數應用到這個量

36
00:02:12,050 --> 00:02:14,850
S型函數的樣子像這樣

37
00:02:14,850 --> 00:02:24,000
如果在橫軸我畫 z 那S型函數 of z 像這樣

38
00:02:24,000 --> 00:02:28,050
它平緩的從 0 到 1

39
00:02:28,050 --> 00:02:30,120
讓我在軸上做標籤

40
00:02:30,120 --> 00:02:34,915
這是 0 而它跟縱軸的交叉是 0.5

41
00:02:34,915 --> 00:02:41,305
這就是S型函數 of z 的樣子而我們將使用 z 來代表這個量

42
00:02:41,305 --> 00:02:43,020
w轉置x 加 b

43
00:02:43,020 --> 00:02:46,230
這是S型函數的公式

44
00:02:46,230 --> 00:02:49,380
S型函數 of z, z 是一個實數

45
00:02:49,380 --> 00:02:52,510
是 1 / 1 加 e 的 負z

46
00:02:52,510 --> 00:02:54,695
請注意幾件事

47
00:02:54,695 --> 00:03:01,255
如果 z 很大那 e 的負z 會趨近於 0

48
00:03:01,255 --> 00:03:03,420
所以S型函數 of z 會是

49
00:03:03,420 --> 00:03:07,255
趨近於 1 除以 1 加這個趨近於0 的東西

50
00:03:07,255 --> 00:03:11,280
因為 e 的負很大的值會趨近於 0

51
00:03:11,280 --> 00:03:13,505
所以這整個趨近於 1

52
00:03:13,505 --> 00:03:16,255
實際上, 如果您看圖的左邊

53
00:03:16,255 --> 00:03:20,475
如果 z 很大這S型函數會趨近於 1

54
00:03:20,475 --> 00:03:24,105
相反的, 如果 z 很小

55
00:03:24,105 --> 00:03:28,970
或者是很大的負數

56
00:03:29,180 --> 00:03:39,640
那S型函數 of z 變成 1 除以 1 加 e 的負 z

57
00:03:39,640 --> 00:03:42,565
這個會變成很大的數

58
00:03:42,565 --> 00:03:47,944
這個想成是 1 除以 1 加很大很大

59
00:03:47,944 --> 00:03:54,473
的數

60
00:03:54,473 --> 00:03:56,570
這整個會趨近於 0

61
00:03:56,570 --> 00:04:00,325
真的, 您看當 z 變成很大的負數

62
00:04:00,325 --> 00:04:03,505
S型函數會趨近於 0

63
00:04:03,505 --> 00:04:06,070
所以當您建置羅吉斯迴歸分析

64
00:04:06,070 --> 00:04:10,350
您的工作是學習參數 w 跟 b 使得

65
00:04:10,350 --> 00:04:15,220
y-hat 變成一個好的預測對於 y 等於 1 的機會

66
00:04:15,220 --> 00:04:18,955
在繼續往前之前, 關於符號的一個註解

67
00:04:18,955 --> 00:04:20,830
當我們做神經網路的程式時

68
00:04:20,830 --> 00:04:26,855
我們通常會將參數 w 跟參數 b 分開

69
00:04:26,855 --> 00:04:30,000
這裡, b 相當於截距

70
00:04:30,000 --> 00:04:31,295
在其他一些課程中

71
00:04:31,295 --> 00:04:35,110
您也許看到一些不同的符號

72
00:04:35,110 --> 00:04:42,205
在某些約定中您定義ㄧ個額外的特徵 x0 而這等於 1

73
00:04:42,205 --> 00:04:47,250
所以 X 是 R 的 nx + 1

74
00:04:47,250 --> 00:04:53,865
而您定義 y-hat 等於 Sigma of theta 轉置 X

75
00:04:53,865 --> 00:04:56,685
在這另一種符號約定中

76
00:04:56,685 --> 00:05:00,510
您有向量參數 theta

77
00:05:00,510 --> 00:05:03,175
theta0, theta1, theat2..

78
00:05:03,175 --> 00:05:09,520
直到 theta nx

79
00:05:09,520 --> 00:05:11,723
theta0 放在 b 的位置

80
00:05:11,723 --> 00:05:13,663
一樣只是個實數

81
00:05:13,663 --> 00:05:18,505
theta1 直到 theta nx 則跟 W 一樣, 實際上

82
00:05:18,505 --> 00:05:20,350
當您建置您的神經網路時

83
00:05:20,350 --> 00:05:26,145
保持 B 跟 W 分開會比較容易

84
00:05:26,145 --> 00:05:27,430
所以, 在這個課程

85
00:05:27,430 --> 00:05:32,087
我們不會使用這個紅色的符號約定

86
00:05:32,087 --> 00:05:36,330
如果您在本課程前不曾見過這些符號, 不用擔心

87
00:05:36,330 --> 00:05:39,610
只是要讓那些見過這樣符號的人我要

88
00:05:39,610 --> 00:05:43,730
明確的提醒我們在這課程不使用這樣的符號

89
00:05:43,730 --> 00:05:45,235
但如果您不曾見過這些

90
00:05:45,235 --> 00:05:48,430
這不重要您不需要擔心

91
00:05:48,430 --> 00:05:52,465
您已經看到羅吉斯迴歸分析模型的樣子

92
00:05:52,465 --> 00:05:57,140
接下來要改變參數 w 跟 b 您需要定義成本函數

93
00:05:57,140 --> 00:05:58,830
讓我們下一段影片談這個部分