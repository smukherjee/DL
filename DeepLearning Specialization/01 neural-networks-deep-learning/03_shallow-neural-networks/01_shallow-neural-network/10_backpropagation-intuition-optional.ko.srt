1
00:00:00,000 --> 00:00:01,230
지난 강의에서

2
00:00:01,230 --> 00:00:03,720
이전 비디오에서는 후 방향전파에 대한 공식들을 봤는데요, 

3
00:00:03,720 --> 00:00:06,900
이번 비디오에서는, 산출 그래프를 이용해서

4
00:00:06,900 --> 00:00:10,515
어떻게 그 공식이 만들어졌는지 직관적인 부분을 다뤄보겠습니다.

5
00:00:10,515 --> 00:00:12,385
이 비디오는 선택적인 비디오입니다.

6
00:00:12,385 --> 00:00:14,106
그렇기 때문에 여러분이 보셔도 안보셔도 됩니다.

7
00:00:14,106 --> 00:00:16,360
이 비디오를 시청하지 않으셔도 작업은 하실 수 있습니다.

8
00:00:16,360 --> 00:00:19,410
로지스틱 회귀 분석을 하는 경우, 기억하시겠지만, 

9
00:00:19,410 --> 00:00:23,685
여기 forward pass를 통해 z의 값을 구했습니다.

10
00:00:23,685 --> 00:00:26,145
그리고 A를 구하고나거, loss를 구했습니다.

11
00:00:26,145 --> 00:00:27,445
그리고 derivative를 계산하기 위해서

12
00:00:27,445 --> 00:00:32,520
이렇게 backward pass를 이용해서 처음에 먼저 da를 계산했는데요, 

13
00:00:32,520 --> 00:00:35,400
그 다음에 dz를 계산하고, 

14
00:00:35,400 --> 00:00:40,720
다음으로 dw 와 db를 계산했습니다.

15
00:00:40,720 --> 00:00:46,970
그러면 loss의 정의는 L(a, y)였습니다.

16
00:00:46,970 --> 00:00:52,655
이 값은 -y log a - (1 - y) 곱하기

17
00:00:52,655 --> 00:00:57,440
log (1-a) 입니다.

18
00:00:57,440 --> 00:00:59,750
그러므로 여러분이 

19
00:00:59,750 --> 00:01:03,600
미적분에 익숙하면, 여기 a에 대해 derivative 값을 구하고, 

20
00:01:03,600 --> 00:01:06,156
그러면 da를 위한 공식을 줄 것입니다.

21
00:01:06,156 --> 00:01:09,060
그러면 da는 이것가 동일한 값이구요, 

22
00:01:09,060 --> 00:01:12,750
미적분을 통해서 이 부분은, 

23
00:01:12,750 --> 00:01:18,808
- y 나누기 a 더하기 (1-y) 나누기 (1-a)입니다.

24
00:01:18,808 --> 00:01:23,040
이것의 derivative를 구함으로써 이 값을 구합니다.

25
00:01:23,040 --> 00:01:26,680
그리고 이제 dz를 구하기 위해 거꾸로 거슬러 올라가면, 

26
00:01:26,680 --> 00:01:32,430
이전에 구한 것은 dz=a-y라는 것입니다. 그 이유는 이전에 설명했었죠. 

27
00:01:32,430 --> 00:01:37,920
미적분에서의 chain rule로 인해, dz=

28
00:01:37,920 --> 00:01:45,425
da 곱하기 g 프라임 z가 됩니다.

29
00:01:45,425 --> 00:01:50,535
여기서 g(z)는 z의 시그모이드가 되죠. 

30
00:01:50,535 --> 00:01:56,245
이것은 여기 로지스틱 회귀분석에서의 결과값에 대한 activation 함수입니다. 맞죠?

31
00:01:56,245 --> 00:02:00,570
그러므로 여기서 기억할 것은 이것이 아직 로지스틱 회귀분석이라는 것이고, 

32
00:02:00,570 --> 00:02:05,757
x1, x2, x3의 값과 한개의 시그모이드 유닛이 여기 A 값, 

33
00:02:05,757 --> 00:02:07,400
ŷ을 줍니다.

34
00:02:07,400 --> 00:02:11,400
여기서는 activation 함수가 시그머이드 함수였는데요, 

35
00:02:11,400 --> 00:02:12,960
따로 또 말씀드리자면, 

36
00:02:12,960 --> 00:02:17,205
미적분학의 chain rule이 익숙한 분들을 위해서인데요, 

37
00:02:17,205 --> 00:02:22,520
그 이유는, A가 z의 시그모이드 함수와 동일하고

38
00:02:22,520 --> 00:02:29,310
그리고 z에 대해서 L의 partial 값은 

39
00:02:29,310 --> 00:02:36,800
a에 대한 partial L 값 곱하기 da.dz입니다.

40
00:02:36,800 --> 00:02:39,611
이것은 z의 시그모이드 

41
00:02:39,611 --> 00:02:42,970
이것은 d/dz

42
00:02:42,970 --> 00:02:49,080
곱하기 g(z)입니다. 이 값은 g 프라임 z죠.

43
00:02:49,080 --> 00:02:54,060
그렇기 때문에, 여기 dz 라고 표현된 코드는 

44
00:02:54,060 --> 00:02:59,484
여기 da 곱하기 g 프라미 z와 동일한 것입니다.

45
00:02:59,484 --> 00:03:05,860
그러므로 이것은 이값이 됩니다.

46
00:03:05,860 --> 00:03:09,172
마지막에 파생된 공식은 

47
00:03:09,172 --> 00:03:13,510
여러분이 미적분학에 익숙한 경우에만 이해를 하실 수 있을텐데요, 

48
00:03:13,510 --> 00:03:15,325
그렇지 않은 경우라면 걱정하지 마십시요, 

49
00:03:15,325 --> 00:03:18,853
여기에 대한 직관적인 부분은 나중에 필요할 때마다 설명 드리겠습니다.

50
00:03:18,853 --> 00:03:22,315
이제 여기 로지스틱 회귀을 위해서 DZ 계산했는데요, 

51
00:03:22,315 --> 00:03:26,335
이제 DW를 계산하겠습니다.

52
00:03:26,335 --> 00:03:31,470
이 값은 dz 곱하기 x 이고 db는 그냥 dz입니다. 한개의 트레이닝 예시의 경우 말이죠.

53
00:03:31,470 --> 00:03:33,822
이것이 로지스틱 회귄분석에 관한 내용이였는데요, 

54
00:03:33,822 --> 00:03:36,700
이제는 후 방향전파으로 계산을 할때, 

55
00:03:36,700 --> 00:03:40,090
신경망을 위해서 말이죠, 이런 계산을

56
00:03:40,090 --> 00:03:46,995
진행할텐데요, 이 절차를 두번 진행할 것입니다. 왜냐면 이제는 x가 결과값 unit으로 가는 것이 아니라

57
00:03:46,995 --> 00:03:50,930
x가 hidden layer로 가고 난 다음, 결과값 unit으로 가기 때문입니다.

58
00:03:50,930 --> 00:03:58,405
그러므로 여기 이런 산출과정과 같이 1단계로 되는 것이 아니라, 

59
00:03:58,405 --> 00:04:04,483
여기와 같이 2개의 층으로 된 신경망은 이렇게 2개의 단계가 있을 것입니다.

60
00:04:04,483 --> 00:04:08,586
여기 2개의 층으로 된 신경망 같은 경우, 입력 층이 있고, 

61
00:04:08,586 --> 00:04:10,138
숨겨진 레이어와 다음 결과 층 이렇게 있습니다.

62
00:04:10,138 --> 00:04:12,070
그러면 살출 과정을 기억해보겠습니다.

63
00:04:12,070 --> 00:04:17,210
처음에는 여기 이 공식을 이용해서 z1을 구합니다.

64
00:04:17,210 --> 00:04:22,177
그 다음에 a1을 계산하고 다음에 z2를 계산합니다.

65
00:04:22,177 --> 00:04:25,505
알아두실 것은 z2가 파라미터 w2 와 b2에 따라 달라진다는 것입니다.

66
00:04:25,505 --> 00:04:27,530
그리고 z2를 바탕으로, 

67
00:04:27,530 --> 00:04:32,815
a2를 계산하고, 마지막으로 이 값이 loss의 값을 줍니다.

68
00:04:32,815 --> 00:04:41,560
후 방향전파이 하는 것은 DA2 와 DZ2를 계산하기 위해 뒤로 가는 것입니다.

69
00:04:41,560 --> 00:04:48,805
그 다음에 DW2와 DP2를 계산하러 뒤로 갑니다.

70
00:04:48,805 --> 00:04:53,232
그 다음에 DA1을 계산합니다.

71
00:04:53,232 --> 00:04:57,278
그 다음 DZ! 등등 이렇게 말이죠. 

72
00:04:57,278 --> 00:05:00,290
그리고 저희는 입력값 x에 대한 derivative값을 구할

73
00:05:00,290 --> 00:05:03,745
필요가 없습니다. 지도학습에서 입력값 X는 고정값입니다.

74
00:05:03,745 --> 00:05:07,845
그렇기 때문에 저희는 이 값을 최적화하려고 하는 것이 아닙니다. 
그러므로 derivative는 구하지 않습니다.

75
00:05:07,845 --> 00:05:09,655
적어도 지도학습 분야에서 x에 대해서

76
00:05:09,655 --> 00:05:15,605
말이죠. 그리고 저는 DA2를 구하는 것을 넘어갈 것인데요.

77
00:05:15,605 --> 00:05:18,110
원하시면, 여러분이 

78
00:05:18,110 --> 00:05:20,750
DA2를 구하셔서 이 값으로 DZ2를 구할 수 있습니다.

79
00:05:20,750 --> 00:05:25,760
하지만 실제로는 이 2가지의 단계를 한가지 단계로 압축시켜

80
00:05:25,760 --> 00:05:31,715
DZ2=A2-Y 가 되게 할 수 있습니다.

81
00:05:31,715 --> 00:05:33,620
그리고 또한, 

82
00:05:33,620 --> 00:05:38,615
dw2 와 db2를 여기다가 적겠습니다.

83
00:05:38,615 --> 00:05:46,700
그리고 여기는 DW2=DZ2 곱하기 A1

84
00:05:46,700 --> 00:05:52,040
transpose 인데요, 그리고 db2는 dz2입니다.

85
00:05:52,040 --> 00:05:55,990
이 과정은 로지스틱 회귀분석에서 저희가 적용한 

86
00:05:55,990 --> 00:06:03,550
DW=DZ 곱하기 X와 비슷한데요, 대신에 여기서는 

87
00:06:03,550 --> 00:06:08,770
A1이 X의 역할을 하고, 여기 추가로 transpose가 있는데요, 그 이유는

88
00:06:08,770 --> 00:06:14,125
대문자 W 매트릭스와 개인별 파라미터 W의 관계 때문에 그렇습니다.

89
00:06:14,125 --> 00:06:16,660
여기 transpose가 있죠, 맞죠?

90
00:06:16,660 --> 00:06:24,370
그 이유는 w 는 row vector 와 같은데요, 로지스틱 회귀분석에서 1개의 결과값이 
있는 경우에 말이죠. 

91
00:06:24,370 --> 00:06:26,980
dw2는 그렇습니다. 반면에 

92
00:06:26,980 --> 00:06:32,440
w는 세로 column vector입니다. 그렇기 때문에 추가적으로 a1 이 있는 것입니다.

93
00:06:32,440 --> 00:06:36,980
여기서 X는 로지스틱 회귀분석에서는 그렇지 않았죠. 

94
00:06:36,980 --> 00:06:40,335
이것이 후 방향전파의 반은 완성입니다.

95
00:06:40,335 --> 00:06:44,045
그리고 여러분이 원하면 DA1도 계산할 수 있습니다.

96
00:06:44,045 --> 00:06:49,440
하지만 실제로는 da1의 계산과 

97
00:06:49,440 --> 00:06:52,330
dz1의 계산은 한단계로 압축되어 진행됩니다.

98
00:06:52,330 --> 00:06:57,130
그러므로 여러분이 주로 도입하는 것은 dz1=w2 transpose 

99
00:06:57,130 --> 00:07:03,480
곱하기 dz2 그리고 g1 프라임 z1이라는 

100
00:07:03,480 --> 00:07:10,383
element wise product를 곱합니다.

101
00:07:10,383 --> 00:07:13,960
그리고 다이멘션을 체크하기 위해서 말이죠

102
00:07:13,960 --> 00:07:19,510
만약 이렇게 생긴 신경망이 있다고 하면, 

103
00:07:19,510 --> 00:07:23,000
여기 Ŷ를 결과값으로 갖겠죠

104
00:07:23,000 --> 00:07:28,265
만약 N0, NX=N0 입력 특성이 있으면

105
00:07:28,265 --> 00:07:30,230
그리고 1개의 숨겨진 유닛이 있고

106
00:07:30,230 --> 00:07:34,275
n2가 있으면, 

107
00:07:34,275 --> 00:07:36,740
n2 같은 경우 저희의 경우, 

108
00:07:36,740 --> 00:07:38,565
1개의 결과값 유닛인데요, 

109
00:07:38,565 --> 00:07:48,795
그러면 w2 매트릭스는 (n2, n1) 차원입니다.

110
00:07:48,795 --> 00:07:57,490
그러면 z2 와 dz2는 (n2, 1) 입니다.

111
00:07:57,490 --> 00:07:59,850
이것은 (1, 1)이 될 것입니다. 이진분류법에서 말이죠.

112
00:07:59,850 --> 00:08:04,750
그러면 z1, 

113
00:08:04,750 --> 00:08:10,045
dz1 같은 경우에도 역시, (n1, 1)이겠죠, 

114
00:08:10,045 --> 00:08:16,115
그렇기 때문에 어떤 foo 변수와 어떤 D foo 이더라고 같은 다이멘션을 가질 것입니다.

115
00:08:16,115 --> 00:08:20,850
그렇기 때문에 W와 DW는 항상 똑같은 다이멘션을 갖게되며, 비슷하게

116
00:08:20,850 --> 00:08:23,680
B와 DB에서도, 그리고 Z와 DZ에서도 동일하게 적용됩니다.

117
00:08:23,680 --> 00:08:26,895
여기서 모든 다이멘션이 매칭되도록, 

118
00:08:26,895 --> 00:08:35,430
DZ1은 W2 transpose 곱하기 DZ2 가 있습니다.

119
00:08:35,430 --> 00:08:44,490
그리고 이것은 element wise product 곱하기 g 프라임 z1인데요, 

120
00:08:44,490 --> 00:08:47,040
위의 다이멘션과 매칭하기 위해

121
00:08:47,040 --> 00:08:52,575
이것은 (n1, 1)이 될 것이고, w2 transpose입니다.

122
00:08:52,575 --> 00:08:57,945
이것을 transpose하는 것인데요, 그러므로 이 값은 (n1, n2) 

123
00:08:57,945 --> 00:09:05,790
이고, dz2는 (n2, 1) 이고, 이것은 

124
00:09:05,790 --> 00:09:07,230
z1과 같은 다이멘션입니다.

125
00:09:07,230 --> 00:09:11,820
그러므로 이 값도 (n1, 1)인데요. 여기는 element wise product입니다.

126
00:09:11,820 --> 00:09:14,350
이제 다이멘션이 말이 되죠. 맞죠?

127
00:09:14,350 --> 00:09:18,330
(N1, 1) 벡터는 (N1, N2) 차원의 매트릭스를

128
00:09:18,330 --> 00:09:23,520
(N2, N1) 차원의 매트릭스를 곱하면서 얻을 수 있게 됩니다. 

129
00:09:23,520 --> 00:09:28,890
그 이유는 이 2개의 매트릭스를 곱하여 (N1, 1) 매트릭스를 주기 때문입니다.

130
00:09:28,890 --> 00:09:34,618
그러면 이것은 (n1, 1) 차원 2개의 element wise product의 값입니다.

131
00:09:34,618 --> 00:09:36,060
그러면 다이멘션이 매칭이 되겠죠. 

132
00:09:36,060 --> 00:09:40,620
후 방향전파를 도입할 때의 팁은, 

133
00:09:40,620 --> 00:09:44,790
여러분의 매트릭스 다이멘션이 매칭이 되게 하면, 

134
00:09:44,790 --> 00:09:47,190
여기 다양한 매트릭스 w1, w1, z1, z2

135
00:09:47,190 --> 00:09:50,430
a1, a2 등등 이 무엇이지 생각하고, 

136
00:09:50,430 --> 00:09:54,180
여기 다인멘션들이 매칭될 수 있도록

137
00:09:54,180 --> 00:09:58,642
확실히 하면, 가끔씩 후 방향전파에서는 이런 절차가

138
00:09:58,642 --> 00:10:03,390
이미 상당 수의 버그를 제거할 것입니다.

139
00:10:03,390 --> 00:10:06,960
그러면 이것은 DZ1을 주고요, 

140
00:10:06,960 --> 00:10:12,160
마지막으로, DW1과 DB1, 

141
00:10:12,160 --> 00:10:13,965
은 여기 적어야 하는데

142
00:10:13,965 --> 00:10:17,200
공간이 부족하므로 여기 오른쪽에 적겠습니다.

143
00:10:17,200 --> 00:10:21,965
DW1 과 DB1은 여기 공식처럼 주어집니다.

144
00:10:21,965 --> 00:10:25,950
이것은 dz1 곱하기 x transpose일 것이구요, 

145
00:10:25,950 --> 00:10:28,905
그리고 이것은 dz와 동일할 것입니다.

146
00:10:28,905 --> 00:10:34,045
그러면 여러분은 비슷한 점을 발견하실텐데요, 여기 이 공식과 이 공식들에서 말이죠.

147
00:10:34,045 --> 00:10:37,095
이것이 우연의 일치는 아닙니다. 왜냐면

148
00:10:37,095 --> 00:10:41,660
X가 a0의 역할을 하기 때문입니다. 그러므로 X transpose 는 a0 transpose입니다. 

149
00:10:41,660 --> 00:10:45,484
여기 이 공식들은 사실 굉장히 유사한데요, 

150
00:10:45,484 --> 00:10:50,260
이렇게 해서 어떻게 후 방향전파 공식이 만들어지는지 봤습니다.

151
00:10:50,260 --> 00:10:54,530
여기 6개의 핵심 공식이 있는데요, dz2, dw2

152
00:10:54,530 --> 00:11:00,190
db2, dz1, dw1 그리고 d1에 대한 공식입니다.

153
00:11:00,190 --> 00:11:05,767
여기 6개의 식을 가지고 복사해서 다음 슬라이드로 넘어가겠습니다.

154
00:11:05,767 --> 00:11:08,950
현재까지 후 방향전파의 derive했는데요, 

155
00:11:08,950 --> 00:11:13,959
1개의 트레이닝 예시의 경우에 말이죠, 

156
00:11:13,959 --> 00:11:21,530
하지만 놀랍지 않게도, 한개의 예시로 작업하는 것보다

157
00:11:21,530 --> 00:11:27,810
다양한 트레이닝 예시에 대해서 vectorize하고 싶을 것입니다. 

158
00:11:27,810 --> 00:11:30,971
기억하시겠지만, 전 방향전파에서는

159
00:11:30,971 --> 00:11:33,545
1개의 예시씩 운영했는데요, 

160
00:11:33,545 --> 00:11:41,665
이러한 공식이 있었습니다. 그리고 a1=g1 (z1) 이런 공식도 있었죠.

161
00:11:41,665 --> 00:11:43,655
vectorize를 위해서, 

162
00:11:43,655 --> 00:11:51,260
z 값을 쌓아 올렸습니다.

163
00:11:51,260 --> 00:12:00,775
이렇게 줄의 형식으로 z1m까지 말이죠, 그리고 이것을
대문자 Z로 불렀습니다. 

164
00:12:00,775 --> 00:12:04,960
그리고 저희는 이렇게 줄에다가 쌓는 방식으로

165
00:12:04,960 --> 00:12:10,240
대문자 버전을 정의했는데요, 

166
00:12:10,240 --> 00:12:17,093
그렇게 하여. Z1=W1 X + B가 생겼습니다.

167
00:12:17,093 --> 00:12:24,700
그리고 A1은 z1의 g1이죠 맞죠?

168
00:12:24,700 --> 00:12:28,645
이 코스 과정에서는 표기법을 조심히 정의합니다. 

169
00:12:28,645 --> 00:12:35,550
왜냐면 다른 줄로 쌓아 올리게 되면, 어느 방법으로도 풀리게 되기 때문입니다.

170
00:12:35,550 --> 00:12:40,105
수학적인 부분을 조심히 보면, 

171
00:12:40,105 --> 00:12:46,645
똑같은 비법이 후 방향전파에서도 쓰일 수 있는 것 알게됩니다. 
vectorize공식은 이렇게 되는데요, 

172
00:12:46,645 --> 00:12:52,250
첫번째로, 여기 다른 트레이닝 예시에 대한 DZ값들을 쌓아올리면, 

173
00:12:52,250 --> 00:12:58,339
매트릭스에서 다른줄로 말이죠, 여기도 동일하게 합니다.

174
00:12:58,339 --> 00:13:03,070
그러면 여기가 vectorized 도입 버전입니다. 그리고 여기가 정의인데여, 

175
00:13:03,070 --> 00:13:05,569
또는 dw2를 계싼하는 방법입니다.

176
00:13:05,569 --> 00:13:11,130
여기 추가적인 1/m 부분이 있습니다. 왜냐면 j비용함수가

177
00:13:11,130 --> 00:13:18,410
1 나누기 m 의 loss합 공식이기 때문이죠 

178
00:13:18,410 --> 00:13:20,615
그러므로 derivative를 계산하는 경우에, 

179
00:13:20,615 --> 00:13:23,885
여기 1/m 추가 항이 있는 것입니다. 저희가 

180
00:13:23,885 --> 00:13:27,982
로지스틱 회귀분석으로 weight 업데이트를 계산할때와 같이 말이죠. 

181
00:13:27,982 --> 00:13:31,790
이것이 db2의 업데이트입니다.

182
00:13:31,790 --> 00:13:40,640
마찬가지로 dz의 합이고, 1/m이 있습니다. 그리고 dz1은 이렇게 산출됩니다.

183
00:13:40,640 --> 00:13:49,109
다시 한번, 이것은 element wise product입니다.

184
00:13:49,109 --> 00:13:56,595
이전 슬라이드에서는 (n1, 1) 차원 벡터였었는데요, 

185
00:13:56,595 --> 00:14:03,185
이제는 (n1, M) 차원이 매트릭스입니다.

186
00:14:03,185 --> 00:14:09,045
여기도 마찬가지로 (n1, m) 차원입니다.

187
00:14:09,045 --> 00:14:19,310
그렇기 때문에 여기 asterisk 별표는 element wise product인 것입니다.

188
00:14:19,310 --> 00:14:21,454
나머지 2개의 업데이트인데요, 

189
00:14:21,454 --> 00:14:25,836
놀랍지 않을텐데요, 

190
00:14:25,836 --> 00:14:29,510
후 방향전파 이 어떻게 파생되었는지 직관적으로 알 수 있는 계기가 되었길 바랍니다.

191
00:14:29,510 --> 00:14:32,205
모든 머신 러닝 분야에서 말이죠. 

192
00:14:32,205 --> 00:14:34,820
후 방향전파 알고리즘의 파생은 

193
00:14:34,820 --> 00:14:38,465
수학적으로 가장 복잡한 부분이기도 한데요, 

194
00:14:38,465 --> 00:14:42,470
선형대수도 알아야하고, 매트릭스의 derivative도

195
00:14:42,470 --> 00:14:46,830
안 상태에서 아무것도 없는 첫번째 상황에서 re-derive 해야합니다.

196
00:14:46,830 --> 00:14:50,165
여러분이 미적분학에 능숙하면, 

197
00:14:50,165 --> 00:14:54,255
이런 절차를 이용해서 derivative알고리즘을 직접 증명할 수 있겠죠.

198
00:14:54,255 --> 00:14:57,500
하지만 제가 생각하기에 상당부분의 딥러닝

199
00:14:57,500 --> 00:15:01,060
종사자들은 지금 여러분이 보신 정도의 

200
00:15:01,060 --> 00:15:04,100
레벨까지 다뤘을 것입니다. 그러면서도

201
00:15:04,100 --> 00:15:08,580
직관적인 부분을 이용해서 알고리즘을 매우 효율적으로 도입할 수 있는 것이죠. 

202
00:15:08,580 --> 00:15:10,070
여러분이 만약 미적분 고수이면, 

203
00:15:10,070 --> 00:15:13,395
아무것도 없는 상황에서 이런 공식을 나오게 할 수 있는지 한번 해보세요.

204
00:15:13,395 --> 00:15:15,665
수학적으로 거의 가장 어려운 것 같습니다.

205
00:15:15,665 --> 00:15:20,010
머신 러닝에서 본 가장 어려운 식의 파생 과정입니다.

206
00:15:20,010 --> 00:15:22,861
어떤 방식이던, 이것을 도입하면, 

207
00:15:22,861 --> 00:15:27,260
잘 작동할 것이고, 지금 상태에서 여러분은 직관적으로 충분히 튜닝하고
작동하게끔 만들 수 있으실 것입니다.

208
00:15:27,260 --> 00:15:30,830
마지막 부분이 있는데요, 

209
00:15:30,830 --> 00:15:34,190
여러분이 신경망을 도입하기 전에, 

210
00:15:34,190 --> 00:15:37,720
신경망에서 weight 비중을 초기화하는 방법입니다.

211
00:15:37,720 --> 00:15:40,600
여러분의 파라미터를 초기화하는 것이

212
00:15:40,600 --> 00:15:42,560
0으로 하는 것이 아니라 임의로 하는 것입니다.

213
00:15:42,560 --> 00:15:45,515
이 신경망을 트레이닝 시키는데 있어 굉장히 중요합니다.

214
00:15:45,515 --> 00:15:48,000
왜 그런지 그 이유는 다음 비디오에서 보실 것입니다.