在上一节课中 我们看到了如何计算神经网络 对于一个训练实例的预测 在这节课中 我们会看到如何对多个训练实例矢量化 这个结果会和我们之前看到的逻辑回归的结果很相似 通过把不同的训练实例按列堆叠在一个矩阵里面 我们就可以拿来之前的视频里的等式 做一些很小的改动 使得神经网络在同时计算 所有训练实例的预测 那么我们来看一下具体怎么做 这些是我们之前视频看到的4个等式 用来计算z1 a1 z2 和a2 (如图示) 这些等式告诉我们 给定一个特征矢量x (如图示) 我们可以用它们来取得一个训练实例的a2=y^ (如图示) 如果我们有m个训练实例 我们需要重复这个过程 比如说 对于第一个训练实例 给定x^(1) 来计算y^(1) (如图示) 这是第一个训练实例的预测 接下来用x^(2)来生成预测y^(2)（如图示） 以此类推 直到用x^(m)来生成预测y^(m) (如图示) 为了用激活函数的标记法 我会(把y^(1))写作a^[2](1)（如图示) 接下来是a^[2](2) (如图示) 然后是a^[2](m) (如图示) 这个记号法a^[2](i) 这个圆括号代表第i个训练实例 而这个方括号代表第2层 这就是方括号和圆括号里的指数代表的意思 所以如果你有一个未矢量化的实现 当你想要计算所有训练实例的预测时 你需要用一个循环 i从1到m (如图示) 实现这4个等式 需要让z^[1](i) (如图示) = W(1) x(i) + b[1] a[1](i) = sigma of z[1](1) z[2](i) = w[2]a[1](i)+ b[2] (如图示 念写的公式) (如图示 念写的公式) 所以基本上就是上面这4个等式 在所有训练实例的因变量上加上上角标(i) 也就是在x z 和a上面加上上角标(i) 来计算所有m个训练实例的输出 我们想做的是矢量化整个计算 来去除这个for循环 万一这部分看起来像是我在讲一些线性代数的繁琐细节 实际上能够正确的实现这个 在这个深度学习的纪元是很重要的 而且我们在这门课里是非常仔细的选择标记法 以便于矢量化的过程尽可能的容易 所以我希望走进这些繁琐细节能够实际上帮助你们 更快得到这些算法的正确实现 那么就让我把这一部分的代码复制到下一页 (如图示) 然后我们会看到如何把这部分矢量化 这是我们前一页看到的 for循环走过所有m个训练实例 记得我们定义矩阵X 是由训练实例按列堆叠组成的 (如图示) 也就是说拿来训练实例 把它们按列堆叠 所有这是一个维度为n 或者说n_x乘以m的矩阵 (如图示) 我就直接说重点 告诉你们需要做什么 来得到这个for循环的矢量化实现 实际上你们需要做的是计算 Z[1] = W[1] X + b[1] (如图示 念写的公式) (如图示 念写的公式) (如图示 念写的公式) (如图示 念写的公式) 所以如果你们想要相似对比 我们就是从小写的x们出发 把小写的x们按列堆叠得到了大写的X矩阵 如果我们对于z做同样的操作 比如说 我们把 z^[1](1) z^[1](2) 等等 (如图示) 注意这些也都是列矢量 直到z^[1](m) (如图示) 这就是这第一个变量 (左上角z^[1](i)) 
所有m个按列堆叠起来的 这就是矩阵Z^[1] (如图示) 类似的 如果我们看这个变量 (左上角a^[1](i)) 把a^[1](1) a^[1](2) 等等 (如图示) 直到a^[1](m) 把它们按列堆叠起来 (如图示) 那么这个 就像我们从小写的x到大写的X 从小写的z到大写的Z 这里我们从小写的列矢量a们到大写的A^[1]矩阵 (箭头指示上面等式里的A^[1]) 
对于Z^[2]和A^[2]是类似的 (如图示) 它们也是通过把这些矢量 横着堆叠起来 也就是把这些矢量横着堆叠起来 (如图示) 来得到大写的Z^[2]和A^[2] 这个标记法有一个性质可能会有助于你们理解它 这些矩阵 比如说Z和A 横着来看我们的指数是训练实例 这也就是为什么横的指数对应不同的训练实例 当我们从左到右扫过时 我们扫描了整个训练集 而竖着来看 竖的指数对应着圣经网络里面的不同结点 比如说 这个结点 这个矩阵最左上角的值 (如图示) 对应着 第一个训练实例的第一个隐藏单元的激活值 它下面一个值对应着第二个隐藏单元 对第一个训练实例的激活值 然后是第三个隐藏单元对第一个训练实例的激活值
以此类推 所以当我们向下扫的时候 我们对应的是隐藏单元号码 而当我们横向扫过的时候 我们是从第一个隐藏单元 对于第一个训练实例
GTC字幕组翻译 到第一个隐藏单元对于第二个
第三个训练实例 等等 直到这个结点 (如图示) 对应第一个隐藏单元对最后第m个训练实例的激活值 所以横向来看矩阵A代表不同训练实例 而纵向来看不同的指数 代表不同的隐藏单元 我们可以把类似的直觉用于矩阵Z 对于X也是一样 它横向来看是不同训练实例 纵向来看是不同的输入特征 这些输入特征其实也就是输入层的不同节点 所以有了这些等式
你们现在知道了怎么实现神经网络矢量化 也就是把多个实例矢量化 在下节课中 我想要多给你们一些理由 来解释为什么这是这种矢量化的一个正确实现 其实这个理由和你们之前看到的逻辑回归的很相似 我们下一节再见