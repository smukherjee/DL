عند تطبيق الانتشار الخلفي
للشبكة العصبية، فإنكم تحتاجون إلى التمكّن من حساب الانحدار
أو المشتقة الخاصة بدوال التنشيط. لذا، دعونا نلقِ نظرة على اختياراتنا لدوال التنشيط وكيف يمكنكم حساب
انحدار هذه الدوال. إليكم دالة التنشيط
السينية الشهيرة. لذلك، لأي قيمة معطاة لـ z، ربما قيمة z هذه. فإن هذه الدالة ستحتوي على
بعض الانحدارات أو المشتقات بصورة متوافقة، إذا رسمتم خطًا صغيرًا هناك، فإنكم تعلمون الارتفاع
على العرض لهذا المثلث الأدنى هنا. لذا إذا كانت g(z)‎ هي الدالة السينية، فإن انحدار الدالة هو d، dz g(z)‎، ونعرف من حساب التفاضل والتكامل أن
هذا هو انحدار g(x)‎ عند z. إذا كنتم على دراية بالتفاضل والتكامل و
تعرفون كيفية أخذ المشتقات، إذا أخذتم المشتقة
من الدالة السينية، فمن الممكن إظهار أنها
تساوي هذه الصيغة. مرة أخرى، لن أقوم بإجراء
خطوات التفاضل والتكامل، ولكن اذا كنتم على دراية بحساب التفاضل والتكامل، فلا تترددوا في إيقاف الفيديو
مؤقتًا ومحاولة إثبات ذلك بأنفسكم. وبالتالي، فإن هذا يساوي g(z)‎، مضروبًا في 1 ناقص g(z)‎. لذا دعونا فقط نتحقق
من أن هذا التعبير له معنى. أولاً، إذا كان z كبيرًا جدًا، فلنقل إن z=10، فسيكون g(z)‎ قريبًا من 1، ولذلك تخبرنا الصيغة الموجودة على اليسار أن
d dz g(z)‎ ستكون قريبة من g(z)‎، وهو ما يساوي 1 في 1-1، والذي هو بالتالي قريب جدًا من 0. وهذا صحيح بالفعل، لأنه
عندما يكون z كبيرًا جدًا، فإن الانحدار يكون قريبًا من 0. وعلى العكس، إذا كانت قيمة z تساوي -10، فإنه يتم قول حسنًا هناك، ثم سيكون g(z)‎ قريبًا من 0، لذلك، تخبرنا الصيغة الموجودة على اليسار أن
d dz g(z)‎‎ ستكون قريبة من g(z)‎، وهو 0 مضروبًا في 1 ناقص 0. وبالتالي تكون قريبة جدًا من 0،
وهو صحيح. أخيرًا، إذا كان z يساوي 0، فإن g(z)‎ يساوي النصف، وهذه الدالة السينية هنا، وهكذا يساوي المشتق
نصفًا مضروبًا في 1 ناقص نصف، وهو ما يساوي ربعًا، ويتبيّن أن هذه هي
القيمة الصحيحة للمشتقة أو الانحدار لهذه الدالة، عندما
تكون قيمة "z" تساوي 0. أخيرًا، فقط لتقديم
رمز إضافي، في بعض الأحيان، بدلاً من كتابة هذا الشيء، اختصار المشتقة هو
g العدد الأولي لـ z. لذا، فإن g العدد الأولي لـ z في التفاضل والتكامل، الشرطة الصغيرة الموجودة بالأعلى تُعرف باسم العدد الأولي (prime)، لذا فإن g هو العدد الأولي لـ z
هو اختصار في حساب التفاضل والتكامل لمشتقة الدالة g،
فيما يتعلق بمتغير الإدخال z. ثم في الشبكة العصبية، لدينا a يساوي g(z)‎، يساوي ذلك، وبعد ذلك يتم تبسيط هذه الصيغة
أيضًا إلى a مضروبة في 1 ناقص a. لذا، أحيانًا خلال التطبيق، قد ترون شيئًا مثل
g عدد أولي لـ z = a مضروبًا في 1 ناقص a، ويشير هذا فقط إلى
ملاحظة أن g عدد أولي، وهو ما يعني أن المشتقة، يساوي هذا هنا. ميزة هذه الصيغة هي أنه
إذا حسبتم قيمة a بالفعل، فحينها من خلال استخدام هذا التعبير، يمكنكم حساب قيمة الانحدار
للعدد الأولي g بسرعة كبيرة أيضًا. حسنًا، كانت هذه دالة التنشيط
السينية. دعونا الآن نلقِ نظرة على
دالة التنشيط Tanh. بصورة مشابهة لما كان لدينا سابقًا، فإن تعريف d dz g(z)‎ هو انحدار g(z)‎
عند نقطة معيّنة لـ z، وإذا نظرتم إلى صيغة
الدالة الزائدة للمماس، وإذا كنتم على دراية بحساب التفاضل والتكامل، يمكنكم أخذ المشتقات
وتوضيح أنه يتم تبسيط هذا إلى هذه الصيغة واستخدام الاختصار الذي كان لدينا سابقًا
عندما نطلق على g هذا العدد الأولي لـ z مرة أخرى. لذا، إذا كنتم ترغبون، يمكنكم التحقق
من أن هذه الصيغة لها معنى. لذا، على سبيل المثال، إذا كان z يساوي 10، فإن Tanh(z)‎ ستكون قريبة جدًا من 1. وهذا ينتقل من +1 إلى -1. ثم g العدد الأولي لـ z، وفقًا لهذه الصيغة، سيكون حوالي 1 ناقص 1 مربع، والذي هو بالتالي قريب من 0. إذًا، كان هذا إذا كان z كبيرًا جدًا، فإن الانحدار يكون قريبًا من 0. وعلى العكس، إذا كانت قيمة z صغيرة جدًا، مثلاً، إذا كانت قيمة z تساوي -10، فستكون Tanh(z)‎ قريبة من -1، وبالتالي سيكون g العدد الأولي لـ z قريبًا من
1 ناقص سالب واحد مربع، وبالتالي سيكون قريبًا من 1 ناقص 1، والذي هو قريب أيضًا من 0. وأخيرًا، إذا كان z يساوي 0، فإن Tanh(z)‎ يساوي 0، وحينها يكون الانحدار
بالفعل مساويًا لـ 1، وهو بالفعل الانحدار
عندما يكون z مساويًا لـ 0. حسنًا، لكي نلخص ذلك، إذا كان a يساوي g(z)‎، إذًا، إذا كان a يساوي
Tanh(z)‎، فإن المشتقة، g العدد الأولي لـ z يساوي
1 ناقص a مربع. لذا، مرة أخرى، إذا كنتم
قد حسبتم قيمة a بالفعل، يمكنكم استخدام هذه الصيغة
لحساب المشتقة سريعًا أيضًا. أخيرًا، إليكم كيفية حساب
مشتقات دوال التنشيط
ReLU وLeaky ReLU، بالنسبة للقيمة، g(z)‎
يساوي الحد الأقصى من (0, z) لذا فإن المشتقة تساوي، يتضح أنها تساوي 0، إذا كان z أقل من 0
و1، إذا كان z أكبر من 0. وهو أمر غير محدد فنيًا،
إذا كان z يساوي 0 بالضبط. ولكن إذا كنتم تنفّذون ذلك
في البرنامج، فقد لا يكون صحيحًا بنسبة 100%
من الناحية الرياضية، ولكن سيكون هذا جيدًا
إذا كان z هو بالضبط 0، إذا قمتم بتعيين المشتقة لتكون
مساوية لـ 1. يتعين أن يكون هذا 0
بصرف النظر عن أي شيء. إذا كنت خبيرًا في التحسين،
فإنه من الناحية الفنية، g العدد الأولي يصبح ما يُعرف باسم
التدرج الفرعي لدالة التنشيط g(z)‎، وهو ما يوضح سبب استمرار
ملاءمة انحدار التدرج. ولكن يمكنكم التفكير فيها باعتبارها فرصة z أن يكون
0.000000 بالضبط. وهي قيمة صغيرة جدًا
حيث لا يهم أينما قمتم بتعيين المشتقة لتكون
مساوية للقيمة المعينة عندما يكون z يساوي 0. فعمليًا، هذا ما يطبقه الناس
على مشتقة z. أخيرًا، في حالة قيامكم
بتدريب شبكة عصبية من خلال دالة تنشيط Leaky ReLU، فإن g(z)‎ سيكون بحد أقصى،
مثلاً 0.01‎ z، z، وبالتالي سيكون العدد الأولي g لـ z مساويًا لـ 0.01، إذا كان z أقل من 0، و1، إذا كان z أكبر من 0. مرة أخرى، يكون التدرج غير
محدد فنيًا عندما يكون z مساويًا لـ 0 بالضبط، ولكن إذا قمتم بتطبيق تعليمات برمجية
تعمل على تعيين المشتقة أو تعيين g
العدد الأولي إلى 0.01 أو 1، في كلتا الحالتين، لا يهم حقًا. عندما يكون z هو 0 بالضبط
فإن التعليمة البرمجية ستعمل جيدًا. إذًا، بموجب هذه الصيغ، يجب عليكم التمكّن من حساب الانحدارات
أو المشتقات الخاصة بدوال التنشيط. الآن، ونحن نتمتع بهذا المكون الأساسي، كونوا على استعداد لمعرفة كيفية
تنفيذ انحدار التدرج للشبكة العصبية. لننتقل إلى الفيديو التالي لرؤية ذلك.