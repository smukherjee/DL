1
00:00:00,390 --> 00:00:03,860
在前面的影片中, 我寫下了
一個表單給

2
00:00:03,860 --> 00:00:05,230
羅吉斯迴歸分析的成本函數

3
00:00:05,230 --> 00:00:09,370
在這個額外的影片, 我要
給您一個快速的道理對於

4
00:00:09,370 --> 00:00:13,490
為什麼我們要使用這個成本函數對於
羅吉斯迴歸分析

5
00:00:13,490 --> 00:00:17,709
快速回顧一下，在羅吉斯迴歸分析中

6
00:00:17,709 --> 00:00:23,704
我們的預測 y-hat 
是S型函數 of w 轉置 x + b

7
00:00:23,704 --> 00:00:27,711
而S型函數是這個熟悉的函數

8
00:00:27,711 --> 00:00:34,353
我們說我們想解釋
y-hat為 p(y = 1 | x)

9
00:00:34,353 --> 00:00:39,776
我們要我們的演算法
輸出 y-hat 是

10
00:00:39,776 --> 00:00:45,030
給予一組輸入特徵 x, y = 1 的機會

11
00:00:45,030 --> 00:00:50,090
另一種方式說法是
如果 y 等於 1

12
00:00:50,090 --> 00:00:56,020
那 y 給予 
x 下等於 y-hat 的機會

13
00:00:56,020 --> 00:00:59,170
反之如果 y 等於 0 

14
00:01:00,310 --> 00:01:05,840
y 為 0 的機會是 1 - y-hat
對吧

15
00:01:05,840 --> 00:01:09,150
如果 y-hat 是 y = 1 的機會

16
00:01:09,150 --> 00:01:13,620
那  1 - y-hat 是 y = 0 的機會

17
00:01:13,620 --> 00:01:18,057
讓我拿最後這兩個方程式
複製它們到下一張投影片

18
00:01:18,057 --> 00:01:22,684
我要做的是
拿這兩個方程式

19
00:01:22,684 --> 00:01:28,010
基本上定義 p(y|x) 
在兩種情況下 y = 0 或 y = 1

20
00:01:28,010 --> 00:01:33,110
然後拿這兩個方程式
總結成為一個方程式

21
00:01:33,110 --> 00:01:37,543
要指出的是 y 一定是 0 
或 1 因為是二元分類

22
00:01:37,543 --> 00:01:41,110
y = 0 或 1 是唯二
的情況

23
00:01:41,110 --> 00:01:44,653
有人將這兩個方程式
總結成為以下

24
00:01:44,653 --> 00:01:48,774
讓我寫下來它的樣子
然後我們來解釋為什麼像這樣

25
00:01:48,774 --> 00:01:54,040
所以 y-hat 的 y 次方乘(1-y-hat) 的 (1-y) 次方

26
00:01:54,040 --> 00:01:58,920
實際上這一行
總結了上面兩個方程式

27
00:01:58,920 --> 00:02:00,500
讓我解釋一下為什麼

28
00:02:00,500 --> 00:02:04,643
第一種情況
假設 y=1, 對吧

29
00:02:04,643 --> 00:02:09,562
如果 y=1 那這個
項目會變成 y-hat

30
00:02:09,562 --> 00:02:13,970
因為這是 y-hat 的 y 次方

31
00:02:13,970 --> 00:02:21,120
這個項目是 1 - y-hat 的
 1 - 1 次方也就是 0 次方

32
00:02:21,120 --> 00:02:26,320
但任何數的 0 次方
等於 1, 所以這會不見

33
00:02:26,320 --> 00:02:33,030
所以這個公式
就是 p(y|x) = y-hat 當 y = 1

34
00:02:33,030 --> 00:02:37,480
這正是我們要的

35
00:02:37,480 --> 00:02:40,250
那第二個情況呢 
如果 y = 0?

36
00:02:40,250 --> 00:02:47,057
如果 y = 0, 那這個上面的公式
會是 p(y|x) = y-hat 的 0 次方

37
00:02:47,057 --> 00:02:51,920
但任何數的 0 次方
等於 1

38
00:02:51,920 --> 00:02:58,267
所以這等於 1 乘
 1- y-hat 的 1-y 次方

39
00:02:58,267 --> 00:03:02,770
而 1 -  y 是 1 - 0, 所以這是 1

40
00:03:02,770 --> 00:03:07,610
所以這等於 1 
乘 (1 - y-hat) 等於 1 - y-hat

41
00:03:10,700 --> 00:03:17,230
這裡我們當 y = 0 時
p(y|x) = 1 - y-hat

42
00:03:17,230 --> 00:03:21,570
這正是我們上面要的

43
00:03:21,570 --> 00:03:23,690
我們剛看過
這個公式

44
00:03:25,330 --> 00:03:30,331
是正確的 p(y|x) 的定義

45
00:03:30,331 --> 00:03:36,513
最後因為對數
函數是嚴格單調

46
00:03:36,513 --> 00:03:43,223
增加函數
您的 log of p(y|x) 最大化會給您

47
00:03:43,223 --> 00:03:48,672
最大值的p(y|x)
如果您計算 log of p(y|x)

48
00:03:48,672 --> 00:03:54,330
等於 log y-hat 
的 y 次方 1 - y-hat 的 1 - y 次方

49
00:03:54,330 --> 00:03:59,818
簡化以後是 y log y-hat

50
00:03:59,818 --> 00:04:05,881
加 1 - y 乘 log 1 - y-hat, 對吧?

51
00:04:05,881 --> 00:04:09,832
所以
這實際上是負的損失

52
00:04:09,832 --> 00:04:14,310
函數我們之前定義的

53
00:04:14,310 --> 00:04:17,470
這裡的負號是因為
通常如果您訓練一個學習

54
00:04:17,470 --> 00:04:20,460
演算法, 您想要
讓機率變大

55
00:04:20,460 --> 00:04:23,980
而在羅吉斯迴歸分析
我們這樣表示

56
00:04:23,980 --> 00:04:25,820
我們要盡量將損失函數最小化

57
00:04:25,820 --> 00:04:30,640
所以最小化損失函數相當於
最大化這個對數函數的機率

58
00:04:30,640 --> 00:04:33,925
這是損失函數
在單一訓練例子的樣子

59
00:04:33,925 --> 00:04:35,435
成本函數呢?

60
00:04:35,435 --> 00:04:40,435
整體的成本函數在
整個訓練集 m 個例子呢?

61
00:04:40,435 --> 00:04:41,385
讓我們看看

62
00:04:41,385 --> 00:04:45,710
所有的
訓練集的標籤的機率

63
00:04:45,710 --> 00:04:47,750
用非正式的方式這樣寫

64
00:04:47,750 --> 00:04:51,945
如果您假設訓練例子們
是獨立的或者說 IID

65
00:04:51,945 --> 00:04:54,198
相同獨立分佈

66
00:04:54,198 --> 00:04:57,810
那所有例子的機率是
機率的乘積

67
00:04:57,810 --> 00:05:03,143
是乘積從 i = 1 到
m p(y(i) | x(i))

68
00:05:03,143 --> 00:05:07,970
如果您要進行
最大似然估計

69
00:05:07,970 --> 00:05:12,476
您想要極大化這個
找到參數來極大化

70
00:05:12,476 --> 00:05:15,948
您的觀察跟訓練集的機率

71
00:05:15,948 --> 00:05:20,200
但極大化這個跟
極大化這個 log 是一樣的

72
00:05:20,200 --> 00:05:22,990
我們放 log 在兩邊

73
00:05:22,990 --> 00:05:28,640
所以 log of 標籤
在訓練集的機率等於

74
00:05:28,640 --> 00:05:30,990
log of 這個乘積, 變成和的 log

75
00:05:30,990 --> 00:05:39,000
就是和從 i=1 到
m log(p(y(i) | x(i))

76
00:05:39,000 --> 00:05:43,582
我們從前面
的投影片知道

77
00:05:43,582 --> 00:05:47,630
這是負的 L of y-hat(i), y(i)

78
00:05:48,850 --> 00:05:55,220
在統計學中, 有一個法則
稱為最大似然法則

79
00:05:55,220 --> 00:06:00,720
估計，也就是說選擇
參數來極大化這個

80
00:06:00,720 --> 00:06:04,220
換句話說
極大化這個

81
00:06:04,220 --> 00:06:09,510
負的和從i=1到
m L(y-hat(i)<i>,y(i)<i>)

82
00:06:09,510 --> 00:06:11,840
只是移動負號
到和的外面

83
00:06:11,840 --> 00:06:15,749
這個說明了我們的

84
00:06:15,749 --> 00:06:21,235
羅吉斯迴歸分析的成本是
 J(w,b) 

85
00:06:21,235 --> 00:06:27,349
因為我們想要最小化
這個成本, 而非最大化之

86
00:06:27,349 --> 00:06:30,095
我們去掉這個負號

87
00:06:30,095 --> 00:06:35,467
最後方便起見
我們確保這個數字是比較好的比例

88
00:06:35,467 --> 00:06:39,310
我們加上 1 除以 m 
額外的縮放比例

89
00:06:39,310 --> 00:06:43,960
總結一下, 最小化
這個成本函數 J(w,b) 我們

90
00:06:43,960 --> 00:06:48,430
使用最大似然估計
在羅吉斯迴歸分析模型

91
00:06:48,430 --> 00:06:53,120
在我們的
訓練集的假設是 IID 或者

92
00:06:53,120 --> 00:06:55,530
相同地獨立分佈

93
00:06:55,530 --> 00:06:59,550
謝謝您觀看這段影片
即使這是可選的

94
00:06:59,550 --> 00:07:03,845
我希望您一些感覺為什麼
我們使用成本函數對於

95
00:07:03,845 --> 00:07:05,200
羅吉斯迴歸分析的假設函數

96
00:07:05,200 --> 00:07:09,287
有了這個, 我希望您進入
練習, 程式練習跟

97
00:07:09,287 --> 00:07:11,277
這個禮拜的測驗

98
00:07:11,277 --> 00:07:14,735
祝您好運在測驗及
以下的練習