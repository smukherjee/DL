Ми всі чули що глибокі нейронні мережі (НМ) добре розв'язують багато задач. Але справа не лише в тому, що вони мають бути великими, а в тому, що вони мають бути глибокими, 
мати багато прихованих шарів. Чому це так? Давай розглянемо кілька прикладів і спробуємо зрозуміти чому глибокі НМ будуть добре працювати. Насамперед, що таке ці "глибокі" обчислення? Якщо ми будуємо систему розпізнавання облич або виявлення облич, то ось що може робити глибока НМ. Скажімо, ми вводимо зображення обличчя. Тоді 1-ий шар НМ можна сказати буде виявляти риси або краї. В цьому прикладі я зобразив як НМ зі, скажімо, 
20 прихованими вузлами буде "обчислювати" це зображення. Тож 20 прихованих вузлів візуалізовані в цих малих прямокутниках. Тож, наприклад, 
ця маленька візуалізація відображає як прихований вузол намагається знайти розташування краю в зображенні. І, можливо, цей прихований вузол намагається знайти горизонтальні краї зображення. Коли ми говоритимемо про згорткові НМ в наступних курсах ця часткова візуалізація матиме більший зміст. Щоб уявити обчислення 1-ого шару НМ, подивись на зображення і спробуй виділити у ньому краї. 
[Напевно маються на увазі перепади кольорів] Тепер давай спробуємо виділити краї у зображенні, групуючи пікселі, щоб сформувати ці краї. Після цього виявлені краї можна згрупувати і сформувати частини облич. Тож, наприклад, може знайтись глибший нейрон, що намагається знайти око. Або інший нейрон, що намагається знайти частину носа. Тож, групуючи багато країв, може початись виявлення різних частин облич. І, нарешті, групуючи різні частини облич (ока, носа чи підборіддя), шар може спробувати розпізнати або виявити різні типи облич. Тож інтуїтивно можна уявити, що попередні шари НМ виявляли прості функції, як от краї, потім групували їх в пізніших шарах НМ, тобто вивчали все складніші і складніші функції. Ці візуалізації матимуть більший зміст, 
коли ми говоритимемо про згорткові НМ. Одна технічна деталь цієї візуалізації: виявлячі країв розглядають відносно маленькі ділянки зображення, можливо дуже маленькі, як ось цей; а виявлячі облич, можливо, - набагато більші ділянки. Але з цього головне зрозуміти, що спочатку шукаються прості речі, як краї, а потім з них будуються або їх групують для виявлення більш складних речей, таких як око або ніс, а потім групують і ці речі, щоб знайте ще більш складні. І це ієрархічне представлення від простого до складного або композиційне представлення може застосовуватись не лише для зображень чи розпізнавання облич. Наприклад, коли ми будуємо систему розпізнавання мовлення, 
то візуалізувати це складно, але, якщо ввести аудіодоріжку, то, можливо, 1-ий шар НМ навчатиметься виявляти низькорівневі характеристики звукових хвиль, наприклад, піднімається чи спадає тон, білий шум це чи свистячий звук і наскільки стрімкі коливання в сенсі низькорівневих характеристик звукових хвиль. А далі [2-ий шар], групуючи низькорівневі характеристики, можливо, навчатиметься виявляти структурні елементи звуку, які в лінгвістиці називаються фонемами. Наприклад, в слові "кіт" "к" - фонема, "і" - фонема, "т" - ще одна фонема. Тож, можливо, навчатиметься знаходити структурні елементи звуку. А далі [3-ій шар], групуючи це, можливо, 
навчатиметься розпізнавати слова в звуковій доріжці. А далі [4-ий шар] групуватиме це, щоб розпізнати цілі фрази або речення. Тож в глибокій НМ з багатьма прихованими шарами, можливо, перші шари виявлятимуть оці низькорівневі прості характеристики, а далі глибші шари групуватимуть простіші речі, які вже виявлені, щоб виявляти більш складні речі, такі як окремі слова або навіть фрази і речення, які ми проговоримо, щоб здійснити розпізнавання мовлення. Тож ми бачимо, що перші шари обчислюють ніби відносно прості функції на вхідних даних, такі як пошук країв, а коли ми заглиблюємось в НМ, то починаємо обчислювати несподівано складні речі, такі як виявлення облич або виявлення слів, фраз чи речень. Дехто проводить аналогію між глибокими НМ і людським мозком, де ми віримо, 
чи нейронауковці вірять, що людський мозок так само починає виявляти прості речі - 
краї, що розрізняють очі - а потім складає ці речі в складніші, 
щоб виявити більш складні речі, такі як обличчя, що ми бачимо. Я думаю, що проведення аналогії між глибокими НМ і людським мозком інколи дещо небезпечне, хоча багато в чому ми не помиляємось, 
припускаючи як працює людський мозок і як він, можливо, 
насправді спочатку виявляє прості речі, такі як краї, а потім складає їх в більш складні об'єкти. Тож це послужило певною формою натхнення 
для деякого глибокого навчання (ГН). Ми поговоримо трохи більше про людський мозок або біологічний мозок пізніше на цьому тижні. Деяке розуміння того, чому глибокі НМ добре працюють, закладене ось тут. Цей висновок виведений з теорії електричних кіл, 
яка намагається визначити які типи функцій можуть бути обчислені за допомогою різних І, 
АБО та НЕ вентилів, тобто логічних вентилів. Грубо кажучи, функції, 
які ми обчислюємо відносно малими, але глибокими НМ (під малими я маю на увазі, 
що кількість прихованих вузлів відносно мала), проте, якщо ми спробуємо обчислити цю ж функцію мілкою НМ, тобто маючи досить небагато прихованих шарів, то, швидше за все, нам потрібно буде 
обчислити експоненційно більше прихованих вузлів. Тож, давай я спробую проілюструвати це в загальних рисах 
на одному прикладі. Скажімо, ми намагаємось обчислити Виключне АБО [XOR] 
або паритет всіх вхідних ознак. Тож ми намагаємось обчислити x₁ XOR x₂ XOR x₃ XOR і так до xₙ (тобто ми маємо n ознак x). Тож, якщо ми побудуємо XOR дерево ось так... 
Тобто спочатку обчислимо XOR для x₁ та x₂, далі візьмемо x₃ та x₄ і обчислимо їхнє XOR... Технічно, якщо ми використовуємо лише вентилі І, АБО і НЕ, 
то нам потрібно буде кілька шарів, щоб обчислити функцію XOR, а не просто 1 шар. 
Проте, ми зможемо обчислити XOR за допомогою відносно невеликої кількості вентилів. 
І так далі. І так ми будемо будувати справжнє XOR дерево... Ось так. Поки, врешті-решт, не отримаємо (назвімо це y) вихідний вентиль ŷ, що =y, Виключному АБО або паритету усіх вхідних бітів. Тож при обчисленні Виключного АБО 
глибина НМ буде пропорційна log(n). [O(log(n))] [незрозуміло] Тож кількість вузлів або кількість кружечків, або кількість вентилів в цій НМ не така вже й велика. Нам непотрібна велика кількість вентилів, 
щоб обчислити Виключне АБО. Але що якщо ми не можемо використовувати НМ з багатьма прихованими шарами 
(в цьому випадку O(log(n)) прихованих шарів)? Що якщо нам необхідно обчислити цю функцію лише одним прихованим шаром? 
Тобто, якщо всі ці ознаки будуть спрямовані у всі ці приховані вузли, а потім всі ці вузли - у вихідне y, то, щоб обчислити паритет (функцію XOR), цей прихований шар повинен буде бути експоненційно великим, тому що ми будемо повинні вичерпно перерахувати 
всі від 2 до n можливих комбінацій. Тобто всі по черзі від 2 до n можливих комбінацій вхідних бітів, щоб отримати в результаті 1 або 0. Тобто наш прихований шар буде експоненційно рости по відношенню до кількості бітів. По-моєму, технічно це буде 2 в степені n-1 прихованих вузлів. А 2 в степені n і є експоненційною залежністю від кількості бітів. Тож, надіюсь, це дає розуміння, що математичні функції набагато легше обчислювати глибокими НМ ніж мілкими. Мушу зазначити, що для мене особисто цей висновок 
теорії електричних кіл не надто простий для розуміння, але його дуже часто цитують коли пояснюють важливість глибокого представлення. Тепер додам ще дещо до причин, через які надають перевагу глибоким НМ. Буду повністю відвертим. Мені здається, що ще однією з причин, 
чому поняття "глибоке навчання" так широко розповсюдилось є брендинг. Ми називаємо ці речі НМ з великою кількістю 
прихованих шарів, але словосполучення "глибоке навчання" стало відомим брендом. Воно просто "дуже глибоке". Тож я думаю, що цей термін з'явився в результаті 
ребрендингу "нейронних мереж" або ребрендингу "нейронних мереж з багатьма прихованими шарами", щоб захопити уяву мас. Вважається, що бренд "глибокі НМ" добре робить свою роботу. Інколи люди дуже захоплюються і наполягають 
на використанні безлічі прихованих шарів, але коли я починаю вирішувати нову задачу, я часто починаю навіть з логістичної регресії, а потім пробую 1 або 2 приховані шари, а потім ніби використовую це 
[кількість прихованих шарів] як гіперпараметр. Використовую це як параметр або гіперпараметр, 
який можна налагоджувати, щоб спробувати знайти правильну глибину НМ. Проте, в останні кілька років була тенденція: люди виявили, що для деяких застосунків потрібні дуже-дуже глибокі НМ, 
інколи з кількома десятками шарів. Інколи це може бути і найкраща модель для задачі. Тож це все щодо розуміння чому ГН добре працює. Тепер давай до прямого розглянемо ще й механіку реалізації зворотного поширення.