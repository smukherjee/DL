1
00:00:00,000 --> 00:00:01,530
在上节课里

2
00:00:01,530 --> 00:00:06,885
我们看到通过将你的训练样本水平地堆叠进矩阵x

3
00:00:06,885 --> 00:00:11,158
能为神经网络中的传播过程推导出一个向量化的实现

4
00:00:11,158 --> 00:00:14,760
接下来我们将给出更多的解释说明

5
00:00:14,760 --> 00:00:19,775
确保我们所写下的公式是对多个样本向量化的正确实现

6
00:00:19,775 --> 00:00:25,590
让我们看一下对于几个样本(前向)传播计算过程的一部分

7
00:00:25,590 --> 00:00:27,645
对于第一个训练样本

8
00:00:27,645 --> 00:00:29,130
你最后会计算得到

9
00:00:29,130 --> 00:00:38,970
(w1)x1加上b1 然后对于第二个训练样本

10
00:00:38,970 --> 00:00:49,310
最终计算得到(w1)x2加上b1

11
00:00:49,310 --> 00:00:50,900
接下来对于第三个训练样本

12
00:00:50,900 --> 00:00:56,064
最终计算得到(w1)x3加上b1

13
00:00:56,064 --> 00:01:00,930
为了简要说明问题，接下来在幻灯片上我将省略b

14
00:01:00,930 --> 00:01:08,395
也就是说 通过令b等于0来做一点简化

15
00:01:08,395 --> 00:01:11,140
但只需对我们将要给出的讨论做一些小的修改

16
00:01:11,140 --> 00:01:14,320
就能在b是非0的情况下适用

17
00:01:14,320 --> 00:01:17,610
这些改动只是为了简化在幻灯片上的描述

18
00:01:17,610 --> 00:01:21,110
那么 w1会是一个矩阵

19
00:01:21,110 --> 00:01:25,625
所以在这个矩阵中会有一定数量的行

20
00:01:25,625 --> 00:01:28,296
观察这个对于x1的运算

21
00:01:28,296 --> 00:01:30,070
你会得到的是

22
00:01:30,070 --> 00:01:40,021
w1乘以x1得到的某个列向量 就像画出来的这样

23
00:01:40,021 --> 00:01:47,420
相似地 如果你看x2这个向量

24
00:01:47,420 --> 00:01:54,730
你会有w1乘以

25
00:01:54,730 --> 00:02:00,460
x2得到另一个列向量

26
00:02:00,460 --> 00:02:03,250
也就是得到这个z12

27
00:02:03,250 --> 00:02:06,730
最后 如果你看x3

28
00:02:06,730 --> 00:02:12,315
你会得到w1乘以x3

29
00:02:12,315 --> 00:02:19,530
得到第三个列向量 也就是这个z13

30
00:02:19,530 --> 00:02:25,250
现在 如果你将训练集记成大写的X

31
00:02:25,250 --> 00:02:31,475
它是通过将所有的训练样本堆叠在一起得到的

32
00:02:31,475 --> 00:02:37,010
矩阵X的构成是在向量x1之上

33
00:02:37,010 --> 00:02:43,430
水平堆叠x2然后再水平堆叠x3

34
00:02:43,430 --> 00:02:46,250
这是只有三个训练样本的情况

35
00:02:46,250 --> 00:02:50,371
如果有更多的样本 会像这样一直水平堆叠下去

36
00:02:50,371 --> 00:02:57,790
将w与这个矩阵x相乘

37
00:02:57,790 --> 00:03:00,190
如果考虑下矩阵乘法的工作原理

38
00:03:00,190 --> 00:03:02,680
你最终得到的第一列会是

39
00:03:02,680 --> 00:03:06,313
这些我画在这里的紫色的数值

40
00:03:06,313 --> 00:03:10,930
第二列会是这四个数值

41
00:03:10,930 --> 00:03:16,612
第三列会是这些橙色的数值

42
00:03:16,612 --> 00:03:19,480
它们最终会是这样

43
00:03:19,480 --> 00:03:27,740
当然这等于是表示为列向量的z11

44
00:03:27,740 --> 00:03:37,185
后面跟着表示为列向量的z12

45
00:03:37,185 --> 00:03:39,273
后面跟着同样被表示为列向量z13

46
00:03:39,273 --> 00:03:41,100
这是当你有三个训练样本的情况

47
00:03:41,100 --> 00:03:44,255
当有更多的样本时矩阵中会有更多的列

48
00:03:44,255 --> 00:03:51,220
因此 这个就是我们的矩阵大写的Z1

49
00:03:51,220 --> 00:03:55,230
我希望这解释了为什么我们先前使用

50
00:03:55,230 --> 00:04:02,830
w1乘以xi等于z1i

51
00:04:02,830 --> 00:04:08,310
这是当我们只考虑一个训练样本的时候

52
00:04:08,310 --> 00:04:12,565
当使用不同训练样本 并把它们堆叠到不同列的时候

53
00:04:12,565 --> 00:04:15,250
对应的结果也变成了

54
00:04:15,250 --> 00:04:18,725
向量z的按列堆叠

55
00:04:18,725 --> 00:04:24,565
我将不再展示 但是你可以利用Python广播机制自己验证

56
00:04:24,565 --> 00:04:26,245
如果把b加进来

57
00:04:26,245 --> 00:04:30,534
这些b的数值仍然是正确的

58
00:04:30,534 --> 00:04:34,540
实际上会发生的是通过Python的广播机制

59
00:04:34,540 --> 00:04:41,790
bi被分别地加到这个矩阵的每一列上

60
00:04:41,790 --> 00:04:48,220
所以在这个幻灯片上 我只证明了Z1等于

61
00:04:48,220 --> 00:04:51,980
w1X加b1

62
00:04:51,980 --> 00:04:54,020
前面幻灯片中四步中的第一步

63
00:04:54,020 --> 00:04:57,493
是一个正确的向量化

64
00:04:57,493 --> 00:04:59,990
但实际上相似的分析方法可以让你

65
00:04:59,990 --> 00:05:02,660
证明其他步骤同样正确

66
00:05:02,660 --> 00:05:08,105
使用非常相似的逻辑
如果你将输入按列进行堆叠 经过计算后

67
00:05:08,105 --> 00:05:11,510
你会得到对应的同样按列堆叠的结果

68
00:05:11,510 --> 00:05:14,970
最后总结一下我们在这个视频中所谈到的内容

69
00:05:14,970 --> 00:05:16,520
如果这是你的神经网络

70
00:05:16,520 --> 00:05:21,693
如果你想实现前向传播 那么你将要做的事情是

71
00:05:21,693 --> 00:05:27,693
从i等于1到m 每次输入一个训练样本

72
00:05:27,693 --> 00:05:34,100
然后我们像这样把训练样本按列堆叠 对于这些数值

73
00:05:34,100 --> 00:05:38,265
z1 a1 z2 a2 像下面这样进行对应列的堆叠

74
00:05:38,265 --> 00:05:43,820
这是一个对于A1的例子 但是对于Z1

75
00:05:43,820 --> 00:05:46,975
A1 Z2和A2都是一样的

76
00:05:46,975 --> 00:05:51,090
我们之前幻灯片所展示的是

77
00:05:51,090 --> 00:05:58,785
这一行向量化操作 让你能同时处理m个样本

78
00:05:58,785 --> 00:06:00,555
用类似的方法

79
00:06:00,555 --> 00:06:03,880
你会发现所有其他行

80
00:06:03,880 --> 00:06:08,811
都是所有这四行代码的正确向量化形式

81
00:06:08,811 --> 00:06:10,675
再提醒一下

82
00:06:10,675 --> 00:06:18,960
因为X也等于A0

83
00:06:18,960 --> 00:06:27,980
因为输入的特征向量x等于a0 所以xi等于a0i

84
00:06:27,980 --> 00:06:30,870
这些公式实际上具有一定的对称性

85
00:06:30,870 --> 00:06:34,110
其中第一个公式也可以被写作

86
00:06:34,110 --> 00:06:41,790
Z1等于w1A0加上b1

87
00:06:41,790 --> 00:06:45,680
可以看到这一对公式和这一对公式

88
00:06:45,680 --> 00:06:51,805
实际上看起来十分相似 只有上标之间相差1

89
00:06:51,805 --> 00:06:55,880
这实际上说明了一个神经网络中的不同层

90
00:06:55,880 --> 00:07:00,585
大致上都在做着相同的事情
或者说一遍又一遍地做着相同的计算

91
00:07:00,585 --> 00:07:04,220
这里我们有两层神经网络

92
00:07:04,220 --> 00:07:08,475
下一周的课程中我们会接触更深的神经网络

93
00:07:08,475 --> 00:07:11,670
你会看到即使是更深的神经网络本质上来说

94
00:07:11,670 --> 00:07:16,215
也只是在进行这两个步骤
仅仅比在这里看到的进行了更多次

95
00:07:16,215 --> 00:07:21,255
这就是基于多个训练样本向量化你的神经网络的方法

96
00:07:21,255 --> 00:07:25,590
到目前为止 我们的神经网络中一直在使用sigmoid函数

97
00:07:25,590 --> 00:07:27,925
这实际上并不是最好的选择

98
00:07:27,925 --> 00:07:29,675
在下个视频中 让我们进一步研究

99
00:07:29,675 --> 00:07:32,450
你该如何使用不同的 所谓的激活函数

100
00:07:32,450 --> 00:07:37,190
sigmoid函数只是其中的一个可能选择