Son videoda tek bir gizli katmanlı yapay sinir ağının neye benzediğini gördünüz. Bu videoda sinir ağlarının çıktılarını tam olarak nasıl hesapladığını detaylarıyla inceleyelim. Gördüğünüz şey biraz bağlanıma benzer ancak bunun defalarca tekrarlanması gibidir. Hadi inceleyelim. İşte iki katmanlı bir yapay sinir ağı. Bu sinir ağının hesaplamaları nasıl yaptığını daha detaylıca görelim. Daha önce söylediğimiz gibi S biçimli bağlanım s biçimli regresyonun içindeki daire, hesaplamanın iki adımını temsil eder. ilk olarak görüldüğü gibi ilk z hesaplanır sonra da z nin sigmoid fonksiyonu ile aktivasyonu hesaplarız böylece sinir ağı bunu defalarca tekrarlar şimdi de gizli katmandaki bir düğüme odaklanalım gizli katmandaki ilk düğüme bir bakalım şimdilik diğer düğümleri gri yapıyorum aynı şekilde soldaki s biçimli regresyonu da gizli katmandaki bu düğümler hesaplamanın iki adımını oluşturur ilk adım, bu düğümün sol yarısında olan ki bu düğüm z=w yi x+b ye geri dönüştürme ve bu ilk gizli katmanla ilişkili tüm miktarları hesaplamasını yapar. bu yüzden burada bir sürü köşeli parantezimiz vardır. işte bu gizli katmandaki ilk düğümdür bu yüzden de buraya indis 1 yazarız. ilk adım bunu yapar ve sonra ikinci adım a[1],1 = sigmoid of z[1],1, gibi hesaplamaları yapar yani hem z hem de a için işaretle anlatma kuralı bu şekildedir bu köşeli parantez içindeki I sizin ilk katman numaranızdır ve alt indis i ise bu katmandaki düğümü ifade eder. yani incelediğimiz düğüm katman 1, yani gizli katman, düğüm 1 bu nedenle hem üssü hem de alt indis 1 dir. böylece bu ilk daire, ilk düğüm, sinir ağındaki iki aşamalı işlemin hesaplanmasını temsil eder. şimdi de sinir ağının gizli katmanındaki ikinci düğüme bakalım. Soldaki s biçimli bağlanım birimine benzer, bu alttaki daire de hesaplamanın iki adımını ifade etmektedir. ilk adım z yi işleme koyar ve bu hala katman birdir ancak bu şimdi ikinci düğüm = W[1],2,T + b[1],2, ve then a[1],2 = sigmoid of (z [1],20, ve tekrar. İsterseniz videoyu duraklatabilirsiniz ya da kontrol edebilirsiniz ki burada morla yazdığımız üssü ve indis işaretlemeleri tutarlıdır. Şimdi de sinir ağındaki ilk iki gizli birimden bahsedeceğiz ki gizli birim üç ve dört de benzer şekilde işlem yapar. Evet şimdi buradaki denklemlerden bahsedelim, buradaki denklem çiftini diğer kopyalayalım İşte bu bizim sinir ağımız, bu birinci, bu ikinci ve bu da önceden birinci ve ikinci birimler için çalıştığımız denklemlerdir Eğer gözden geçirip üçüncü ve dördüncü birimler için uyuşan tüm denklemleri yazarsanız, şunu elde edersiniz ve işaretlemelerin doğru olduğundan emin olmalısınız, burada vector w [1] 1, bu vektör yeri değiştirilmiş x , tamam? İşte buradaki üssü T yeri değiştirilmiş vektörü temsil eder. Şimdi tahmin edebileceğiniz üzere, eğer gerçekten bir sinir ağı çalışıyorsanız bunu sadece 4 döngü ile yapmanın etkisiz olduğunu göreceksiniz. burada bu dört denklemi alıp vektörleştireceğiz. burada Z nin nasıl vektör olarak işlem yapacağını ve dönüştürmeyi göreceğiz ki siz de takip ederek yapabilirsiniz. şimdi bu W ları alalım ve bir matrise yükleyelim ve şimdi elimizde yer değiştiren have w[1], 1, var, bu bir dizin vektörü ya da bu sütun vektörü, size yer değiştirmiş dizin vektörünü sağlar w[1], 2 transpoze, w[1], 3 transpoze, w[1],4 transpoze, ve böylece 4 w vektörünü bir araya getirerek bir matris elde edersiniz. böyle düşünmenin bir diğer yolu da şudur: burada dört s bağlanımlı birimimiz var ve her bir s bağlanımlı biriminin karşılıklı W vektör parametresi var ve bu dört vektörü bir araya getirerek bu 4 e 3 matrisini elde ederiz. eğer bu matrisi alır ve girdilerinizle çarparsanız x1, x2, x3, matrisin çarpma işlemini nasıl yaptığını görerek W1[1] transpoze x , W2[1] transpoze x, W3[1] transpoze x, W4[1] transpoze x elde edersiniz ve b'leri unutmayınız. ve şimdi buna b[1],1, b[1], 2, b[1], 3 b[1],4, ekliyoruz ki temel olarak böyledir sonra da b[1]1, b[1]2, b[1]3, and b[1]4, ve görüyouz ki buradaki dört çıktı dizini her bir çıktı dizini ile yukarıdakiler birbirine tamamen uyumlu. diğer bir deyişle görüyoruz ki burası sonuç olarak z [1], 1 z[1], 2, z[1], 3, z[1],4, ye burada tanımlandığı gibi eşit ve doğal olarak buradaki her şeye z[1] diyeceğiz çünkü hepsi sütun vektöründeki z'lerden alınarak bir araya getirilmiştir. vektör oluştururken konumlama yapmakta size yardımcı olabilecek şeylerden biri de katman içerisinde farklı düğümlerimizin olması ve bunları dikey olarak birleştireceğimizdir. İşte bu yüzden Z[1]1 den Z[1]4, gizli katmanda birbiriyle iletişimli dört farklı düğümümüz var ve biz bu dört numarayı z[1] vektöründen bir tane daha işaretleme kullanmak için dikey olarak alırız, bu 4 e 3 matrisini küçük w[1],1, w[1],2, ve diğerlerini yığınlayarak elde ettik ki bu matrise W[1] diyeceğiz, benzer olarak bu vektör b üssü köşeli parantez 1 olarak gösterilecek ki bu da (4,1) vektörüdür. Şimdi vektör matris gösterimi ile Z yi işleme koyduk yapmamız gereken son şey buradaki a değerlerini işleme koymak ki doğal olarak bu gördüğünüzü a[1] diye tanımlayacağız çünkü bu a[1],1 den a[1],4. e kadar olan değerlerden gelir. şimdi bu dört değeri alıp a[1] de bir araya getirelim, bu da (z[1]) in sigmoid i olacak, çünkü bu z nin dört elementi sigmoid fonksiyonunun uygulanmasından gelmektedir. Şimdi toparlayalım, anladık ki z[1] = W[1] x + b[1] ve a[1] defa z[1]. Şimdi bunu bir sonraki slayta kopyalayalım ve görüyoruz ki sinir ağının ilk katmanında x girdisi için elimizde z[1] = W[1] x + b[1] ve (z[1]) in sigmoid i a[1] var ki bunlar da (4,1) = (4,3) x (3,1) + (4,1), ve (4,1) gibi boyutlarıdır. Hatırlayalım ki x = a[0] tıpkı y-hat = a[2] dediğimiz gibi bu yüzden isterseniz buradaki x'i alıp 0 ile yer değiştirebilirsiniz ki isterseniz bu x girdisinin vektörünün diğer yanı, başka adıdır. Şimdi benzer bir türetme ile bir sonraki katmanı anlayabiliriz ki bu katman da çıktı katmanına çok benzerdir başlangıç kısmıyla alakalı olarak w[2] ve b[2] diyoruz bu durum için w[2] (1,4) matrisi olacaktır ve b[2] ise (1,1) gibi gerçek sayı olacaktır. z[2] nin matris olarak gerçek sayı ile yazımı (1,1) dir burası ise (1,4) defa (4,1) artı b(1,1) olacak ve bu sadece gerçek bir sayı verir ve bu son çıkış birimi w ve b parametreleri olan örneksel bir mantıksal ilerleme olarak düşünülebilir W , w[2] devriğinin örneksel rolünü oynar ya da W2 gerçekten w'nin devriğidir ve b, b[2] ye eşittir. şimdi bu soldaki ağı işaretleyelim ve şimdilik görmezden gelelim evet şimdi bu son üst birim daha çok mantıksal regresyon gibidir parametreleri w ve b olarak yazmanın dışında bunun yerine biz onları (1,4 ve (1,1) boyutları ile w[2] ve b[2] olarak yazacağız. Tekrarlamak adına; ayarlama regresyonunu çıktıya entegre etmek için ya da tahmine entegre etmek için z = w , x + b, ve y-hat = a = sigmoid of (z) yi işleme koyun Gizli katmanlı bir sinir ağı biriminiz olduğunda çıktıyı işleyebilmek için sadece bu dört denkliği uygulamanız gerekir. Bunları gizli katmandaki bu dört mantıksal regresyon çıktısının işleme konmasının uygulamasının vektörleştirilmesi olarak düşünebilirsiniz ki öyledir Umarım bu tanım size mantıklı gelmiştir ancak sinir ağı çıktısını işleme koymanız için gereken her şey bu dört kod satırıdır. Evet şimdi, dört satır kod ile tek x vektör girdisinin sinir ağı çıktısını nasıl işleme koyulduğunu gördünüz. mantıksal regresyonda yaptığınıza benzer olarak çapraz çoklu çalışma örneklerimizi vektörize etmek isteyebilirsiniz de ki bunu göreceğiz bu da matristeki farklı sütunlardaki üç örneği bir araya getirerek az değişiklikle olur Ek olarak, mantıksal regresyonda gördüğünüze benzer şekilde sadece bir örnek üzerinden değil, tüm eğitiminiz boyunca örnekler üzerinden sinir ağının çıktısının işleyebilir olacaksınız. Şimdi detayları bir sonraki videoda görelim.