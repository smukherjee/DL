1
00:00:00,000 --> 00:00:03,840
Certo, acho que este é vídeo legal.

2
00:00:01,800 --> 00:00:06,240
Neste vídeo você verá como

3
00:00:03,840 --> 00:00:08,730
implementar gradiente decrescente para sua

4
00:00:06,240 --> 00:00:10,530
rede neural com uma camada oculta.

5
00:00:08,730 --> 00:00:12,809
Neste vídeo já darei a você

6
00:00:10,530 --> 00:00:14,639
as equações que você 
precisa implementar

7
00:00:12,809 --> 00:00:17,039
para que possa fazer a 
retropropagação, que

8
00:00:14,639 --> 00:00:19,410
faz o gradiente decrescente 
funcionar e, depois, no próximo

9
00:00:17,039 --> 00:00:21,510
vídeo, darei uma noção

10
00:00:19,410 --> 00:00:24,150
maior sobre porque estas equações

11
00:00:21,510 --> 00:00:26,070
específicas são as mais apropriadas, ou

12
00:00:24,150 --> 00:00:27,630
as mais corretas, para o cálculo de

13
00:00:26,070 --> 00:00:28,289
gradientes, que você precisa na sua rede

14
00:00:27,630 --> 00:00:30,090
neural.

15
00:00:28,289 --> 00:00:32,520
Então, sua rede neural com uma única

16
00:00:30,090 --> 00:00:39,930
camada oculta, agora terá

17
00:00:32,520 --> 00:00:44,760
parâmetros w'¹' , b'¹', w'²' 
e b'²', e assim por diante.

18
00:00:39,930 --> 00:00:50,399
Como um lembrete, se 
você tem nₓ, ou de outra forma,

19
00:00:44,760 --> 00:00:56,640
n'⁰' características de entrada, 
n'¹' unidades ocultas

20
00:00:50,399 --> 00:00:59,149
e n'²' unidades de saída no nosso

21
00:00:56,640 --> 00:01:05,670
exemplo, até agora, n'²' = 1
[uma única unidade de saída].

22
00:00:59,149 --> 00:01:08,880
Então, a matriz w'¹' 
terá dimensões n'¹' por n'⁰',

23
00:01:05,670 --> 00:01:11,250
b'¹' será um vetor de dimensão n'¹'.

24
00:01:08,880 --> 00:01:13,350
Então, você pode escrever 
como uma matriz de

25
00:01:11,250 --> 00:01:16,500
dimensão n'¹' x 1, na 
realidade um vetor

26
00:01:13,350 --> 00:01:20,750
coluna. A dimensão de 
w'²'será n'²' por

27
00:01:16,500 --> 00:01:26,759
n'¹' e a dimensão de b'²' será

28
00:01:20,750 --> 00:01:28,590
n'²' por 1. Certo, até 
o momento vimos

29
00:01:26,759 --> 00:01:30,930
somente exemplos onde n'²' é igual a

30
00:01:28,590 --> 00:01:36,930
1, onde temos apenas uma única

31
00:01:30,930 --> 00:01:39,570
unidade oculta. Então, 
você também tem uma função

32
00:01:36,930 --> 00:01:41,340
de custo para sua rede neural, e

33
00:01:39,570 --> 00:01:44,220
por agora, irei 
assumir que você está

34
00:01:41,340 --> 00:01:48,659
fazendo classificação 
binária. Então, neste

35
00:01:44,220 --> 00:01:51,740
caso, o custo dos 
seus parâmetros, como

36
00:01:48,659 --> 00:01:57,090
segue, será 1/m da

37
00:01:51,740 --> 00:01:59,969
média desta função 
perda, e assim, o ʆ aqui

38
00:01:57,090 --> 00:02:03,420
é a perda quando a sua rede neural

39
00:01:59,969 --> 00:02:06,240
prevê ŷ. Este é 
na verdade a'²',

40
00:02:03,420 --> 00:02:07,649
quando o rótulo base 
deveria ser igual a y,

41
00:02:06,240 --> 00:02:09,629
e se você está fazendo classificação

42
00:02:07,649 --> 00:02:12,510
binária, a função de perda pode ser

43
00:02:09,629 --> 00:02:15,030
exatamente o que você 
usou previamente na regressão

44
00:02:12,510 --> 00:02:18,420
logística. Então, para 
treinar os parâmetros no seu

45
00:02:15,030 --> 00:02:21,450
algoritmo, você 
precisará realizar gradiente

46
00:02:18,420 --> 00:02:23,189
decrescente. Ao treinar 
uma rede neural,

47
00:02:21,450 --> 00:02:25,379
é importante inicializar os

48
00:02:23,189 --> 00:02:26,129
parâmetros aleatoriamente ao invés de

49
00:02:25,379 --> 00:02:28,140
inicializá-los todos com zero.

50
00:02:26,129 --> 00:02:30,030
Veremos mais tarde o porquê, mas

51
00:02:28,140 --> 00:02:32,069
depois de inicializar 
os parâmetros com algo,

52
00:02:30,030 --> 00:02:34,140
cada laço de repetição, 'loop',
 do gradiente decrescente

53
00:02:32,069 --> 00:02:36,780
calcularia as previsões.

54
00:02:34,140 --> 00:02:42,359
Então, você basicamente
calcula ŷ⁽ⁱ⁾,

55
00:02:36,780 --> 00:02:44,519
para i = 1 até m, e depois

56
00:02:42,359 --> 00:02:49,440
você precisa calcular 
as derivadas. Então,

57
00:02:44,519 --> 00:02:51,420
você precisa calcular 
dw'¹' e esta é

58
00:02:49,440 --> 00:02:54,359
a derivada da função de custo

59
00:02:51,420 --> 00:02:56,489
com relação ao parâmetro w'¹'.

60
00:02:54,359 --> 00:02:59,220
Você precisa calcular 
uma outra variável

61
00:02:56,489 --> 00:03:00,870
que chamamos db'¹', que é a

62
00:02:59,220 --> 00:03:04,109
derivada ou a 
inclinação da sua função

63
00:03:00,870 --> 00:03:07,349
de custo com 
relação à variável b'¹'

64
00:03:04,109 --> 00:03:10,170
e assim por diante, da 
mesma forma para os outros

65
00:03:07,349 --> 00:03:12,629
parâmetros w'²' e b'²'. 
E finalmente,

66
00:03:10,170 --> 00:03:17,879
a atualização do 
gradiente decrescente seria

67
00:03:12,629 --> 00:03:22,709
atualizar w'¹' como 
w'¹' menos alfa, a

68
00:03:17,879 --> 00:03:26,129
a taxa de aprendizagem 
vezes dw'¹' recebe

69
00:03:22,709 --> 00:03:31,620
b'¹' menos a taxa de aprendizagem

70
00:03:26,129 --> 00:03:34,739
db'¹', e da mesma 
forma para w'²' e b'²'.

71
00:03:31,620 --> 00:03:36,299
E às vezes eu uso ":=",

72
00:03:34,739 --> 00:03:38,489
e às vezes "=", qualquer uma das

73
00:03:36,299 --> 00:03:40,829
notações funciona bem. 
Assim, esta seria

74
00:03:38,489 --> 00:03:42,510
uma iteração do 
gradiente decrescente, e então

75
00:03:40,829 --> 00:03:44,280
você repete isto algumas vezes

76
00:03:42,510 --> 00:03:46,079
até os seus parâmetros pareçam

77
00:03:44,280 --> 00:03:48,150
estar convergindo. 
Em vídeos anteriores,

78
00:03:46,079 --> 00:03:50,099
falamos sobre como calcular

79
00:03:48,150 --> 00:03:51,629
previsões, como calcular as saídas,

80
00:03:50,099 --> 00:03:54,060
e também vimos como fazer isso

81
00:03:51,629 --> 00:03:56,269
na forma de vetorização. 
Então, a chave é

82
00:03:54,060 --> 00:04:00,180
saber como calcular estas derivadas

83
00:03:56,269 --> 00:04:04,079
parciais, 
 dw'¹', db'¹', e também

84
00:04:00,180 --> 00:04:06,780
as derivadas dw'²' 
e db'²'. Então, o que

85
00:04:04,079 --> 00:04:09,419
eu gostaria de fazer é apenas dar as

86
00:04:06,780 --> 00:04:12,150
equações que você 
precisa para calcular

87
00:04:09,419 --> 00:04:14,699
essas derivadas e vou deixar para o

88
00:04:12,150 --> 00:04:17,430
próximo vídeo, que é um vídeo opcional,

89
00:04:14,699 --> 00:04:19,090
para aprofundarmos 
em como chegamos

90
00:04:17,430 --> 00:04:21,400
nestas fórmulas.

91
00:04:19,090 --> 00:04:26,169
Deixe-me resumir novamente as

92
00:04:21,400 --> 00:04:33,250
equações para propagação para frente.

93
00:04:26,169 --> 00:04:37,900
Temos Z'¹' = w'¹'X + b'¹' e então

94
00:04:33,250 --> 00:04:41,680
A'¹' igual a função de ativação,
g'¹'

95
00:04:37,900 --> 00:04:49,690
nesta camada, aplicado ao Z'¹',
 A'¹' = g'¹' (Z'¹')

96
00:04:41,680 --> 00:04:53,530
e para Z'²' = w'²' A'¹' + b'²',

97
00:04:49,690 --> 00:04:55,180
e finalmente, estas são todas

98
00:04:53,530 --> 00:05:01,210
vetorizadas através dos conjuntos de treinamento certo,

99
00:04:55,180 --> 00:05:02,740
A'²' = g'²' (Z'²'). Novamente,

100
00:05:01,210 --> 00:05:04,870
se assumimos que 
estamos fazendo classificação

101
00:05:02,740 --> 00:05:06,610
binária, então esta função de

102
00:05:04,870 --> 00:05:08,560
ativação realmente deveria ser a função

103
00:05:06,610 --> 00:05:11,080
sigmoide σ ( Z'²' ).
Então, vou jogar ela aqui.

104
00:05:08,560 --> 00:05:13,870
Esta é a propagação para frente ou

105
00:05:11,080 --> 00:05:15,729
cálculos para frente, 
da esquerda para direita, na

106
00:05:13,870 --> 00:05:18,430
sua rede neural. 
A seguir, vamos calcular

107
00:05:15,729 --> 00:05:24,750
as derivadas. Então, esta é a etapa de

108
00:05:18,430 --> 00:05:30,750
retropropagação. Ela calcula

109
00:05:24,750 --> 00:05:33,610
dz'²' = A'²' - Y,

110
00:05:30,750 --> 00:05:36,580
e somente como 
um lembrete, tudo isso

111
00:05:33,610 --> 00:05:41,289
é vetorizado através do exemplo 
de treinamento, então a matriz Y

112
00:05:36,580 --> 00:05:45,280
é esta matriz 1 x m,
 que lista todos

113
00:05:41,289 --> 00:05:51,370
os m exemplos,
empilhados horizontalmente.

114
00:05:45,280 --> 00:05:55,330
Resulta que dw'²' é 
igual a isso. Na verdade,

115
00:05:51,370 --> 00:05:58,870
estas 3 primeiras equações são muito

116
00:05:55,330 --> 00:06:00,900
parecidas com gradiente 
decrescente em regressão

117
00:05:58,870 --> 00:06:00,900
logística.

118
00:06:00,960 --> 00:06:12,610
axis = 1, keepdims = true.

119
00:06:07,419 --> 00:06:15,580
Só um pequeno detalhe:
este np.sum

120
00:06:12,610 --> 00:06:18,070
é um comando Python numpy para o

121
00:06:15,580 --> 00:06:21,100
somatório através de uma dimensão de

122
00:06:18,070 --> 00:06:24,810
uma matriz, neste caso, 
somando horizontalmente.

123
00:06:21,100 --> 00:06:27,600
E o que keepdims faz é prevenir Python

124
00:06:24,810 --> 00:06:31,230
de jogar na saída 
aqueles vetores

125
00:06:27,600 --> 00:06:34,650
estranhos com 1, onde as dimensões eram

126
00:06:31,230 --> 00:06:37,010
"n,". Então, este comando

127
00:06:34,650 --> 00:06:41,280
assegura que

128
00:06:37,010 --> 00:06:44,580
a saída do Python 
para o db'²', um vetor que é

129
00:06:41,280 --> 00:06:47,820
n por 1. Na verdade, 
tecnicamente, isto será

130
00:06:44,580 --> 00:06:50,130
imagino que n'²' por 1. 
Neste caso, é

131
00:06:47,820 --> 00:06:53,520
apenas um número 1 x 1, 
então, na verdade

132
00:06:50,130 --> 00:06:56,790
não importa, mas depois veremos

133
00:06:53,520 --> 00:06:58,500
quando realmente importar. 
Então, até o momento,

134
00:06:56,790 --> 00:07:01,320
o que fizemos é muito similar à regressão

135
00:06:58,500 --> 00:07:03,919
logística, mas agora conforme você

136
00:07:01,320 --> 00:07:14,370
continua a rodar retropropagação, você

137
00:07:03,919 --> 00:07:19,380
calcularia dz'²' = W'²' ⸆ dz'²' vezes g'¹' ' (Z'¹').
 [W'²' transposta vezes a derivada de Z'²' e sigmoide g'¹' linha de Z'¹'].

138
00:07:14,370 --> 00:07:20,880
Então, esta quantidade g'¹' linha é a

139
00:07:19,380 --> 00:07:22,919
derivada da função de 
qualquer que seja a

140
00:07:20,880 --> 00:07:25,770
derivada da função de 
ativação que você usa para a

141
00:07:22,919 --> 00:07:27,030
camada oculta, e para 
a camada de saída, eu

142
00:07:25,770 --> 00:07:29,400
assumo que você 
esteja fazendo classificação

143
00:07:27,030 --> 00:07:30,780
binária com a função sigmoide.

144
00:07:29,400 --> 00:07:34,620
Então, isso já está embutido naquela

145
00:07:30,780 --> 00:07:39,090
fórmula para dz'²'. 
E esta multiplicação é um

146
00:07:34,620 --> 00:07:43,050
Produto de Hadamard, então isso

147
00:07:39,090 --> 00:07:46,950
será uma matriz
n'¹' por m,

148
00:07:43,050 --> 00:07:48,990
e isso aqui é um 
Produto de

149
00:07:46,950 --> 00:07:52,680
Hadamard, e que também será

150
00:07:48,990 --> 00:07:54,720
uma matriz n'¹' por m, e 
assim, esta multiplicação ali

151
00:07:52,680 --> 00:07:59,669
é outro Produto de Hadamard de duas

152
00:07:54,720 --> 00:08:08,490
matrizes. Finalmente, dw'¹' é igual a

153
00:07:59,669 --> 00:08:18,950
isto, e db'¹' é igual a isso, e

154
00:08:08,490 --> 00:08:21,900
np.sum (dz'¹' , axis = 1,

155
00:08:18,950 --> 00:08:23,430
keepdim = True), em que,

156
00:08:21,900 --> 00:08:27,210
anteriormente o keepdim importa

157
00:08:23,430 --> 00:08:28,590
menos se n'²' é igual a 1, então isso

158
00:08:27,210 --> 00:08:35,729
é uma dimensão 1 x 1, 
logo apenas um número

159
00:08:28,590 --> 00:08:38,370
real. Aqui, db'¹' será 
um vetor n'¹' por 1,

160
00:08:35,729 --> 00:08:40,110
então no Python você quer que

161
00:08:38,370 --> 00:08:43,110
np.sum devolva uma saída nessa

162
00:08:40,110 --> 00:08:46,529
dimensão, ao invés de um vetor

163
00:08:43,110 --> 00:08:48,360
estranho desta 
dimensão, que poderia acabar

164
00:08:46,529 --> 00:08:50,580
esculhambando alguns de seus cálculos

165
00:08:48,360 --> 00:08:53,310
posteriores. Outra forma seria não

166
00:08:50,580 --> 00:08:56,910
ter o parâmetro keepdim, mas

167
00:08:53,310 --> 00:09:00,060
chamar explicitamente 
"reshape" para remodelar

168
00:08:56,910 --> 00:09:04,400
a saída de np.sum nessa

169
00:09:00,060 --> 00:09:08,310
dimensão que você 
quer que db tenha. Então, isso

170
00:09:04,400 --> 00:09:11,339
foi propagação para frente em, acredito,

171
00:09:08,310 --> 00:09:14,310
quatro equações e 
retropropagação em, mais ou

172
00:09:11,339 --> 00:09:16,680
menos, seis equações. Eu 
sei que eu acabei de escrever

173
00:09:14,310 --> 00:09:18,870
estas equações, mas no próximo vídeo

174
00:09:16,680 --> 00:09:22,050
optativo, vamos rever alguns

175
00:09:18,870 --> 00:09:23,940
conceitos de como as seis equações dos

176
00:09:22,050 --> 00:09:25,830
algoritmos de retropropagação foram

177
00:09:23,940 --> 00:09:27,750
derivados. Por favor, fique 
à vontade para assisti-los

178
00:09:25,830 --> 00:09:30,000
ou não, mas de qualquer 
forma, se você implementar

179
00:09:27,750 --> 00:09:32,730
esses algoritmos, você 
terá uma implementação

180
00:09:30,000 --> 00:09:34,650
correta de propagação 
para frente e retropropagação,

181
00:09:32,730 --> 00:09:36,750
e você será capaz de calcular as

182
00:09:34,650 --> 00:09:39,029
derivadas que você precisa para aplicar

183
00:09:36,750 --> 00:09:41,520
gradiente decrescente de 
forma a aprender os parâmetros

184
00:09:39,029 --> 00:09:43,680
de sua rede neural. É possível

185
00:09:41,520 --> 00:09:45,209
implementar este 
algoritmo e fazê-lo funcionar

186
00:09:43,680 --> 00:09:47,130
sem conhecer profundamente os

187
00:09:45,209 --> 00:09:50,520
cálculos. Muitos profissionais 
de aprendizagem profunda

188
00:09:47,130 --> 00:09:52,320
de sucesso fazem isso, mas se você

189
00:09:50,520 --> 00:09:54,180
quer, você também 
pode assistir o próximo vídeo

190
00:09:52,320 --> 00:09:56,580
apenas para ter mais noção sobre

191
00:09:54,180 --> 00:09:58,820
a origem destas 
[Tradução: Renato Barata Gomes | Revisão: Carlos Lage]

192
00:09:56,580 --> 00:09:58,820
equações