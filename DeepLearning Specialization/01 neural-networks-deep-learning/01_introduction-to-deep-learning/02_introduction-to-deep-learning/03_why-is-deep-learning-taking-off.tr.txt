deep learning ve neural networks arkasındaki temel fikirler onlarca yıldır mevcut ise neden şuanda popülerliğini kazanıyor bu videoda, deep learning'deki yükselen popülerliğini nedenlerini ve arkasındaki sebepleri inceleyeceğiz Çünkü, inanıyorum ki bu size organizasyonunuzun içerisinde hangi fırsatları daha iyi kullanabileceğiniz konusunda yardımcı olacaktır. Son birkaç yıldır bir çok kişi bana "Andrew, neden derin öğrenmenin çok daha iyi sonuç veriyor" şeklinde sorular soruyor ve bana bu soru sorulduğunda genellikle bu resmi onlar için çizerim. Diyelim ki yatay eksende bir görev için elimizde mevcut olan veri miktarını belirttiğimiz bir şekil çizelim. ve diyelim ki dikey eksende öğrenme algoritmamızın performansını çizelim örneğin spam sınıflandırıcısı veya reklam tıklamamızın ya da sürücüsüz araba sistemimizin diğer arabaların konumlarının belirlenmesindeki keskinlik oranı gibi. Görünen o ki eğer geleneksel öğrenme algoritmasının performansını veri miktarına göre çizecek olursak örneğin destek yöney makinası(support vector machine) geleneksel bir öğrenme algoritmasının performansını çizerseniz, buna benzer bir eğriye elde edebiliriz. Bu eğride veri ekledikçe performans artış gösterir fakat belli bir süre sonunda performans plato şekline dönüşür yani düz şekilde ilerler. Bunun düz bir çizgi olması gerekiyordu, çok iyi çizemedim. Burada algoritma sanki çok fazla veriyi nasıl kullanacağını bilmiyormuş gibi davranış sergilemektedir. Toplumumuzda son 20 yılda olan şey ise, birçok problem için eskiden az miktarda veriye sahipken şu anda çok daha fazla miktarda veriye sahibiz ve bunun en temel sebebi ise toplumumuzdaki dijitalleşmeden kaynaklı artık insan aktivitelerinin birçoğunun verisi dijital ortamda mevcut çünkü artık bilgisayarda, internet sitelerinde, mobil uygulamalarda, çok daha fazla vakit geçiriyoruz ve bu dijital cihazlar fazla miktarda veri oluşturuyorlar ayrıca artık telefonlarımıza dahil olan ucuz kameralara sahip olmamız ve nesnelerin interneti çağında(internet of things) telefonlarımızda ivmeölçer gibi diğer birçok sensöre sahip olmamız da bir diğer sebep. Dolayısıyla, son 20 yılda birçok uygulamada geleneksel öğrenme algoritmalarının avantaj sağlayabileceğinden daha fazla veriyi biriktirmiş bulunuyoruz. Sinir ağlarında ise eğer küçük bir sinir ağı eğitirseniz, bu durumda performans şuna benzeyecektir ve bir miktar daha büyük bir sinir ağı eğitirseniz, diyelim ki orta büyüklükte bir sinir ağı olsun, performans bir miktar daha iyi olur ve çok büyük bir sinir ağı eğitirseniz bu durumda yine performans daha iyi olmaya devam edecektir. Birkaç gözlemde bulunalım; birinci olarak, eğer yüksek seviye bir performansı hedefliyorsanız bu durumda iki şeye sahip olmalısınız, birincisi büyük miktardaki veriden yararlanabilmek için büyük bir sinir ağını eğitebilmelisiniz ikincisi ise fazla miktarda veriye sahip olmalısınız ve genellikle bu durumu 'ölçek' derin öğrenme ilerleyişinin temel sebebidir olarak açıklıyoruz ve ölçekten kastetiğimiz şey hem sinir ağının büyüklüğü yani çok fazla gizli üniteye sahip,çok fazla bağlantı ve çok fazla parametreye sahip bir sinir ağı ve aynı zamanda verinin ölçeği. Günümüzde bir sinir ağından daha iyi performans almanın yöntemi ya daha büyük bir sinir ağı eğitmek
 ya da daha fazla veriyle beslemekten geçiyor ve bunlar sadece belli bir noktaya kadar çalışır çünkü eninde sonunda elinizdeki veri miktarı yeterli gelmemeye
başlar ya da elinizdeki sinir ağı çok büyük olduğundan eğitmesi çok uzun sürer fakat sadece 'ölçeği' geliştirmek bize derin öğrenme dünyasında çok fazla şey kazandırmıştır Bu şemayı teknik olarak daha doğru bir hale getirmek için birkaç şey eklememe izin verin. X eksenine veri miktarı yazdım, teknik olarak buna etiketli veri miktarı diyebiliriz burada etiketli veriden kastım input x ve etiket y ikilisidir. Bu kursun ilerleyen kısımlarında da kullanılmak üzere bir notasyon paylaşmam gerekirse, eğitim setimizin boyutunu ya da eğitim örneklerinin sayısını belirtmek için küçük m harfini kullanacağız.Yani yatay ekseni burada m ile belirtebiliriz. Bu şekil hakkında birkaç detay daha vermek gerekirse, burada az sayıda eğitim örneğinin olduğu durumlarda, algoritmaların sıralanışı aslına bakılırsa çok belli değildir. Eğer çok fazla eğitim veriniz yoksa elle mühendislikle oluşturulmuş özellikler performansın belirleyici faktörü olacaktır dolayısıyla örnek verirsek az sayıda eğitim örneğinde elle mühendislik yaparak SVM algoritması ağiten biri büyük derin öğrenme ağı eğiten birinden az verinin olduğu durumlarda daha iyi performansa ulaşabilir. Dolayısıyla bunun gibi az sayıda örneğe sahip olduğumuz durumlarda algoritmaların performanslarının karşılaştırılması çok iyi tanımlanmamıştır ve performans elle mühendislikle bulunmuş özelliklerden ve algoritmanızın diğer küçük ayrıntılarından çok daha fazla etkilenecektir. Sadece çok fazla veriye sahip olduğumuzda daha kararlı olarak büyük sinir ağlarının daha iyi performans göstereceğini ve listede üst sıralara yerleşeceğini söyleyebiliriz. Eğer arkadaşlarınızdan biri gelip size sinir ağlarının neden popülerleştiğini sorarsa, size onlar için bu çizimi çizmenizi tavsiye ediyorum. Modern derin öğrenmenin yükselişinin ilk zamanlarında veri miktarındaki artış ve işlem kapasındaki artış yani Cpu ya da Gpu üzerinde büyük sinir ağları eğitebiliyor olmamız bize epey ilerleme kaydettirdi fakat özellikle son birkaç yıldır algoritmalarda büyük innovasyonlara şahit olduk bunun önemini de azımsamak istemem. İlginç olarak algoritmalardaki innovasyonların birçoğu sinir ağının daha hızlı çalışması için yapılıyor. Somut bir örnek vermek gerekirse, sinir ağlarındaki en büyük devrimlerden bir tanesi burada gördüğünüz sigmoid fonksiyonundan önceki videoda kısaca bahsemiş olduğumuz şuraya gözüken relu fonksiyonuna geçmekti. Söyleyeceklerimin detaylarını anlamıyorsanız bunun hakkında endişelenmeyin. Makine öğreniminde sigmoid fonksiyonunu kullanmanın problemlerinden bir tanesi fonksiyonun eğimininin neredeyse sıfır olması dolayısıyla buradaki bölgelerde öğrenme çok yavaş bir hale gelir çünkü gradyan inişini burada uyguladığınız zaman eğim sıfır olduğundan parametreler çok yavaş değişir yani öğrenme çok yavaşlar bunun yanısıra Rectified Linear Unit (RELU)(doğrultulmuş lineer ünite) adı verilen bu fonksiyonu kullandığımızda ise gradyan inişi pozitif her girdi için 1 e eşitlenmiş olur dolayısıyla gradyan çok daha az bir ihtimalle yavaşça sıfıra iner. Bu bölgedeki eğim, doğrunun eğimi sıfırdır fakat görünen o ki sadece sigmoidten relu ye geçmemiz gradyan inişi algoritmasının çok daha hızlı ve verimli çalışmasına vesile olur. Bu örnek göreceli olarak basit bir algoritmasal innovasyondu fakat sonuç olarak bu innovasyonun amacı işlem zamanını azaltmaktı buna benzer şekilde algoritmaları daha hızlı çalışması için değiştirdiğimiz bir sürü örnek mevcut çünkü algoritmaların daha hızlı çalışması daha büyük sinir ağları eğitebilmemizi ya da büyük sinir ağlarını kabul edilebilir zaman içerinde çalıştırmamızı sağladı. Hızlı işlem yapabilmenin önemli olmasının bir diğer sebebine gelelim. görünen o ki Sinir ağlarını eğitmek tekrarlayıcı bir işlemdir. Genellikle sinir ağı mimarisi için bir fikriniz olur ve bu fikri uygularsınız ve kodlarsınız bu size sizir ağınızın nasıl sonuç verdiğini anlamanızı sağlayan bir deneyi çalıştırma imkanı verir ve buna bakarak sinir ağınızın detaylarında değişikliğe gidersiniz ve daha sonra bu döngüde tekrar tekrar devam edersiniz. Eğer sinir ağınız eğitim için çok fazla zaman alıyorsa, bu döngü etrafında hareket etmek çok fazla zaman alır ve bir fikriniz olduğunda bunu uygulayıp çalışıp çalışmadığına örneğin 10 dk da bakabilmekle 1 günde bakabilmek veya belki de 1 ayda bakabilmek(bazen 1 aya kadar sürebilir) arasında üretkenliğiniz açısından ciddi fark vardır. Kısa zamanda sonuç alarak çok daha fazla farklı fikir deneyebilir ve uygulamanız için iyi çalışan bir sistem geliştirme ihtimaliniz artacaktır. Dolayısıyla hızlı işlem yapabilme yeteneği deneysel sonuçları alabilme hızınızı artırır ve bu hem sinir ağı uygulayamacılarına hem de derin öğrenme üzerine çalışan araştırmacılara çok daha hızlı ve tekrarlayıcı çalışabilmek adına yardım etmektedir. Tüm bunlar tüm derin öğrenme topluluğuna yeni algoritmalar inşa etme ve durma olmaksızın ilerleme kaydetme konusunda büyük faydalar sağlamaktadır. Bunlar derin öğrenmenin yükselişinde etkili olan bazı güçlerdir. İyi haber ise bu güçlerin derin öğrenmenin gelişmesinde hala epey etkili olduklarıdır. Veriyi düşünün, toplum gün geçtikçe daha fazla dijital veri üretmektedir ya da işlem kapasitesini düşünün Gpu'lar ve hızlı ağlar gibi birçok özelleştirilmiş donanımla beraber eminim ki büyük sinir ağları eğitme kapasitemiz daha da gelişecektir. Ve algoritmaları ele alın çok sayıda araştırma topluluğu yeni algoritmalar keşfetme konusunda çok iyi işler çıkarmakta. Bence tüm bunlardan dolayı gelecekte uzun bir süre boyunca derin öğrenmenin daha iyiye gideceği konusunda iyimser olabiliriz. Bunlarla beraber, hadi bir sonraki videoda bu kursta neler öğreneceğiniz hakkında konuşalım.