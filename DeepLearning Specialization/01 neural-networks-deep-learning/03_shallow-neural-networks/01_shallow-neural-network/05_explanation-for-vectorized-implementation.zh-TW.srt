1
00:00:00,000 --> 00:00:01,530
在先前的影片中,

2
00:00:01,530 --> 00:00:06,885
我們看到您個訓練例子水平疊成了矩陣 X

3
00:00:06,885 --> 00:00:11,158
您可以推導出向量化正向傳播在您的神經網路

4
00:00:11,158 --> 00:00:14,760
讓我們給多一點理由為什麼這個我寫的方程式

5
00:00:14,760 --> 00:00:19,775
是正確的方式來建置向量化跨越多重例子

6
00:00:19,775 --> 00:00:25,590
讓我們先談一下部分的正向傳播計算在一些例子

7
00:00:25,590 --> 00:00:27,645
假設對於第一個訓練例子

8
00:00:27,645 --> 00:00:29,130
您的計算為

9
00:00:29,130 --> 00:00:38,970
這個項目 x1 加 b1, 然後第二個訓練例子

10
00:00:38,970 --> 00:00:49,310
您計算這個 x2 加 b1

11
00:00:49,310 --> 00:00:50,900
第三個訓練例子

12
00:00:50,900 --> 00:00:56,064
您計算為這個 x3 加 b1

13
00:00:56,064 --> 00:01:00,930
所以為了簡化解釋在這個投影片, 我先忽略 b

14
00:01:00,930 --> 00:01:08,395
假設, 簡化這個證明一點點設 b 等於 0

15
00:01:08,395 --> 00:01:11,140
但以下我們將進行的議論還是可行

16
00:01:11,140 --> 00:01:14,320
當 b 不為 0 時只要一點點改變

17
00:01:14,320 --> 00:01:17,610
這只是為了要簡化這張投影片的描述而已

18
00:01:17,610 --> 00:01:21,110
w1 會是某個矩陣, 是吧 ？

19
00:01:21,110 --> 00:01:25,625
我有一些行在這個矩陣

20
00:01:25,625 --> 00:01:28,296
如果您看這個 x1 的計算

21
00:01:28,296 --> 00:01:30,070
您有的是

22
00:01:30,070 --> 00:01:40,021
w1 乘 x1 給您一些列向量您可以這樣畫

23
00:01:40,021 --> 00:01:47,420
同樣地, 如果您看向量 x2

24
00:01:47,420 --> 00:01:54,730
您有 w1 乘

25
00:01:54,730 --> 00:02:00,460
x2 給您另一個列向量, 是吧?

26
00:02:00,460 --> 00:02:03,250
這會給您 z[1](2)

27
00:02:03,250 --> 00:02:06,730
最後您看 x3

28
00:02:06,730 --> 00:02:12,315
您有 w1 乘 x3

29
00:02:12,315 --> 00:02:19,530
給您第三個列向量, 也就是 z[1](3)

30
00:02:19,530 --> 00:02:25,250
如果您看這個訓練集大寫 X

31
00:02:25,250 --> 00:02:31,475
是我們用所有訓練例子疊起來

32
00:02:31,475 --> 00:02:37,010
這個矩陣大寫 X 是拿向量 x1 

33
00:02:37,010 --> 00:02:43,430
跟 x2 還有 x3垂直疊起來

34
00:02:43,430 --> 00:02:46,250
這是假設總共有三個訓練例子

35
00:02:46,250 --> 00:02:50,371
如果有更多, 就像這樣繼續垂直疊起來

36
00:02:50,371 --> 00:02:57,790
如果您拿這個矩陣 X 跟 W 相乘您會得到

37
00:02:57,790 --> 00:03:00,190
如果您回想矩陣相乘如何作用

38
00:03:00,190 --> 00:03:02,680
您會得到第一列是

39
00:03:02,680 --> 00:03:06,313
相等於我用紫色畫的這些值

40
00:03:06,313 --> 00:03:10,930
第二列會是跟這四個值一樣

41
00:03:10,930 --> 00:03:16,612
第三列會是這些橘色的值

42
00:03:16,612 --> 00:03:19,480
最終會是這樣

43
00:03:19,480 --> 00:03:27,740
但當然這只是等於 z[1](1) 表示成

44
00:03:27,740 --> 00:03:37,185
列向量, 接著是 z[1](2) 表示成列向量, 接著是 z[1](3)

45
00:03:37,185 --> 00:03:39,273
也是列向量

46
00:03:39,273 --> 00:03:41,100
而這前提是您有三個訓練例子

47
00:03:41,100 --> 00:03:44,255
您有更多例子, 那您會有更多的列

48
00:03:44,255 --> 00:03:51,220
這只是我們的矩陣大寫 Z1

49
00:03:51,220 --> 00:03:55,230
我希望這給您一些理由為什麼我們用

50
00:03:55,230 --> 00:04:02,830
前面的 W1 乘 Xi 等於

51
00:04:02,830 --> 00:04:08,310
Z[1](i), 當我們只看單一訓練例子時

52
00:04:08,310 --> 00:04:12,565
當您拿不同的訓練例子把它們疊在不同的列時

53
00:04:12,565 --> 00:04:15,250
那這相對的結果是

54
00:04:15,250 --> 00:04:18,725
z 也是疊在列上

55
00:04:18,725 --> 00:04:24,565
我不證明但您可以說服您自己如果您用 Python 傳播

56
00:04:24,565 --> 00:04:26,245
如果您加入

57
00:04:26,245 --> 00:04:30,534
這些 b 的值, 這些值還是正確的

58
00:04:30,534 --> 00:04:34,540
實際上會發生的是您用 Python 傳播

59
00:04:34,540 --> 00:04:41,790
您最終會有一個個 bi 在這個矩陣的每個列上

60
00:04:41,790 --> 00:04:48,220
在這個投影片, 我只說明了 z1 等於

61
00:04:48,220 --> 00:04:51,980
w1x 加 b1 是

62
00:04:51,980 --> 00:04:54,020
正確的向量化在

63
00:04:54,020 --> 00:04:57,493
前面的投影片四個步驟中的第一個步驟

64
00:04:57,493 --> 00:04:59,990
但事實上同樣的分析讓您

65
00:04:59,990 --> 00:05:02,660
明白其他的步驟也是可以

66
00:05:02,660 --> 00:05:08,105
用相同的邏輯來作用, 如果您將輸入疊成列在方程式中

67
00:05:08,105 --> 00:05:11,510
您會得到相對的輸出也是疊在列上

68
00:05:11,510 --> 00:05:14,970
最後, 讓我們回顧一下我們在這段影片中談過的

69
00:05:14,970 --> 00:05:16,520
如果者是您的神經網路

70
00:05:16,520 --> 00:05:21,693
我們說這是您要做的如果您在建置正向傳播

71
00:05:21,693 --> 00:05:27,693
一次一個訓練例子從 i 等於 1 到 m, 然後我們說

72
00:05:27,693 --> 00:05:34,100
讓我們將訓練例子疊在列上像這樣, 對於每個 Z1,

73
00:05:34,100 --> 00:05:38,265
A1, Z2, A2 讓我們疊在相對應的列上

74
00:05:38,265 --> 00:05:43,820
這是 A1 的例子, 但對於 Z1,

75
00:05:43,820 --> 00:05:46,975
A1, Z2 跟 A2 也是相同

76
00:05:46,975 --> 00:05:51,090
然後我們在前面的投影片說明了

77
00:05:51,090 --> 00:05:58,785
這一行程式讓您向量化同時處理所有 m 個例子

78
00:05:58,785 --> 00:06:00,555
同樣的原因

79
00:06:00,555 --> 00:06:03,880
您也可以說明其他行的程式也是

80
00:06:03,880 --> 00:06:08,811
正確的向量化對於這四行程式

81
00:06:08,811 --> 00:06:10,675
只是要提醒一下

82
00:06:10,675 --> 00:06:18,960
因為 X 也是等於 A0, 因為記得

83
00:06:18,960 --> 00:06:27,980
輸入特徵向量 X 等於 A0, 而 xi 等於 a[0](i)

84
00:06:27,980 --> 00:06:30,870
那這實際上也一定的對稱性對於

85
00:06:30,870 --> 00:06:34,110
這些方程式, 第一個方程式也可以

86
00:06:34,110 --> 00:06:41,790
寫成 Z[1] 等於 W[1] A[0] 加 b[1]

87
00:06:41,790 --> 00:06:45,680
所以, 您看這一對方程式跟這一對

88
00:06:45,680 --> 00:06:51,805
方程式實際上很類似, 只是所有指標前進 1

89
00:06:51,805 --> 00:06:55,880
這也說明了神經網路的不同層

90
00:06:55,880 --> 00:07:00,585
大約做同樣的事或者說做同樣的計算一再重複

91
00:07:00,585 --> 00:07:04,220
這裡我們的是兩層神經網路而當我們用

92
00:07:04,220 --> 00:07:08,475
更深入的神經網路在下一週的影片

93
00:07:08,475 --> 00:07:11,670
您看到即使更深入的深度學習基本上也是用

94
00:07:11,670 --> 00:07:16,215
這兩個步驟只是比我們這裡做的更多次

95
00:07:16,215 --> 00:07:21,255
所以這是您如何向量化您的神經網路跨多重的訓練例子

96
00:07:21,255 --> 00:07:25,590
下一步是, 我們目前使用 S型函數在我們的神經網路

97
00:07:25,590 --> 00:07:27,925
實際上這並不是最好的選擇

98
00:07:27,925 --> 00:07:29,675
在下一段影片, 我們更進一步

99
00:07:29,675 --> 00:07:32,450
我們如何使用, 被稱為

100
00:07:32,450 --> 00:07:37,190
啟動函數而S型函數只是其中一種選擇