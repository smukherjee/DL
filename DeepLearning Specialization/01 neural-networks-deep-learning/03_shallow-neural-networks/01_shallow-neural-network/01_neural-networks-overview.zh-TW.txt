歡迎回來。這個星期 你學會建置一個神經網路。 在深入探討技術細節之前, 我想在這個影片中, 為您提供本周影片中
您將看到的內容概述。 如果您跟不上這些細節, 別擔心,我們將在接下來的
幾個影片中深入探討技術細節。 但是現在,讓我們快速概述一下,
如何建置你的網路。 上周,我們討論了羅吉斯迴歸分析 我們看到這個模型,如何對應于下面的計算圖形, 在那裡,您加入特徵 x 和參數 w 和 b, 那允許您計算 z,然後用於計算 a, 我們使用 a 可互換為 輸出 y-hat,然後您可以計算損失函數 L, 一個神經網路,看起來像這樣. 正如我之前已經提到的, 你可以通過堆疊大量的小sigmoid單位,
來形成一個神經網路。 而以前,此節點對應于兩個計算步驟。 第一個是計算 z 值, 其次,它計算這 a 值。 在這個神經網路中, 這堆節點將對應于,這樣類似的 z 的計算, 以及,類似這樣 a 的計算。 然後,該節點將對應于另一個 z 
和另一個 a 的類似計算。 因此,我們稍後將介紹的標記法,如下所示。 首先輸入特徵, X 連同一些參數 W 和 b, 這將允許您計算 z_1.。 因此,我們將介紹的新標記法是,我們將使用 上標方括弧 1 指的是 與此節點堆疊關聯的數值,它被稱為層。 稍後,我們將使用上標方括弧 2 來標示與該節點關聯的數值。 這稱為神經網路的另一層。 上標方括弧, 就像我們這裡一樣 不要混淆於 上標圓形括弧,我們用它來標示單ㄧ訓練例子。 所以, x 上標圓括弧,我指的是第 i 個訓練示例, 上標方括弧 1 和 2 指這些不同的層; 第一層和第二層在這個神經網路。 計算完 z[1] 後類似于羅吉斯迴歸分析之後, 會有一個計算 a[1]的計算, 那只是 sigmoid(z[1]), 然後使用另一個線性方程式計算 z[2],然後計算 a[2]。 a[2] 是最終的輸出 在這神經網路,也將與 y-hat互換使用。 所以,我知道這有很多的細節,但關鍵 記住的是,對於羅吉斯迴歸分析, 我們這個 z ,跟著是 a 的計算. 在這個神經網路中, 在這裡,我們只是做很多次, 這個 z, 跟著是 a 的計算, 這個 z, 跟著是 a 的計算 最後計算損失。 您還記得,對於羅吉斯迴歸分析 我們有這個反向的計算來 計算導數,或者在計算 da, dz 等等。 所以,以同樣的方式, 神經網路也會做向後計算,看起來像 這樣,您會計算 da[2], dz[2],允許您計算 dw[2], db[2],等等。 此從右至左向後計算,用紅色箭頭來表示。 這為您提供了神經網路的快速概述。 它基本上,採取羅吉斯迴歸分析和重複兩次。 我知道有很多新的記號法, 新的細節,不要擔心保存他們, 遵循一切,最有可能,我們將在
接下來的幾個影片進入細節。 因此,讓我們繼續下一個影片 我們將開始討論怎樣表示神經網路。