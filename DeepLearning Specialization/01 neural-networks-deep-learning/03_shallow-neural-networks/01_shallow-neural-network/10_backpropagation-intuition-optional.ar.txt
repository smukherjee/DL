في الفيديو السابق، شاهدتم معادلات الانتشار الخلفي. في هذا الفيديو سوف نقوم بتوضيح بعض المفاهيم باستخدام مخطط حسابي لكيفية اشتقاق هذه المعادلات. مشاهدة هذا الفيديو اختيارية. لذا لديك حرية المشاهدة أم لا. ويمكنك أداء واجبك بالكامل بغض النظر. أتذكرون عندما تحدثنا عن الانحدار اللوجستي وكان لدينا هذا المسار للأمام حيث نحتسب Z ثم A وبعد ذلك الخسارة. ثم نأخذ المشتقات وكان لدينا هذا المسار للخلف حيث نحتسب أولاً DA ثم نتابع لاحتساب DZ ثم بعد ذلك نحتسب DW وDB. إذن تعريف الخسارة كان L(A,Y) Y = -Y log a - 1، Y-) مضروبًا في log(1ِ-A). لذا إذا كنت على علم بالتفاضل والتكامل وأخذتم المشتقة من هذا مع وضع A في الاعتبار فستحصلون على معادلة DA. إذن DA تساوي هذا. وإذا كنتم على دراية بالحساب، فستدركون أن ‎.-Y/A + 1-Y/1-A فأنتم اشتققتم هذا من التفاضل والتكامل بإيجاد مشتقات هذا. ويتضح أنه بالرجوع خطوة للخلف لحساب DZ عرفنا أن DZ =A - Y. وقد شرحت السبب سابقًا. واتضح من مربع التفاضل والتكامل أن DZ تساوي DA مضروبة في G '(Z). حيث هنا G (Z) تساوي سينية (Z) هي دالة تنشيطنا لوحدة الإخراج هذه في الانحدار اللوجستي، صحيح؟ لذا تذكروا أن هذا انحدار لوجستي حيث لدينا X1 وX2 وX3 ووحدة سينية واحدة وهذا يعطينا A مما يعطينا Y hat. لذا احتوت هنا دالة التنشيط على دالة سينية. وكمعلومة إضافية فقط الذين على دراية منكم بمربع التفاضل والتكامل يعرفون أن سبب ذلك أن A تساوي سينية (Z). إذن Z∂/‏L∂ يساوي A‏∂/‏L∂ مضروبًا في DA/DZ. بما أن A تساوي ‏Z∂ فهذا يساوي D/DZ G(Z) والذي يساوي G'(Z). هذا هو السبب أن هذا التعبير أي D/Z في تعليمتنا البرمجية يساوي هذا التعبير أي DA في تعليمتنا البرمجية مضروبًا فيG'(Z). أي هذا هو نفسه ذاك. إذن عملية الاشتقاق الأخيرة هذه سيكون لها معنى إذا كنتم على دراية بالتفاضل والتكامل وخاصة مربع التفاضل. لكن إن لم تعرفوها، فلا تقلقوا بشأن هذا. سأحاول شرح المفاهيم كلما احتاج الأمر. والآن بعدما قمنا بحساب DZ لهذا الانحدار سنقوم بحساب DW والذي اتضح أنه DZ مضروبًا في X وDB والذي وجدنا أنه DZ في مثال تدريبي واحد. كان هذا الانحدار اللوجستي. إذن، ما سنقوم به عند احتساب الانتشار الخلفي لشبكة عصبية هو الكثير من العمليات الحسابية مثل هذه، لكننا سنقوم به مرتين لأن X لدينا لا تذهب إلى وحدة إخراج لكنها تذهب إلى طبقة مخفية، ثم تذهب إلى وحدة الإخراج. بدلاً من أن تكون العمليات الحسابية من خطوة واحدة كما لدينا هنا سيكون لدينا خطوتين هنا في هذه الشبكة العصبية من طبقتين. إذن في هذه الشبكة العصبية من طبقتين، لدينا طبقة إدخال طبقة مخفية،ثم طبقة إخراج. تذكروا خطوات عملية الحساب. أولاً نحتسب Z1 باستخدام هذه المعادلة ثم نحتسب A1 وبعد ذلك نحتسب Z2. ولاحظوا أن Z2 تعتمد على المعلمات W2 وB2. ثم بناءً على Z2 نحتسب A2 مما يعطينا في النهاية الخسارة. ما يفعله الانتشار الخلفي هو العودة للخلف لاحتساب DA2 ثم DZ2. ثم تعود لاحتساب DW2 وDP2 وتعود أكثر لاحتساب DA1 وDZ1 وهكذا. لا نحتاج إلى احتساب العاكس وفقًا للإدخال X لإن الإدخال X للتعلم الخاضع للإشراف هو بادئة. لذا لا نحاول تحسين X ولهذا لن نأخذ العاكسات. على الأقل للتعلم الخاضع للإشراف نضع X في الاعتبار. سأقوم بتخطي شرح احتساب DA2 بالتفصيل. إذا أردتم، فيمكنكم احتساب DA2 ثم استخدامه لاحتساب DZ2 لكن خلال التدريب يمكنك جعل هاتين الخطوتين خطوة واحدة لذا تنتهي بـ DZ2= A2-Y كما قلنا سابقًا. وسيكون لديك أيضًا سأكتب DW2 وDB2 هنا بالأسفل. لديك DW2=DZ2*A1ᵀ وDB2=DZ2. هذه الخطوة مماثلة إلى حد ما للانحدار اللوجستي حيث كان لدينا DW=DZ*X باستثناء أن الآن A1 تلعب دور X وهناك ᵀ إضافي لأن العلاقة بين W المصفوفة الكبيرة وw المعلمات الفردية لدينا يوجد ᵀ إضافي هناك، صحيح؟ لأن W=[---] في حالة الانحدار الخطي بإخراج واحد. DW2 مماثلة لذلك أيضًا حيث كانت W هنا متجه عمود لذلك تشتمل على ᵀ إضافي لـ A1 بينما لم يكن لدينا لـ X هنا للانحدار اللوجستي. هذا يمثل نصف الانتشار الخلفي. ثم يمكنك احتساب DA1 مجددًا إذا أردت. لكن عمليًا تندمج عملية احتساب DA1 وDZ1 عادة في خطوة واحدة لذا ما تقومون بتطبيقه فعليًا هو أن DZ1=W2ᵀ*DZ2 ثم بعد ذلك عنصر ناتج Y لـ G1'(z1). لفحص الأبعاد فحسب، صحيح؟ إذا كان لديك شبكة عصبية تبدو كهذه وتخرج ŷ، إذن إذا كان لديك N0 وكانت NX=N0 سمات الإدخال N1 وحدات مخفية وN2 حتى الآن. N2 في حالتنا وحدة إخراج واحدة. إذا المصفوفة W2 هي (N2,N1) بعدية Z2 ولذا DZ2 ستكون (N2,1) بعدي. سيكون هذا فعليًا (1,1) عندما نتعامل مع التصنيف الثنائي. وZ1 ولذا DZ1 ستكون (N1,1) بعدي، صحيح؟ لاحظ أنه لأي متغير، سيكون لـ foo وdfoo البعد نفسه دائمًا. لذا سيكون لـ W وDW البعد نفسه دائمًا، وبالمثل لـ B وDB ولـ Z وDZ وهكذا. لذا للتأكد أن أبعاد هذا متماثلة تمامًا لدينا DZ1=W2ᵀ*DZ2 هذا ناتج العنصر Y مضروبًا في G1'(Z1). بمطابقة الأبعاد بالأعلى سيكون هذا ‎(N1,1)=‎W2ᵀ نحن ننقل هذا لذا ستكون الصيغة (N1,N2) بعدية. وDZ2 ستكون كما يلي (N2,1) بعدي، ثم هذا هذا مثل بعد Z1. سيكون هذا أيضًا (N1,1) بعدي، أي ناتج العنصر Y. لذا الأبعاد ذات معنى فعلا، صحيح؟ يمكن الحصول على متجه (N1,1) بعدي من خلال مصفوفة (N1,N2)*(N2,1) لأن ناتج هذين يعطيك مصفوفة (N1,1) بعدي وبذلك يصبح هذا ناتج العنصر Y لمتجهين (N1,1) بعدي، مما يعني أن الأبعاد متطابقة فعلاً. ملحوظة عند تنفيذ الانتشار الخلفي. إذا كنت تتأكد من تماثل أبعاد مصفوفاتك لذا تمعن التفكير في أبعاد المصفوفات المختلفة، بما في ذلك W1 وW2 وZ1 وZ2 وA1 وA2 وهكذا. وتتأكد من أن أبعاد عمليات تلك المصفوفات متماثلة فهذا سيمنع حدوث الكثير من الأخطاء في الانتشار الخلفي. حسنًا. هذا يعطينا DZ1. وأخيرًا لتلخيص ما شرحنا، DW1 وDB1 عليَّ أن أكتبهما هنا على ما أظن لكن بما أنه ليس لدي مساحات فارغة، فسأكتبهما على يمين اللوحة DB1 وDB1 ناتجين عن المعادلات التالية. هذا سيساوي ‎DZ1*Xᵀ‎ وهذا سيساوي DZ1. ربما ستلاحظ التماثل بين هذه المعادلات وهذه المعادلات. وهذه ليست صدفة لأن X تلعب دور A0، إذن Xᵀ‎ هي A0ᵀ‎. هذه المعادلات متماثلة جدًا في الواقع. وهذا يعطينا رؤية عن كيفية اشتقاق الانتشار الخلفي. لدينا ست معادلات رئيسية هنا لـ DZ2 وDW2 وDB2 وDZ1 وDW1 وDB1. إذن لنأخذ هذه المعادلات الستة وننسخها إلى الشريحة التالية، ها نحن ذا. إلى الآن، قمنا باشتقاق الانتشار الخلفي إذا كنت تعمل على مثال تدريبي واحد في هذا الوقت ولكنها ليست مفاجأة أنه بدلاً من العمل على مثال واحد في كل مرة نود أن نعالج عدة عناصر بمتجه واحد عبر عدة أمثلة تدريبية في نفس الوقت. تتذكرون أنه عند الانتشار الأمامي عندما نعمل على مثال واحد في وقت ما كانت لدينا معادلات مثل هذه حيث (A1=G1(Z1. وللمعالجة بمتجه واحد كنا نأخذ عناصر Z ونكدسها في أعمدة كهذه إلى Z1M وندعوها حرف Z الكبير. ثم وجدنا أنه بتكديس الأشياء في أعمدة وتحديد نسخ الحروف الكبيرة من هذه كان لدينا Z1=W1 X + B فحسب. و(A1=G1(Z1، صحيح؟ وقد حددنا الترميز في هذه الدورة بعناية للتأكد من أن تكديس الأمثلة في أعمدة مختلفة من المصفوفة، سيجعل كل شيء يعمل بشكل صحيح. إذن اتضح أنه إن كنتم على علم بقواعد الرياضيات فإن الفكرة نفسها تعمل للانتشار الخلفي. إذن معادلات المتجه الواحد هي كما يلي: أولا، إذا أخذتم حرفي DZ من أمثلة تدريبية مختلفة وكدستموهم في أعمدة مختلفة من المصفوفة والمثل لهذا وهذا أيضًا فسيكون هذا هو تطبيق المعالجة بمتجه واحد، ثم هذا تعريف أو كيفية احتساب DW2. هناك 1‎/M إضافي لأن دالة التكلفة J هي 1‎/M من مجموع Y يساوي واحد من M للخسائر. عند احتساب المشتقات لدنيا عنصر 1‎/M الإضافي كما كان لدينا بالضبط عندما كنا نحتسب أيام الانتظار للانحدار اللوجستي. وهذا هو الجديد فيما يتعلق بـ DB2. مرة أخرى، حرفي DZ من عدة أمثلة و1‎/M، ثم يحتسب DZ1 كما يلي: مرة أخرى، هذا ناتج العنصر Y لكن رأينا في الشريحة السابقة أن هذا كان متجه (N1,1) بعدي. لكن الآن هذه مصفوفة (N1,M) بعدي. وهذان أيضًا (N1,M) بعدي. لذا علامة النجمة (*) هذه هي ناتج العنصر Y وأخيرًا التحديثين الأخيرين. يجب ألا يكون هذا مفاجئًا جدًا. أتمنى أن يوضح لكم هذا كيفية اشتقاق خوارزمية الانتشار الخلفي. في تعلم الآلات عامة أعتقد أن اشتقاق خوارزمية الانتشار الخلفي هي واحدة من أعقد العمليات الرياضية التي رأيتها يومًا وتتطلب معرفة الجبر الخطي، بالإضافة إلى اشتقاق المصفوفات. أن تشتقها من الصفر، البدايات الأولى. إذا كنتم خبراء في حساب تفاضل وتكامل المصفوفات فباستخدام هذه العملية، يمكنكم اشتقاق الخوارزمية بأنفسكم. لكني أظن أن هناك الكثير من متخصصي التعلم العميق الذين رأوا عملية الاشتقاق على نفس المستوى الذي رأيتموه في هذا الفيديو والمفاهيم وهم قادرون بالفعل على تطبيق هذه الخوارزمية بفاعلية. إذا كنتم خبراء في حساب التفاضل والتكامل فاكتشفوا ما إذا كان بإمكانكم اشتقاقها من الصفر. إنها إحدى أعقد العمليات الرياضية. وإحدى أصعب عمليات الاشتقاق التي رأيتها يومًا في تعلم الآلات. على أي حال، إذا قمتم بتطبيقها فستعمل جيدًا وأظن أن لديكم المفاهيم الكافية لتعديلها وجعلها تعمل. والآن، هناك تفصيل أخير أود مشاركته معكم قبل البدء بتطبيق شبكتكم العصبية وهو كيفية تهيئة أوزان شبكتكم العصبية. يتضح أن تهيئة معلماتكم ليس للصفر ولكن عشوائيًا اتضح أنه مهم للغاية في تدريب شبكتكم العصبية. في الفيديو التالي، ستعرفون لماذا.