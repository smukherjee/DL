В цьому відео ми розглянемо логістичну регресію (ЛР). Це поширений навчальний алгоритм 
для виведення міток y в задачі керованого навчання, які мають значення 0 або 1, тобто в задачі двійкової класифікації. Припустимо, що заданий вхідний вектор ознак X відповідає зображенню, яке ми хочемо розпізнати і помітити як "кіт" або "не кіт". Нам потрібен алгоритм, який обчислить передбачення, яке ми називатимемо ŷ (y-циркумфлекс), яке є розрахунковим y. Формально, ми хочемо, щоб ŷ визначав імовірність, при якій y=1 при заданому векторі ознак x. Тож, іншими словами, якщо x - це зображення, яке ми бачили в останньому відео, то ми хочемо, щоб ŷ визначив, яка ймовірність, що це зображення - "кіт". Тож x, як ми сказали в попередньому відео, - це вектор розміру nₓ. Це значить, що параметри ЛР будуть наступними: w - також вектор розміром nₓ, а b - просто дійсне число. Тож, маючи вхідний x і параметри w і b, як ми можемо вивести вихідну ŷ? Що ж, один з варіантів, непрацюючих, буде ŷ=wᵀx+b - щось на зразок лінійної функції від вхідного x. І дійсно, це те, що ми використовуємо, реалізуючи ЛР. Але цей алгоритм не дуже підходить для двійкової класифікації, тому що нам потрібно, щоб ŷ 
відображав імовірність того, що y=1. Тож ŷ повинен бути між 0 та 1. А це важко реалізувати, тому wᵀx+b може бути набагато більше 1 або, навіть, негативним. А це не має сенсу для визначення імовірності, бо ми ж хочемо отримати число між 0 і 1. Тож натомість для ЛР, ми обчислюватимемо ŷ як сигмоїду від цього виразу. Ось як виглядає сигмоїда. Якщо на горизонтальній осі позначати z, 
то сигмоїда від z буде ось такою. Вона плавно переходить від 0 до 1 (я підпишу свою вісь - це 0) і перетинає вертикальну вісь в значенні 0,5. Тож сигмоїда від z виглядає ось так. 
Ми використовуватимемо z для позначення цього рівняння: wᵀx+b. Ось формула сигмоїди. Сигмоїда від z, де z - дійсне число, це 1/(1+e⁻ᶻ). Тож, занотуй наступне. Якщо z дуже велике, то e⁻ᶻ буде близьким до 0, отже, сигмоїда від z буде наближена до 1/(1+ дещо дуже близьке до 0), тому що, e в степені дуже великого негативного числа 
буде наближатись до 0. Тож оце буде наближатись до 1. І справді, якщо глянути на графік зліва, якщо z дуже велике, то сигмоїда від z - дуже близька до 1. І навпаки, якщо z дуже мале, іншими словами, дуже велике від'ємне число, то сигмоїда від z буде 1поділити на 1+ e в степені від'ємне z, тобто це буде великим числом. Тож це буде, 1 поділене на 1 плюс число, що є дуже дуже великим. Тож оце буде наближатись до 0. І справді, ми бачимо, що коли z - велике від'ємне число, сигмоїда від z наближається до 0. Отже, при реалізації ЛР, наша робота - спробувати знайти такі параметри w і b, щоб ŷ став якомога точнішою розрахунковою імовірністю того, що y=1. Перед тим як піти далі - ще одна примітка до позначень. Коли ми програмуємо НМ, ми, зазвичай, зберігаємо параметри w і параметри b окремо. Тут b в якості перехоплювача. В деяких інших курсах ти, можливо, бачив/ла інші позначення. Деякі конвенції визначають екстра ознаки x₀, що =1. Тож, таким чином, x є множиною nₓ+1 дійсних чисел. А ŷ визначається як сигмоїда від Θᵀx. В цій альтернативній конвенції позначень ми маємо вектор параметрів Θ: Θ₀, Θ₁, Θ₂... і так до Θₙₓ. І, таким чином, Θ₀ грає роль b і є просто дійсним числом. А Θ₁ і далі до Θₙₓ грають роль w. Виявляється, що для реалізації НМ, буде простіше відокремити параметри b і w. Тому в цьому курсі ми не будемо використовувати жодних позначень конвенції, 
яку я щойно записав червоним кольором. Якщо ти не зустрічав/ла цієї конвенції раніше в інших курсах, не переймайся, це лише для тих, хто раніше бачив ці позначення. Я просто хочу явно зазначити, що цих позначень 
ми не будемо використовувати в цьому курсі. Але якщо раніше ти цього не бачив/ла, то це - не важливо, тобі не потрібно про це турбуватись. Тож, тепер ти побачив/ла як виглядає модель ЛР. Далі, щоб натренувати параметри w і b, 
тобі потрібно визначити функцію Витрат. Давай зробимо це в наступному відео.