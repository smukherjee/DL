1
00:00:00,000 --> 00:00:01,619
Коли ти тренуєш свою нейронну мережу (НМ)

2
00:00:01,619 --> 00:00:03,955
дуже важливо ініціалізувати коефіцієнти в випадкові значення.

3
00:00:03,955 --> 00:00:08,426
Для логістичної регресії (ЛР) підходила й ініціалізація в 0.

4
00:00:08,426 --> 00:00:12,258
Але для НМ ініціалізація всіх коефіцієнтів або параметрів в 0

5
00:00:12,258 --> 00:00:14,706
із наступним застосуванням Градієнтного спуску не буде працювати.

6
00:00:14,706 --> 00:00:15,289
Давай подивимось чому.

7
00:00:15,289 --> 00:00:20,173
Тож ми маємо 2 вхідні ознаки -

8
00:00:20,173 --> 00:00:25,953
n[0]=2 - і 2 прихованих вузли - n[1]=2.

9
00:00:25,953 --> 00:00:31,547
Тож матриця, пов'язана з прихованим шаром,

10
00:00:31,547 --> 00:00:35,373
w[1] матиме розмір 2*2.

11
00:00:35,373 --> 00:00:41,230
Скажімо, ми ініціалізуємо її в 0, тобто 0, 0, 0, 0 - матрицю 2*2.

12
00:00:41,230 --> 00:00:45,531
І, скажімо, b[1] теж буде 0, 0.

13
00:00:45,531 --> 00:00:50,788
Виявляється, ініціалізація в 0 зсувів b є прийнятною,

14
00:00:50,788 --> 00:00:54,240
але ініціалізація w в 0 буде проблемою.

15
00:00:54,240 --> 00:00:59,625
А проблема такої ініціалізації заключається в тому,

16
00:00:59,625 --> 00:01:05,522
що всі зразки - a[1]₁ і

17
00:01:05,522 --> 00:01:09,253
a[1]₂ - будуть однаковими. Правильно?

18
00:01:09,253 --> 00:01:12,613
Тобто ця активація і оця активація будуть однаковими,

19
00:01:12,613 --> 00:01:17,170
бо обидва приховані вузли будуть обчислювати ідентичні функції.

20
00:01:17,170 --> 00:01:21,810
А потім, коли буде обчислюватись зворотне поширення,

21
00:01:21,810 --> 00:01:24,478
виявиться, що dz[1]₁ і

22
00:01:24,478 --> 00:01:30,165
dz[1]₂ теж будуть однаковими або симетричними. Правильно?

23
00:01:30,165 --> 00:01:33,720
Обидва приховані вузли ініціалізуватимуться однаковим способом.

24
00:01:33,720 --> 00:01:36,080
Я хочу сказати, що технічно

25
00:01:36,080 --> 00:01:39,851
я припускаю, що вихідні коефіцієнти теж будуть ідентичними.

26
00:01:39,851 --> 00:01:45,122
Тож w[2] буде 0, 0.

27
00:01:45,122 --> 00:01:48,691
Тож, якщо ініціалізувати таким чином НМ,

28
00:01:48,691 --> 00:01:53,590
то оцей і оцей приховані вузли будуть повністю ідентичні.

29
00:01:53,590 --> 00:01:57,011
Інколи ще кажуть, що вони повністю симетричні,

30
00:01:57,011 --> 00:02:01,687
що означає, що вони просто обчислюють одну і ту ж функцію.

31
00:02:01,687 --> 00:02:03,765
Індуктивне доведення

32
00:02:03,765 --> 00:02:08,064
показує, що під час кожної ітерації тренування обидва

33
00:02:08,064 --> 00:02:11,272
приховані вузли будуть обчислювати ту саму функцію.

34
00:02:11,272 --> 00:02:17,521
Можна показати, що матриця dw виглядатиме отак.

35
00:02:17,521 --> 00:02:20,681
Кожний її рядок міститиме ідентичні величини.

36
00:02:20,681 --> 00:02:23,318
Тож застосуємо коригування коефіцієнтів.

37
00:02:23,318 --> 00:02:30,163
Коли ми застосуємо коригування коефіцієнтів - w[1]:=w[1]-αdw -

38
00:02:30,163 --> 00:02:33,740
то побачимо, що після кожного ітерації

39
00:02:33,740 --> 00:02:37,616
1-ий рядок w[1] = 2-ому.

40
00:02:37,616 --> 00:02:41,487
Тож можна індукційно довести, що

41
00:02:41,487 --> 00:02:44,688
ініціалізація всіх w в 0

42
00:02:44,688 --> 00:02:49,164
(через те, що обидва приховані вузли 
обчислюють ту саму функцію

43
00:02:49,164 --> 00:02:53,541
і обидва приховані вузли мають однаковий 
вплив на вихідний вузол),

44
00:02:53,541 --> 00:02:57,542
то після однієї ітерації наше твердження справджуватиметься -

45
00:02:57,542 --> 00:03:00,273
обидва приховані вузли залишаться симетричними.

46
00:03:00,273 --> 00:03:04,507
І, отже, індукційно, після 2-ох ітерацій, після 3-ох ітерацій і т.д. -

47
00:03:04,507 --> 00:03:07,013
незалежно від тривалості тренувань НМ -

48
00:03:07,013 --> 00:03:10,373
обидва приховані вузли все ще 
обчислюватимуть ту саму функцію.

49
00:03:10,373 --> 00:03:15,212
Тож в цьому випадку, насправді, 
нема потреби мати більше одного прихованого вузла,

50
00:03:15,212 --> 00:03:17,692
тому що всі вони обчислюватимуть те саме.

51
00:03:17,692 --> 00:03:22,378
І, звичайно, для більших НМ, скажімо, з 3-ома ознаками і,

52
00:03:22,378 --> 00:03:24,972
можливо, з дуже великою кількістю прихованих вузлів

53
00:03:24,972 --> 00:03:29,239
нічого не зміниться. Можна показати, що якщо в отакої НМ

54
00:03:29,239 --> 00:03:34,107
(не буду малювати всіх зв'язків) коефіцієнти будуть ініціалізовані в 0,

55
00:03:34,107 --> 00:03:37,103
то всі приховані вузли будуть симетричними і

56
00:03:37,103 --> 00:03:40,603
не матиме значення скільки часу працюватиме Градієнтний спуск.

57
00:03:40,603 --> 00:03:44,037
Всі вузли продовжуватимуть обчислювати одну й ту ж функцію.

58
00:03:44,037 --> 00:03:48,785
Тож це не приносить користі, бо ми хочемо, щоб

59
00:03:48,785 --> 00:03:52,835
приховані вузли обчислювали різні функції. 
Виходом в цьому випадку є

60
00:03:52,835 --> 00:03:57,748
ініціалізація в випадкові значення. 
Тож ось що ми робимо. Ми встановлюємо

61
00:03:57,748 --> 00:03:58,677
w[1]=

62
00:03:58,677 --> 00:04:04,053
np.random.randn -

63
00:04:04,053 --> 00:04:07,037
це генерує Гаусівські випадкові значення - (2, 2).

64
00:04:07,037 --> 00:04:12,358
І далі, зазвичай, ми множимо це на маленьке число - типу 0,01.

65
00:04:12,358 --> 00:04:14,951
Тож генеруються маленькі випадкові значення.

66
00:04:14,951 --> 00:04:20,590
А тепер b. Виявляється, що b не має проблеми симетрії,

67
00:04:20,590 --> 00:04:24,735
так званої проблеми порушення рівноваги [symmetry breaking].

68
00:04:24,735 --> 00:04:29,370
Тож b можна ініціалізувати в 0.

69
00:04:29,370 --> 00:04:32,166
Тому що через те, що w ініціалізується в випадкові значення,

70
00:04:32,166 --> 00:04:36,769
приховані вузли вже будуть обчислювати різні функції.

71
00:04:36,769 --> 00:04:40,912
Тож проблеми порушення рівноваги вже не буде.

72
00:04:40,912 --> 00:04:43,795
Далі, аналогічно w[2] ініціалізуємо в випадкові значення.

73
00:04:43,795 --> 00:04:48,858
А b[2] можемо теж ініціалізувати в 0.

74
00:04:48,858 --> 00:04:55,321
Можливо, трохи дивно звідки взялась константа і чому вона =0,01.

75
00:04:55,321 --> 00:04:58,478
Чому не 100 чи 1000?

76
00:04:58,478 --> 00:05:02,313
Що ж, зазвичай, ми ініціалізуємо

77
00:05:02,313 --> 00:05:05,763
коефіцієнти в дуже маленькі випадкові значення.

78
00:05:05,763 --> 00:05:10,443
Тому що, якщо використовується, скажімо, функція th або сигмоїда,

79
00:05:10,443 --> 00:05:14,047
або сигмоїда є лише в вихідному вузлі,

80
00:05:14,047 --> 00:05:17,922
то, якщо коефіцієнти надто великі,

81
00:05:17,922 --> 00:05:23,967
при обчисленні значень активаторів

82
00:05:23,967 --> 00:05:28,621
(пам'ятаєш, що z[1]=w[1]x+b[1],

83
00:05:28,621 --> 00:05:34,094
а потім a[1] - функція активації - обчислюється на основі z[1])...

84
00:05:34,094 --> 00:05:39,097
Тож, якщо w дуже велике, z теж буде дуже велике або,

85
00:05:39,097 --> 00:05:44,235
принаймі, якісь значення z будуть дуже великими, а інші - дуже малими,

86
00:05:44,235 --> 00:05:49,789
то в цьому випадку є більшою ймовірність попасти в оцю ділянку функції th

87
00:05:49,789 --> 00:05:55,699
або сигмоїди, де нахил або градієнт - дуже малі.

88
00:05:55,699 --> 00:05:58,302
А це значить, що Градієнтний спуск буде дуже повільним.

89
00:05:58,302 --> 00:05:59,730
А, отже, і навчання буде дуже повільним.

90
00:05:59,730 --> 00:06:04,133
Тож, резюмуємо. Якщо w надто велике, то збільшується ймовірність що,

91
00:06:04,133 --> 00:06:08,633
навіть на початку тренування, z буде дуже великим.

92
00:06:08,633 --> 00:06:13,525
А це призводить до перенасичення функція th або сигмоїди,

93
00:06:13,525 --> 00:06:15,418
що сповільнює навчання.

94
00:06:15,418 --> 00:06:17,231
Якщо сигмоїди чи th

95
00:06:17,231 --> 00:06:22,149
немає серед активаційних функцій твоєї НМ, то цієї проблеми не буде.

96
00:06:22,149 --> 00:06:26,506
Але, якщо ти робиш двійкову класифікацію і на виході маєш сигмоїду,

97
00:06:26,506 --> 00:06:30,806
то просто не потрібно ініціалізувати параметри в надто великі значення.

98
00:06:30,806 --> 00:06:35,435
Тож ось чому варто застосувати множення на 0,01 або

99
00:06:35,435 --> 00:06:36,872
інше маленьке число.

100
00:06:36,872 --> 00:06:38,536
І те саме для w[2]. Правильно?

101
00:06:38,536 --> 00:06:44,295
Це буде random.randn...

102
00:06:44,295 --> 00:06:49,545
повинно бути 1*2 в цьому випадку... помножити на 0,01.

103
00:06:49,545 --> 00:06:51,404
Пропустив тут s.

104
00:06:51,404 --> 00:07:00,085
Отже, на останок. Виявляється, 
що інколи можна підібрати кращу константу ніж 0,01.

105
00:07:00,085 --> 00:07:04,304
Коли ми тренуємо НМ з лише одним

106
00:07:04,304 --> 00:07:09,129
прихованим шаром, тобто неглибоку НМ, 
не з багатьма прихованими шарами,

107
00:07:09,129 --> 00:07:12,392
то використовувати 0,01 може підійти.

108
00:07:12,392 --> 00:07:15,705
Але коли ми тренуємо дуже-дуже глибоку НМ,

109
00:07:15,705 --> 00:07:19,294
то, можливо, треба буде підібрати константу відмінну від 0,01.

110
00:07:19,294 --> 00:07:23,642
Тож на наступному тижні ми поговоримо трохи про те, як

111
00:07:23,642 --> 00:07:27,925
і коли може бути потрібно вибрати константу відмінну від 0,01.

112
00:07:27,925 --> 00:07:32,008
Але, в будь-якому випадку, вона завжди буде відносно малим числом.

113
00:07:32,008 --> 00:07:34,584
Тож це все на цей тиждень.

114
00:07:34,584 --> 00:07:38,348
Тепер ти знаєш як налаштовувати приховані шари НМ,

115
00:07:38,348 --> 00:07:42,430
як ініціалізувати параметри, 
як обчислювати передбачення, використовуючи пряме поширення

116
00:07:42,430 --> 00:07:45,445
і як обчислювати похідні і реалізовувати Градієнтний спуск, 
використовуючи зворотне поширення.

117
00:07:45,445 --> 00:07:46,275
Тож,

118
00:07:46,275 --> 00:07:48,654
з цими знаннями, ти повинен/на пройти тестування

119
00:07:48,654 --> 00:07:51,166
і виконати цьоготижневі програмувальні вправи.

120
00:07:51,166 --> 00:07:52,143
Успіхів тобі у цьому!

121
00:07:52,143 --> 00:07:54,802
Я надіюсь тобі сподобаються програмувальні вправи

122
00:07:54,802 --> 00:07:57,728
і чекаю тебе на наступному тижні.