1
00:00:00,028 --> 00:00:04,605
深層ニューラルネットワークの実装で
コードが正しいかを確認するために

2
00:00:04,605 --> 00:00:08,118
私がよく使うデバッグツールの一つは
紙を持ってきて

3
00:00:08,118 --> 00:00:11,727
自分が扱っている行列の次元を
確認していくという方法です

4
00:00:11,727 --> 00:00:15,895
これによってあなたもより簡単に
深層ニューラルネットワークを

5
00:00:15,895 --> 00:00:18,275
実装できるようになってほしいので
その方法を説明します

6
00:00:18,275 --> 00:00:23,174
大文字のLは入力層を含めないので
５になりますね

7
00:00:23,174 --> 00:00:27,390
５つの層があります
４つの隠れ層と１つの出力層です

8
00:00:27,390 --> 00:00:34,878
なので 順伝播を実装すると

9
00:00:34,878 --> 00:00:41,408
一つ目のステップは
z[1]=W[1]x+b[1]です

10
00:00:41,408 --> 00:00:48,144
バイアス項のbを無視して
パラメータWに着目しましょう

11
00:00:48,144 --> 00:00:54,501
この一つ目の隠れ層には
三つの隠れユニットがあります

12
00:00:54,501 --> 00:00:59,517
これが０層目
そして１、２、３、４、５層目です

13
00:00:59,517 --> 00:01:05,741
前回の動画での表記法を用いると

14
00:01:05,741 --> 00:01:11,265
１層目の隠れユニットの数であるn[1]は
３となります

15
00:01:11,265 --> 00:01:16,202
n[2]は５で

16
00:01:16,202 --> 00:01:23,018
n[3]は４、n[4]は２、
n[5]は１です

17
00:01:23,018 --> 00:01:27,715
ここまでは出力ユニットが一つだけの
ニューラルネットワークでしたが

18
00:01:27,715 --> 00:01:32,497
後のコースでは出力層が複数
あるものについてもお話しします

19
00:01:32,497 --> 00:01:36,989
最後に入力層は

20
00:01:36,989 --> 00:01:40,443
n[0]=nx=2です

21
00:01:40,443 --> 00:01:45,860
ではz、W、xの次元を考えましょう

22
00:01:45,860 --> 00:01:49,120
zは一つ目の隠れ層への
活性化のベクトルなので

23
00:01:49,120 --> 00:01:54,244
zは3×1になります

24
00:01:54,244 --> 00:01:58,675
つまり３次元のベクトルです

25
00:01:58,675 --> 00:02:03,093
なのでn[1]×1次元のベクトルを書きます

26
00:02:03,093 --> 00:02:08,546
n[1]×1次元の行列ですね
この場合3×1です

27
00:02:08,546 --> 00:02:12,319
それでは入力の特徴量xはどうでしょう
二つの特徴量があります

28
00:02:12,319 --> 00:02:18,622
そのためこの例ではxは2×1ですが
一般化するとn[0]×1になります

29
00:02:18,622 --> 00:02:24,082
するとW[1]は
n[0]×1を掛けたときに

30
00:02:24,082 --> 00:02:30,181
n[1]×1のベクトルになる行列
ということになりますね

31
00:02:30,181 --> 00:02:34,747
３次元のベクトルが

32
00:02:34,747 --> 00:02:38,600
何かに２次元のベクトルをかけたものと
等しくなります

33
00:02:38,600 --> 00:02:42,993
行列の掛け算の法則を考えると

34
00:02:42,993 --> 00:02:46,041
これは3×2の行列となります

35
00:02:46,041 --> 00:02:51,138
3×2の行列に
2×1の行列またはベクトルをかけると

36
00:02:51,138 --> 00:02:56,249
3×1のベクトルが得られるからです

37
00:02:56,249 --> 00:03:02,771
一般化すると これは
n[1]×n[0]次元の行列です

38
00:03:02,771 --> 00:03:07,167
ここでわかったことは

39
00:03:07,167 --> 00:03:12,665
W[1]はn[1]×n[0]でなければいけない
ということです

40
00:03:12,665 --> 00:03:20,191
より一般的に言うと
W[l]はn[l]×n[l-1]次元になります

41
00:03:20,191 --> 00:03:26,021
例えば この場合W[2]は

42
00:03:26,021 --> 00:03:31,508
5×3次元となります

43
00:03:31,508 --> 00:03:35,119
というのは
n[2]×n[1]次元ということです

44
00:03:35,119 --> 00:03:40,036
なぜならz[2]は

45
00:03:40,036 --> 00:03:45,132
W[2]×a[1]から計算されるからです

46
00:03:45,132 --> 00:03:50,059
ここでもバイアス項は無視しましょう

47
00:03:50,059 --> 00:03:54,584
これは3×1となり

48
00:03:54,584 --> 00:03:59,432
これは5×1となるので

49
00:03:59,432 --> 00:04:03,169
これは5×3になるはずだからです

50
00:04:03,169 --> 00:04:10,273
同様にW[3]も

51
00:04:10,273 --> 00:04:15,501
次の層の次元数 コンマ
前の層の次元数になるので

52
00:04:15,501 --> 00:04:19,266
これは4×5ですね

53
00:04:22,055 --> 00:04:27,489
W[4]は2×4

54
00:04:27,489 --> 00:04:34,405
そしてW[5]は1×2となります
いいですか？

55
00:04:34,405 --> 00:04:38,730
つまり確認すべきことは

56
00:04:38,730 --> 00:04:43,416
l層目についての行列を
実装するとき

57
00:04:43,416 --> 00:04:48,475
その行列がn[l]×n[l-1]となっているか
ということです

58
00:04:48,475 --> 00:04:55,362
ではベクトルbの次元数についても
考えましょう

59
00:04:55,362 --> 00:05:01,017
これは3×1のベクトルなので
答えに3×1のベクトルを得るには

60
00:05:01,017 --> 00:05:06,008
別の3×1のベクトルを
足さなければなりません

61
00:05:06,008 --> 00:05:11,287
この例では これは5×1なので

62
00:05:11,287 --> 00:05:14,823
これも5×1のベクトルになります

63
00:05:14,823 --> 00:05:19,122
すると四角で囲った二つの合計が

64
00:05:19,122 --> 00:05:22,767
ちゃんと5×1となります

65
00:05:22,767 --> 00:05:30,090
より一般的なルールとしては
左の例では

66
00:05:30,090 --> 00:05:35,470
b[1]はn[1]×1 つまり3×1ですね

67
00:05:35,470 --> 00:05:41,156
二つ目の例ではn[2]×1です

68
00:05:41,156 --> 00:05:45,891
一般化すると

69
00:05:45,891 --> 00:05:50,637
b[l]はn[l]×1次元になる
ということです

70
00:05:50,637 --> 00:05:56,402
これら二つの式が
行列Wとベクトルbの次元を

71
00:05:56,402 --> 00:06:02,091
確認する助けになればと思います

72
00:06:02,091 --> 00:06:06,206
もちろん逆伝播の実装でも

73
00:06:06,206 --> 00:06:10,657
dWはWと同じ次元になるはずです

74
00:06:10,657 --> 00:06:16,373
dWはWと同じ次元数で

75
00:06:16,373 --> 00:06:22,276
dbはbと同じ次元数です

76
00:06:22,276 --> 00:06:28,399
他に次元数を確認すべきものは
ここではあまり話しませんでしたが

77
00:06:28,399 --> 00:06:33,658
z、x、そしてa[l]です

78
00:06:33,658 --> 00:06:39,856
z[l]はg[l]がa[l]に
要素ごとに適用されたものなので

79
00:06:39,856 --> 00:06:46,914
こういう種類のネットワークでは
zとaは同じ次元数を持つはずです

80
00:06:46,914 --> 00:06:51,582
では一回に複数のサンプルについて
計算する

81
00:06:51,582 --> 00:06:53,258
ベクトル化された実装では
どうでしょう

82
00:06:53,258 --> 00:06:56,092
ベクトル化された実装でも

83
00:06:56,092 --> 00:07:00,687
もちろんW、b、dW、dbの
次元数は同じです

84
00:07:00,687 --> 00:07:04,929
しかしz、a、そしてxの次元は

85
00:07:04,929 --> 00:07:09,771
少し変わります

86
00:07:09,771 --> 00:07:13,420
前回は

87
00:07:13,420 --> 00:07:18,372
z[1]=W[1]x+b[1]でした

88
00:07:18,372 --> 00:07:23,845
そしてこれはn[1]×1

89
00:07:23,845 --> 00:07:28,276
これはn[1]×n[0]

90
00:07:28,276 --> 00:07:35,846
xはn[0]×1で
bはn[1]×1でした

91
00:07:35,846 --> 00:07:40,979
そしてベクトル化された

92
00:07:40,979 --> 00:07:46,398
実装では

93
00:07:46,398 --> 00:07:53,536
Z[1]=W[1]X+b[1]という式になります

94
00:07:53,536 --> 00:07:58,023
Z[1]はそれぞれのサンプルについての
z[1]を取ることで得られます

95
00:07:58,023 --> 00:08:03,575
つまりz[1](1)、z[1](2)から

96
00:08:03,575 --> 00:08:10,207
z[1](m)までをこのように並べると
Z[1]になるのです

97
00:08:10,207 --> 00:08:15,042
するとZ[1]はn[1]×1ではなく

98
00:08:15,042 --> 00:08:20,285
n[1]×mになります
ここでmは教師セットのサイズです

99
00:08:20,285 --> 00:08:26,140
W[1]の次元数はそのままなので
n[1]×n[0]です

100
00:08:26,140 --> 00:08:29,201
Xはn[0]×1ではなくなって

101
00:08:29,201 --> 00:08:33,431
全ての教師サンプルが
水平に並べられたものなので

102
00:08:33,431 --> 00:08:38,565
今はn[0]×m次元になっています

103
00:08:38,565 --> 00:08:43,833
n[1]×n[0]の行列に
n[0]×mの行列をかけると

104
00:08:43,833 --> 00:08:50,160
期待通りn[1]×mの行列が
得られることがわかりますね

105
00:08:50,160 --> 00:08:55,030
最後になりますが
b[1]はn[1]×1のままですが

106
00:08:55,030 --> 00:09:01,147
これをbに足すと
Pythonのブロードキャスティングによって

107
00:09:01,147 --> 00:09:08,218
n[1]×mに複製され
要素ごとに足されることになります

108
00:09:08,218 --> 00:09:14,977
前のスライドではW、b、dW、dbの
次元数について話しました

109
00:09:14,977 --> 00:09:21,143
ここでお話ししているのは
z[l]とa[l]は

110
00:09:21,143 --> 00:09:26,922
n[l]×1なのに対して

111
00:09:26,922 --> 00:09:34,650
大文字のZ[l]とA[l]は
n[l]×mだということです

112
00:09:34,650 --> 00:09:40,410
特殊なのはlが０のときです

113
00:09:40,410 --> 00:09:45,188
そのときはA[0]は

114
00:09:45,188 --> 00:09:49,543
教師サンプルの特徴量Xに等しく

115
00:09:49,543 --> 00:09:54,616
予想できる通りn[0]×m次元になります

116
00:09:54,616 --> 00:10:01,259
もちろん逆伝播で実装するときにも

117
00:10:01,259 --> 00:10:06,749
後で学びますが
dZやdAを計算します

118
00:10:06,749 --> 00:10:11,327
これらは当然

119
00:10:11,327 --> 00:10:15,736
ZとAと同じ次元数となります

120
00:10:15,736 --> 00:10:19,467
ここまで見てきた例によって

121
00:10:19,467 --> 00:10:21,685
扱う行列の次元数を明確に
できたならうれしいです

122
00:10:21,685 --> 00:10:25,947
深層ニューラルネットワークを
実装するときに

123
00:10:25,947 --> 00:10:30,350
全ての行列が一貫していることを
確認するのは

124
00:10:30,350 --> 00:10:31,825
助けになるはずです

125
00:10:31,825 --> 00:10:35,908
きっと あり得るバグの原因を
潰してくれます

126
00:10:35,908 --> 00:10:40,325
いろいろな行列の次元数を求める
練習問題が

127
00:10:40,325 --> 00:10:41,979
役に立っていたら幸いです

128
00:10:41,979 --> 00:10:44,788
深層ニューラルネットワークの実装で

129
00:10:44,788 --> 00:10:48,241
これらの行列やベクトルの次元数を
きちんと整えていたら

130
00:10:48,241 --> 00:10:52,162
いくつかのバグの原因を消すのに
役立つでしょう

131
00:10:52,162 --> 00:10:54,467
実際私のコードを正しくするのを
助けてくれています

132
00:10:54,467 --> 00:10:58,882
ここまではニューラルネットワークで
順伝播を行う

133
00:10:58,882 --> 00:11:01,227
方法を見てきました

134
00:11:01,227 --> 00:11:04,163
しかしなぜ深層ニューラルネットワークは
こんなに効果的で

135
00:11:04,163 --> 00:11:07,243
浅いものよりも効果があるのでしょう

136
00:11:07,243 --> 00:11:09,939
次の動画で数分間
そのことについて話しましょう