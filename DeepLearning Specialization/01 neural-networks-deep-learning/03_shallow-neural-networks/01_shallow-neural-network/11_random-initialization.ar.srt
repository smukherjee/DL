1
00:00:00,000 --> 00:00:01,619
عندما تقوم بتغيير شبكتك العصبية،

2
00:00:01,619 --> 00:00:03,955
يكون من المهم تهيئة
الأثقال بشكل عشوائي.

3
00:00:03,955 --> 00:00:08,426
بالنسبة للانحدار اللوجستي، كان من
المقبول تهيئة الأثقال إلى صفر.

4
00:00:08,426 --> 00:00:12,258
ولكن بالنسبة للشبكة العصبية، إذا قمت
بتهيئة الأثقال للمتغيرات إلى صفر

5
00:00:12,258 --> 00:00:14,706
ثم تطبيق الانحدار التدريجي،
فإنها لن تعمل.

6
00:00:14,706 --> 00:00:15,289
دعونا نرى السبب.

7
00:00:15,289 --> 00:00:20,173
إذًا، لديك هنا خاصيتا إدخال، إذًا

8
00:00:20,173 --> 00:00:25,953
n0=2، ووحدتان مخفيتان، إذًا n1=2.

9
00:00:25,953 --> 00:00:31,547
وبالتالي فإن المصفوفة المرتبطة
بالطبقة المخفية،

10
00:00:31,547 --> 00:00:35,373
w1، ستكون 2 في 2.

11
00:00:35,373 --> 00:00:41,230
لنفترض أنك قمت بتهيئته للكل إلى 0،
إذًا مصفوفة 0 0 0 0، 2 في 2.

12
00:00:41,230 --> 00:00:45,531
ولنفترض أن B1 يساوي أيضًا 0 0.

13
00:00:45,531 --> 00:00:50,788
يتضح أن تهيئة مصطلحات
التحيز b إلى 0 مقبولة في الواقع،

14
00:00:50,788 --> 00:00:54,240
ولكن تهيئة w لجميع 0 تمثل مشكلة.

15
00:00:54,240 --> 00:00:59,625
إذًا، فالمشكلة في هذه
الصياغة هي أنه

16
00:00:59,625 --> 00:01:05,522
بالنسبة لأي مثال تقوم بتقديمه،
فسوف يكون لديك a1,1

17
00:01:05,522 --> 00:01:09,253
وa1,2، سيكونان متساويين، تمام؟

18
00:01:09,253 --> 00:01:12,613
إذًا، فهذا التنشيط وذاك
التنشيط سيكونان متشابهين،

19
00:01:12,613 --> 00:01:17,170
لأن كلتا الوحدتين المخفيتين تقومان
بحساب نفس الدالة بالضبط.

20
00:01:17,170 --> 00:01:21,810
وبعد ذلك، عندما تقوم
بحساب الانتشار الخلفي،

21
00:01:21,810 --> 00:01:24,478
يتبين أن dz11

22
00:01:24,478 --> 00:01:30,165
وdz12 سيكونان أيضًا بنفس
اللون بالتناظر، تمام؟

23
00:01:30,165 --> 00:01:33,720
كلا هاتين الوحدتين المخفيتين
سيقومان بالتهيئة بالطريقة نفسها.

24
00:01:33,720 --> 00:01:36,080
من الناحية الفنية، بالنسبة لما أقول،

25
00:01:36,080 --> 00:01:39,851
أفترض أن الأثقال الصادرة
متطابقة أيضًا.

26
00:01:39,851 --> 00:01:45,122
إذًا، w2 هذا يساوي 0 0.

27
00:01:45,122 --> 00:01:48,691
لكن إذا قمت بتهيئة الشبكة
العصبية بهذه الطريقة،

28
00:01:48,691 --> 00:01:53,590
فإن هذه الوحدة المخفية وتلك
الوحدة المخفية تتطابقان تمامًا.

29
00:01:53,590 --> 00:01:57,011
في بعض الأحيان،
تكونان متماثلتين تمامًا،

30
00:01:57,011 --> 00:02:01,687
مما يعني أنهما تكملان
نفس الدالة بالضبط.

31
00:02:01,687 --> 00:02:03,765
وعن طريق نوع من الإثبات بالاستقراء،

32
00:02:03,765 --> 00:02:08,064
يتبين أنه بعد كل تكرار فردي
للتدريب، لا تزال الوحدتان

33
00:02:08,064 --> 00:02:11,272
المخفيتان تقومان بحساب
الدالة نفسها بالضبط.

34
00:02:11,272 --> 00:02:17,521
بما أن [كلام غير مسموع] يبين أن dw
سوف يكون مصفوفة تبدو هكذا.

35
00:02:17,521 --> 00:02:20,681
حيث يأخذ كل صف نفس القيمة.

36
00:02:20,681 --> 00:02:23,318
إذًا، نقوم بتنفيذ تحديث للثقل.

37
00:02:23,318 --> 00:02:30,163
إذًا، عند تنفيذ تحديث للثقل،
يتم تحديث w1 ليكون w1 - ألفا×dw

38
00:02:30,163 --> 00:02:33,740
تجد أن w1، بعد كل تكرار،

39
00:02:33,740 --> 00:02:37,616
سيكون صفها الأول
مساويًا للصف الثاني.

40
00:02:37,616 --> 00:02:41,487
لذلك من الممكن إنشاء إثبات
بالاستقراء أنه إذا قمت

41
00:02:41,487 --> 00:02:44,688
بتهيئة جميع الطرق،
فإن كل قيم w إلى 0،

42
00:02:44,688 --> 00:02:49,164
ثم لأن كلتا الوحدتين المخفيتين
تبدأ بحساب الدالة نفسها.

43
00:02:49,164 --> 00:02:53,541
وكلا الوحدتين المخفيتين لهما التأثير
نفسه على وحدة الإخراج،

44
00:02:53,541 --> 00:02:57,542
ثم بعد تكرار واحد،
يظل نفس المنطوق صحيحًا،

45
00:02:57,542 --> 00:03:00,273
والوحدتان المخفيتان تظلان متماثلتان.

46
00:03:00,273 --> 00:03:04,507
وبالتالي، عن طريق الاستقراء،
بعد تكرارين، وثلاثة تكرارات وهكذا،

47
00:03:04,507 --> 00:03:07,013
بغض النظر عن مدة تدريب
شبكتك العصبية،

48
00:03:07,013 --> 00:03:10,373
تظل كلتا الوحدتين المخفيتين
تحسبان الدالة نفسها بالضبط.

49
00:03:10,373 --> 00:03:15,212
وبالتالي في هذه الحالة، لا توجد في الواقع
نقطة تمتلك أكثر من وحدة مخفية.

50
00:03:15,212 --> 00:03:17,692
لأنها جميعًا تحسب
الشيء نفسه.

51
00:03:17,692 --> 00:03:22,378
وبالطبع، بالنسبة للشبكات العصبية الأكبر،
مثل التي تتكون من ثلاث خصائص

52
00:03:22,378 --> 00:03:24,972
وربما عدد كبير جدًا من الوحدات المخفية،

53
00:03:24,972 --> 00:03:29,239
فإن وسيطة مشابهة تعمل على إظهار ذلك
مع شبكة عصبية مثل هذه.

54
00:03:29,239 --> 00:03:34,107
[كلام غير مسموع] نرسم جميع الحواف،
إذا قمت بتهيئة الأثقال إلى صفر،

55
00:03:34,107 --> 00:03:37,103
ثم كانت كل الوحدات
المخفية لديك متماثلة.

56
00:03:37,103 --> 00:03:40,603
وبغض النظر عن المدة التي
تقوم فيها بترقية المركز،

57
00:03:40,603 --> 00:03:44,037
تستمر جميعها في حساب
الدالة نفسها بالضبط.

58
00:03:44,037 --> 00:03:48,785
إذًا، هذا ليس مفيدًا،
لأنك تريد أن تقوم

59
00:03:48,785 --> 00:03:52,835
الوحدات المخفية المختلفة
بحساب دوال مختلفة.

60
00:03:52,835 --> 00:03:57,748
الحل لذلك هو تهيئة
المتغيرات بشكل عشوائي.

61
00:03:57,748 --> 00:03:58,677
إذًا، إليك ما تفعله.

62
00:03:58,677 --> 00:04:04,053
يمكنك تعيين w1 = np.random.randn.

63
00:04:04,053 --> 00:04:07,037
هذا يولّد متغير
غاوسي عشوائيًا (2,2).

64
00:04:07,037 --> 00:04:12,358
ثم عادةً، تضرب هذا العدد
الصغير جدًا، مثل 0,01.

65
00:04:12,358 --> 00:04:14,951
إذًا، قم بتهيئته إلى قيم
عشوائية صغيرة جدًا.

66
00:04:14,951 --> 00:04:20,590
ومن ثم b، يتبين أن b لا
يعاني من مشكلة التناظر،

67
00:04:20,590 --> 00:04:24,735
ما تسمى مشكلة
كسر التناظر.

68
00:04:24,735 --> 00:04:29,370
إذًا، لا بأس من تهيئة b
إلى أصفار فقط.

69
00:04:29,370 --> 00:04:32,166
لأنه طالما تمت
تهيئة w بشكل عشوائي،

70
00:04:32,166 --> 00:04:36,769
فستبدأ بالوحدات المخفية المختلفة
التي تحسب أشياء مختلفة.

71
00:04:36,769 --> 00:04:40,912
وهكذا تتخلص من مشكلة
كسر التناظر هذه.

72
00:04:40,912 --> 00:04:43,795
وبالمثل، بالنسبة إلى w2، ستقوم
بتهيئة ذلك بشكل عشوائي.

73
00:04:43,795 --> 00:04:48,858
وكذلك b2، يمكنك تهيئة ذلك إلى 0.

74
00:04:48,858 --> 00:04:55,321
لذا قد تتساءل، من أين جاء
هذا الثابت ولماذا يساوي 0,01؟

75
00:04:55,321 --> 00:04:58,478
لماذا لا نضع الرقم 100 أو 1000؟

76
00:04:58,478 --> 00:05:02,313
تبين أننا عادة
نفضل تهيئة

77
00:05:02,313 --> 00:05:05,763
الأثقال إلى قيم عشوائية صغيرة جدًا.

78
00:05:05,763 --> 00:05:10,443
لأنه إذا كنت تستخدم دالة تنشيط
سينية أو ظل زائدي، أو

79
00:05:10,443 --> 00:05:14,047
دالة سينية أخرى،
حتى عند طبقة الإخراج.

80
00:05:14,047 --> 00:05:17,922
إذا كانت الأثقال كبيرة جدًا،

81
00:05:17,922 --> 00:05:23,967
فعند حساب
قيم التنشيط،

82
00:05:23,967 --> 00:05:28,621
تذكر أن z[1]=w1 x + b.

83
00:05:28,621 --> 00:05:34,094
ثم a1 هي دالة التنشيط
المطبقة على z1.

84
00:05:34,094 --> 00:05:39,097
إذًا، إذا كان w كبيرًا جدًا، فسيكون z كبيرًا،
أو على الأقل ستكون بعض

85
00:05:39,097 --> 00:05:44,235
قيم z كبيرة جدًا أو
صغيرة جدًا.

86
00:05:44,235 --> 00:05:49,789
وهكذا في هذه الحالة، من المرجح أن ينتهي
بك الأمر في هذه الأجزاء المكتظة من

87
00:05:49,789 --> 00:05:55,699
دالة الظل الزائدي أو الدالة السينية، حيث
يكون المنحدر أو التدرج صغيرًا جدًا.

88
00:05:55,699 --> 00:05:58,302
مما يعني أن الانحدار التدريجي
سيكون بطيئًا جدًا.

89
00:05:58,302 --> 00:05:59,730
لذلك كان التعلم بطيئًا جدًا.

90
00:05:59,730 --> 00:06:04,133
إذًا، باختصار، إذا كان حجم w كبيرًا جدًا،
فمن المرجح أن ينتهي بك المطاف،

91
00:06:04,133 --> 00:06:08,633
حتى في بداية التدريب،
مع قيم كبيرة جدًا من z.

92
00:06:08,633 --> 00:06:13,525
مما يؤدي إلى تشبع دالة الظل الزائدي
أو دالة التنشيط السينية،

93
00:06:13,525 --> 00:06:15,418
وبالتالي إبطاء التعلم.

94
00:06:15,418 --> 00:06:17,231
إذا لم تكن لديك أي دوال تنشيط

95
00:06:17,231 --> 00:06:22,149
سينية أو ظل زائدي في جميع أنحاء
الشبكة العصبية، فهذه مشكلة ثانوية.

96
00:06:22,149 --> 00:06:26,506
ولكن إذا كنت تقوم بتصنيف ثنائي،
ووحدة الإخراج لديك دالة سينية،

97
00:06:26,506 --> 00:06:30,806
فأنت لا تريد أن تكون المتغيرات
الأولية كبيرة جدًا.

98
00:06:30,806 --> 00:06:35,435
ولهذا السبب، فإن الضرب في 0.01
سيكون أمرًا يستحق المحاولة، أو

99
00:06:35,435 --> 00:06:36,872
أي رقم صغير آخر.

100
00:06:36,872 --> 00:06:38,536
ونفس الشيء بالنسبة إلى w2، تمام؟

101
00:06:38,536 --> 00:06:44,295
وهذا يمكن أن يكون random.random.

102
00:06:44,295 --> 00:06:49,545
أعتقد أن هذا سيكون 1 في 2
في هذا المثال، مضروبًا في 0.01.

103
00:06:49,545 --> 00:06:51,404
هناك s مفقودة.

104
00:06:51,404 --> 00:07:00,085
وأخيرًا، تبين أنه في بعض الأحيان يمكن أن
تكون هناك ثوابت أفضل من 0.01.

105
00:07:00,085 --> 00:07:04,304
عندما تقوم بتدريب شبكة عصبية
ذات طبقة مخفية واحدة فقط،

106
00:07:04,304 --> 00:07:09,129
فهي عبارة عن شبكة عصبية سطحية نسبيًا،
بدون طبقات مخفية كثيرة.

107
00:07:09,129 --> 00:07:12,392
بتعيينها إلى 0.01 ستعمل على ما يرام على الأرجح.

108
00:07:12,392 --> 00:07:15,705
ولكن عندما تقوم بتدريب شبكة
عصبية عميقة للغاية،

109
00:07:15,705 --> 00:07:19,294
فقد تحتاج إلى اختيار ثابت
مختلف عن 0.01.

110
00:07:19,294 --> 00:07:23,642
وفي مادة الأسبوع المقبل،
سنتحدث قليلاً عن كيفية

111
00:07:23,642 --> 00:07:27,925
وتوقيت وجوب اختيار
ثابت مختلف عن 0.01.

112
00:07:27,925 --> 00:07:32,008
لكن في كلتا الحالتين، عادة ما ينتهي الأمر
به إلى أن يكون عددًا صغيرًا نسبيًا.

113
00:07:32,008 --> 00:07:34,584
إذًا، سنتناول هذا في مقاطع فيديو هذا الأسبوع.

114
00:07:34,584 --> 00:07:38,348
أنت الآن تعرف كيفية إعداد شبكة
عصبية ذات طبقة مخفية،

115
00:07:38,348 --> 00:07:42,430
وتهيئة المتغيرات وإجراء التنبؤات
باستخدام [كلام غير مسموع].

116
00:07:42,430 --> 00:07:45,445
وكذلك حساب المشتقات
وتنفيذ الانحدار التدريجي،

117
00:07:45,445 --> 00:07:46,275
وذلك باستخدام الانتشار الخلفي.

118
00:07:46,275 --> 00:07:48,654
إذًا، يجب أن تكون قادرًا
على إجراء الاختبارات الموجزة،

119
00:07:48,654 --> 00:07:51,166
بالإضافة إلى تمارين البرمجة
لهذا الأسبوع.

120
00:07:51,166 --> 00:07:52,143
حظًا موفقًا مع ذلك.

121
00:07:52,143 --> 00:07:54,802
آمل أن تستمتع
بالتدريب على المسائل،

122
00:07:54,802 --> 00:07:57,728
وأتطلع إلى رؤيتك في
مواد الأسبوع الرابع.