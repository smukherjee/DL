1
00:00:00,320 --> 00:00:04,140
यदि मूल तकनीकी विचार डीप लर्निंग के पीछे,

2
00:00:04,140 --> 00:00:07,110
न्यूरल नेटवर्क्स के पीछे, हैं कई दशकों से,

3
00:00:07,110 --> 00:00:08,970
तो वे अब क्यों प्रचलित हो रहे हैं?

4
00:00:08,970 --> 00:00:11,310
इस वीडियो में, चलो देखते हैं कुछ

5
00:00:11,310 --> 00:00:14,690
मुख्य घटक डीप लर्निंग की प्रसिद्धि के पीछे 
क्योंकि मैं सोचता हूँ यह

6
00:00:14,690 --> 00:00:17,540
मदद करेगा आपकी जानने के लिए बढ़िया अवसर

7
00:00:17,540 --> 00:00:20,560
आपके अपने संगठन में इन्हें अप्लाई करने के लिए.

8
00:00:20,560 --> 00:00:21,690
पिछले कुछ सालों में,

9
00:00:21,690 --> 00:00:23,095
बहुत से लोगों ने मुझसे पूछा है,

10
00:00:23,095 --> 00:00:25,880
"एंड्रयू, क्यों डीप लर्निंग निश्चित रूप से इतनी अच्छी तरह 
से काम कर रहा है?"

11
00:00:25,880 --> 00:00:30,460
जब मुझसे पूछा जाता है वह सवाल,
 मैं आम तौर पर यह चित्र उनके लिए बनाता हूँ.

12
00:00:30,460 --> 00:00:33,830
मान लो हम प्लॉट करते हैं एक चित्र 
जहाँ हॉरिज़ॉंटल ऐक्सिस पर

13
00:00:33,830 --> 00:00:37,675
प्लॉट करते हैं डेटा की मात्रा जो हमारे पास है 
एक टास्क के लिए.

14
00:00:37,675 --> 00:00:40,040
मान लो वर्टिकल ऐक्सिस पर,

15
00:00:40,040 --> 00:00:43,610
प्लॉट करते हैं हमारे लर्निंग अल्गोरिद्म की पर्फ़ॉर्मन्स.

16
00:00:43,610 --> 00:00:47,660
जैसे एक्यूरेसी हमारे स्पैम क्लैसिफ़ायअर की या हमारे

17
00:00:47,660 --> 00:00:50,870
ऐड क्लिक प्रिडिक्टर की या एक्यूरेसी

18
00:00:50,870 --> 00:00:55,980
हमारे न्यूरल नेट की समझने के लिए स्थिति 
अन्य कारों की हमारी सेल्फ़-ड्राइविंग कार के लिए.

19
00:00:55,980 --> 00:01:00,680
ऐसा होता है यदि आप प्लॉट करते हैं पर्फ़ॉर्मन्स 
एक पारम्परिक लर्निंग अल्गोरिद्म की जैसे

20
00:01:00,680 --> 00:01:02,090
सपोर्ट वेक्टर मशीन या

21
00:01:02,090 --> 00:01:06,340
लॉजिस्टिक रेग्रेशन की डेटा की मात्रा के 
एक फ़ंक्शन की तरह जो आपके पास है,

22
00:01:06,340 --> 00:01:09,185
आपको शायद मिले एक कर्व जो शायद दिखे ऐसा,

23
00:01:09,185 --> 00:01:14,220
जहां पर्फ़ॉर्मन्स में सुधार होता है कुछ समय के लिए 
जैसे आप जोड़ते हैं अधिक डेटा लेकिन कुछ समय के बाद,

24
00:01:14,220 --> 00:01:17,565
पर्फ़ॉर्मन्स समतल हो जाती है.

25
00:01:17,565 --> 00:01:20,100
यह हो जाती है एक हॉरिज़ॉंटल लाइन, 
बहुत अच्छा बनाया नहीं.

26
00:01:20,100 --> 00:01:26,760
यह ऐसा था कि उन्हें नहीं पता कि क्या करें डेटा
 की विशाल मात्रा से.

27
00:01:26,760 --> 00:01:29,270
हमारे समाज में क्या हुआ कुछ

28
00:01:29,270 --> 00:01:32,360
पिछले 20 सालों में शायद कि बहुत से प्राब्लम्ज़ के लिए,

29
00:01:32,360 --> 00:01:35,900
हम पहुँचे अपेक्षाकृत बहुत कम मात्रा से डेटा की

30
00:01:35,900 --> 00:01:39,985
डेटा की एक काफी बड़ी मात्रा पर

31
00:01:39,985 --> 00:01:43,430
जहाँ इसमें से बहुत था- डिजिटलीकरण की वजह से

32
00:01:43,430 --> 00:01:48,020
एक समाज के जहाँ इतनी मानवीय गतिविधि है 
अब डिजिटल क्षेत्र में.

33
00:01:48,020 --> 00:01:50,810
हम अपने कंप्यूटर पर इतना समय खर्च करते हैं,
 वेबसाइटों पर,

34
00:01:50,810 --> 00:01:56,475
ऐप्स पर और गतिविधियाँ डिजिटल डिवाइसेज़ 
पर बनाती हैं डेटा.

35
00:01:56,475 --> 00:01:59,720
वृद्धि सस्ते कैमरों की,

36
00:01:59,720 --> 00:02:01,970
जो हमारे सेल फ़ोन में हैं, एक्सेलेरोमीटर्स,

37
00:02:01,970 --> 00:02:05,645
सभी तरह के सेन्सर्ज़ इंटर्नेट ऑफ़ थिंग्ज़ में,

38
00:02:05,645 --> 00:02:09,350
हम इसके अलावा भी इकट्ठा कर रहे हैं और अधिक डेटा.

39
00:02:09,350 --> 00:02:12,590
तो, पिछले 20 सालों में बहुत सी ऐप्लिकेशन्स के लिए,

40
00:02:12,590 --> 00:02:15,935
हम सिर्फ संचित कर रहे हैं बहुत अधिक डेटा जो ज़्यादा है

41
00:02:15,935 --> 00:02:20,015
उससे जिसका पारम्परिक लर्निंग अल्गोरिद्म्स
 लाभ उठा पा रहे थे.

42
00:02:20,015 --> 00:02:23,465
न्यूरल नेटवर्क्स से, ऐसा हुआ कि यदि आप ट्रेन करते हैं

43
00:02:23,465 --> 00:02:28,865
एक छोटा न्यूरल नेट, तब इसकी पर्फ़ॉर्मन्स होगी 
शायद दिखेगी ऐसी,

44
00:02:28,865 --> 00:02:31,765
यदि आप ट्रेन करते हैं एक थोड़ा बड़ा न्यूरल नेटवर्क,

45
00:02:31,765 --> 00:02:34,840
जिसे कहते हैं एक मध्यम साइज़ का न्यूरल नेट,

46
00:02:34,840 --> 00:02:40,275
पर्फ़ॉर्मन्स होगी थोड़ी बेहतर, 
और यदि आप ट्रेन करते हैं एक बहुत बड़ा न्यूरल नेट,

47
00:02:40,275 --> 00:02:43,750
तब पर्फ़ॉर्मन्स होती जाती है बेहतर और बेहतर.

48
00:02:43,750 --> 00:02:45,965
तो, कुछ पर्यवेक्षण, एक है,

49
00:02:45,965 --> 00:02:49,150
यदि आप चाहते हैं इस बहुत ऊँचे स्तर पर पर्फ़ॉर्मन्स तब,

50
00:02:49,150 --> 00:02:50,720
दो चीज़ों की जरूरत है.

51
00:02:50,720 --> 00:02:53,660
पहला, अक्सर आपको ट्रेन कर पाना चाहिए

52
00:02:53,660 --> 00:02:56,540
एक पर्याप्त बड़ा न्यूरल नेटवर्क ले पाने के लिए

53
00:02:56,540 --> 00:02:59,630
लाभ एक बड़ी विशाल मात्रा में डेटा का और दूसरा,

54
00:02:59,630 --> 00:03:01,760
आपको होना चाहिए यहाँ x ऐक्सिस पर,

55
00:03:01,760 --> 00:03:03,850
आपको चाहिए बहुत सा डेटा.

56
00:03:03,850 --> 00:03:09,590
तो, हम अक्सर कहते है कि स्केल चला रहा है 
डीप लर्निंग की प्रगति और स्केल से,

57
00:03:09,590 --> 00:03:12,380
मेरा मतलब है दोनो साइज़ न्यूरल नेटवर्क का,

58
00:03:12,380 --> 00:03:16,090
मतलब सिर्फ़ एक न्यूरल नेटवर्क जिसमें हैं 
बहुत सी हिडन यूनिट्स, बहुत से पेरमिटरज़,

59
00:03:16,090 --> 00:03:21,055
बहुत से कनेक्शनज़ तथा डेटा का स्केल भी.

60
00:03:21,055 --> 00:03:25,190
वास्तव में आज सबसे भरोसेमंद तरीकों में से 
एक तरीका बेहतर पर्फ़ॉर्मन्स का

61
00:03:25,190 --> 00:03:29,690
एक न्यूरल नेटवर्क में है अक्सर या तो ट्रेन करना 
एक बड़ा नेटवर्क या डालना अधिक डेटा

62
00:03:29,690 --> 00:03:32,210
उसमें और वह काम करता है केवल एक पोईँट तक क्योंकि

63
00:03:32,210 --> 00:03:35,510
अंततः आपका सभी डेटा समाप्त हो जाएगा
 या अंततः न्यूरल नेटवर्क हो जाएगा

64
00:03:35,510 --> 00:03:38,120
इतना बड़ा कि इसे बहुत समय लगेगा ट्रेन करने में. लेकिन

65
00:03:38,120 --> 00:03:43,325
सिर्फ़ बढ़ाने से स्केल हम पहुँच पाए हैं 
इतना आगे डीप लर्निंग की दुनिया में.

66
00:03:43,325 --> 00:03:46,860
बनाने के लिए इस चित्र को थोड़ा और तकनीकी रूप से सही,

67
00:03:46,860 --> 00:03:48,815
मैं जोड़ता हूँ इसमें कुछ और चीज़ें.

68
00:03:48,815 --> 00:03:51,465
मैंने लिखा डेटा की मात्रा x ऐक्सिस पर.

69
00:03:51,465 --> 00:03:56,240
तकनीकी रूप से, यह है मात्रा लेबल्ड डेटा की

70
00:03:56,240 --> 00:04:02,795
जहाँ लेबल्ड डेटा से मेरा मतलब है ट्रेनिंग इग्ज़ाम्पल्ज़ 
जिनमें है दोनो इनपुट x और लेबल y.

71
00:04:02,795 --> 00:04:07,500
देने के लिए परिचय थोड़ी और नोटेशन का जो 
हम बाद में इस्तेमाल करेंगे इस कोर्स में,

72
00:04:07,500 --> 00:04:12,460
हम अपने प्रशिक्षण सेट के आकार को निरूपित करने के 
लिए लोअरकेस वर्णमाला के m का उपयोग करेंगे.

73
00:04:12,460 --> 00:04:17,390
तो, ट्रेनिंग इग्ज़ाम्पल्ज़ की संख्या है लोअरकेस m. 
तो वह है हॉरिज़ॉंटल ऐक्सिस.

74
00:04:17,390 --> 00:04:20,300
कुछ और विवरण इस चित्र में,

75
00:04:20,300 --> 00:04:24,080
छोटे ट्रेनिंग सेट की इस प्रणाली में,

76
00:04:24,080 --> 00:04:28,970
आपेक्षिक क्रम अल्गोरिद्म्स का वास्तव में 
अच्छे ढंग से परिभाषित नहीं है.

77
00:04:28,970 --> 00:04:32,690
आपके पास नहीं है बहुत अधिक ट्रेनिंग डेटा, यह है अक्सर

78
00:04:32,690 --> 00:04:37,120
आपके ऊपर ख़ुद से बनाना फ़ीचर्स 
जो निर्धारित करता है पर्फ़ॉर्मन्स.

79
00:04:37,120 --> 00:04:41,810
तो, यह काफ़ी सम्भव है कि यदि कोई ट्रेन
 कर रहा है एक SVM वह है

80
00:04:41,810 --> 00:04:46,640
अधिक प्रेरित हाथ से फ़ीचर बनाने में अपेक्षा में किसी से 
जो ट्रेन कर रहा है एक उससे बड़ा न्यूरल नेट,

81
00:04:46,640 --> 00:04:49,260
शायद छोटे ट्रेनिंग सेट क्षेत्र में,

82
00:04:49,260 --> 00:04:50,815
SVM शायद बेहतर काम करे.

83
00:04:50,815 --> 00:04:54,140
तो इस क्षेत्र में बाईं तरफ़ चित्र में,

84
00:04:54,140 --> 00:04:56,990
आपेक्षिक क्रम अल्गोरिद्म्स का वास्तव में 
नहीं है उतने

85
00:04:56,990 --> 00:05:00,800
अच्छे ढंग से परिभाषित और पर्फ़ॉर्मन्स निर्भर करता है 
अधिक आपके कौशल पर

86
00:05:00,800 --> 00:05:03,380
फ़ीचर्ज़ को हैंडल करने की तथा अन्य विवरण पर

87
00:05:03,380 --> 00:05:07,700
अल्गोरिद्म्स के और केवल इस बिग डेटा के क्षेत्र में,

88
00:05:07,700 --> 00:05:09,080
बहुत बड़े ट्रेनिंग सेट्स,

89
00:05:09,080 --> 00:05:12,320
बहुत बड़े m के क्षेत्र में दाईं तरफ़ जहाँ हम अधिक

90
00:05:12,320 --> 00:05:16,960
लगातार रूप से देखते हैं न्यूरल नेट श्रेष्ठ है 
अन्य तरीक़ों की अपेक्षा.

91
00:05:16,960 --> 00:05:21,315
यदि आपके कोई मित्र पूछते हैं कि 
क्यों न्यूरल नेट प्रचलित हो रहे हैं?

92
00:05:21,315 --> 00:05:24,820
आपको प्रोत्साहित करूँगा बनाने के लिए
 यह चित्र उनके लिए भी.

93
00:05:24,820 --> 00:05:27,695
तो, मैं कहूंगा कि शुरुआती दिनों में,

94
00:05:27,695 --> 00:05:29,450
आधुनिक डीप लर्निंग की प्रगति के पीछे,

95
00:05:29,450 --> 00:05:33,395
यह था स्केल किया गया डेटा और स्केल कॉम्प्यूटेशन का.

96
00:05:33,395 --> 00:05:36,425
और हमारी क्षमता ट्रेन करने की बहुत बड़े न्यूरल नेटवर्क्स,

97
00:05:36,425 --> 00:05:38,420
एक CPU पर या एक GPU पर,

98
00:05:38,420 --> 00:05:41,555
जिसने हमें सक्षम किया बहुत सी प्रगति करने में.

99
00:05:41,555 --> 00:05:44,320
लेकिन तेज़ी से, विशेष रूप से पिछले कुछ सालों में

100
00:05:44,320 --> 00:05:47,980
हम देख रहे हैं ज़बरदस्त नवीनता अल्गोरिद्म्स में भी.

101
00:05:47,980 --> 00:05:50,830
तो, मैं नहीं चाहता उसे कम श्रेय देना.

102
00:05:50,830 --> 00:05:54,995
दिलचस्प रूप से, बहुत से अल्गोरिद्म्स में नवीनता है

103
00:05:54,995 --> 00:06:00,285
करने के लिए कि न्यूरल नेट्वर्क अधिक तेज़ी से काम करें.

104
00:06:00,285 --> 00:06:02,940
एक ठोस उदाहरण के जैसे,

105
00:06:02,940 --> 00:06:05,210
एक बहुत बड़ी सफलता न्यूरल नेटवर्क्स में हुई है

106
00:06:05,210 --> 00:06:08,125
परिवर्तन एक सिग्मोईड फ़ंक्शन से जो,

107
00:06:08,125 --> 00:06:09,840
जो ऐसा दिखता है,

108
00:06:09,840 --> 00:06:13,535
से एक ReLu फ़ंक्शन पर जिसकी हमने बात की

109
00:06:13,535 --> 00:06:17,855
संक्षेप में एक पहले के वीडियो में जो दिखता है ऐसा.

110
00:06:17,855 --> 00:06:22,010
यदि आप नहीं समझ पा रहे हैं जो मैं कहना चाहता हूँ, 
चिंता न करें इसकी.

111
00:06:22,010 --> 00:06:25,820
लेकिन ऐसा होता है कि बहुत में से एक प्रॉब्लम इस्तेमाल 
करने में सिग्मोईड फ़ंक्शनज़

112
00:06:25,820 --> 00:06:29,980
मशीन लर्निंग में है कि ऐसे क्षेत्र हैं यहाँ 
जहाँ स्लोप फ़ंक्शन की,

113
00:06:29,980 --> 00:06:31,905
जहां ग्रैडिएंट लगभग शून्य है,

114
00:06:31,905 --> 00:06:34,685
और इसलिए लर्निंग बहुत धीरे होती है

115
00:06:34,685 --> 00:06:37,945
क्योंकि जब आप इम्प्लमेंट करते हैं 
ग्रेडीयंट डिसेंट और ग्रेडीयंट है ज़ीरो,

116
00:06:37,945 --> 00:06:41,820
पेरमिटर्स बदलते हैं बहुत धीरे और 
इसलिए लर्निंग बहुत धीरे होती है.

117
00:06:41,820 --> 00:06:47,090
जबकि बदलने से ऐक्टिवेशन फ़ंक्शन न्यूरल नेटवर्क का

118
00:06:47,090 --> 00:06:52,835
इस फ़ंक्शन में जिसे कहते हैं ReLu फ़ंक्शन 
या रेक्टिफ़ायड लिनीअर यूनिट,

119
00:06:52,835 --> 00:06:58,350
ReLu, ग्रेडीयंट है बराबर एक सभी पॉज़िटिव
 वैल्यूज़ के लिए इनपुट की, ठीक है.

120
00:06:58,350 --> 00:07:03,130
तो ग्रेडीयंट की काफ़ी कम सम्भावना है 
धीरे-धीरे शून्य होने की.

121
00:07:03,130 --> 00:07:05,720
ग्रेडीयंट यहाँ, स्लोप इस लाइन की है ज़ीरो

122
00:07:05,720 --> 00:07:08,960
बाईं तरफ़, लेकिन ऐसा होता है सिर्फ़ बदलने से

123
00:07:08,960 --> 00:07:12,320
सिग्मोईड फ़ंक्शन को ReLu फ़ंक्शन में

124
00:07:12,320 --> 00:07:16,380
बना दिया है एक अल्गोरिद्म को जिसे कहते हैं 
ग्रेडीयंट डिसेंट, काम अधिक तेज़ गति से.

125
00:07:16,380 --> 00:07:21,350
तो यह है उदाहरण शायद अपेक्षाकृत 
सरल नवीनता का अल्गोरिद्म में लेकिन

126
00:07:21,350 --> 00:07:26,755
अंततः प्रभाव जो यह एल्गोरिद्म की नवीनता का था 
इसने वास्तव में गणना में मदद की.

127
00:07:26,755 --> 00:07:29,765
तो, वास्तव में ऐसे बहुत से उदाहरण हैं इस तरह के

128
00:07:29,765 --> 00:07:33,230
जहाँ हमने अल्गोरिद्म को बदला क्योंकि 
इसमें हम चला पाए हमारा कोड

129
00:07:33,230 --> 00:07:37,400
तेज़ी से और इससे हम ट्रेन कर पाए एक बड़ा न्यूरल 
नेटवर्क या अनुमान कर पाए

130
00:07:37,400 --> 00:07:42,065
उचित मात्रा कोड की जब हमारे पास है
 एक बड़ा नेटवर्क या बहुत सारा डेटा.

131
00:07:42,065 --> 00:07:47,060
अन्य कारण कि त्वरित कॉम्प्यूटेशन है महत्वपूर्ण

132
00:07:47,060 --> 00:07:52,525
है कि ऐसा होता है कि प्रक्रिया न्यूरल नेटवर्क 
को ट्रेन करने की बहुत इटरेटिव है.

133
00:07:52,525 --> 00:07:54,050
अक्सर, आपके पास होता हैं एक आइडिया

134
00:07:54,050 --> 00:07:58,285
एक न्यूरल नेटवर्क के आर्किटेक्चर का और इसलिए 
आप इम्प्लमेंट करते हैं आपका आइडिया एक कोड में.

135
00:07:58,285 --> 00:08:01,999
जो इम्प्लमेंट करता है आपका आइडिया 
फिर करने देता है आपको रन एक प्रयोग

136
00:08:01,999 --> 00:08:05,880
जो बताता है आपको कितना बेहतर काम कर रहा है 
आपका न्यूरल नेटवर्क और फिर उसको देखने उसे,

137
00:08:05,880 --> 00:08:08,240
आप वापिस जाते हैं बदलने के लिए विवरण

138
00:08:08,240 --> 00:08:13,190
आपके न्यूरल नेटवर्क का और फिर आप जाते हैं 
इस चक्र में बार-बार.

139
00:08:13,190 --> 00:08:17,089
जब आपका न्यूरल नेट्वर्क लेता है बहुत 
अधिक समय ट्रेन होने में,

140
00:08:17,089 --> 00:08:21,140
सिर्फ़ अधिक समय लगता है इस साइकल में और वहाँ है

141
00:08:21,140 --> 00:08:27,215
एक बहुत अंतर आपकी उत्पादकता में बनाने में प्रभावी 
न्यूरल नेटवर्कस जब आपके पास हो सकता है

142
00:08:27,215 --> 00:08:32,455
एक आइडिया और आप परीक्षण कर सकते हैं और 
देख सकते हैं कि यह करेगा या नहीं 10 मिनट में.

143
00:08:32,455 --> 00:08:38,570
या शायद अधिक से अधिक एक दिन में तुलना में
 यदि आप ट्रेन करते हैं एक न्यूरल नेटवर्क एक महीने में.

144
00:08:38,570 --> 00:08:41,480
बेशक, कभी-कभी यह होता भी है 
क्योंकि यदि आपको मिलता है

145
00:08:41,480 --> 00:08:44,630
एक परिणाम वापिस 10 मिनट में या शायद एक दिन में,

146
00:08:44,630 --> 00:08:47,750
आप सिर्फ़ परीक्षण कर सकते है और 
अधिक आइडिया और पा सकते हैं

147
00:08:47,750 --> 00:08:52,005
संभवत: एक न्यूरल नेटवर्क जो बेहतर 
काम करता हैं आपकी ऐप्लिकेशन के लिए.

148
00:08:52,005 --> 00:08:57,320
तो, त्वरित कॉम्प्यूटेशन ने वास्तव में सहायता की है

149
00:08:57,320 --> 00:09:02,495
गति बढ़ाने में दर की जिस में आपको मिल सकता है 
प्रयोग का परिणाम वापिस और इससे

150
00:09:02,495 --> 00:09:06,950
वास्तव में मदद हुई है दोनो न्यूरल 
नेटवर्क बनाने वालों की तथा

151
00:09:06,950 --> 00:09:10,550
शोधकर्ताओं की जो डीप लर्निंग में 
काम कर रहे हैं इटरेट करने में

152
00:09:10,550 --> 00:09:14,530
अधिक तेज़ी से और सुधारने में आपके 
आइडिया अधिक तेज़ी से.

153
00:09:14,530 --> 00:09:17,570
तो, इस सब से आयी है अधिक तेज़ी

154
00:09:17,570 --> 00:09:22,390
पूरे डीप लर्निंग के शोध समुदाय में जो 
मुझे लगता है अविश्वसनीय है,

155
00:09:22,390 --> 00:09:27,470
नए अल्गोरिदम्स की खोज में और
 लगातार प्रगति कर रही है उस मोर्चे पर.

156
00:09:27,470 --> 00:09:33,200
तो हैं कुछ शक्तियाँ जो मदद कर रही हैं वृद्धि में 
डीप लर्निंग की लेकिन अच्छी ख़बर है

157
00:09:33,200 --> 00:09:38,825
कि ये ताक़तें अभी भी काम कर रही हैं शक्तिशाली
 रूप से बनाने के डीप लर्निंग और भी बेहतर डेटा को.

158
00:09:38,825 --> 00:09:42,890
समाज अभी भी बना रहा है अधिक और 
अधिक डिजिटल डेटा या ले जा रहा है

159
00:09:42,890 --> 00:09:47,630
कॉम्प्यूटेशन को, उदय से विशेष हार्डवेयर के 
जैसे GPU और तेज़ नेटवर्किंग से,

160
00:09:47,630 --> 00:09:49,595
कई प्रकार के हार्डवेयर.

161
00:09:49,595 --> 00:09:54,260
मुझे वास्तव में काफी विश्वास है कि हमारी क्षमता बहुत 
बड़े न्यूरल नेटवर्क का निर्माण करने की में हिस्सा होगा

162
00:09:54,260 --> 00:09:59,300
कॉम्प्यूटेशन दृष्टिकोण होगा आगे बेहतर और बनाएगा अल्गोरिद्म.

163
00:09:59,300 --> 00:10:02,150
जबकि पूरे डीप लर्निंग शोध समुदाय हालाँकि

164
00:10:02,150 --> 00:10:06,170
बने हुए है अभूतपूर्ण खोज करने में नए अल्गोरिद्म्स की.

165
00:10:06,170 --> 00:10:07,550
तो, इस वजह से,

166
00:10:07,550 --> 00:10:09,530
मुझे लगता है कि हम आशावादी हो सकते हैं.

167
00:10:09,530 --> 00:10:11,720
मैं निश्चित रूप से आशावादी हूं कि डीप लर्निंग होता रहेगा

168
00:10:11,720 --> 00:10:14,570
बेहतर और बेहतर आने वाले बहुत से वर्षों में.

169
00:10:14,570 --> 00:10:17,855
तो, उसके साथ, चलो जाते हैं इस अनुभाग के अंतिम वीडियो पर

170
00:10:17,855 --> 00:10:21,200
हम थोड़ा और अधिक बात करेंगे कि 
आप क्या सीखेंगे इस कोर्स में.