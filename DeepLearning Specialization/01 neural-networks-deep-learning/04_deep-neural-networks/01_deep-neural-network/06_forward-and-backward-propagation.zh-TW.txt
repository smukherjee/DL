在先前的影片中, 您看到了基本的礎石
在建置一個深度神經網路 對於每一層正向傳播步驟 跟相對的反向傳播步驟 讓我們來看看您如何實際去建置這些步驟 我們從正向傳播開始 記得這個做的是輸入 a[l-1], 輸出 a[l] 然後快取 z[l] 我們之前也說過用建置觀點來看 也許也存 w[l] 跟 b[l] 在快取上 讓函數取值容易一些在程式練習裡 所以, 這個方程式應該看起來很眼熟 建置正向函數的方式就是
這個等於 w[l] * a[l-1] + b[l] 然後 a[l] 等於啟動函數應用到 z 如果您想做向量化建置 就會是那個乘上 a[l-1] + b 加上 b 使用 Python 傳播 然後 a[l] = g 逐元素應用到 z[l] 您記得, 在正向步驟的圖形上 記得我們用一些箱子向前連接起來 您初始時餵入 a[0] 也就是等於 X 所以您初始化這個 真的, 什麼是第一個輸入值? 其實是 a[0] 也就是輸入特徵對於一個訓練例子 如果您一次做一個例子 或者 A[0], 整個訓練集 如果您同時處理整個訓練集 這個是第一個連鎖正向函數的輸入 然後只要重複這個會讓您 計算正向傳播從左到右 下一步, 我們來談反向傳播步驟 這裡您的目標是輸入 da[l] 然後輸出 da[l-1] 跟 dw[l], db[l] 讓我寫下這些您需要做的步驟
去計算這些東西: dz[l] = da[l] 逐元素乘積 g[l] 一撇 z[l] 然後計算導數 dw[l] = dz[l] * a[l-1] 我沒有明顯將這個放在快取， 但實際上 您需要這樣做 然後 db[i] = dz[l], 最後 da[l-1] = w[l]轉置 乘 dz[l], 對吧? 我不去詳細談怎樣做這些導數 但實際上, 如果您拿這個
 da 的定義代入這個公式 您會得到跟我們前一個禮拜
用的相同的公式 對於您如何計算 dz[l] 是個函數
對於前一個 dz[l], 實際上 如果我將其代入這裡 您最後得到 dz[l] = w[l+1]轉置 dz[l+1] 乘 g[l] 一撇 z[l] 我知道這有很多的代數 實際上您可以自己檢查 這就是我們之前寫下的公式 在上個禮拜反向傳播， 當我們 作用一個神經網路在單一隱藏層時 提醒一下, 這個乘
是逐元素乘積 您所需的是這四個方程式
來建置您的反向函數 最後, 我會寫直接寫下向量化版本 第一行變成 dz[l] = dA[l] 逐元素乘積 g[l] 一撇 Z[l] 也許不意外 dw[l] 變成 1/m dz[l] 乘 A[l-1]轉置 db[l] 變成 1/m np.sum dZ[l] 然後 axis=1, keepdims = True 我們在上個禮拜談過用 np.sum 來計算 db 最後, dA[l-1] 是 w[l]轉置 乘 dz[l] 這會讓您輸入這個量 dA, 這裡 輸出 dw[l], db[l] 您需要的導數 跟 dA[l-1] 像這樣 這是您如何來建置反向函數 總結一下 拿輸入 X 您也許用第一層 也許是 ReLU 啟動函數 然後到第二層 也許另一個 ReLU 啟動函數 到第三層 也許是S形函數如果您做二元分類的話 而這會輸出 y-hat 然後, 用 y-hat 您可以計算損失 而這可以讓您開始反向的遞迭 我先來畫這些箭頭 這樣我不用常常換畫筆 您會用反向傳播來計算導數, 計算 dw[3], db[3], dw[2], db[2], dw[1], db[1] 一路下來，您也計算了 快取會傳遞 z[1], z[2], z[3] 這裡反向的傳送 da[2], da[1] 這也可以計算 da[0] 但我們用不到 所以您可以忽略它，是吧? 這是您如何去建置正向傳播跟反向傳播 對於一個三層的神經網路 有最後一個細節我還沒談到 也就是對於正向遞迴 我們用輸入資料 X 來初始化 反向遞迴呢? 實際上 da[l] 當您做羅吉斯迴歸分析時 當您做二元分類時 是等於 y/a + 1-y/1-a 事實上損失函數的導數 相對於輸出 相對於 y-hat, 可以證明是等於這樣的 如果您熟悉微積分 如果您查損失函數 l 求導數相對於 y-hat, 或相對於 a 您可以證明您得到這個公式 所以您應該用這個公式在最後一層, 大寫 L 層的 da 當然，如果您用向量化建置 您初始這個反向遞迴 不是這個, 而是 dA 大寫A, 對於 L 層 會是對每個不同的例子做同樣的事 第一個訓練例子 除以 a + 1-y 對於第一個例子 除以 1-a  對於第一個訓練例子 ... 直到 m 訓練例子,  1-a(m) 這是您用來建置向量化的版本 這是您用來初始
向量化版本的反向傳播 現在您看到基本的建構礎石對於 正向傳播跟反向傳播 如果您建置這些方程式 您會得到正確的 正向傳播，反向傳播建置
來獲得您需要的導數 您也許會想這麼多的方程式 我略感困惑， 我不是很清楚這些如何作用 如果您覺得如此, 我的建議是 當您做這個星期的程式作業時 您會自己建置這些 它們會變得更具體 我知道有很多的方程式 也許一些方程式看起來並不合理 但您可以通過微積分 跟線性代數來驗證, 雖然真的不容易 不妨試試 但這真的是在機器學習裡
最難推導的部分之一 實際上推導這些方程式 就是在反向傳播時用微積分方程式
來計算這些導數 但再一次說明, 
如果覺得這有一點抽象 您覺得有一點點神秘 我的建議是, 當您做完程式練習後 您會覺得它具體了一些 雖然我必須說, 即使在今天 當我建置學習算法, 有時候 我也會驚訝於 我的學習算法建置可行， 因為 機器學習的複雜度來自於資料
而不是幾行程式 所以有時候, 您覺得 您建置了幾行程式 不確定它們做了什麼 但幾乎神奇的運作了 因為許多的神奇實際上
並非來自於您寫的幾行程式 通常不會太長的程式 它並不是真的簡單 但也不是那種上萬 或幾十萬行的程式 但您餵了這麼多資料, 有時候 即使我做了這麼久時間
的機器學習 有時候，它還是讓我驚艷當 我的學習演算法可行因為
您的學習演算法很多複雜度 來自於資料而 不一定來自於成千上萬行的程式 好的, 這是您如何建置一個深度神經網路 當您完成這個禮拜的程式作業
這些會變得更具體 更進一步之前，在下一段影片 我想討論超參數及參數 實際上當您訓練深度網路時 能夠組織好您的超參數 也會幫助您建置您的網路更有效率 下一段影片，我們來談談這是什麼意思