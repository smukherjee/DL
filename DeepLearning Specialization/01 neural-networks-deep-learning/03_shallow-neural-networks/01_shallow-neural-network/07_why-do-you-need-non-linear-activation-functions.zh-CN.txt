为什么神经网络需要使用非线性激活函数 事实上 想要让神经网络 实现一些有趣的功能 非线性激活函数是不可或缺的 我们来看下原因 这是神经网络前向传播的方程 为什么我们不能把这个去掉？ 去掉函数g 并让a[1]等于z[1] 或者说g(z)等于z 通常 我们把这称为线性激活函数 有时候我们也会用专业性更强的名称 恒等激活函数 因为它直接将输入的值输出 为了说明问题 我们一起来看看如果a2等于z2会怎样 假如这么做 那么这个模型计算的仅仅是输入特征x的线性函数y或y帽 根据前两个等式 如果令a[1]等于z[1]等于W[1]*x+b[1] 并令a[2]等于z[2]等于W[2]*a[1]+b[2] 如果把a[1]的定义代入 你就会发现a[2]等于 W[2]*(W[1]*x+b[1])+b[2] 这一部分是a[1]+b[2] 简化一下就是 W[2]*W[1]*x 加上 W[2]*b[1]+b[2] 然后 让我们把这两项标记为W' b' 表达式就等于W'*x+b' 如果你使用线性激活函数 或者叫恒等激活函数 那么神经网络的输出 仅仅是输入函数的线性变化 我们之后会讲到深度网络 也就是有很多层 很多隐藏层的神经网络 以上推导证明 如果你使用线性激活函数 或者说 没有使用激活函数 那么无论你的神经网络有多少层 它所做的仅仅是计算线性激活函数 这还不如去除所有隐藏层 在上述例子中 如果 在这里使用线性激活函数 在这里使用sigmoid激活函数 那么这个模型其实和没有任何隐藏层的 逻辑回归模型相同 我不会在这里证明 如果你有时间 可以自己尝试 但请记得 线性的隐藏层没有任何用处 因为两个线性函数的组合 仍然是线性函数 除非你在这里引入非线性函数 否则无论神经网络模型包含多少隐藏层 都无法实现更有趣的功能 只有一个地方会使用线性激活函数 当g(z)等于z 就是使用机器学习解决回归问题的时候 y是一个实数 比如你想预测房价 y不是0或1 而是一个实数 从0美元到可能存在的最高价格 任何值都有可能 也许要几百万美元 但无论数据集中的房价是多少 只要y是实数 那么在这里使用线性激活函数就没问题 这样的预测输出y帽 也是从负无穷到正无穷的任意实数 但此时 隐藏单元不应该使用线性激活函数 可以使用ReLU或者Tanh 或者leaky ReLU等 所以唯一可以使用线性激活函数的地方 通常就是输出层 除此之外 只有在一些极为特殊的情况下 才会在隐藏层使用线性激活函数 比如与压缩有关的处理 我们就不进行深入讨论了 此外 使用线性激活函数的情况极为少见 当然 如果你要预测房价 例如在第一周视频中看到的 因为房价是非负数 这种情况下也可以使用ReLU激活函数 使你输出的y帽都大于等于0 希望以上这些已经让你了解 为什么使用非线性激活函数 对神经网络至关重要 下期预告 我们将会讨论梯度下降 为了先让大家熟悉梯度下降的基础 在下个视频中 我会展示如何估计以及 如何计算单个激活函数的斜率或者导数 下个视频见<br />翻译 | 审阅：Cousera Global Translator Community