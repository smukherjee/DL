Ми поговорили 
як векторизація дає змогу значно пришвидшити код. В цьому відео ми поговоримо як можна векторизувати реалізацію логістичної регресії (ЛР). Тож ти зможеш обробити весь тренувальний набір, тобто застосувати одну ітерацію Градієнтного спуску для всього тренувального набору, 
не використовуючи жодного явного циклу for. Я дуже захоплююсь цією методикою. І пізніше ми поговоримо про нейронні мережі (НМ) без використання жодного явного циклу for. Давай почнемо. Давай спочатку розглянемо кроки прямого поширення ЛР. Тож, якщо ми маємо m тренувальних зразків, то щоб зробити передбачення для першого зразка нам потрібно обчислити ось це. Обчислити z, використовуючи цю знайому вже формулу. Потім обчислити активатори, обчислити ŷ першого зразка. Потім, щоб зробити передбачення для другого тренувального зразка, потрібно обчислити ось це. Потім, щоб зробити передбачення для третього зразка, потрібно обчислити ось це і т.д. І це потрібно зробити m разів, якщо ми маємо m тренувальних зразків. А, виявляється, що щоб реалізувати кроки прямого поширення, які обчислюють передбачення для наших m тренувальних зразків, є спосіб, що дозволяє це зробити без використання явного циклу for. Давай подивимось як це зробити. Спочатку, згадаймо, що ми визначили матрицю 
вхідних тренувальних даних і позначили її великою буквою X. Ці дані розміщені ось так в окремих стовпцях. Тож це - матриця розміру (nₓ, m). Я записав розмір як в Python NumPy. Це позначає те саме - матрицю розміру nₓ*m. Тепер, спочатку ми покажемо як можна обчислити z⁽¹⁾, z⁽²⁾, z⁽³⁾ і т.д. в одному кроці. Фактично, одним рядком коду. Тож я побудую матрицю розміру 1*m, що насправді є вектором, 
в якому я обчислюватиму z⁽¹⁾, z⁽²⁾ і т.д. до z⁽ᵐ⁾ одночасно. Виявляється, що це можна записати як wᵀ [транспоноване], велика X + ось такий вектор: b, b, b і т.д., де ось це, ось це b-b-b-b-b, є вектором розміру 1*m або матрицею 1*m, точніше m-розмірним вектором-рядком. Залежно від того наскільки ти обізнаний/а з множенням матриць, ти можеш побачити, що wᵀ, x⁽¹⁾, x⁽²⁾ і т.д. до x⁽ᵐ⁾... wᵀ може бути вектором-рядком. Тож wᵀ буде ось таким вектором-рядком. І, отже, цей перший вираз буде обчислюватись як wᵀx⁽¹⁾, wᵀx⁽²⁾ і т.д. крапка-крапка-крапка, wᵀx⁽ᵐ⁾. І тепер ми додаємо другий вираз - b-b-b і т.д. - додаємо b до кожного елементу. Тож, в результаті отримуємо інший вектор розміру 1*m. Ось це - перший елемент, ось це - другий елемент і т.д., а це - m-ий елемент. І якщо ми глянемо на визначення вгорі, то цей перший елемент - це точне визначення z⁽¹⁾, другий елемент - точне визначення z⁽²⁾ і т.д. Тож, аналогічно, як було отримано X, коли ми брали тренувальні зразки і складали їх один до одного горизонтально, я визначу отак велике Z. Ми візьмемо малі z і складемо їх горизонтально. Тож, складаючи малі x, що відповідають окремим тренувальним зразкам горизонтально, ми отримуємо велике X. І, аналогічно, ми беремо ці малі змінні z і складаємо їх горизонтально, щоб отримати змінну, яку ми позначаємо великим Z. І виявляється, що це можна реалізувати командою NumPy: "Z=np.dot(w.T," (що позначає транспоноване w) і далі "X)+b". В Python є деякі тонкощі. Одна з них це те, що b - дійсне число або ж матриця розміру 1*1, що і є дійсним числом. Проте, якщо додавати цей вектор до цього дійсного числа, Python автоматично бере це дійсне число b і 
розширює його до вектора-рядка розміру 1*m. Тож, якщо ця операція виглядає дещо таємниче (це називається посівом [broadcasting] в Python), то тобі не треба поки цим перейматись, ми поговоримо про це більше в наступному відео. Потрібно запам'ятати, що лише одним рядком коду, 
ось цим рядком, ми можемо обчислити велике Z, а велике Z буде матрицею розміру 1*m, яка містить малі z (від маленького z⁽¹⁾ до маленького z⁽ᵐ⁾). Тож це було Z. Що ж відносно оцих значень A? Далі ми спробуємо знайти спосіб обчислення a⁽¹⁾, a⁽²⁾ і т.д. до a⁽ᵐ⁾ одночасно. Тож, аналогічно, як ми складали малі x в результуюче велике X і як складали (горизонтально) малі z в результуюче велике Z, складаючи малі A, ми повинні отримати нову змінну, яка буде визначена як велике A. В програмувальному завданні ти побачиш як реалізувати векторизовану сигмоїду так, щоб ця сигмоїда прийняла на вхід це велике Z і дуже швидко віддала велике A. Тож ти побачиш це детально в програмувальному завданні. Тож резюмуємо. На цьому слайді ми побачили, що замість проходження циклу по m тренувальних зразках, щоб обчислити малі z і малі a по черзі, ми можемо записати це ось цим одним рядком коду, щоб обчислити всі z одночасно. А потім ось цим рядком коду правильно використати малу σ, щоб обчислити всі малі a одночасно. Отже, ось так реалізується векторизована версія прямого поширення для всіх m тренувальних зразків одночасно. Тож підіб'ємо підсумки. Ти щойно побачив/ла як можна використати векторизацію, щоб дуже ефективно обчислити всі активатори (всі малі a) одночасно. Далі, виявляється, можна також використати векторизацію, щоб дуже ефективно обчислити зворотне поширення, обчислити градієнти. Давай подивимось як це зробити в наступному відео.