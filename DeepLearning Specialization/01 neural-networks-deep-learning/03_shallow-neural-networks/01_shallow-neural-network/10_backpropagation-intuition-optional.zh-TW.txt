在上一段影片中 您看過反向傳播的方程式 在這段影片, 讓我們用直觀的方式 使用計算圖形來看這些方程式的推導 這段影片是可選 隨您意看或不看 不管如何您都可以做作業 記得當我們談論羅吉斯迴歸分析時 我們用這個正向方向來計算 z 然後 a 然後損失 然後求導數時 我們用反向方向來首先計算 da 然後計算 dz 然後計算 dw 跟 db 損失的定義是  L of a, y 等於負 y log a - 1 - y 乘 log 1 - a 如果您熟悉微積分 您可以求這項目相對於 a 的導數 這會給您 da 的公式 所以 da 等於這個 從微積分可以導出這是 負 y 除以 a 加 1 - y 除以 1 - a 您只需用微積分導出來這個導數 實際上當您反向下一步計算 dz 我們計算出 dz 等於 a - y, 我在前面解釋過 從微積分的連鎖律 dz 是等於 da 乘 g prime of z 這裡 g of z 等於S型函數 of z 是我們的啟動函數對於輸出單元在羅吉斯迴歸分析中,對吧? 只是請記得這還是羅吉斯迴歸分析而我們有 x1, x2 x3 跟一個S型函數單元來給我們 a 給我們 y-hat 這裡的啟動函數是S型函數 並順便說一句 只對那些熟悉微積分連鎖律的您 這裡的原因是因為 a 等於S型函數 of z 所以 偏微 of L 相對於 z 等於 偏微 of L 相對於 a 乘上 da/dz 這裡的 a 是S型函數 of z 這等於是 d/dz g(z) 也就是 g prime of z 這是為什麼這個算式 dz 在我們程式中是等於 這個算式也就是 da 在我們的程式乘上 g prime of z 所以這個就是那個 最後這個推導會是否合理在於 您是否熟悉微積分特別是微積分的連鎖律 但如果您不熟悉也不用擔心 我會試著解釋這些直觀當需要時 最後計算完 dz 對於這個迴歸分析 我們來計算 dw , 這會是 dz 乘 x 而 db 就是 dz 當您只有一個訓練例子 所以這是羅吉斯迴歸分析 我們將要做的是當計算 神經網路的反向傳播時是很像這樣的計算 只是要做兩次因為我們沒有 x 進入輸出單元 x 進入隱藏層然後到輸出單元 與其像這樣的計算只用一個步驟 我們會有兩個步驟在這種兩層的神經網路 在這樣兩層神經網路我們有輸入層 一個隱藏層然後是輸出層 記得計算的步驟 首先我們計算 z1 用這個方程式 然後計算 a1 然後計算 z2 注意到 z2 也取決於參數 w2 跟 b2 然後基於 z2 計算 a2, 最後計算損失 而反向傳播做的是反方向計算 da2 然後 dz2 然後反向計算 dw2 跟 db2 反向計算 da1 dz1 等等 我們不用求導數相對於 輸入 x 因為輸入 x 對於監督式學習是固定的(不是變數) 我們不會試著優化 x 所以我們不用對其求導數 至少對於監督式學習 相對於 x, 我將跳過 da2 的一步一步計算 如果您要的話您可以計算 da2 然後用來計算 dz2, 但實用上 您可以折疊這兩個步驟變成一個步驟您會 得到 dz2 = a2 - y 跟以前一樣 您也會有 我將 dw2 跟 db2 寫在下面 您有 dw2 = dz2 * a1轉置 db2 = dz2 這一步驟類似於羅吉斯迴歸分析, 我們有 dw = dz 乘 x 除了現在 a1 扮演了 x 的角色且有一個額外的轉置在這裡因為 介於大寫矩陣 W 跟我們的個別參數 w 的關係 有一個轉置在那裡, 對吧? 因為 W=[---] 行向量在羅吉斯迴歸分析中使用單一個輸出 dw2 像這樣而 這裡的 w 是列向量, 所以這是為什麼有一個額外的轉置在 a1 而在羅吉斯迴歸分析中我們的 x 不用這樣 這樣完成了一半的反向傳播 然後, 同樣的, 您可以計算 da1 如果您要的話 雖然在實用上, 計算 da1 跟 dz1 通常折疊為一步, 所以 您真正建置的是 dz1 = w2 轉置乘 dz2 然後乘上逐元素 乘積對於 g1 prime of z1 我們來檢查一下維度 如果您有一個神經網路像這樣 輸出 y 如果您有 n0, nx=n0 個輸入特徵 n1 隱藏單元 跟 n2 n2 在我們的例子 只是一個輸出單元 那這個矩陣 w2 是 (n2, n1) 維度 z2 跟 dz2 將會是 (n2, 1)  維度 這實際上是 1 乘 1 維度當我們做二元分類時 z1 跟 dz1 將是 n1 乘 1 維度, 對吧? 注意對於任何變數 foo 跟 d foo 永遠是一樣的維度 這是為什麼 w 跟 dw 有相同的維度, 同樣的 b 跟 db, z 跟 dz 等等 為了確定這些維度都配對了 我們有 dz1 = w2 轉置乘 dz2 然後這是逐元素乘積 乘 g1 prime of z1 對應上面的維度 這是 n1 乘  1 = w2 轉置 我們轉置這個所以這會是 n1 乘 n2 維度 dz2 是 n2 乘 1 維度, 而這個是 這是跟 z1 同維度 這也是 n1 乘 1 維度在逐元素乘積中 這個維度是合理的,對吧? n1 乘 1 維度向量可以從 n1 乘 n2 維度矩陣乘 n2 乘 n1 因為 這兩項的乘積會是 n1 乘 1 維度矩陣所以這變成 逐元素乘積對於兩個 n1 乘 1 維度向量 所以維度相符合 一個建置反向傳播的技巧 如果您確認了您的矩陣的維度都配對了 所以您確認過了這些維度在 這些矩陣包含 w1, w2, z1 z2, a1, a2 等等確認了 這些矩陣運算的維度配對了 有時候這已經去除了很多臭蟲在反向傳播中 這會給我們 dz1 最後 只要搞定 dw1 跟 db1 我們應該寫在這裡 但這裡沒有位置了, 我把它寫在投影片右邊 dw1 跟 db1 給予了下面的公式 這個是等於 dz1 乘 x轉置 這個將是等於 dz 您也許注意到這個方程式跟這個方程式的相似性 這真的不是巧合, 因為 x 扮演 a0 的角色, 所以 x轉置就是 a0 轉置 這些方程式實際上非常相似的 這給了一些感覺反向傳播如何推導 我們有六個方程式, dz2, dw2, db2, dz1, dw1 跟 db1 讓我複製這六個方程式到下一張投影片 我們已經推導了反向傳播 對於如果您訓練單一訓練例子 但這不意外的是, 與其一次作用一個訓練例子 我們想向量化同時做整個訓練例子 我們記得在正向傳播 當我們運作於一次一個例子時 我們有方程式像這樣 a1 = g1 of z1 為了向量化 我們拿 z 在列疊起來 像這樣 z[1]1, ... z[1]m 稱這個為大寫 Z 然後我們發現把它們用列疊起來 然後定義大寫版本 我們就有 Z1 = W1X + b 跟 A1 = g1 of Z1, 對吧? 我們在這個課程很小心定義這些符號為了確定 將這些例子疊在矩陣中不同的列使得這一切可行 實際上, 如果您仔細透過數學 同樣的技巧在反向傳播也是可行, 所以向量化方程式如下 首先, 如果您拿這些 dz 對於不同的訓練例子，將它們 疊在矩陣中不同列， 這個也一樣，這個也一樣 那這就是向量化的建置, 這個是這樣定義 您如何來計算 dw2 有一個額外的 1/m 因為成本函數 J 是 1/m 和對於 i 從 1 到 m of 損失 當計算導數時 我們有這個額外的 1/m 項目, 就像我們 計算羅吉斯迴歸分析的加權平均 這是您得到 db2 的更新 對於 dz 的和然後 1/m 然後 dz1 如此計算 再一次, 這是逐元素乘積只是之前 我們前面投影片上是 n1 乘 1 維度向量 現在則是 n1 乘 m 維度矩陣 而這些也是 n1 乘 m 維度 這是為什麼這個星號是逐元素乘積, 最後 剩下兩個更新 也許這不應該太意外 我希望給您一些直觀有關於反向傳播如何推導出來 在所有機器學習中 我想反向傳播演算法的推導 真的是我看過最複雜的數學之一 而它需要線性代數跟 矩陣導數從第一個定理開始推導 如果您是矩陣微積分的專家 使用這個方法, 您或許可以自己導出這些演算法 但我想很多深度學習從業者 看過這樣的推導過程的水準像您 看到這段影片, 已經 有所有這些直觀能夠很有效地建置這個演算法 如果您是微積分的專家 看看是否能推導這些從零開始 這是很難的數學 我看過所有機器學習中很難的導數之一 不管如何, 如果您建置這個 這個可行而我相信您有足夠直觀來調整它讓它可行 最後一個細節我想要 跟您分享在您建置您的神經網路之前 也就是如何初始您神經網路的權值 實際上初始化您的參數 不是0而是隨機值 變成很重要在您訓練您的神經網路 在下一段影片, 您會看到為什麼