上一段影片您看到了一個單一
隱藏層神經網路的樣子 這段影片讓我們更進一步
談論細節 這個神經網路是如何計算這些輸出的。 你看到的是如同羅吉斯迴歸分析 重複很多次。 讓我們來看看。這是一個雙層神經網路。 讓我們更進一步看
這個神經網路怎麼做計算的. 我們之前已經說過,羅吉斯迴歸分析 羅吉斯迴歸分析中的圓圈, 實際上代表兩個步驟的計算。 首先您計算 z 像這樣,
再來 您計算這個 z 的
S型啟動函數. 神經網路只是
重複做這樣很多次. 讓我們開始專注於隱藏層的
一個節點. 讓我們看在隱藏層的
第一個節點 我先將其他節點掩藏灰掉, 類似于左邊的羅吉斯迴歸分析 這個在隱藏層的節點
做這兩個步驟的計算 第一步, 您想成是
這個節點的左半部, 它計算 z 等於 w轉置, x, 加 b, 我們用的標記法是, 這些都是與第一個隱藏層關聯的數值。 所以,這就是為什麼我們有一堆方括弧。 這是隱藏層中的第一個節點。 所以這是為什麼在這裡
我們有下標 1 所以首先,它這樣做, 然後第二步 是它計算 a[1]_1 等於 z[1]_1 的 sigmoid,如下所示。 所以,對於 z 和 a, 符號慣例是 a, [l] , i, 在這裡的上標方括弧 [l], 指的是層號數值, 在這裡下標的 i , 是該層中的節點。 所以,我們查看的節點是在第一層, 是隱藏的層第一個節點。 這是為什麼上標跟
下標都是 1, 1 那麼,那個小圓圈, 神經網路中的第一個節點, 表示執行這兩個計算步驟。 現在，讓我們看在這個神經網路
的第二個節點, 在這個神經網路的
隱藏層的第二個節點. 類似於在左邊的
羅吉斯迴歸分析 這個小圓圈代表兩個計算步驟。 第一步是計算 z。 這仍然是第一層 但現在為第二個節點,等於 w 轉置, x, 加上 b[1]_2,然後 a[1]_2,等於 z[1]_2 的 sigmoid。 再次,請隨時暫停影片,如果你想, 但您可以仔細檢查上標和 下標符號是跟
上面用紫色寫的公式一致的 因此,我們已經討論了神經網路中的
前兩個隱藏單元, 隱藏單元 3 跟 4 
代表類似的計算. 現在
讓我拿著這對方程式 和這兩對方程式, 並把它們複製到下一張投影片。 所以這是我們的神經網路, 這是第一個, 這裡是我們計算出的第二個方程式 以前用於第一個和第二個隱藏單位。 如果您用同樣方式寫下
對應方程式 對於第三和第四個隱藏單位,您將獲得以下內容。 所以,讓我弄清一下這個記號是正確的, 這是向量 w[1]_1, 這是一個向量轉置,乘 x, 好吧 這就是上標 T 所代表的。 這是一個向量轉置。 你可能已經猜到了, 當您建置神經網路時, 用 for loop 來做這個,真的效率不高。 那麼,我們要做的是, 就是向量化這四個方程式。 因此,我們將首先展示
如何用向量的方式計算 z 您可以這樣做. 讓我們拿這些 w 
將他們疊成一個矩陣 然後你有 w[1]_1 轉置, 這是一個行向量, 或此列向量轉置為行向量,然後 w[1]_2 轉置, w[1]_3 轉置, w[1]_4 轉置。 所以,通過把這四個w向量堆疊在一起, 你最終有一個矩陣。 所以,另一個可以思考的方法是,
我們有四個羅吉斯迴歸分析單位 以及每個羅吉斯迴歸分析單位, 具有相應的參數向量, w. 通過把這四個向量堆疊在一起, 您會得到這個 (4, 3) 矩陣. 如果您拿這個矩陣,
乘上您的輸入特徵 x1, x2, x3 您會得到,
使用矩陣乘法. 您會得到 w[1]_1轉置, x; w[1]_2 轉置 x; w[1]_3 轉置, x; w[1]_4 轉置, x. 然後,我們不要忘記 b。 因此,我們現在添加一個向量 b[1]_1 1, b[1]_2, b[1]_3, b[1]_4。 基本上就是這個, 然後這是 b[1]_1 1, b[1]_2, b[1]_3, b[1]_4。 所以,你看到這四行 結果與這四行中的每一行完全一致, 這四行每一個
都相配我們上面的方程式 換句話說, 我們剛剛展示了
這個東西等於 z[1], z[1]_1 , z[1]_2 , z[1]_3 , z[1]_4 
像這裡的定義. 也許並不奇怪,我們要稱這
整個東西為,向量z[1], 也就是把這些每一個 z 
疊起來變成一個列向量. 當我們進行向量化時,一個經驗法則
來幫助您導引這些, 是當您有不同的節點
在這一層時, 我們將它們垂直疊起來. 這就是為什麼我們有z[1]_1 到 z[1]_4, 那些對應在隱藏層中的四個不同的節點, 因此,我們垂直堆疊這四個數字,以形成向量 z[1]。 最後, 再介紹一個符號, 這裡四乘三矩陣,我們通過堆疊小寫 w[1]_1, w[1]_2 ,等等,我們將稱為矩陣,大寫 W[1]。 同樣,這個向量,我們將稱為 b上標[1]方括弧。 所以,這是一個 4 乘 1 向量。 現在,我們使用向量矩陣標記法計算 z, 最後一件事是我們也需要
計算這些 a 的值 或許不意外您看到
我們將定義 a[1] 為 一樣堆在一起, 這些啟動值, a[1]_1, 到 a[1]_4。 就是拿這四個值
疊起來成為一個向量稱為 a[1] 這將會是  sigmoid z[1], 而現在這建置是 S型 sigmoid 函數,拿 z 的四個元素, 然後逐元素的應用S型函數作用在它們. 總結一下, 我們發現 z[1] 等於 W[1] 乘 x 加上向量 b[1], 和 a[1] 是 sigmoid z[1]。 讓我們將其複製到下一張投影片。 我們看到的是,對於給定輸入 x 
的神經網路的第一層, 我們有 z[1] 等於 W[1] 乘 x 加上 b[1], 和 a[1] 是 sigmoid z[1]。 這個維度是 (4,1), 這是一個 (4,3) 矩陣,乘以 (3,1) 向量,加 (4,1) 向量 b, 這是 (4,1) 個相同的維度作為結束。 記得我們說 x = a[0], 且 y-hat = a[2] 如果您要的話, 您可以將這個
a[0] 取代 x , 因為 a[0] 因為 a[0] 是,如果您要的話,作為輸入特徵向量 x 的別名。 現在,通過類似的推導, 你可以斷定出下一層的表現形式也可以 寫成類似的, 而輸出層做的是 與它相關的值。 因此參數 W[2] 和 b[2]。 所以,在這種情況下,W[2] 將是一個 (1,4) 矩陣, b[2] 就只是一個實數 (1,1) z[2] 會是一個實數
寫成 (1,1) 矩陣 這是 (1,4) 矩陣, 乘 a 是 (4,1) 加上b[2] 是 (1,1), 就只是一個實數 如果你想象這最後一個輸出單位只是 類似于具有參數 w 和 b 的羅吉斯迴歸分析 w 確實扮演了與 w[2] 轉置,類似的角色, 或 w[2] 實際上是 W 轉置,b 等於 b[2]。 如果您將左邊這個網路
蓋起來, 先忽略他們 那麼最後一個輸出單位
就像羅吉斯迴歸分析一樣, 除了與其寫成
參數 w 跟 b 我們將它們寫成 w[2] 和 b[2], 有 (1,4) 和 (1,1) 的維度。 總結一下, 對於羅吉斯迴歸分析,建置輸出或建置預測, 您計算 z 等於 w 轉置, x 加 b, a 或 y-hat 等於 a, 等於 sigmoid z。 當你有一個神經網路與一個隱藏的層, 您需要建置的 就只是這四個方程式,去計算這個輸出。 您可以將其視為向量化建置的計算 首先,這四個隱藏層中羅吉斯迴歸分析單位的輸出. 這就是它所做的, 然後輸出層中的這種羅吉斯迴歸分析,這就是它所做的。 我希望這個描述有道理, 但要點是,計算這個神經網路的輸出, 您需要的就是這四行程式. 現在您看到,給予一個輸入特徵, 向量 a ,你可以用四行代碼, 計算此神經網路的輸出。 類似于我們在羅吉斯迴歸分析所做的, 我們還需要向量化多個訓練例子。 我們看到,通過堆疊訓練示例在矩陣中的不同的列, 稍作修改,你也 類似于你在此迴歸分析中所看到的, 能夠計算這個神經網路的輸出, 不只是一次一個例子, 一次用到你的整個訓練集. 讓我們在下一段影片看這些細節