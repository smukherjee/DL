1
00:00:01,050 --> 00:00:03,840
深層学習という用語は
ニューラルネットワークに学習させることを意味します

2
00:00:03,840 --> 00:00:06,050
時に非常に大きなニューラルネットワークを対象にします

3
00:00:06,050 --> 00:00:08,400
ではニューラルネットワークとは
一体何なのでしょうか

4
00:00:08,400 --> 00:00:11,340
このビデオでは基本的で
直感的なところを身につけてもらいます

5
00:00:12,850 --> 00:00:16,540
住宅価格を予測する例から
見てみましょう

6
00:00:16,540 --> 00:00:20,599
６件の住宅のデータセットがあります
平方フィートや平方メートルで

7
00:00:20,599 --> 00:00:24,478
広さが分かります
住宅の価格もわかります

8
00:00:24,478 --> 00:00:28,501
広さの関数で住宅価格を
予測するようにフィッティングさせたいとします

9
00:00:28,501 --> 00:00:33,509
線形回帰を知っていれば
このデータに

10
00:00:33,509 --> 00:00:38,450
直線を引こうとするでしょう
このような直線を引きましょう

11
00:00:38,450 --> 00:00:41,850
少し変わっています
価格はマイナスには

12
00:00:41,850 --> 00:00:43,770
ならないでしょう

13
00:00:43,770 --> 00:00:48,050
マイナスになってしまう直線で
フィッティングするのではなく

14
00:00:48,050 --> 00:00:49,960
ここで曲げましょう

15
00:00:49,960 --> 00:00:51,530
ここで０になるようにします

16
00:00:51,530 --> 00:00:56,770
この青の線が関数です

17
00:00:56,770 --> 00:00:59,760
広さによって住宅価格を
予測する関数です

18
00:00:59,760 --> 00:01:03,310
ここは０でそこより右は
直線でフィッティングしています

19
00:01:04,408 --> 00:01:08,735
この住宅価格にフィッティングさせた関数を
とてもシンプルな

20
00:01:08,735 --> 00:01:11,880
ニューラルネットワークと考えてください

21
00:01:11,880 --> 00:01:14,230
最もシンプルなニューラルネットワークです

22
00:01:14,230 --> 00:01:15,000
ここに書きます

23
00:01:17,220 --> 00:01:22,170
ニューラルネットワークの入力に
x と呼ぶ住宅の広さがあります

24
00:01:22,170 --> 00:01:26,791
この円で示したノードに
入っていきます

25
00:01:26,791 --> 00:01:30,940
そしてy と呼ぶ価格を出力します

26
00:01:30,940 --> 00:01:37,183
この小さな円は
ニューラルネットワークの小さなニューロンです

27
00:01:37,183 --> 00:01:41,830
円は左に描いた関数を
実装しています

28
00:01:43,350 --> 00:01:48,940
ここでニューロンが行っているのは
広さを入力し線形関数を計算し

29
00:01:48,940 --> 00:01:51,960
0を引数に持つmax を返し
予測した価格を出力しているだけです

30
00:01:53,190 --> 00:01:58,230
ところでニューラルネットワークに関する文献では
この関数をよく目にします

31
00:01:58,230 --> 00:02:00,992
この関数は０がしばらく続き

32
00:02:00,992 --> 00:02:03,550
その後直線で昇っていきます

33
00:02:03,550 --> 00:02:09,108
この関数はReLU と呼ばれ
次の略語です

34
00:02:09,108 --> 00:02:17,620
Rectified Linear Units.

35
00:02:17,620 --> 00:02:18,252
R-E-L-U です

36
00:02:18,252 --> 00:02:22,520
R は０を引数にもつmax を計算するということで
このため関数はこのような形になります

37
00:02:23,640 --> 00:02:25,550
ReLU 部分は今は気にしないでください

38
00:02:25,550 --> 00:02:30,200
このコースで再度触れるものと捉えてください

39
00:02:30,200 --> 00:02:33,790
これがひとつのニューラルネットワークとすれば

40
00:02:33,790 --> 00:02:38,870
本当に小さなニューラルネットワークですが
より大きなニューラルネットワークというのは

41
00:02:38,870 --> 00:02:44,520
１つ１つの小さなニューロンを使い
積み重ねたものです

42
00:02:44,520 --> 00:02:50,700
このニューロンを１つのレゴのブロックと考えれば

43
00:02:50,700 --> 00:02:55,270
多くのブロックを積み重ねれば
より大きなニューラルネットワークができます

44
00:02:55,270 --> 00:02:56,110
例を見てみましょう。

45
00:02:57,260 --> 00:03:02,220
例えば広さだけを手掛かりに
住宅価格を予測するのではなく

46
00:03:02,220 --> 00:03:04,330
他の特徴を使うとします

47
00:03:04,330 --> 00:03:08,164
例えば寝室の数以外の住宅の特徴もわかります

48
00:03:08,164 --> 00:03:13,630
sを付けるべきでした
住宅価格を決めるものの１つに

49
00:03:13,630 --> 00:03:18,820
家族の人数があると考えるでしょう

50
00:03:18,820 --> 00:03:21,882
住宅には３人、４人
５人などで住めます

51
00:03:21,882 --> 00:03:22,687
よね？

52
00:03:22,687 --> 00:03:26,351
平方メートルなどの広さと
寝室の数こそが

53
00:03:26,351 --> 00:03:28,960
ある人数の家族が

54
00:03:28,960 --> 00:03:31,462
その住宅に住めるかを決めます

55
00:03:31,462 --> 00:03:34,909
そのほかにZIP コードがあります

56
00:03:34,909 --> 00:03:40,520
国によっては郵便番号と呼ばれています

57
00:03:40,520 --> 00:03:48,820
ZIP コードによって徒歩が可能な地域かどうかが
分かるかもしれません

58
00:03:48,820 --> 00:03:51,434
徒歩で行動できる地域でしょうか

59
00:03:51,434 --> 00:03:53,635
食料品店や学校まで歩ける

60
00:03:53,635 --> 00:03:54,194
地域か？

61
00:03:54,194 --> 00:03:55,250
車が必要かどうか

62
00:03:55,250 --> 00:03:57,870
徒歩で行動できる地域を好む人もいます

63
00:03:57,870 --> 00:04:06,145
ZIP コードとともに
裕福さが

64
00:04:06,145 --> 00:04:09,200
アメリカでも他の国でもですが

65
00:04:09,200 --> 00:04:13,590
学校の質の高さを決めます

66
00:04:13,590 --> 00:04:17,820
この小さな円はそれぞれが
ReLU になりえます

67
00:04:17,820 --> 00:04:22,670
Rectified Linear Units か
どちらかというと非線形関数にもなりえます

68
00:04:22,670 --> 00:04:24,936
まとめると広さと寝室の数から
家族の人数が分かり

69
00:04:24,936 --> 00:04:28,420
ZIP コードから徒歩行動の可否が分かり

70
00:04:28,420 --> 00:04:32,050
ZIP コードと裕福さから学校の質が分かります

71
00:04:32,050 --> 00:04:35,660
そして人が住宅にいくらまで支払うかは

72
00:04:35,660 --> 00:04:38,880
本当に重要と考える要素によって
決まると考えるでしょう

73
00:04:38,880 --> 00:04:43,060
この場合には家族の人数、徒歩行動の可否、学校の質をもとに

74
00:04:43,060 --> 00:04:45,210
価格を予測しようとします

75
00:04:46,330 --> 00:04:51,740
この例ではx はこれらの４つの入力で

76
00:04:53,470 --> 00:04:56,460
y は予測しようとする価格です

77
00:04:57,960 --> 00:05:03,350
先のスライドで見たニューロン
または予測因子を重ねることで

78
00:05:03,350 --> 00:05:07,360
少しだけ大きなニューラルネットワークができました

79
00:05:07,360 --> 00:05:10,850
強力な魔法のようですが
ニューラルネットワークを実装するときには

80
00:05:10,850 --> 00:05:15,860
入力にはx と出力のy だけを使い

81
00:05:15,860 --> 00:05:20,740
トレーニングセットに使用し

82
00:05:20,740 --> 00:05:23,580
中間のものは
自ら勝手に決まります

83
00:05:25,435 --> 00:05:29,225
実際に実装するのはこれです

84
00:05:29,225 --> 00:05:32,055
ここに４つの入力を持つ
ニューラルネットワークがあります

85
00:05:32,055 --> 00:05:35,455
入力の特徴量は広さ、寝室の数

86
00:05:35,455 --> 00:05:40,365
郵便番号、地域の裕福さです

87
00:05:40,365 --> 00:05:44,805
これらの入力の特徴量を与えると

88
00:05:44,805 --> 00:05:50,200
ニューラルネットワークの仕事は
価格のy を予測することです

89
00:05:50,200 --> 00:05:55,942
よく見てください
この部分の小さな円はニューラルネットワークでは

90
00:05:55,942 --> 00:06:02,310
隠れユニットと呼ばれ
入力特徴量の４つ全てを入力します

91
00:06:02,310 --> 00:06:08,139
例えばこの１つ目のノードはX1とX2の特徴量のみから

92
00:06:08,139 --> 00:06:12,056
家族の人数を表すというのではなく

93
00:06:12,056 --> 00:06:15,302
ニューラルネットワークに対して

94
00:06:15,302 --> 00:06:18,200
自らこのノードの表すものを決めさせます

95
00:06:18,200 --> 00:06:21,070
このため４つの特徴量をすべて渡し
自由に計算させます

96
00:06:21,070 --> 00:06:26,170
ニューラルネットワークの
入力の層と中間の層を

97
00:06:26,170 --> 00:06:28,960
密に接続されていると言います

98
00:06:28,960 --> 00:06:31,740
全ての入力特徴量は中間のすべての円に

99
00:06:31,740 --> 00:06:33,980
接続されているからです

100
00:06:33,980 --> 00:06:38,630
ニューラルネットワークの素晴らしいところは
xとyに関する十分なデータを与えると

101
00:06:38,630 --> 00:06:43,290
十分な学習データをxとyで与えると
ニューラルネットワークは

102
00:06:43,290 --> 00:06:47,450
正確にxからyを割り出す関数を
非常に上手に探し出します

103
00:06:48,990 --> 00:06:51,680
これがニューラルネットワークの基本です

104
00:06:51,680 --> 00:06:54,290
自分自身でニューラルネットワークを作ることで

105
00:06:54,290 --> 00:06:57,130
教師あり学習では最も便利で
強力なものと恐らく思われたでしょう

106
00:06:57,130 --> 00:07:01,620
入力xを使って出力のyに
割り当てようとしました

107
00:07:01,620 --> 00:07:06,980
住宅価格の予測の例で見たようにです

108
00:07:06,980 --> 00:07:11,490
次のビデオでは
さらに教師あり学習の例を見てみましょう

109
00:07:11,490 --> 00:07:15,670
これらの例では作るニューラルネットワークが
適用例に驚異的に役立つと思われるかも

110
00:07:15,670 --> 00:07:16,670
しれません