이 비디오에서는 신경망을 도입하는 방법을 배울 것입니다. 기술적인 부분을 다루기에 앞서, 이 비디오에서는 이번주에 보게될 내용의 개용를 간단히 설명하고자 합니다. 여기 비디오에서 다루는 내용을 전부 이해하시지 못하더라도 걱정하지 마십시요. 다음 몇개의 비디오에서 조금 더 상세적인 내용에대해 알아보도록 하겠습니다. 신경망 네트워크를 어떻게 도입하는지에 대해 간략히 이야기해보겠습니다. 저번 주에는 로지스틱 회귀분석법에 대해 이야기 했었는데요, 이 모델이 어떻게 여기 보이는 산출 그래프로 이어지는지 알아보았습니다. 여기서는 x 특성을 z를 산출해주는 w 와 B 파라미터에 넣지 않았는데요. 여기 z는 그 다음에 a를 산출하는데 쓰입니다. 여기 ŷ값가 교차로 쓰입니다. 그 다음에 여기 loss 함수를 계산을 할 수 있게 됩니다. 신경망은 여기 이렇게 생겼습니다. 이전에 말했듯이, 신경망은 여거 시그모이드 유닛을 쌓아 합쳐서 구성할 수 있습니다. 이전에는 이 노드 부분이 2가지의 계산 단계를 따랐었는데요. 처음에는 여기 z 의 값을 구하고요, 두번째로는 여기 a의 값을 구합니다. 
신경망에서는 여기 노드 뭉치는 여기 z의 값과 같을 것이고요, 그리고 여기 a와 같을 것입니다. 그리고 여기 노드는 여기 또 하나의 z와 또 하나의 a의 값을 나타낼 것입니다. 표기법은 나중에 소개하겠지만 여기 이렇게 생겼습니다. 처음에는 입력 특성 x을 넣을 것인데요. 그 다음에 w와 b의 파라미터와 함께 말이죠. 그러면 이것이 z1을 계산하게 해줄 것인데요, 그러면 새로 소개할 표기는 위 첨자 괄호 1 여기 레어라고 부르는 노드 뭉치의 양을 나타낼 것인데요, 그리고 다음에는 위첨자 괄호 2로 Daniel이라고 불리는 것과 연관된 양입니다. 이것은 또 하나의 신경망 층입니다. 여기 있는 위첨자는 괄호는 개인 트레이닝 예시를 나타낼 때 쓰는 소괄호와 헷갈리시면 안됩니다. 반면에 X 위첨자 소괄호 I는 트레이닝 예시를 나타내고, 위첨자 대괄호 1과 2는 여기 이렇게 다른 층을 나타냅니다. 1번 층, 2번 층 이렇게 여기 네트워크에서 말이죠, z1을 산출한 다음에는, 로지스틱 회귀분석과 유사하게, a1을 계산하는 방식이 있고, 이것은 그냥 z1으 ㅣ시그모이드인데요, 그 다음에는 z2를 계산하는데요, 또 하나의 선형 식을 이용해서 말이죠, 그 다음에는 a2를 계산합니다. a2는 신경망 네트워크의 최종 결과값입니다. 이것은 ŷ과 교차적으로 쓰일텐데요, 지금 이야기한 내용이 많은 내용인 것을 저도 알고 있는데요, 여러분이 기억하실 핵심 부분은 로지스틱 회위분석에서는 여기 Z값과 그 다음에 a 계산이 따랐고, 그 다음에 신경망 네트워크는 여러번 진행합니다. z 다음에 a 계산 다음에 또, Z 계산 진행 후, 다음에 a 계산이 따릅니다. 그리고 최종적으로 loss의 값을 구합니다. 기억하시겠지만 로지스틱 회귀분석에서는 여기 이런 backward calculation 이 있었습니다. derivative를 계산하기 위해서 말이죠. 여기서는 da와 dz를 계산하는 것이죠. 같은 방법으로, 신경망에서는 backward calculation을 할텐데요, 이렇게 생겼습니다. 이 경우에서는 dz1과 dz2 를 계산해서 dw2 와 db2의 값을 얻을 수 있게 합니다. 이것은 어떻게보면 오른쪽에서 왼쪽으로 진행되는 backward calculation인데요, 여기 이렇게 빨간색 화살표 방향으로 말이죠. 이 내용은 신경망이 어떻게 작동하는지 간단한 오버뷰를 제공해주는데요, 로지스틱 회귀분석법을 이용해서 2번 반복하는 것입니다. 이 강의에서는 표기를 많이 사용했는데요, 새로운 정보와 표기방식이 많았는데요, 모든 내용을 이해하지 못하셨더라도 걱정하지 마십시요. 조금 더 천천히 관련 내용을 다음 몇개의 비디오에서 다루겠습니다. 그럼 이제 다음 비디오로 넘어가서 신경망 representation에 대해 이야기하겠습니다.