当你在神经网络中用反向传播算法时，你需要 计算激活函数的导数（斜率）。 那就让我们一起看看有哪些可选择的激活函数， 而又该怎样计算这些函数的斜率。 这是大家都熟知的sigmoid激活函数。 所以给定z一个任意值 比如这个值 这个函数将 给出对应的某个斜率，或者某个导数， 如果你在那里画一条短线，以短线为弦的小三角形的高与底边的比值就是斜率。 所以如果g(z)是Sigmoid函数，那么函数的斜率就是d/dz g(z)。 我们通过计算得到g(x)在z值上的斜率。 如果你对微积分十分熟悉并且知道怎样计算导数， 如果对sigmoid函数求导， 可以导出和这里一样的公式。 再一次声明，我不会在这里做演算步骤。 但如果你熟悉微积分 可以随意暂停视频，尝试自己证明求导过程。 所以这里的导数刚好等于g(z)*(1-g(z))。 让我们对这个表达式做个完整性检查看是否合理。 首先，如果z非常大，假如z=10, 那么g(z)将接近于1，并且 左边的公式告诉我们d/dz g(z)的值接近于g(z) 这里g(z)等于1,g(z)*(1-g(z))等于1乘以(1-1), 因此非常接近于0。 这确实是正确的，因为当z值非常大时,斜率接近于0。 相反的，如果z=-10,远在那头，那么g(z)值接近于0。 那左边的公式告诉我们， d/dzg(z)将接近于，这里g(z)等于0，g(z)*(1-g(z))等于0乘以(1-0)。 所以这也非常接近于0，这也是正确的。 最后如果z等于0，那么g(z)等于0.5。 这就是sigmoid函数。 因此导数等于0.5乘以(1-0.5), 导数值等于1/4。 这也是当z等于0时导数的正确取值 或者说函数的斜率。 最后，再介绍一点标记上的问题 有时，代替d/dz g(z)这种写法 导数的简写法是g'(z)。 所以在微积分中，g'(z)这里的短撇也叫做prime(角分号)。 因为g'(z)是微积分中的简写 代表对函数g求对输入变量z的求导。 在神经网络中，我们令a等于g(z), 就像这样。 那么这里也可以简写为a乘以(1-a)。 所以书写上有时 你会看到g'(z)等于a乘以(1-a)。 那就说明，g'这个表达式 即表示导数，就等于这边的式子。 然后这个公式的优点在于
如果你已经计算出了a的值 那么通过这个表达式，你可以很快算出 g'的斜率。 好了，所以这就是sigmoid激活函数的导数。 现在我们来看tanh激活函数。 和前面的讨论相似，d/dz g(z)的定义 就是g(z)在特定z点上的斜率。 观察一下双曲正切函数的例子， 如果你微积分学得不错，你就可以求导 并证明这个式子可以简化为这个式子。 我们可以用之前的写法 称这个为g'(z)。 你可以去检查这个式子有没有错。 例如，当z=10，tanh(z)就会非常接近1。 这是从+1到-1的函数。 那么根据这个公式，g'(z) 大概就是1-1^2，即为0. 所以如果z非常大，斜率就接近于0. 相对地，如果z非常小，比如z=-10, tanh(z)就会接近-1 那么g'(z)就约等于是1-(-1)^2， 也就是约等于1-1，约等于0。 最后，若z=0，那么tanh(z)=0. 从而斜率就等于1， 这就是z=0时的斜率。 总而言之，如果a=g(z)，如果a等于这个tanh(z) 那么导数g'(z)就等于1-a^2 那么再次，如果你已经算出了a的值 你可以用这个式子来很快算出导数。 最后，我们来看看如何计算ReLU的导数 以及带泄漏的ReLU激活函数的导数。 对于ReLU, g(z)=Max(0,z) 那么当z<0时，导数就是0 z>0导数就是1 然后在z精确等于0处，斜率是没有定义的 但如果你在软件中实现这个算法 如果z刚好在0处 可能数学上不是百分之百正确 但实际上是行得通的 你可以令导数为1，或者令导数为0 这都是可以的。 如果你对优化领域的术语很熟悉， g‘就变成所谓激活函数g(z)的次梯度 这样梯度下降法仍然有效 但你可以这样想，z精确为0的概率非常小 所以你把z=0处的导数设为哪个值都无所谓 <br /> 所以在实践中，人们一般这么定z的导数 最后，如果你在训练自己的神经网络 使用带泄漏的ReLU激活函数时 g(z)就是max(0.01z,z) 那么z<0时，g'(z)就等于0.01 z>0时，g’(z)就等于1 我们再来看看我们再来看看 z精确为0时的导数也是没有定义的 但你可以写一段代码去定义这个梯度 令g'(z)等于0.01或1，都是可以的 你的代码都可以运行 掌握了这些式子，你应该可以计算 你的激活函数的斜率或导数 现在，你有了这个基础工具 就已经准备好去学习如何在你的神经网络上实现梯度下降算法了。 让我们来看下一个视频