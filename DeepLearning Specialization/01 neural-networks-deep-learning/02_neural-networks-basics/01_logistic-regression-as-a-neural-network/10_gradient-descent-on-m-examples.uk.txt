В попередньому відео ми бачили як обчислювати похідні і застосовувати Градієнтний спуск для одного тренувального зразка для логістичної регресії (ЛР). Тепер ми хочемо це зробити для m тренувальних зразків. Спочатку, давай згадаємо визначення функції Витрат J. Функція Витрат від w і b, яка нам потрібна, = середньому, тобто 1/m, від суми Втрат при i від 1 до m, коли алгоритм видає a⁽ⁱ⁾ для зразка y, де a⁽ⁱ⁾ - передбачення для i-ого тренувального зразка, що = σ від z⁽ⁱ⁾, що = σ від wᵀ [транспоноване] помноженого на x⁽ⁱ⁾) плюс b. Тож, на попередньому слайді ми показали, як для кожного окремого тренувального зразка обчислити похідні. Ми мали лише один тренувальний зразок. Тож dw₁, dw₂ і db тепер будуть з верхнім індексом ⁽ⁱ⁾ і позначатимуть відповідні значення, які ми отримаємо, якщо будемо робити те, що ми робили на попередньому слайді для одного тренувального зразка (x⁽ⁱ⁾, y⁽ⁱ⁾). Вибачте, тут теж повинно бути ⁽ⁱ⁾. Як бачимо, загальна функція Витрат є сумою розділеною на m, а тому дійсно є середньою окремою втратою. Виявляється, що похідна загальної функції Витрат, скажімо, по w₁ також буде середнім від похідних по w₁ від окремих втрат. Проте, раніше ми вже бачили як обчислюється цей вираз, тобто dw₁⁽ⁱ⁾. Ми вже знаємо з попереднього слайду як його обчислювати для окремого тренувального зразка. Тож що нам потрібно зробити, це обчислити ці похідні так само як на попередньому тренувальному зразку і знайти середнє. І це дасть нам загальний градієнт, який ми можемо використати для реалізації Градієнтного спуску. Отже, я розумію, що тут багато інформації, проте, давай спробуємо використати це в конкретному алгоритмі. Тобто розглянемо, що потрібно зробити, щоб змусити працювати зв'язку ЛР і Градієнтного спуску. Тож ось що можна зробити. Давай встановимо J в 0, dw₁ в 0, dw₂ в 0, db в 0. І давай використаємо цикл for для тренувального набору і обчислимо похідні для кожного тренувального зразка і просумуємо їх. Тож ось що ми робимо. Для i від 1 до m (m - це кількість тренувальних зразків) обчислюємо z⁽ⁱ⁾= =wᵀx⁽ⁱ⁾+b, передбачення a⁽ⁱ⁾=σ(z⁽ⁱ⁾) і давай просумуємо J: J+=-(y⁽ⁱ⁾log(a⁽ⁱ⁾) +(1-y⁽ⁱ⁾)(1-log(a⁽ⁱ⁾))). Поставимо знак "-" перед усім виразом. І, як ми бачили раніше, dz⁽ⁱ⁾ =a⁽ⁱ⁾-y⁽ⁱ⁾ і dw₁+=x₁⁽ⁱ⁾dz⁽ⁱ⁾, dw₂+=x₂⁽ⁱ⁾dz⁽ⁱ⁾. Я записав це таким чином, тому що ми маємо лише 2 ознаки, тобто m=2. В іншому випадку це потрібно буде зробити для dw₁, dw₂, dw₃ і т.д. Далі db+=dz⁽ⁱ⁾. І це кінець циклу for. І, нарешті, зробивши це для m тренувальних зразків, нам потрібно буде поділити це на m, щоб обчислити середнє. Тож dw₁/=m, dw₂/=m, db/=m (щоб обчислити середні). Отже, у всіх цих щойно проведених обчисленнях ми знайшли [часткові] похідні функції Витрат J по кожному параметру w₁, w₂ і b. Кілька коментарів про те, що ми робимо. Ми використовуємо dw₁, dw₂ і db як накопичувачі. Тобто після цих обчислень ми знаємо, що dw₁= [частковій] похідній всієї функції Витрат по w₁ і, аналогічно, по dw₂ і db. Відміть, що dw₁ і dw₂ не мають верхнього індексу ⁽ⁱ⁾, тому що ми використовуємо їх в коді як накопичувачі, щоб просумувати для всього тренувального набору. На противагу dz⁽ⁱ⁾ ось тут, який відноситься до окремого тренувального зразка. Тому воно має верхній індекс ⁽ⁱ⁾, бо відноситься до одного тренувального зразка, який обчислюється. Отже, щоб закінчити всі ці обчислення і реалізувати один крок Градієнтного спуску ми виконуємо коригування: w₁ як w₁ мінус навчальний темп помножений на dw₁, w₂ як w₂ мінус навчальний темп помножений на dw₂ і b як b мінус навчальний темп помножений на db. Де dw₁, dw₂ і db ми обчислюємо і, нарешті, J отримує правильне значення для функції Витрат. Тож, все на цьому слайді реалізується всього за один крок Градієнтного спуску і ми повинні повторювати все, що є на цьому слайді, багато разів, щоб здійснити багато кроків Градієнтним спуском. Якщо це здається дуже складним, то, знову ж, не дуже хвилюйся поки. Я надіюсь, воно стане зрозумілішим коли почнеш реалізовувати це в програмувальному завданні. Виявляється, що ми маємо 2 слабких місця в обчисленнях, які ми тут описали. Вони наступні. Щоб реалізувати ЛР цим способом, ми повинні написати 2 цикли for. Перший - для всіх (m) тренувальних зразків, а другий - для всіх ось цих характеристик. В цьому прикладі ми маємо лише 2 ознаки - m=2. (і nₓ=2) Проте, коли ознак буде більше, потрібно буде записати dw₁, dw₂ і, аналогічно, для dw₃ і так далі до dwₘ. Тож виявиться, що тут потрібно мати цикл for для цих ознак, для всіх m ознак. Коли реалізуєш алгоритми глибокого навчання (ГН), то починаєш розуміти, що явні цикли for в коді значно знижують швидкість роботи алгоритму. А, так як в епоху ГН ми використовуємо все більші і більші набори даних, то можливість реалізувати алгоритми без використання явних циклів for стає дуже важливою і допоможе перейти до набагато більших наборів даних. Виявляється, що є набір методик, які називаються методиками векторизації і які дозволяють позбавитись явних циклів for в коді. Я думаю, що в пре-ГН епоху, тобто перед злетом ГН, векторизація було добре мати: можна було інколи прискорити код, а інколи цього можна було не робити. Проте, в епоху ГН векторизація, яка прибирає цикли for (як ось цей чи ось цей), стала дуже важливою. Тому що ми все частіше і частіше тренуємо дуже великі набори даних, тож, справді, потребуємо, щоб код був дуже ефективним. В кількох наступних відео ми поговоримо про векторизацію і як реалізувати все ось це без використання навіть одного циклу for. Тож тепер, я надіюсь ти маєш розуміння як реалізувати ЛР або Градієнтний спуск для ЛР. Це стане ще зрозуміліше коли ти виконаєш програмувальні вправи. Проте, перед тим як робити програмувальні вправи, давай спочатку поговоримо про векторизацію, щоб ти міг/могла виконати всі ці розрахунки в одній ітерації Градієнтного спуску без використання циклів for.