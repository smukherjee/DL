1
00:00:00,060 --> 00:00:03,750
在之前的课程中，你们学习了怎样

2
00:00:01,890 --> 00:00:05,819
计算导数以及怎样实现

3
00:00:03,750 --> 00:00:07,500
梯度下降 在只有

4
00:00:05,819 --> 00:00:09,929
一个训练样例的逻辑回归情况下

5
00:00:07,500 --> 00:00:12,450
现在 我们来讨论

6
00:00:09,929 --> 00:00:14,429
m个训练样例的情况

7
00:00:12,450 --> 00:00:17,460
我们先回顾一下

8
00:00:14,429 --> 00:00:19,380
代价函数J(w,b)

9
00:00:17,460 --> 00:00:22,699
我们关心的J(w,b)是个平均数

10
00:00:19,380 --> 00:00:25,350
1/m乘上求和从i取1到m时的

11
00:00:22,699 --> 00:00:29,519
这个损失函数L 当你的算法

12
00:00:25,350 --> 00:00:33,510
输出关于样本y的a^i

13
00:00:29,519 --> 00:00:36,120
你知道a^i是训练样本的预测值

14
00:00:33,510 --> 00:00:40,620
也就是𝜎(z^i)

15
00:00:36,120 --> 00:00:46,800
等于𝜎函数作用于w的转置乘上x^i

16
00:00:40,620 --> 00:00:48,510
加上b 所以我们在

17
00:00:46,800 --> 00:00:51,600
前几页幻灯片中展示的是对任意单个

18
00:00:48,510 --> 00:00:55,620
训练样本如何计算导数

19
00:00:51,600 --> 00:01:00,180
当你只有一个

20
00:00:55,620 --> 00:01:03,809
训练样本时 dw1 dw2和

21
00:01:00,180 --> 00:01:06,689
db 加上上标i

22
00:01:03,809 --> 00:01:08,369
表示你求得的相应值

23
00:01:06,689 --> 00:01:10,799
如果你现在在做我们在

24
00:01:08,369 --> 00:01:15,030
之前的幻灯片中演示的情况

25
00:01:10,799 --> 00:01:17,850
但只使用一个训练样本(x^i,y^i)

26
00:01:15,030 --> 00:01:20,759
抱歉 我这里少了个i

27
00:01:17,850 --> 00:01:22,530
现在你发现了带求和的全局代价函数

28
00:01:20,759 --> 00:01:26,220
实际上是从第1项到第m项

29
00:01:22,530 --> 00:01:29,369
各项损失的平均

30
00:01:26,220 --> 00:01:32,810
所以这表明全局代价函数

31
00:01:29,369 --> 00:01:38,600
对w_1的导数

32
00:01:32,810 --> 00:01:42,600
也同样是各项损失

33
00:01:38,600 --> 00:01:46,170
对w_1导数的平均值

34
00:01:42,600 --> 00:01:48,240
但之前我们

35
00:01:46,170 --> 00:01:54,119
已经演示了如何计算

36
00:01:48,240 --> 00:01:55,890
这项 也就是我所写的

37
00:01:54,119 --> 00:01:57,659
即之前幻灯片中演示的如何

38
00:01:55,890 --> 00:02:00,450
对单个训练样本进行计算

39
00:01:57,659 --> 00:02:03,119
所以你真正需要做的是计算

40
00:02:00,450 --> 00:02:04,680
这些导数 正如我们在

41
00:02:03,119 --> 00:02:07,350
之前的训练例子中演示的

42
00:02:04,680 --> 00:02:10,379
并且求平均 这会给你

43
00:02:07,350 --> 00:02:10,830
全局梯度值 你能够

44
00:02:10,379 --> 00:02:12,870
直接把它实现

45
00:02:10,830 --> 00:02:15,390
到梯度下降法中 所以这里有

46
00:02:12,870 --> 00:02:17,730
很多细节 但让我们把这些

47
00:02:15,390 --> 00:02:19,920
装进一个具体的算法

48
00:02:17,730 --> 00:02:21,690
你需要实现的就是

49
00:02:19,920 --> 00:02:24,960
使逻辑回归和其中的

50
00:02:21,690 --> 00:02:28,350
梯度下降法生效

51
00:02:24,960 --> 00:02:37,770
我们可以初始化J等于0

52
00:02:28,350 --> 00:02:40,140
dw_1等于0 dw_2等于0 db等于0

53
00:02:37,770 --> 00:02:43,190
我们将要做的是使用一个

54
00:02:40,140 --> 00:02:45,690
for循环遍历训练集

55
00:02:43,190 --> 00:02:47,670
同时计算相应的每个

56
00:02:45,690 --> 00:02:49,020
训练样本的微分并把它们加起来

57
00:02:47,670 --> 00:02:51,480
好的 如我们所做的让i

58
00:02:49,020 --> 00:02:54,360
取1到m m是训练样本数

59
00:02:51,480 --> 00:02:57,090
我们计算z^i就等于

60
00:02:54,360 --> 00:03:00,360
w的转置乘上x^i加上b

61
00:02:57,090 --> 00:03:04,020
a^i的预测值等于σ(z^i)

62
00:03:00,360 --> 00:03:09,120
然后我们要累加J

63
00:03:04,020 --> 00:03:12,360
J加等于(y^i)log(a^i)加上

64
00:03:09,120 --> 00:03:14,010
(1-y^i)log(1-a^i) 然后加一个

65
00:03:12,360 --> 00:03:15,959
负号在整个公式的前面

66
00:03:14,010 --> 00:03:20,580
然后如我们早前所见

67
00:03:15,959 --> 00:03:28,500
我们有dz^i或者它等于a^i减去y^i

68
00:03:20,580 --> 00:03:33,180
dw加等于(x_1)^i乘上dz^i dw_2加

69
00:03:28,500 --> 00:03:35,280
等于(x_2)^i乘上dz^i 我做这个计算

70
00:03:33,180 --> 00:03:37,680
已经假设你只有

71
00:03:35,280 --> 00:03:41,070
两个特征 所以n等于2

72
00:03:37,680 --> 00:03:45,480
否则你需要对dw_1 dw_2 dw_3等等

73
00:03:41,070 --> 00:03:47,430
做同样的计算 同时db加等于dz^i

74
00:03:45,480 --> 00:03:49,350
我觉得这个for循环就到此结束

75
00:03:47,430 --> 00:03:51,900
最终对所有的m个训练样本

76
00:03:49,350 --> 00:03:54,959
都进行这个计算后 你还需要

77
00:03:51,900 --> 00:03:56,880
除以m 因为我们是在

78
00:03:54,959 --> 00:04:01,920
计算平均值

79
00:03:56,880 --> 00:04:04,260
因此dw_1除等于m dw_2除等于m

80
00:04:01,920 --> 00:04:07,019
db除等于m 全都以平均的形式

81
00:04:04,260 --> 00:04:09,060
做完所有这些计算后

82
00:04:07,019 --> 00:04:11,160
你已经计算了

83
00:04:09,060 --> 00:04:14,250
代价函数J

84
00:04:11,160 --> 00:04:17,010
对各个参数w_1 w_2

85
00:04:14,250 --> 00:04:22,079
和b的导数 回顾我们正在做的细节

86
00:04:17,010 --> 00:04:25,020
我们使用dw_1 dw_2和db

87
00:04:22,079 --> 00:04:28,169
作为累加器

88
00:04:25,020 --> 00:04:31,500
所以算完这个以后

89
00:04:28,169 --> 00:04:33,509
dw_1等于你全局代价函数

90
00:04:31,500 --> 00:04:36,780
对w_1的导数

91
00:04:33,509 --> 00:04:39,720
对dw_2和db也是一样

92
00:04:36,780 --> 00:04:41,520
注意dw_1和dw_2没有上标i

93
00:04:39,720 --> 00:04:43,379
这是因为我们在这代码中把它们作为

94
00:04:41,520 --> 00:04:45,690
累加器去求取整个

95
00:04:43,379 --> 00:04:48,960
训练集上的和

96
00:04:45,690 --> 00:04:51,539
然而dz^i是对应于

97
00:04:48,960 --> 00:04:53,490
单个训练样本的dz

98
00:04:51,539 --> 00:04:55,740
这也就是为什么这里会有个上标i

99
00:04:53,490 --> 00:04:58,379
指代对应的第i个训练样本

100
00:04:55,740 --> 00:05:00,960
所以完成所有

101
00:04:58,379 --> 00:05:03,449
这些计算后实现一步

102
00:05:00,960 --> 00:05:06,360
梯度下降来更新w_1

103
00:05:03,449 --> 00:05:10,710
即w_1减去学习率

104
00:05:06,360 --> 00:05:13,740
乘上dw_1 而w_2更新为w_2减去

105
00:05:10,710 --> 00:05:17,190
学习率乘上dw_2

106
00:05:13,740 --> 00:05:21,000
同时b更新为b减去学习率乘上db

107
00:05:17,190 --> 00:05:23,879
这里dw_1 dw_2和db都是如之前所说那样计算的

108
00:05:21,000 --> 00:05:27,000
最终这里的J也会

109
00:05:23,879 --> 00:05:28,590
是你代价函数的正确值

110
00:05:27,000 --> 00:05:31,050
所以幻灯片上的所有东西

111
00:05:28,590 --> 00:05:33,060
只实现了一步

112
00:05:31,050 --> 00:05:35,699
梯度下降 因此你需要

113
00:05:33,060 --> 00:05:37,680
重复以上内容很多次

114
00:05:35,699 --> 00:05:40,469
以完成多次梯度下降

115
00:05:37,680 --> 00:05:41,819
这些细节看起来

116
00:05:40,469 --> 00:05:43,830
似乎很复杂

117
00:05:41,819 --> 00:05:45,960
但目前不要担心太多

118
00:05:43,830 --> 00:05:48,599
所有这些将会变的更加清楚

119
00:05:45,960 --> 00:05:50,520
当你继续尝试并在编程作业中

120
00:05:48,599 --> 00:05:54,120
实现这些方法的时候

121
00:05:50,520 --> 00:05:57,300
但它表明计算中有两个缺点

122
00:05:54,120 --> 00:05:59,729
如果按照这里的方法实现的话

123
00:05:57,300 --> 00:06:01,440
那就是这样实现

124
00:05:59,729 --> 00:06:03,960
逻辑回归时

125
00:06:01,440 --> 00:06:05,490
你需要两个for循环 第一个for循环

126
00:06:03,960 --> 00:06:07,770
是一个小循环用于遍历m个训练样本

127
00:06:05,490 --> 00:06:10,919
第二个for循环是一个

128
00:06:07,770 --> 00:06:13,139
遍历所有特征的for循环

129
00:06:10,919 --> 00:06:15,930
这个例子中我们只有2个特征

130
00:06:13,139 --> 00:06:17,879
所以n等于2并且n_x等于2

131
00:06:15,930 --> 00:06:21,000
但如果你有更多特征

132
00:06:17,879 --> 00:06:23,099
你需要编写dw_1 dw_2

133
00:06:21,000 --> 00:06:25,979
以及类似地计算dw_3等等

134
00:06:23,099 --> 00:06:29,009
直到dw_n 所以看来你

135
00:06:25,979 --> 00:06:31,279
需要一个for循环遍历

136
00:06:29,009 --> 00:06:33,199
所有n个特征

137
00:06:31,279 --> 00:06:36,049
当你实现深度学习算法时

138
00:06:33,199 --> 00:06:38,419
你会发现在代码中显式地

139
00:06:36,049 --> 00:06:41,839
使用for循环会使你的

140
00:06:38,419 --> 00:06:44,149
算法不够高效

141
00:06:41,839 --> 00:06:46,669
在深度学习时代会有

142
00:06:44,149 --> 00:06:48,649
越来越大的数据集

143
00:06:46,669 --> 00:06:50,779
所以不使用显式的for循环来

144
00:06:48,649 --> 00:06:52,969
实现你的算法

145
00:06:50,779 --> 00:06:55,129
是非常重要的而且会帮你

146
00:06:52,969 --> 00:06:56,719
适用于更大的数据集

147
00:06:55,129 --> 00:06:58,129
所以这里有一些技术

148
00:06:56,719 --> 00:07:01,159
叫做矢量化技术

149
00:06:58,129 --> 00:07:03,559
它可以帮助你的代码摆脱

150
00:07:01,159 --> 00:07:06,169
这些显式的for循环

151
00:07:03,559 --> 00:07:08,199
我想在前深度学习的时代

152
00:07:06,169 --> 00:07:11,239
也就是深度学习兴起之前

153
00:07:08,199 --> 00:07:13,159
矢量化是有着两面性的

154
00:07:11,239 --> 00:07:15,589
有时候能加速你的代码

155
00:07:13,159 --> 00:07:17,749
有时候也未必能加速

156
00:07:15,589 --> 00:07:20,029
但在深度学习时代

157
00:07:17,749 --> 00:07:22,699
矢量化 也就是像这样摆脱for循环

158
00:07:20,029 --> 00:07:25,039
已经变得相当重要

159
00:07:22,699 --> 00:07:26,989
因为我们越来越多地训练

160
00:07:25,039 --> 00:07:29,239
非常大的数据集

161
00:07:26,989 --> 00:07:31,209
因此你需要代码变得非常高效

162
00:07:29,239 --> 00:07:34,219
所以在接下来的几个视频中

163
00:07:31,209 --> 00:07:37,339
我们将讨论矢量化以及如何实现这些

164
00:07:34,219 --> 00:07:40,879
同时连一个for循环都不使用

165
00:07:37,339 --> 00:07:43,069
所以到这里 我希望你明白

166
00:07:40,879 --> 00:07:44,299
如何实现逻辑回归

167
00:07:43,069 --> 00:07:46,339
或者逻辑回归中的梯度下降法

168
00:07:44,299 --> 00:07:47,959
这一切都会变得更加清晰

169
00:07:46,339 --> 00:07:50,299
当你进行编程练习以后

170
00:07:47,959 --> 00:07:51,829
但在真正做编程练习前

171
00:07:50,299 --> 00:07:54,079
让我们先谈谈矢量化

172
00:07:51,829 --> 00:07:56,419
所以到时候你可以实现

173
00:07:54,079 --> 00:07:58,369
全部这些东西 实现一个

174
00:07:56,419 --> 00:08:01,479
梯度下降的迭代

175
00:07:58,369 --> 00:08:01,479
而不使用任何for循环
GTC字幕组翻译