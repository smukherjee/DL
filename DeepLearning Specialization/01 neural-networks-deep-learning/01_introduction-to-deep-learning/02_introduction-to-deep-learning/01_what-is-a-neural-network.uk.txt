Поняття глибоке навчання (ГН) відноситься 
до тренування нейронних мереж (НМ), інколи дуже великих НМ. Тож, що таке НМ? В цьому відео давай спробуємо дати тобі 
базове розуміння. Давай почнемо з прикладу передбачення вартості житла. Скажімо, ми маємо набір даних про 6 будинків. 
Тож, ми знаємо площу будинків в квадратних футах чи метрах і знаємо їх ціну. І хочемо підібрати функцію, щоб передбачатиме ціну будинків, 
як функцію від площі. Тож, якщо ти знайомий з лінійною регресією, 
то, можливо, скажеш: що ж, давайте намалюємо пряму через ці точки. 
Тож, скажімо, ми отримаємо приблизно таку пряму. Включивши уяву, ми можемо сказати: що ж, ми знаємо, що ціни не можуть бути від'ємними. Правильно? Тож, замість підбору прямої, яка, врешті-решт, 
стане від'ємною, давай зробимо її ламаною ось тут. Так, щоб вона закінчувалась нулем. Тож, ця товста синя лінія буде нашою функцією передбачення вартості будинку, як функція від площі. Тож ми маємо отут 0, а далі вправо - пряму, що підходить для наших точок. Тож, ця функція, яку ми щойно підібрали по цінах будинків, - є дуже простою НМ. Це майже найпростіша НМ. Давай я її тут опишу. Вхідними даними для НМ у нас тут є житлова площа. 
Ми називаємо їх x. x входить в цей вузол (це маленьке коло) і виходить ціною, яку ми називаємо y. Тож, це маленьке коло, 
що являє собою єдиний нейрон нашої НМ, реалізує цю функцію, яку ми намалювали зліва. І все, що робить цей нейрон, це - приймає площу, 
обчислює цю лінійну функцію, бере максимум відносно 0, 
а потім видає обчислену ціну. І, до речі, в літературі по НМ ця функція зустрічається дуже часто. Ця функція, яка рівна 0 на певному проміжку, а потім піднімається вгору прямою лінією. Ця функція називається функцією ВЛВ [ReLU], що означає випрямлений лінійний вузол 
[rectified linear unit] (R-E-L-U). Випрямлений означає максимум відносно 0. 
Ось що надає їй такої форми. Не переймайся щодо ВЛВ поки що. Просто ти ще побачиш його пізніше в цьому курсі. Тож, якщо цей єдиний нейрон НМ (дійсно дуже маленької НМ), то більша НМ потім формується з більшої кількості таких нейронів, 
складених докупи. Тож, якщо ти уявиш, що цей нейрон - 
окрема деталь Lego, то більшу НМ можна отримати, з'єднавши ці деталі Lego. Давай розглянемо приклад. Скажімо, замість лише самої площі будинку 
для передбачення вартості житла нам дано ще й інші характеристики. Ми знаємо і інші характеристики будинку, 
такі як кількість спалень. Запишу це "№ спалень". 
Якщо задуматись, то ще одна з характеристик, яка справді впливає на ціну, це - розмір сім'ї, правильно? 
Тобто, скажімо, чи підходить цей будинок сім'ї з 3-ох, чи 4-ох, чи 5-ти людей. І дійсно, базуючись на розмірі в квадратних футах чи метрах і кількості спалень, можна визначити чи підходить будинок сім'ї, розміром як твоя. Крім цього, можливо, нам відомі zip коди. В інших країнах вони називаються 
поштовими кодами будинку. І zip код, можливо, - характеристика, 
що говорить про пішу доступність. Тобто, що знаходиться в пішій доступності. Тобто, чи можна пішки дійти до продуктового магазину чи до школи? Чи потрібно їхати машиною? Декому подобається, щоб все було в пішій доступності. Тож, zip коди вкупі з, можливо, інфраструктурою, 
дають нам розуміння (зазвичай в США, але і в інших країнах також) наскільки якісна шкільна освіта. Тож, кожне з цих маленьких кіл, що я 
намалював, може бути однією з тих ВЛВ (випрямлених лінійних вузлів) або 
іншою нелінійною функцією. Отож, базуючись на площі будинку і кількості спалень, ми можемо визначити розмір сім'ї. 
Zip код визначає пішу доступність. Базуючись на zip коді і інфраструктурі, ми можемо 
визначити якість шкільної освіти. І, врешті-решт, ми можемо подумати: 
що ж, люди вирішують скільки готові заплатити за будинок, 
оцінюючи важливі для них характеристики. В цьому випадку: розмір сім'ї, що в пішій доступності 
і якість шкільної освіти. І це допомагає нам передбачити ціну. Тож, в цьому прикладі, x - це оці 4 вхідні ознаки, а y - ціна, яку ми намагаємося передбачити. Тож, групуючи кілька одиночних нейронів або простих предикторів, що ми бачили на попередньому слайді, 
ми отримуємо дещо більшу НМ. Магія НМ заключається в тому, що ми її реалізуємо, ми просто даємо їй вхідні x і вихідні y для певної кількості елементів тренувального набору, а всі оці штуки посередині зроблять все самі. Тож ми впровадили ось це. Тут ми маємо НМ з 4-ма вхідними ознаками. Цими вхідними ознаками можуть бути площа, кількість спалень, zip або поштовий код і інфраструктура. Тож, маючи ці вхідні параметри, роботою НМ буде - передбачення ціни y. Також варто відмітити, що кожен з цих кружечків 
(вони називаються прихованими вузлами НМ), кожен з них приймає всі ці 4 вхідні ознаки. Тож, наприклад, замість казати, 
що оцей перший вузол відображає розмір сім'ї, а розмір сім'ї залежить лише від ознак x₁ і x₂, ми збираємось сказати: "Що ж, НМ, вирішуй сама, що це має бути за вузол, Бо ми даємо тобі всі 4 вхідні ознаки, 
а ти вже дій як вважаєш за потрібне". Тож, ми кажемо, що ці шари - цей вхідний шар і цей шар всередині НМ - щільно зв'язані. Тому що кожна вхідна ознака пов'язана з кожним із цих кружечків всередині. Тож дивовижність НМ в тому, що, 
маючи достатньо відомостей про x та y, маючи достатньо тренувальних зразків, 
що містять з x та y, НМ дивовижно впоруються з обчисленням 
функцій, що співвідносять x до y. Отож, це - базова НМ. Виявляється, що розбудовуючи власну НМ, може дійти висновку, що вона найбільш корисна і потужна для налаштування керованого навчання. 
Тобто, коли ми намагаємось співвіднести вхідне x до деякого вихідного y, як ми щойно бачили 
на прикладі передбачення вартості житла. В наступному відео давай розглянемо трохи 
більше прикладів керованого навчання і кілька прикладів, де ти, можливо, виявиш, 
що твоя НМ неймовірно корисна також і для твоїх застосунків.