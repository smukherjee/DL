क्यों चाहिए एक न्यूरल नेटवर्क को 
एक नॉन-लीनियर ऐक्टिवेशन फ़ंक्शन? ऐसा होता है कि आपके न्यूरल नेटवर्क 
करने के लिए दिलचस्प फ़ंक्शन्स, आपको अवश्य लेना चाहिए 
एक नॉन-लीनियर ऐक्टिवेशन फ़ंक्शन. चलिए देखते हैं क्यों. तो यहाँ है 
फ़ॉर्वर्ड प्रॉपगेशन न्यूरल नेटवर्क का. हम इससे छुटकारा क्यों नहीं पा लेते? हटा दें फ़ंक्शन g और सेट करें a1 बराबर z1 के. या वैकल्पिक रूप से, आप कह सकते है 
कि g ऑफ़ z बराबर है z के, है न? कभी-कभी इसे कहते हैं लीनियर ऐक्टिवेशन फ़ंक्शन. शायद इसके लिए एक बेहतर नाम होगा आयडेंटिटी ऐक्टिवेशन फ़ंक्शन क्योंकि यह 
सिर्फ़ आउट्पुट करता है जो भी इनपुट था. इस उद्देश्य से, क्या हो यदि a2 सिर्फ़ बराबर हो z2 के? परिणाम यह निकला कि, यदि आप ऐसा करते हैं, तब यह मॉडल सिर्फ़ कम्प्यूट कर रहा है y या y-हैट 
इनपुट फ़ीचर्ज़ x के एक लीनियर फ़ंक्शन की तरह. लेने के लिए पहली दो इक्वेज़न्स, यदि आपके पास है वह a1 बराबर z1 है बराबर w1 x प्लस b और सिर्फ़ तब a2 है बराबर z2 है बराबर w2, a1 प्लस b, तब यदि आप लेते हैं 
परिभाषा a1 और रखते हैं यहाँ, आपको मिलेगा a2 है बराबर w2 गुणा w1 x प्लस b1, ठीक है? तो, यह है a1 प्लस b2 और यह सरलीकृत होता है w2, w1, x प्लस w2, b1 प्लस b2. तो यह है सिर्फ़, चलिए कहते हैं इसे w प्राइम, b प्राइम. तो यह है सिर्फ़ w प्राइम x प्लस b प्राइम. यदि आपको लेना होता लीनियर ऐक्टिवेशन फ़ंक्शन या जिन्हें हम आयडेंटिटी ऐक्टिवेशन फ़ंक्शन भी कहते हैं, तब न्यूरल नेटवर्क सिर्फ़ आउट्पुट करता 
एक लीनियर फ़ंक्शन इनपुट का. और हम बात करेंगे डीप नेटवर्क्स की बाद में. न्यूरल नेटवर्कस जिनमे हैं बहुत सी, बहुत सी लेयर्स, बहुत अधिक हिडन लेयर्स. और ऐसा होता है कि यदि आप लेते हैं एक 
लीनियर ऐक्टिवेशन फ़ंक्शन या वैकल्पिक रूप से, यदि आपके पास नहीं है एक ऐक्टिवेशन फ़ंक्शन तब कोई अंतर नहीं है कि कितनी लेयर्स हैं आपके न्यूरल नेटवर्क में, यह सिर्फ़ कम्प्यूट कर रहा है एक लीनियर ऐक्टिवेशन फ़ंक्शन. तो शायद आपको चाहिए ही नहीं कोई हिडन लेयर्स. जैसे कि मैंने संक्षेप में उल्लेख किया है, ऐसा होता है कि अगर आपके पास है एक लीनियर ऐक्टिवेशन फ़ंक्शन यहाँ और 
एक सिग्मोईड ऐक्टिवेशन फ़ंक्शन यहाँ, तब यह मॉडल कुछ ज़्यादा नहीं बता रहा तुलना में स्टैंडर्ड लॉजिसिटिक रेग्रेशन के बिना किसी हिडन लेयर के. तो मैं उसे साबित नहीं करूँगा लेकिन 
आप कर सकते हैं यदि आप चाहें. लेकिन शुद्ध संदेश है कि, एक हिडन लेयर लगभग बेकार है क्योंकि संरचना दो लीनियर फ़ंक्शनज़ की
 ख़ुद भी एक लिनीयर फ़ंक्शन है. तो, जबतक आप वहाँ नॉन-लिनियरिटी नहीं लाते हैं, तब आप नहीं कम्प्यूट कर रहे अधिक दिलचस्प फ़ंक्शन्स, चाहे आप डीप 
भी जाते हैं नेटवर्क में. वहाँ सिर्फ़ एक स्थान है जहाँ आप शायद ले 
सकते हैं एक लीनियर ऐक्टिवेशन फ़ंक्शन, g ऑफ़ z बराबर है z और वह है जब आप कर रहे हैं
 मशीन लर्निंग एक रेग्रेशन प्रॉब्लम पर. तो, यदि y है एक रियल नम्बर. तो उदाहरण के लिए, यदि आप प्रिडिक्ट करना 
चाह रहे हैं क़ीमत घरों की, तो y नहीं है 0,1, लेकिन यह है एक रियल नम्बर, कहीं $0 क़ीमत से आगे कितना महँगा घर हो सकता है. मुझे लगता है शायद, घर हो सकते हैं संभवत: मिल्यन डालर्ज़ तक. हालाँकि, कितनी भी क़ीमत है घर की आपके डेटा सेट में, परंतु यदि y लेता है ये रियल वैल्यूज़, तब हो सकता है सही लेना 
लीनियर ऐक्टिवेशन फ़ंक्शन यहाँ ताकि आपकी आउट्पुट y-हैट है एक रियल नम्बर ही, माइनस इन्फ़िनिटी से 
प्लस इन्फ़िनिटी तक कुछ भी. लेकिन तब, हिडन यूनिट्स को नहीं इस्तेमाल करना 
चाहिए लीनियर ऐक्टिवेशन फ़ंक्शन. वे ले सकते हैं ReLU या tanh या 
leaky ReLU या शायद कुछ और. तो, स्थान जहाँ आप शायद ले सकते हैं 
एक लीनियर ऐक्टिवेशन है अक्सर आउट्पुट लेयर. लेकिन उसके अलावा, लेना एक लीनियर ऐक्टिवेशन फ़ंक्शन एक हिडन लेयर में विशेष परिस्थियों के अलावा, कम्प्रेशन से सम्बंधित जिसकी हम बात नहीं करेंगे लेना एक लीनियर ऐक्टिवेशन फ़ंक्शन बहुत ही कम है. ओह, और हां, अगर आप वास्तव में घरों की कीमतें 
प्रिडिक्ट कर रहे है जैसे आपने देखा सप्ताह एक के वीडियो में क्योंकि 
घरों की क़ीमतें हैं नॉन-नेगेटिव शायद तब भी, आप ले सकते हैं ReLU ऐक्टिवेशन फ़ंक्शन ताकि आपकी आउट्पुट्स y-हैट हैं सभी बड़ी या बराबर ज़ीरो के. तो मुझे आशा है कि यह देता है आपको एक ज्ञान 
 कि क्यों होना एक नॉन-लीनियर ऐक्टिवेशन फ़ंक्शन है 
एक महत्वपूर्ण हिस्सा न्यूरल नेटवर्कस का. आगे, हम बात करेंगे ग्रेडीयंट डिसेंट के बारे में, 
और वह करने के लिए, ग्रेडीयंट डिसेंट के बारे में चर्चा करने के लिए, अगले वीडियो में, मैं आपको दिखाना चाहता हूँ कैसे अनुमान करना है, कैसे कम्प्यूट करनी है स्लोप या डेरिवेटिव्स ऐक्टिवेशन फ़ंक्शन्स के. तो चलिए चलते हैं उस पर.