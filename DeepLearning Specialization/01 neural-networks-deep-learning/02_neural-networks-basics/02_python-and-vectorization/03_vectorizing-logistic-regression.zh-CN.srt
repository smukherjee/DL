1
00:00:00,860 --> 00:00:05,760
我们已经谈过向量化能够
大大地加速你的代码

2
00:00:05,760 --> 00:00:08,160
在本节课中 我们会讲讲逻辑回归的

3
00:00:08,160 --> 00:00:10,545
向量化实现

4
00:00:10,545 --> 00:00:12,960
使得它们可以被用于处理整个训练集

5
00:00:12,960 --> 00:00:15,930
也就是说 可以用梯度下降法的一次迭代来处理

6
00:00:15,930 --> 00:00:22,330
整个训练集 甚至不需要使用任何一个for循环

7
00:00:22,330 --> 00:00:24,039
我对于这个技术兴奋不已

8
00:00:24,039 --> 00:00:26,670
后面当我们讨论神经网络时

9
00:00:26,670 --> 00:00:30,050
也不需要使用任何一个for循环

10
00:00:30,050 --> 00:00:35,965
那么就让我们开始吧
首先让我们看看逻辑回归的前向传播

11
00:00:35,965 --> 00:00:37,860
假设我们有m个训练样本

12
00:00:37,860 --> 00:00:40,605
为了预测第一个样本

13
00:00:40,605 --> 00:00:42,105
你需要计算这个

14
00:00:42,105 --> 00:00:45,480
计算z 使用这个我们熟悉的公式

15
00:00:45,480 --> 00:00:47,370
来计算激活函数

16
00:00:47,370 --> 00:00:49,485
即计算第一个例子的y值

17
00:00:49,485 --> 00:00:52,705
然后预测第二个样本

18
00:00:52,705 --> 00:00:54,405
你需要做同样的计算

19
00:00:54,405 --> 00:00:57,085
然后预测第三个样本

20
00:00:57,085 --> 00:00:59,045
你还需要做这个计算 以此类推

21
00:00:59,045 --> 00:01:01,020
如果你有m个训练样本

22
00:01:01,020 --> 00:01:03,855
你可能需要重复m次

23
00:01:03,855 --> 00:01:08,250
不过为了实现前向传播

24
00:01:08,250 --> 00:01:13,435
即计算出m个训练样本的预测结果

25
00:01:13,435 --> 00:01:14,865
有一种实现方法

26
00:01:14,865 --> 00:01:17,925
可以不需要for循环

27
00:01:17,925 --> 00:01:20,450
让我们看看如何做到

28
00:01:20,450 --> 00:01:26,455
首先回忆一下 我们曾把矩阵X
定义为训练的输入值

29
00:01:26,455 --> 00:01:30,895
就像这样排列在不同的列中

30
00:01:30,895 --> 00:01:33,810
这就是一个矩阵

31
00:01:33,810 --> 00:01:38,425
一个nx乘m的矩阵

32
00:01:38,425 --> 00:01:41,885
我现在用Python numpy的shape形式来写

33
00:01:41,885 --> 00:01:50,350
这是说X是一个nx乘m维的矩阵

34
00:01:50,350 --> 00:01:54,670
现在我先告诉你如何计算z(1)和z(2)

35
00:01:54,670 --> 00:01:56,512
以及z(3)等等

36
00:01:56,512 --> 00:01:58,665
以上全部在一个步骤中完成

37
00:01:58,665 --> 00:02:01,195
实际上 仅用一行代码即可实现

38
00:02:01,195 --> 00:02:06,930
所以我要先构造一个

39
00:02:06,930 --> 00:02:13,100
1xm维的行向量 方便计算z(1)

40
00:02:13,100 --> 00:02:15,405
z(2)等等

41
00:02:15,405 --> 00:02:18,480
直到z(m) 都是在同一时间内(完成计算)

42
00:02:18,480 --> 00:02:22,175
实际上它可以表达成

43
00:02:22,175 --> 00:02:29,225
W的转置矩阵(W^T)与矩阵X相乘
再加上这个向量

44
00:02:29,225 --> 00:02:31,040
b b b ...

45
00:02:31,040 --> 00:02:33,315
其中这个[b b ... b b]的东西

46
00:02:33,315 --> 00:02:34,480
其中这个[b b ... b b]的东西

47
00:02:34,480 --> 00:02:38,980
是个1*m维的向量

48
00:02:38,980 --> 00:02:46,725
或者称为一个1*m维的矩阵
或一个m维的行向量

49
00:02:46,725 --> 00:02:50,495
如果你比较熟悉矩阵乘法的话
你可能就能看出

50
00:02:50,495 --> 00:02:56,300
W的转置乘上X 其实就等价于
W的转置乘上x(1)

51
00:02:56,300 --> 00:02:58,760
x(2)等等直到x(m)

52
00:02:58,760 --> 00:03:05,755
其中W^T应该是一个行向量

53
00:03:05,755 --> 00:03:10,655
W^T应该是一个这样的行向量

54
00:03:10,655 --> 00:03:18,614
所以第一项就等于W^T*x(1)

55
00:03:18,614 --> 00:03:22,970
W^T*x(2)等等...

56
00:03:22,970 --> 00:03:29,840
直到W^T*x(m) 而当你加上第二项

57
00:03:29,840 --> 00:03:30,960
[b b ... b]时

58
00:03:30,960 --> 00:03:33,565
就等同于你把每一项加上b

59
00:03:33,565 --> 00:03:37,650
所以你得到了另一个1*m的向量

60
00:03:37,650 --> 00:03:38,955
这是第一个元素

61
00:03:38,955 --> 00:03:40,590
这是第二个元素 等等

62
00:03:40,590 --> 00:03:42,810
这是第m个元素

63
00:03:42,810 --> 00:03:45,605
如果你参照上面的定义

64
00:03:45,605 --> 00:03:51,250
第1个元素就是z(1)的定义

65
00:03:51,250 --> 00:03:57,305
第2个元素就是z(2)的定义 等等

66
00:03:57,305 --> 00:04:00,035
就如同我们构造X那样

67
00:04:00,035 --> 00:04:02,870
把你的训练样本水平排列在一起

68
00:04:02,870 --> 00:04:07,400
把你的训练样本水平排列在一起

69
00:04:07,400 --> 00:04:11,069
我将把Z定义为把z(1) z(2) ... z(m)们

70
00:04:11,069 --> 00:04:16,385
水平排列在一起

71
00:04:16,385 --> 00:04:21,080
所以当你把代表着不同训练样本的x们

72
00:04:21,080 --> 00:04:24,350
水平排列在一起 你得到了X

73
00:04:24,350 --> 00:04:27,420
当你用同样的方法把这些z们

74
00:04:27,420 --> 00:04:28,805
水平排列在一起

75
00:04:28,805 --> 00:04:34,050
你会得到Z

76
00:04:34,050 --> 00:04:37,400
为了实现这些计算

77
00:04:37,400 --> 00:04:45,773
numpy命令为Z=np.dot(W.T,X)+b

78
00:04:45,773 --> 00:04:51,095
其中W.T为W的转置

79
00:04:51,095 --> 00:04:53,645
这里有一个python的精妙的地方

80
00:04:53,645 --> 00:04:59,405
这里b是一个实数 或者说一个1*1的矩阵

81
00:04:59,405 --> 00:05:01,330
即一个普通的实数

82
00:05:01,330 --> 00:05:06,230
但是 当你把这个实数b
加到这个向量上的时候

83
00:05:06,230 --> 00:05:13,235
python自动把这个实数b
扩展为一个1*m的行向量

84
00:05:13,235 --> 00:05:16,490
为了防止你觉得这个运算难以理解

85
00:05:16,490 --> 00:05:20,120
这个在python中叫做广播(broadcasting)

86
00:05:20,120 --> 00:05:22,210
现在你不需要担心这个

87
00:05:22,210 --> 00:05:25,760
我们会在下一个视频中详细讨论

88
00:05:25,760 --> 00:05:29,180
这里值得学习的是 你可以用这一行代码

89
00:05:29,180 --> 00:05:33,290
来计算Z

90
00:05:33,290 --> 00:05:37,698
其中Z是一个包含z(1) z(2) ... z(m)的1*m矩阵

91
00:05:37,698 --> 00:05:41,200
其中Z是一个包含z(1) z(2) ... z(m)的1*m矩阵

92
00:05:41,200 --> 00:05:46,255
所以这是求z的过程 那我们怎么求这些a们的值呢

93
00:05:46,255 --> 00:05:48,260
我们下一步要做的

94
00:05:48,260 --> 00:05:52,685
就是找到一个同时求

95
00:05:52,685 --> 00:05:57,220
a(1), a(2), ..., a(m)

96
00:05:57,220 --> 00:05:58,700
的方法

97
00:05:58,700 --> 00:06:03,350
就像把所有x排列在一起可以得到X

98
00:06:03,350 --> 00:06:08,870
把所有z排列在一起可以得到Z

99
00:06:08,870 --> 00:06:10,810
把所有a排列在一起

100
00:06:10,810 --> 00:06:12,470
可以得到一个新的变量

101
00:06:12,470 --> 00:06:15,200
我们将它定义为A

102
00:06:15,200 --> 00:06:18,075
在程序作业中

103
00:06:18,075 --> 00:06:22,790
你会看到如何实现一个
(输入输出为)向量值的sigmoid函数

104
00:06:22,790 --> 00:06:24,480
所以将这个Z作为sigmoid函数的输入值

105
00:06:24,480 --> 00:06:32,380
会非常高效地得到A

106
00:06:32,380 --> 00:06:36,620
你将在程序作业中看到细节

107
00:06:36,620 --> 00:06:38,110
复习一下

108
00:06:38,110 --> 00:06:42,655
我们在这个幻灯片中学到的是

109
00:06:42,655 --> 00:06:47,515
我们不需要遍历m个训练样本来一次一次计算z和a

110
00:06:47,515 --> 00:06:52,090
你可以用一行代码来实现

111
00:06:52,090 --> 00:06:54,290
同时计算所有的z

112
00:06:54,290 --> 00:06:57,100
用这一行代码

113
00:06:57,100 --> 00:06:59,260
以及sigma函数的恰当实现

114
00:06:59,260 --> 00:07:04,115
来同时计算所有的a

115
00:07:04,115 --> 00:07:05,965
这就是你如何通过向量化同时实现

116
00:07:05,965 --> 00:07:07,948
这就是你如何通过向量化同时实现

117
00:07:07,948 --> 00:07:11,560
所有m个训练样本的前向传播

118
00:07:11,560 --> 00:07:13,985
总结一下 你刚刚看到了你如何使用向量化

119
00:07:13,985 --> 00:07:18,100
来高效同时计算所有的激活函数

120
00:07:18,100 --> 00:07:21,700
所有a(1) a(2) ... a(m)

121
00:07:21,700 --> 00:07:24,860
下一步 我们会证明你还可以用向量化来

122
00:07:24,860 --> 00:07:27,910
高效计算反向传播

123
00:07:27,910 --> 00:07:29,650
计算导数

124
00:07:29,650 --> 00:07:32,000
让我们在下一节课中学习如何实现它
GTC字幕组翻译