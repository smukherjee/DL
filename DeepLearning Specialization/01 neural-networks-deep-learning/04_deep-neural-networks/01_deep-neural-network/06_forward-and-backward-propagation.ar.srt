1
00:00:00,000 --> 00:00:01,350
في الفيديو السابق

2
00:00:01,350 --> 00:00:05,065
رأيتم اللبنات الأساسية لتنفيذ الشبكة العصبية العميقة.

3
00:00:05,065 --> 00:00:07,696
ورأينا خطوة الانتشار الأمامي لكل طبقة

4
00:00:07,696 --> 00:00:09,975
وخطوة الانتشار الخلفي المقابلة لها.

5
00:00:09,975 --> 00:00:12,370
دعونا نرى كيف يمكن أن نقوم بتنفيذ
هذه الخطوات بشكل فعلي.

6
00:00:12,370 --> 00:00:14,230
سوف نبدأ بالانتشار الأمامي.

7
00:00:14,230 --> 00:00:20,045
تذكروا أن ما سيؤدي ذلك إلى إنتاجه هو
المدخلات [l-1] والمخرجات a[l]

8
00:00:20,045 --> 00:00:21,795
والذاكرة الوسيطة z[l].

9
00:00:21,795 --> 00:00:24,615
كما قلنا للتو أنه من وجهة نظر التنفيذ

10
00:00:24,615 --> 00:00:28,145
ربما حيث تكون الذاكرة الوسيطة w[l] وb[l] كذلك

11
00:00:28,145 --> 00:00:31,585
فقط لتسهيل ظهور الدوال في التدريب الخاص بالمشكلة.

12
00:00:31,585 --> 00:00:35,438
وبالتالي، فإن الدوال المتعلقة بذلك يجب أن تكون معروفة لكم.

13
00:00:35,438 --> 00:00:46,813
طريقة تنفيذ دالة أمامية تكون يساوي
w[l] * a[l-1] + b[l]

14
00:00:46,813 --> 00:00:53,620
ثم a[l] يساوي دالة إلغاء
التنشيط المطبقة على "z".

15
00:00:53,620 --> 00:00:57,250
وإذا كنت ترغب في توجيه التنفيذ

16
00:00:57,250 --> 00:01:06,218
يكون فقط من خلال هذا عدة مرات a[l-1] + b

17
00:01:06,218 --> 00:01:09,930
مع إضافة "b"، التي تشير إلى الإفراط في البث

18
00:01:09,930 --> 00:01:15,296
وa[l] = g المطبقة بشكل يعتمد
على العناصر على "z".

19
00:01:15,296 --> 00:01:20,128
وكما تذكرون، في مخطط الخطوة الأمامية

20
00:01:20,128 --> 00:01:22,565
تذكرون أننا كان لدينا هذه السلسلة من المربعات
التقدمية نحو الأمام

21
00:01:22,565 --> 00:01:26,775
ويمكن تهيئة ذلك من خلال إدخال a[0]

22
00:01:26,775 --> 00:01:29,305
والتي تساوي "X".

23
00:01:29,305 --> 00:01:31,440
وبالتالي، تقوم بتهيئة ذلك.

24
00:01:31,440 --> 00:01:33,380
في الواقع، ما المدخلات الخاصة بالشيء الأول؟

25
00:01:33,380 --> 00:01:40,207
في الواقع، تكون المدخلات a[0]
وهي سمات المدخلات لمثال تدريب واحد

26
00:01:40,207 --> 00:01:42,255
إذا كنت تقوم بعمل مثال واحد في كل مرة

27
00:01:42,255 --> 00:01:45,800
أو A[0]، لمجموعة التدريب برمتها

28
00:01:45,800 --> 00:01:48,215
إذا كنت تقوم بمعالجة مجموعة التدريب برمتها.

29
00:01:48,215 --> 00:01:51,990
إذًا، هذه هي مدخلات أول أربع دوال في السلسلة

30
00:01:51,990 --> 00:01:53,970
ومجرد تكرار ذلك يسمح لك

31
00:01:53,970 --> 00:01:57,085
بحساب الانتشار الأمامي من اليسار إلى اليمين.

32
00:01:57,085 --> 00:02:00,110
بعد ذلك، سوف نتحدث عن خطوة الانتشار الخلفي.

33
00:02:00,110 --> 00:02:03,585
هنا، هدفك إدخال da[l]

34
00:02:03,585 --> 00:02:08,040
وإخراج da[l-1] وdW[l] وdb.

35
00:02:08,040 --> 00:02:16,240
دعوني أشير إلى الخطوات التي تحتاج إليها لحساب
هذه الأشياء: dz[l] = da[l]

36
00:02:16,240 --> 00:02:23,866
منتج يعتمد على العنصر مع g[l]` z[l]

37
00:02:23,866 --> 00:02:27,405
ثم، لحساب المشتقات

38
00:02:27,405 --> 00:02:34,420
dW[l] = dz[l] * a[l - 1].

39
00:02:34,420 --> 00:02:37,445
لم أضع ذلك بشكل صريح في
الذاكرة الوسيطة، إلا أنه يتضح

40
00:02:37,445 --> 00:02:39,114
أنك تحتاج إلى ذلك كذلك.

41
00:02:39,114 --> 00:02:47,560
وبعد ذلك، db[l] = dz[l]، وفي النهاية

42
00:02:47,560 --> 00:02:59,089
da[l-1] = w[l]_نقل في dz[l]، حسنًا؟

43
00:02:59,089 --> 00:03:02,310
ولا أريد الخوض في المشتقات التفصيلية لذلك

44
00:03:02,310 --> 00:03:06,605
لكن، يتضح أنك إذا أخذت هذا
التعريف لـ da ووضعته هنا

45
00:03:06,605 --> 00:03:10,260
تحصل على نفس الصيغة التي
كانت لدينا الأسبوع الماضي

46
00:03:10,260 --> 00:03:16,617
حيال كيفية حساب dz[l] كدالة
لقيمة dz[l] السابقة في الواقع، حسنًا

47
00:03:16,617 --> 00:03:18,135
إذا وضعت ذلك هنا

48
00:03:18,135 --> 00:03:33,817
ينتهي الأمر بـ dz[l] = w[l+1]_نقل
dz[l+1] * g[l]` z[l].

49
00:03:33,817 --> 00:03:36,165
أنا أعلم أن هذا يبدو مثل الجبر كثيرًا

50
00:03:36,165 --> 00:03:38,360
يمكنك التحقق بنفسك من أن هذه

51
00:03:38,360 --> 00:03:40,820
هي نفس المعادلة التي كتبناها

52
00:03:40,820 --> 00:03:43,055
للانتشار الخلفي في الأسبوع الماضي عندما

53
00:03:43,055 --> 00:03:45,930
كنا نقوم بعمل الشبكات العصبية
بطبقة مخفية واحدة فقط.

54
00:03:45,930 --> 00:03:48,602
وأود أن أذكركم أنه في هذه المرة
هذا المنتج يعتمد على العناصر

55
00:03:48,602 --> 00:03:54,950
وبالتالي كل ما تحتاج إليه هو هذه المعادلات الأربعة
لتنفيذ الدالة الخلفية.

56
00:03:54,950 --> 00:03:58,735
وفي النهاية، سوف أكتب نسخة موجهة.

57
00:03:58,735 --> 00:04:04,135
وبالتالي يكون أول سطر dz[l] = dA[l]

58
00:04:04,135 --> 00:04:11,045
منتج يعتمد على العناصر مع g[l]` of z[l].

59
00:04:11,045 --> 00:04:13,060
وربما لا تكون هناك أي مفاجآت هنا.

60
00:04:13,060 --> 00:04:23,715
فـ dW[l] تصبح 1/m, dz[l] * a[l-1]_نقل، ثم

61
00:04:23,715 --> 00:04:30,844
db[l] تصبح 1/m np.sum dz[l]

62
00:04:30,844 --> 00:04:37,970
ثم، axis = 1 وkeepdims = true.

63
00:04:37,970 --> 00:04:44,095
لقد تحدثنا عن استخدام np.sum
في الأسبوع الماضي لحساب db.

64
00:04:44,095 --> 00:04:56,155
وفي النهاية dA[l-1] تكون W[l]_نقل * dz[l].

65
00:04:56,155 --> 00:05:02,435
إذًا، يسمح لك ذلك بإدخال هذه الكمية، da، هنا

66
00:05:02,435 --> 00:05:07,950
وإخراج dW[l], db[l]

67
00:05:07,950 --> 00:05:10,100
المشتقات التي تحتاج إليها

68
00:05:10,100 --> 00:05:16,022
وكذلك dA[l-1]، حسنًا؟ كما يلي.

69
00:05:16,022 --> 00:05:18,905
إذًا تلك هي طريقة تنفيذ الدالة الخلفية.

70
00:05:18,905 --> 00:05:20,560
حسنًا، لكي نلخص ذلك

71
00:05:20,560 --> 00:05:23,585
قم بأخذ المدخلات "X"

72
00:05:23,585 --> 00:05:25,060
يمكن أن يكون لديك أول طبقة

73
00:05:25,060 --> 00:05:28,445
وربما لديك دالة تنشيط ReLU.

74
00:05:28,445 --> 00:05:30,570
ثم انتقل إلى الطبقة الثانية

75
00:05:30,570 --> 00:05:33,340
ربما تستخدم دالة تنشيط ReLU أخرى

76
00:05:33,340 --> 00:05:35,175
وتنتقل إلى الطبقة الثالثة

77
00:05:35,175 --> 00:05:39,595
وربما يكون لديك دالة تنشيط Sigmoid إذا كنت
تقوم بإجراء التصنيف الثنائي

78
00:05:39,595 --> 00:05:41,875
ويؤدي ذلك إلى إنتاج y_hat.

79
00:05:41,875 --> 00:05:43,890
بعد ذلك، باستخدام y_hat

80
00:05:43,890 --> 00:05:46,265
يمكنك حساب الخسائر

81
00:05:46,265 --> 00:05:49,685
ويسمح لك ذلك ببدء التكرار الخلفي.

82
00:05:49,685 --> 00:05:51,775
سوف أرسم الأسهم أولاً، حسنًا؟

83
00:05:51,775 --> 00:05:54,320
حتى لا أحتاج إلى تغيير الأقلام كثيرًا.

84
00:05:54,320 --> 00:06:03,430
سيقوم الانتشار الخلفي بحساب المشتقات، وحساب

85
00:06:03,430 --> 00:06:16,063
dW[3] وdb[3] وdW[2]
وdb[2] وdW[1] وdb[1]

86
00:06:16,063 --> 00:06:18,865
وطوال الطريق، سوف تقوم بالحساب

87
00:06:18,865 --> 00:06:24,820
وعلى ما أعتقد، يمكن أن تقوم الذاكرة الوسيطة بنقل
z[1] وz[2] وz[3]

88
00:06:24,820 --> 00:06:32,250
وهنا، توقف da[2] وda[1] الخلفية مؤقتًا.

89
00:06:32,250 --> 00:06:34,730
يمكن أن يؤدي ذلك لحساب da[0]

90
00:06:34,730 --> 00:06:35,880
إلا أننا لا نريد استخدام ذلك.

91
00:06:35,880 --> 00:06:37,935
لذا، يمكنك تجاهل ذلك، حسنًا؟

92
00:06:37,935 --> 00:06:40,785
وبالتالي، تلك هي طريقة حساب
الانتشار الأمامي والانتشار الخلفي

93
00:06:40,785 --> 00:06:44,040
لشبكة عصبية مكونة من ثلاث طبقات.

94
00:06:44,040 --> 00:06:46,140
الآن، هناك تفصيل أخير لم

95
00:06:46,140 --> 00:06:48,735
أتحدث عنه وهو أنه بالنسبة للتكرار الأمامي

96
00:06:48,735 --> 00:06:52,420
سوف نقوم بالبدء فيه باستخدام بيانات المدخلات "X".

97
00:06:52,420 --> 00:06:54,090
وماذا عن التكرار الخلفي؟

98
00:06:54,090 --> 00:06:59,235
حسنًا، يتضح أن da[l]

99
00:06:59,235 --> 00:07:01,065
عندما تستخدم التراجع اللوجيستي

100
00:07:01,065 --> 00:07:02,977
أثناء إجراء التصنيف الثنائي

101
00:07:02,977 --> 00:07:09,685
يساوي y/a + 1-y/1-a.

102
00:07:09,685 --> 00:07:12,575
لذا، يتضح أن مشتقة دالة الخسارة

103
00:07:12,575 --> 00:07:14,180
فيما يتعلق بالمخرجات

104
00:07:14,180 --> 00:07:17,475
وفيما يتعلق بـ y_hat،
يمكن أن يظهر على ما هو عليه.

105
00:07:17,475 --> 00:07:19,105
إذا كنت على دراية بحساب التفاضل والتكامل

106
00:07:19,105 --> 00:07:21,328
إذا بحثت عن دالة الخسارة "l"

107
00:07:21,328 --> 00:07:24,150
وأخذت المُشتقات، فيما يتعلق بـ y_hat أو a

108
00:07:24,150 --> 00:07:26,505
يتضح أنك تحصل على هذه الصيغة.

109
00:07:26,505 --> 00:07:31,350
إذًا، هذه هي الصيغة التي يجب أن تستخدمها
لـ da للطبقة النهائية حرف "L" كابيتال.

110
00:07:31,350 --> 00:07:35,715
وبالقطع، إذا كنت ترغب في إجراء التنفيذ الموجه

111
00:07:35,715 --> 00:07:38,351
تقوم ببدء تكرار خلفي

112
00:07:38,351 --> 00:07:43,706
ليس من خلال ذلك، ولكن باستخدام da للطبقة l

113
00:07:43,706 --> 00:07:48,465
ويسري نفس الأمر على الأمثلة المختلفة

114
00:07:48,465 --> 00:07:54,011
عبر "a"، لمثال التدريب الأول، + 1-y

115
00:07:54,011 --> 00:07:55,285
لمثال التدريب الأول

116
00:07:55,285 --> 00:07:58,153
عبر 1-a، لمثال التدريب الأول

117
00:07:58,153 --> 00:08:05,185
...حتى نهاية مثال التدريب، حيث 1-a[m].

118
00:08:05,185 --> 00:08:09,423
إذًا، تلك هي طريقة تنفيذ النسخة الموجهة.

119
00:08:09,423 --> 00:08:13,055
تلك هي طريقة تهيئة النسخة
الموجهة من الانتشار الخلفي.

120
00:08:13,055 --> 00:08:16,100
إذًا، لقد رأيت الآن اللبنات الأساسية

121
00:08:16,100 --> 00:08:20,030
للانتشار الأمامي وكذلك الانتشار الخلفي.

122
00:08:20,030 --> 00:08:22,340
والآن، إذا قمنا بتنفيذ هذه المعادلات

123
00:08:22,340 --> 00:08:24,530
فسوف يتم تنفيذ الانتشار الأمامي

124
00:08:24,530 --> 00:08:27,640
والانتشار الخلفي بشكل صحيح للحصول
على المشتقات التي تريدها.

125
00:08:27,640 --> 00:08:29,660
وربما يراود ذهنك أنه، لأننا لدينا الكثير من المعادلات

126
00:08:29,660 --> 00:08:32,080
تشعر بالارتباك قليلاً، وتشعر أنك لست متأكدًا تمامًا
من أنك ترى كيف يعمل ذلك.

127
00:08:32,080 --> 00:08:34,645
إذا كنت تشعر بذلك، فإن نصيحتي لك

128
00:08:34,645 --> 00:08:37,205
أنك عندما تصل إلى مهمة البرمجة الخاصة بهذا الأسبوع

129
00:08:37,205 --> 00:08:40,175
سوف تتمكن من تنفيذ ذلك بنفسك

130
00:08:40,175 --> 00:08:42,020
وستكون الأشياء ملموسة بشكل أكبر.

131
00:08:42,020 --> 00:08:43,805
وأنا أدرك أن هناك الكثير من المعادلات

132
00:08:43,805 --> 00:08:46,265
وربما بعض المعادلات لا يمكن فهمها منطقيًا
بشكل كامل

133
00:08:46,265 --> 00:08:49,055
لكن، إذا كنت تتعامل مع حساب التفاضل والتكامل

134
00:08:49,055 --> 00:08:50,905
والجبر الخطي، وهو أمر ليس سهلاً

135
00:08:50,905 --> 00:08:52,430
يمكن أن تبدأ في التجربة

136
00:08:52,430 --> 00:08:56,390
إلا أن هذا من أصعب الاشتقاقات في التعلم الآلي.

137
00:08:56,390 --> 00:08:57,950
ويتضح أن المعادلات تتدفق

138
00:08:57,950 --> 00:09:02,685
أو مجرد معادلات التفاضل والتكامل لحساب المشتقات
خصوصًا في الانتشار الخلفي.

139
00:09:02,685 --> 00:09:04,750
لكن، مرة أخرى، إذا كان
ذلك يبدو أمرًا تجريديًا بشدة

140
00:09:04,750 --> 00:09:06,400
وغامضًا بعض الشيء بالنسبة لك

141
00:09:06,400 --> 00:09:09,108
فإني أقول لك إنه بعد
الانتهاء من التدريب الرئيسي

142
00:09:09,108 --> 00:09:11,465
فإنك ستشعر أنها ملموسة
بشكل أفضل بالنسبة لك.

143
00:09:11,465 --> 00:09:14,120
رغم أنني مجبر على أن أقول الآن

144
00:09:14,120 --> 00:09:16,805
أنه عند تنفيذ خوارزمية
تعلم، في بعض الأحيان

145
00:09:16,805 --> 00:09:18,000
فإنه حتى أنا أشعر بالاندهاش عندما

146
00:09:18,000 --> 00:09:21,170
يؤتي تنفيذ خوارزميات التعلم الخاصة بي ثماره
وهذا يرجع إلى

147
00:09:21,170 --> 00:09:25,670
أن الكثير من التعقيد في التعلم الآلي يأتي
من البيانات وليس من سطور التعليمات البرمجية.

148
00:09:25,670 --> 00:09:27,095
وبالتالي، في بعض الأحيان، تشعر

149
00:09:27,095 --> 00:09:28,685
بأنك تقوم بتنفيذ عدد قليل من سطور
التعليمات البرمجية.

150
00:09:28,685 --> 00:09:30,110
ولا تكون متأكدًا تمامًا من نتيجتها

151
00:09:30,110 --> 00:09:31,625
إلا أنها تعمل بشكل سحري

152
00:09:31,625 --> 00:09:35,296
لأن القدر الأكبر من السحر لا يكمن في
التعليمات البرمجية التي تكتبها

153
00:09:35,296 --> 00:09:37,090
والتي غالبًا لا تكون طويلة.

154
00:09:37,090 --> 00:09:38,705
إنها ليست بسيطة تمامًا

155
00:09:38,705 --> 00:09:40,730
لكنها لا تكون عشرات الآلاف

156
00:09:40,730 --> 00:09:42,115
ولا مئات الآلاف من سطور التعليمات البرمجية

157
00:09:42,115 --> 00:09:44,750
لكنك تقوم بإدخال قدر كبير من البيانات التي ما زالت

158
00:09:44,750 --> 00:09:46,850
رغم أنني عملت لفترة طويلة في مجال التعلم الآلي

159
00:09:46,850 --> 00:09:49,400
تدهشني إلى حد ما عندما

160
00:09:49,400 --> 00:09:53,274
تؤتي خوارزميات التعلم الخاصة بي ثمارها لأن قدر
كبير من تعقيد خوارزميات

161
00:09:53,274 --> 00:09:55,970
التعلم الخاصة بك يأتي من البيانات وليس

162
00:09:55,970 --> 00:10:01,020
بالضرورة من كتابة الآلاف والآلاف من سطور
التعليمات البرمجية.

163
00:10:01,020 --> 00:10:05,935
حسنًا. إذًا تلك هي طريقة تنفيذ الشبكات العصبية العميقة.

164
00:10:05,935 --> 00:10:10,325
ومرة أخرى، سوف يصبح ذلك ملموسًا بشكل أفضل
لك عندما تنتهي من التدريب الرئيسي لك.

165
00:10:10,325 --> 00:10:14,220
وقبل المتابعة، في الفيديو التالي

166
00:10:14,220 --> 00:10:17,480
أريد مناقشة المتغيرات المفرطة والمتغيرات.

167
00:10:17,480 --> 00:10:19,681
فقد اتضح أنك عندما تقوم بتدريب الشبكات العميقة

168
00:10:19,681 --> 00:10:22,225
فإن التمكن من ترتيب المتغيرات
المفرطة لديك بشكل جيد

169
00:10:22,225 --> 00:10:25,400
يساعدك على أن تكون أكثر فاعلية في تطوير
الشبكات الخاصة بك.

170
00:10:25,400 --> 00:10:29,000
في الفيديو التالي، سوف نتحدث
عن ماهية ذلك بدقة.