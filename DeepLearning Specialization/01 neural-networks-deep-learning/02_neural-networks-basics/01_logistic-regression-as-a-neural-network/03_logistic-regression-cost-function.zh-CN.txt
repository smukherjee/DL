在之前的视频中 我们学习了逻辑回归模型 为了优化逻辑回归模型的参数W和B 需要定义一个代价函数 现在 就来了解一下可以优化逻辑回归的代价函数 先来回顾一下 我们之前的讲义中用过的这个方程式 将输出值y ̂设为S(W*X^t+b) 将此处的S(z)设定为x+b 所以 为了了解你的模型参数 这里有一组用于优化m参数的示例 你会很自然想找出参数W和B 至少在获得优化集合和输出值时 对于优化集合的假定 我们只提出y ̂(i)将接近 从优化集合中获得的实标y_i 为了更详细地描述上诉方程式 我们已经提及y ̂已经在之前 为优化示例x和其他示例进行了设置 我们使用上标 圆括号标注索引 区分示例 对优化示例(i) 也就是y ̂(i) 的假定 将通过S函数应用到WX' 输入值(i) 优化示例+V Z(i)也可以如此设定 Z(i)等于WX'+b 所以本课中 我们就使用这个标记法则 上标括号指的是数据 X Y Z 以及其他字母与i-th优化示例相关联 也与i-th示例相关 这是括弧里上标i的意义 现在 我们来学习损失函数或 误差函数可以用来检测算法运行情况 如在算法输出时定义损失 y ̂和实标Y有可能是一个或半个平方误差 你可以如此操作 但一般在逻辑回归里不进行此操作 因为当研究参数时 我们讨论的优化问题将会变成非凸问题 所以 优化问题会产生多个局部最优解 梯度下降算法也就无法找到全局最优解 如果你暂时不理解以上知识点 不用担心 接下来的视频中会详细讲解 但有一点请记住 函数L被称为损失函数 需要进行设定 才能在实标为y时对输出值y ̂进行检测 平方误差整体是个合理的选择 除了无法让梯度下降算法良好运行 所以 在逻辑回归中 我们会设定 一个不同的损失函数充当平方误差 这样能产生一个凸象最优问题 将使之后的优化变得更容易 所以 实际要使用的逻辑回归 是我写在屏幕右上方的损失函数 [公式]-ly log (y^)+(1-y)log(1-y^) [公式] 这里说明下为什么这个损失函数有意义 请记住 如果使用平方误差 平方误差需要尽可能的小 在此逻辑回归损失函数里 我们也要让这个数值尽可能小 为了帮助理解 我们举两个例子 第一个 我们假定Y=1 [公式] L (y^, y) -log(y ̂) 如果y等于1 第二项1-y等于0 所以如果y=1，-logy ̂就必须尽可能大 因此log y ̂数值必须大 这也就是要求y ̂的数值必须大 但是y ̂是S函数 它无法大于1 这也就是说y等于1时 y ̂值要尽可能的大 但却不能大过1 所以y ̂值要无限接近1 另一个例子是 如果y等于0 损失函数中的第一项也等于0 因为y等于0 第二项设定损失函数 所以损失是-log1-y ̂ 所以在学习过程中 你需要把损失函数值变小 这意味着log1-y ̂的值要大 又因为有负号 可以总结出类似的原因 就是损失函数会使y ̂尽可能变小 因为y ̂的数值在0~1之间 如果y等于0 损失函数将作用于参数使y ̂无限趋近于0 目前有很多函数有拉斐拉效应 也就是 如果y等于1 y ̂值要变大 如果y等于0 y ̂值要变小 就像这边绿色的内容 关于这个损失函数 有一段非正式的解释之后 另一段视频将更详细地解释 为什么在逻辑回归中 这个特定函数更适用 最后 这个损失函数被单一的优化示例所定义 它将检测单一优化示例的运行情况 接下来，我要设定代价函数 来检测优化组的整体运行情况 所以 运用于参数W和B的代价函数J 是取m平均值 损失函数的总和运用于优化示例 这里的y ̂ 你的逻辑回归算法预计的输出值 用一组特定的W和B参数 所以 展开后 这个等于-1/m 在设定的损失函数中i等于1 所以 这就是y(i)logy ̂ (i)+1 y(i) y ̂(i) 我想我可以在这里添加括号 这样减号可以在最外面 所以我要用的术语是 损失函数适用于像这样单一的优化示例 损失函数反映的是你的参数成本 所以 在优化你的逻辑回归模型时 我们要试着去找参数W和B 以此来缩小J的整体成本 所以 你刚看到的就是设置逻辑回归算法 优化示例的损失函数 以及算法参数的总体损失函数 逻辑回归可被视为一个非常小的神经网络 在下一个视频中我们会提及相关内容 大家对神经网络也就会有初步的认识 接下来请大家观看下一个视频 逻辑回归是一个很小的神经网络
GTC字幕组翻译