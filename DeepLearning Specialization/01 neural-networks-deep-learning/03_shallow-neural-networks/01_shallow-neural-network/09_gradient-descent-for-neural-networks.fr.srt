1
00:00:00,000 --> 00:00:02,520
Je pense que cette vidéo 
va être très intéressante.

2
00:00:02,520 --> 00:00:04,635
Dans cette vidéo, vous allez voir 
comment implémenter

3
00:00:04,635 --> 00:00:08,490
la descente de gradient pour un réseau 
de neurones avec une couche cachée.

4
00:00:08,490 --> 00:00:12,090
Dans cette vidéo, je vais juste vous
 donner les équations nécessaires pour

5
00:00:12,090 --> 00:00:16,245
implémenter la rétro propagation,
 c'est à dire pour faire fonctionner
 une descente de gradient.

6
00:00:16,245 --> 00:00:18,555
Puis dans la vidéo suivante,

7
00:00:18,555 --> 00:00:20,940
Je vais vous donner une compréhension 
plus intuitive de pourquoi

8
00:00:20,940 --> 00:00:24,150
ces équations sont en effet 
les bonnes équations pour

9
00:00:24,150 --> 00:00:28,320
calculer les gradients nécessaires
 pour votre réseau de neurones.

10
00:00:28,320 --> 00:00:29,910
Bien, votre réseau de neurones,

11
00:00:29,910 --> 00:00:31,875
avec une seule couche cachée pour l’instant,

12
00:00:31,875 --> 00:00:34,964
aura les paramètres W[1],

13
00:00:34,964 --> 00:00:39,285
b[1], W[2] et b[2].

14
00:00:39,285 --> 00:00:40,800
Ainsi, à titre de rappel,

15
00:00:40,800 --> 00:00:48,150
Si vous avez nx, ou autrement n[0] 
caractéristiques en entrée,

16
00:00:48,150 --> 00:00:51,090
et n[1] unités cachées

17
00:00:51,090 --> 00:00:57,260
et n[2] unités de sortie dans notre exemple.

18
00:00:57,260 --> 00:00:59,690
Jusqu'ici on avait toujours n[2] = 1,

19
00:00:59,690 --> 00:01:05,720
alors la matrice W[1] sera 
de dimensions n[1] par n[0].

20
00:01:05,720 --> 00:01:08,870
b[1] sera un vecteur de dimension n[1],

21
00:01:08,870 --> 00:01:12,770
Donc je vais l'écrire comme
 une matrice de dimensions n[1] par 1,

22
00:01:12,770 --> 00:01:14,120
c'est à dire un vecteur colonne.

23
00:01:14,120 --> 00:01:18,395
Les dimensions de W[2] seront n[2] par n[1],

24
00:01:18,395 --> 00:01:25,485
et les dimensions de b[2] seront n[2] par 1.

25
00:01:25,485 --> 00:01:28,925
Jusqu'ici, nous avons toujours eu n[2] = 1,

26
00:01:28,925 --> 00:01:32,180
où vous n'avez qu'une seule 
unité cachée.

27
00:01:32,180 --> 00:01:39,405
Vous avez également une fonction
 de coût pour le réseau de neurones.

28
00:01:39,405 --> 00:01:43,370
Pour le moment je suppose que
 vous faites de la classification binaire.

29
00:01:43,370 --> 00:01:45,110
Dans ce cas,

30
00:01:45,110 --> 00:01:50,600
le coût de vos paramètres va être

31
00:01:50,600 --> 00:01:56,520
1 sur m fois la moyenne
 de cette fonction de perte.

32
00:01:56,520 --> 00:02:02,580
L ici est la perte quand 
votre réseau de neurones prédit ŷ,

33
00:02:02,580 --> 00:02:06,750
qui est égal à a[2], quand l'étiquette vaut Y.

34
00:02:06,750 --> 00:02:08,560
Si vous faites de
 la classification binaire,

35
00:02:08,560 --> 00:02:13,310
la fonction de perte peut être la même 
que pour la régression logistique.

36
00:02:13,310 --> 00:02:15,890
Donc pour entrainer
 les paramètres de votre algorithme,

37
00:02:15,890 --> 00:02:19,705
vous devrez faire une descente de gradient.

38
00:02:19,705 --> 00:02:21,570
Lors de l'entraînement
 d'un réseau de neurones,

39
00:02:21,570 --> 00:02:26,435
il est important d’initialiser les poids
 au hasard, plutôt que tous à zéro.

40
00:02:26,435 --> 00:02:28,340
Nous verrons plus tard pourquoi, mais

41
00:02:28,340 --> 00:02:31,110
après l'initialisation des paramètres,

42
00:02:31,110 --> 00:02:34,555
chaque boucle de descente de gradients 
va calculer des prédictions.

43
00:02:34,555 --> 00:02:38,270
C'est à dire que vous calculez ŷ(i)

44
00:02:38,270 --> 00:02:41,765
pour i de 1 à m, et puis

45
00:02:41,765 --> 00:02:44,450
vous devez calculer les dérivées.

46
00:02:44,450 --> 00:02:47,750
Vous devez calculer dW[1],

47
00:02:47,750 --> 00:02:54,279
qui est la dérivée de la fonction de coût 
par rapport au paramètre W[1].

48
00:02:54,279 --> 00:02:56,499
Vous devez calculer une autre variable,

49
00:02:56,499 --> 00:02:58,375
qu'on appellera db[1],

50
00:02:58,375 --> 00:03:02,260
la dérivée,
 ou la pente, de la fonction de coûts

51
00:03:02,260 --> 00:03:06,190
par rapport à la valeur b[1] et ainsi de suite.

52
00:03:06,190 --> 00:03:09,685
Même chose pour
 les autres paramètres W[2] et b[2].

53
00:03:09,685 --> 00:03:17,775
Enfin, la mise à jour de la descente 
de gradient sera de remplacer

54
00:03:17,775 --> 00:03:21,150
W[1]par W[1] moins alpha, le taux d'apprentissage, fois dW[1].

55
00:03:21,150 --> 00:03:26,310
b[1] est remplacé par b[1] - le taux 
d'apprentissage fois db[1]

56
00:03:26,310 --> 00:03:32,280
et c'est la même chose pour W[2] et b[2].

57
00:03:32,280 --> 00:03:35,560
Et parfois, j'utilise deux points égal,

58
00:03:35,560 --> 00:03:37,630
les deux notations sont biens.

59
00:03:37,630 --> 00:03:40,790
Voilà donc une itération 
de descente de gradient.

60
00:03:40,790 --> 00:03:42,580
puis vous répétez ça 
un certain nombre de fois

61
00:03:42,580 --> 00:03:45,100
jusqu'à ce que vos paramètres convergent.

62
00:03:45,100 --> 00:03:46,300
Dans une vidéo précédente,

63
00:03:46,300 --> 00:03:49,055
nous avons parlé de comment
 calculer les prédictions,

64
00:03:49,055 --> 00:03:50,400
comment calculer les sorties,

65
00:03:50,400 --> 00:03:52,960
et nous avons aussi vu 
comment le faire de façon vectorisée.

66
00:03:52,960 --> 00:03:57,820
Donc, la clé est de savoir comment 
calculer ces termes de dérivées partielles,

67
00:03:57,820 --> 00:04:03,010
dW[1], db[1] et les dérivées dW[2] et db[2].

68
00:04:03,010 --> 00:04:06,730
Donc, ce que je voudrais faire, 
c’est juste vous donner

69
00:04:06,730 --> 00:04:11,050
les équations dont vous avez besoin
 afin de calculer ces dérivés.

70
00:04:11,050 --> 00:04:15,130
Je reporte à la vidéo suivante, 
qui est une vidéo optionnelle, l'explication

71
00:04:15,130 --> 00:04:19,030
détaillée de comment 
nous avons trouvé ces formules.

72
00:04:19,030 --> 00:04:25,360
Donc, je voudrais juste résumer à nouveau
 les équations de propagation vers l'avant.

73
00:04:25,360 --> 00:04:32,510
Donc, vous avez Z[1] égal à W[1] * X + b[1],

74
00:04:32,510 --> 00:04:42,560
et puis A[1] est égal à la fonction d’activation 
dans cette couche appliquée à chaque

75
00:04:42,560 --> 00:04:46,610
élément de Z[1], et puis Z[2] est égal à

76
00:04:46,610 --> 00:04:52,595
W[2]* A[1] + b[2], et puis enfin,

77
00:04:52,595 --> 00:04:55,295
tout ça est vectorisé sur
 tout le set d'apprentissage,

78
00:04:55,295 --> 00:05:00,580
A[2] est égal à g[2] de Z[2].
Encore une fois,

79
00:05:00,580 --> 00:05:03,605
pour l’instant, nous supposons que
 nous faisons de la classification binaire,

80
00:05:03,605 --> 00:05:07,120
alors cette fonction d'activation
 devrait être la fonction sigmoïde,

81
00:05:07,120 --> 00:05:08,995
au moins pour ce neurone de sortie.

82
00:05:08,995 --> 00:05:11,900
Donc, voilà la propagation vers l’avant,
 de la gauche vers la droite

83
00:05:11,900 --> 00:05:14,690
pour le calcul de votre réseau de neurones.

84
00:05:14,690 --> 00:05:16,730
Ensuite, nous allons calculer les dérivées.

85
00:05:16,730 --> 00:05:21,725
Donc, c’est l’étape de rétro propagation.

86
00:05:21,725 --> 00:05:30,900
Alors je calcule dZ[2], égal à
 A[2] moins Y,

87
00:05:30,900 --> 00:05:33,130
et juste pour rappel,

88
00:05:33,130 --> 00:05:35,240
tout cela est vectorisé sur les exemples.

89
00:05:35,240 --> 00:05:38,540
Ainsi, la matrice Y est de dimensions 1 par m,

90
00:05:38,540 --> 00:05:44,600
elle contient les m exemples 
empilés horizontalement.

91
00:05:44,600 --> 00:05:50,599
On trouve que dW[2] est égal à ceci.

92
00:05:50,599 --> 00:05:54,920
Et en fait ces trois premières équations sont

93
00:05:54,920 --> 00:06:00,630
très similaires à la descente de gradient 
pour la régression logistique.

94
00:06:00,910 --> 00:06:03,170
axis = 1, virgule,

95
00:06:03,170 --> 00:06:08,635
keepdims=true

96
00:06:08,635 --> 00:06:13,600
Juste un petit détail, ce np.sum est

97
00:06:13,600 --> 00:06:18,700
une commande de Python NumPy pour
 sommer sur une dimension d'une matrice.

98
00:06:18,700 --> 00:06:21,450
Dans ce cas, on va sommer horizontalement

99
00:06:21,450 --> 00:06:25,645
et keepdims empêche Python

100
00:06:25,645 --> 00:06:30,750
de retourner un de
 ces bizarres tableaux de rang 1

101
00:06:30,750 --> 00:06:33,525
avec la dimension (n, ).

102
00:06:33,525 --> 00:06:36,045
Donc ce keepdims=True nous assure

103
00:06:36,045 --> 00:06:43,210
que Python retourne pour db[2] 
un vecteur de dimension (n,1).

104
00:06:43,210 --> 00:06:47,145
En fait, techniquement ça va être (n[2], 1),

105
00:06:47,145 --> 00:06:49,680
et dans ce cas,
 c'est juste un nombre, donc (1, 1),

106
00:06:49,680 --> 00:06:51,795
donc ça n'a peut être pas d'importance.

107
00:06:51,795 --> 00:06:55,350
Mais plus tard, nous verrons 
que c’est vraiment important.

108
00:06:55,350 --> 00:06:59,825
Jusqu'ici, ce que nous avons fait est 
très proche d'une régression logistique.

109
00:06:59,825 --> 00:07:04,260
Mais pour poursuivre la rétro propagation,

110
00:07:04,260 --> 00:07:05,790
vous devez calculer ceci :

111
00:07:05,790 --> 00:07:16,340
dZ[1] = (W[2].T*dZ[2]) .* (g[1]'(Z[1])

112
00:07:16,340 --> 00:07:19,190
donc cette quantité g[1] prime est

113
00:07:19,190 --> 00:07:23,945
la dérivée de la fonction d'activation
 de la couche cachée.

114
00:07:23,945 --> 00:07:25,750
Pour la couche de sortie,

115
00:07:25,750 --> 00:07:29,470
j'ai supposé que vous faisiez une classification
 binaire avec la fonction sigmoïde,

116
00:07:29,470 --> 00:07:32,630
donc la dérivée est déjà inclue 
dans la formule pour dZ[2].

117
00:07:32,630 --> 00:07:37,735
Rappelez vous bien, ce fois est 
un produit élément par élément,

118
00:07:37,735 --> 00:07:45,650
donc ceci sera une matrice n[1] par m, et ceci

119
00:07:45,650 --> 00:07:51,545
cette dérivée élément par élément sera
 également une matrice n[1] par m,

120
00:07:51,545 --> 00:07:55,910
et ce fois ici est la multiplication
 élément par élément de deux matrices.

121
00:07:55,910 --> 00:08:00,950
Enfin, dW[1] est égal à ça,

122
00:08:00,950 --> 00:08:07,010
et db[1] est égal à :

123
00:08:07,010 --> 00:08:14,930
np.sum( dZ[1], axis=1, keepdims=True)

124
00:08:14,930 --> 00:08:20,820
np.sum( dZ[1], axis=1, keepdims=True)

125
00:08:20,820 --> 00:08:26,455
Ce keepdims avait avant peu 
d'importance car n[2] est égal à 1,

126
00:08:26,455 --> 00:08:29,475
donc le résultat était une matrice
 de 1 par 1, un nombre réel.

127
00:08:29,475 --> 00:08:36,330
Ici, db[1] sera un vecteur 
de dimensions n[1] par 1

128
00:08:36,330 --> 00:08:39,180
donc on veut que np.sum retourne

129
00:08:39,180 --> 00:08:43,990
quelque chose de cette dimension
 plutôt qu'un bizarre tableau de rang 1

130
00:08:43,990 --> 00:08:49,720
de cette dimension qui pourrait gêner
 certains de vos calculs de données.

131
00:08:49,720 --> 00:08:52,914
Une autre façon serait de ne pas
 utiliser le paramètre keepdims,

132
00:08:52,914 --> 00:09:01,470
mais d'appeler explicitement reshape 
sur le résultat de np.sum pour s'assurer

133
00:09:01,470 --> 00:09:04,665
que db ait les bonnes dimensions.

134
00:09:04,665 --> 00:09:09,655
Donc c'était la propagation vers 
l'avant en quatre équations,

135
00:09:09,655 --> 00:09:13,400
et la rétro propagation en six équations.

136
00:09:13,400 --> 00:09:15,590
Je sais que je n'ai fait 
qu'écrire ces équations,

137
00:09:15,590 --> 00:09:17,990
mais dans la prochaine vidéo, optionnelle,

138
00:09:17,990 --> 00:09:20,690
nous verrons quelques 
intuitions de comment

139
00:09:20,690 --> 00:09:24,730
on trouve les 6 équations
 de la rétro propagation.

140
00:09:24,730 --> 00:09:26,455
Vous pouvez choisir de la regarder ou pas,

141
00:09:26,455 --> 00:09:28,875
mais de toutes façons, 
si vous implémentez ça, vous aurez

142
00:09:28,875 --> 00:09:33,255
une implémentation correcte de la propagation 
directe et de la rétro propagation et

143
00:09:33,255 --> 00:09:38,120
vous pourrez calculer toutes les dérivées pour 
mettre en place la descente de gradient,

144
00:09:38,120 --> 00:09:40,360
et faire apprendre ses paramètres 
à votre réseau de neurones.

145
00:09:40,360 --> 00:09:43,190
Il est possible d'implémenter
 cet algorithme et

146
00:09:43,190 --> 00:09:46,000
de la faire fonctionner sans pour autant 
comprendre en profondeur les dérivées.

147
00:09:46,000 --> 00:09:49,505
Beaucoup de praticiens de
 l’apprentissage profond le font.

148
00:09:49,505 --> 00:09:50,975
Mais, si vous le voulez,

149
00:09:50,975 --> 00:09:52,505
regardez la vidéo suivante

150
00:09:52,505 --> 00:09:57,680
pour avoir un peu plus d’intuition sur
 la façon d'obtenir ces équations.