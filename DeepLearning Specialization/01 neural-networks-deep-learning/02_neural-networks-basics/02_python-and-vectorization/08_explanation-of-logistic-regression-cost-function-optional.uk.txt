Раніше я записав формулу функції Витрат для логістичної регресії (ЛР). В цьому необов'язковому відео я хочу дати коротке обґрунтування чому ми використовуємо функцію Витрат для ЛР. Згадаємо, що в ЛР ми маємо передбачення ŷ = сигмоїді від w транспоноване x плюс b, де сигмоїда - ось ця знайома функція. І ми казали, що будемо розцінювати ŷ як ймовірність того, що y=1 при заданому x. Тож ми хочемо, щоб наш алгоритм вивів y як імовірність, що y=1 для заданих вхідних ознак x. Іншими словами, якщо y=1, тоді ймовірність від y при заданому x =ŷ. І навпаки, якщо y=0, тоді ймовірність (того, що y=0) =1-ŷ. Тож якщо ŷ є ймовірністю, що y=1, тоді 1-ŷ є ймовірністю, що y=0. Тож давай візьмемо 2 останніх рівняння і 
скопіюємо на наступний слайд. Тож я візьму ці 2 рівняння, які, по суті, визначають p(y|x) для двох випадків: y=0 та y=1. Тож я візьму ці 2 рівняння і об'єднаю в одне рівняння. Зазначимо, що y має бути або 0 або1, 
тому що в нас двійкова класифікація. Тож y= або 0 або 1 - лише 2 можливих випадки. Якщо взяти ці 2 рівняння і об'єднати їх наступним чином... Давай я спочатку запишу це, а потім поясню чому це так. Тож 1-ŷ в степені 1-y. Виявляється цей один рядок відображає два верхніх. Давай поясню чому. Тож, перший випадок: коли y=1. Тож, якщо y=1, тоді цей вираз буде = ŷ, тому що це буде ŷ в степені 1. Оцей вираз буде: 1-ŷ в степені 1-1, тобто в степені 0. Так як будь-яке число в степені 0 буде 1, то цей вираз відпадає. І, отже, це рівняння стане p(y|x)=ŷ при y=1. А це саме те, що нам треба. Як щодо другого випадку? Що буде якщо y=0? Якщо y=0, тоді це рівняння вгорі буде p(y|x)=... ŷ в степені 0. Але будь-яке число в степені 0 =1. Тому це просто = 1 помножене на 1-ŷ в степені 1-y. А 1-y буде 1-0, тобто просто =1. І, отже, це = 1 помножити на 1-ŷ, а це =1-ŷ. І, отже, ми маємо, що при y=0, p(y|x)=1-ŷ, А це саме те, що ми записали вгорі. Тож ми щойно показали, що ось це рівняння є правильним визначенням для p(y|x). Тепер, врешті, через те, що логарифмічна функція є 
строго монотонно зростаючою функцією, яку ми намагаємось максимізувати, то логарифм від y 
при заданому x повинен дати результат схожий на p(y|x) при оптимізації. 
І ми можемо обчислити log(p(y|x)), який = log(ŷʸ(1-ŷ)⁽¹⁻ʸ⁾). А це спрощується до y*log(ŷ)+(1-y)*log(1-ŷ), правильно? А це, насправді, протилежне до функції Втрати, яку ми раніше визначили. Знак мінус тут тому, що зазвичай при тренуванні навчального алгоритму, ми хочемо щоб імовірності були якомога більшими, а для ЛР це значить, що ми намагаємось мінімізувати функцію Втрати. Тож мінімізація Втрати 
означає максимізацію логарифму від ймовірності. Тож ось що являє собою функція Втрати для одного зразка. А що ж з функцією Витрат, загальною функцією Витрат для всього тренувального набору 
з m зразків? Давай обчислимо. Тож імовірності всіх позначок тренувального зразка (запишемо це дещо неформально), якщо припустити, що тренувальні зразки я намалював незалежно 
або що вони - НРВ/IID (нормально розподілені величини/identically independently distributed), то імовірність всіх зразків буде добутком їх імовірностей. Отже, добуток по i від 1 до m усіх p(y⁽ⁱ⁾|x⁽ⁱ⁾). Якщо ми хочемо застосувати метод максимальної правдоподібності, то нам потрібно максимізувати або знайти параметри, що максимізують ймовірність спостережень і тренувального набору. Проте, максимізація цього - це те саме, що максимізація логарифму. Тож поставмо логарифм з обох боків. Логарифм ймовірності позначок тренувального набору = логарифму добутку, тобто сумі логарифмів, - сумі по i від 1 до m усіх log(p(y⁽ⁱ⁾|x⁽ⁱ⁾)). Нещодавно ми побачили (на попередньому слайді), що це =-L(ŷ⁽ⁱ⁾, y⁽ⁱ⁾). В статистиці, цей метод називається 
методом максимальної правдоподібності, що означає, що щоб вибрати параметри, які максимізують ось це, або іншими словами, максимізують ось це: від'ємна сума по i від 1 до m усіх L(ŷ⁽ⁱ⁾, y⁽ⁱ⁾).<i><i> Я просто виніс мінус перед суму. Тож це обґрунтування наших Витрат для ЛР, тобто J(w, b)= оцьому. А через те, що ми хочемо мінімізувати Витрати, 
а не максимізувати правдоподібність, ми позбудемося знаку мінус. І, нарешті, згідно конвенції, 
щоб звести наш результат до відповіднішого діапазону, додамо тут масштабуючий фактор 1/m. Тож підсумуємо. Мінімізуючи функцію Витрат J(w, b), ми, насправді, обчислюємо максимальну правдоподібність для моделі ЛР. При цьому ми припускаємо, що тренувальні зразки - НРВ/IID або нормально розподілені величини/identically independently distributed. Тож дякую за перегляд цього відео, хоча це було й необов'язково. Надіюсь, 
ти отримав/ла розуміння чому ми використовуємо функцію Витрат для ЛР. І, надіюсь, ти перейдеш до вправ (програмувальної вправи) і цьоготижневого опитування. Успіхів тобі на опитуванні і вправах!