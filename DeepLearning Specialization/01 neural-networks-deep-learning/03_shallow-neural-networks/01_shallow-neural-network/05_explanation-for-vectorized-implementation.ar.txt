لاحظنا في الفيديو السابق، كيف استطعنا من خلال مقارنة نماذج
التدريب أفقيًا في المصفوفة x اشتقاق تطبيق موجّه للانتشار
الأمامي من خلال شبكتك العصبية. دعونا نتناول اليوم تفسيرات أخرى
لكتابة هذه المعادلات باعتبارها تطبيقًا صحيحًا للطريقة
المتجهية على عدة أمثلة. ولنستعرض الآن جزءًا من العملية
الحسابية للانتشار لبضعة أمثلة. لنفترض أنه في نموذج التدريب الأول ستجدون أنفسكم تقومون بحساب التالي x1 زائد b1، وفي نموذج التدريب الثاني ستقومون بحساب x2 زائد b1 وفي نموذج التدريب الثالث ستقومون بحساب 3 زائد b1. لذا، لتبسيط الشرح الموجود في هذه الشريحة، سأتجاهل b. لنفترض أن b تساوي صفرًا،
حتى يكون هذا الشرح أبسط قليلاً. ومع ذلك، سنُجري بعض التغييرات
على الوسيطة التي سنعرضها حتى لو لم تكن قيمة b تساوي صفرًا. وما فعلناه هو لمجرد تبسيط شرح
محتوى هذه الشريحة. حسنًا، سنفترض أن w1
هي إحدى المصفوفات. وبداخل هذه المصفوفة
يوجد عدد من الصفوف. وبالنظر إلى هذه العملية الحسابية لـ x1 فستجدون أن حاصل ضرب w1 في x1 هو متجه
عمود يجب رسمه كالتالي. وبالمثل، إذا نظرتم إلى المتجه x2 هذا وقمنا بضرب w1 في x2 فسنحصل على متجه عمود ثانٍ، صحيح؟ وهو يعطينا z12. وأخيرًا، إذا نظرنا إلى x3، وقمنا بضرب w1 في x3، فسنحصل على متجه عمود ثالث وهو
ما أطلقنا عليه z1 والآن، إذا فكرتم في مجموعة التدريب X "كابيتال"، التي حصلنا عليها بتكديس
كل نماذج التدريب. تكّونت المصفوفة X "كابيتال" بتكديس كل من المتجه x1 عموديًا مع المتجه x2 ثم المتجه x3. وذلك إذا كان عدد نماذج التدريب
هو ثلاثة فقط. وإذا كان العدد أكثر من ذلك، فسنستمر
في تكديسها أفقيًا على هذا النحو. ولكن إذا أخذنا المصفوفة x، وقمنا
بضربها في w، فسنحصل على إذا أخذنا كيفية عمل مضاعفة
المصفوفات بعين الاعتبار فسيتضمن العمود الأول القيم نفسها التي كتبتها هنا قبل قليل
باللون الأرجواني. وسيتضمن العمود الرابع هذه القيم
الموجودة هنا نفسها. وبالمثل سيتضمن العمود الثالث هذه القيم
الموجودة هنا باللون البرتقالي. هذه هي النتيجة النهائية أمامكم. وهي بالطبع تساوي z11
التي عبّرنا عنها بمتجه عمود تلاه z12 في صورة
متجه عمود تلاه z13، التي عبّرنا عنها أيضًا بمتجه عمود. وهذا في حالة وجود ثلاثة
نماذج تدريب فقط. فإذا كانت هناك مزيد من النماذج،
فسنحصل على مزيد من الأعمدة. وهذه ببساطة هي المصفوفة Z1 "كابيتال". وأتمنى أن تكونوا قد فهمتهم السبب وراء ما قمنا به مسبقًا بضرب w1 في xi
وكان الناتج هو Z1i، وذلك في حالة نموذج
تدريب واحد في المرة الواحدة. وعند أخذ نماذج التدريب
المختلفة وتكديسها في أعمدة مختلفة، ستكون النتيجة المقابلة
التي سنحصل عليها هي كل قيم Z مكدّسة في الأعمدة. لن أتناول هذا الجزء، ولكن يمكنكم
تجربة ذلك إن أردتم في Python broadcasting، وإذا قمتم بإضافة قيم b هذه إلى القيم المعروضة
فستظل النتيجة صحيحة. وما سيحدث فعليًا هو تنفيذ Python broadcasting، الذي يقوم بتوزيع القيمة bi على كل
عمود من أعمدة هذه المصفوفة حسنًا، إذًا ما قمت به في هذه الشريحة
هو أنني قدمت تفسيرًا للمعادلة Z1 =w1X + b1 وهو تطبيق صحيح للطريقة المتجهية للخطوة الأولى من الخطوات الأربع
التي تحدثنا عنها في الشريحة السابقة لكننا توصلنا في الوقت نفسه إلى أن
التحليل المماثل سيوضح لنا أنه يمكننا تطبيق الخطوات الأخرى
بنجاح أيضًا باتّباع منطق مشابه حيث إنه إذا قمنا بتكديس
المدخلات في أعمدة فستكون النتيجة بعد المعادلة الحصول على مخرجات
مقابلة مُكدسة في أعمدة. وأخيرًا دعونا نلخص سريعًا ما تحدثنا
عنه في هذا الفيديو. إذا كانت هذه هي الشبكة العصبية
الخاصة بك، فلقد ذكرنا أن هذا هو ما ينبغي فعله، في حال أردت تنفيذ انتشار أمامي، نموذج تدريب واحد في المرة الواحدة والبدء
من i = 1 حتى m، وتناولنا بعد ذلك كيفية تكديس نماذج التدريب في أعمدة
مثل هذه وتوزيع قيم z1، a1 وz2 وa2 التي تكدست في الأعمدة
المقابلة كما هو واضح أمامكم. هذا نموذج للقيمة A1 "كابيتال" ولكنه صحيح
أيضًا لكل من z1، وa1 وz2 وa2. وما عرضناه على الشريحة السابقة أن هذا السطر يتيح لنا استخدام مُوجه لذلك
على كل نماذج m في وقت واحد. واتضح لنا أنه باتّباع المنطق نفسه، يمكننا إثبات أن كل السطور الأخرى تُعد طُرقًا متجهية صحيحة للسطور
الأربعة الخاصة باللغة البرمجية. أود أن أذكركم لأن x تساوي أيضًا a0، وذلك لأن كما تتذكرون أن متجه ميزة الإدخال x كان يساوي
a0 وبالتالي فإن xi يساوي a0i. وحينها سنلاحظ وجود تناظر بين هذه المعادلات، ويمكن كتابة المعادلة
الأولى هذه على النحو الآتي. z1 تساوي w1 a0 زائدb1 وسنلاحظ أن هاتين المعادلتين وهاتين المعادلتين متشابهتان جدًا
ولكن كل المؤشرات تزيد بواحد فقط. وبالتالي يتضح لنا أن الطبقات المختلفة
من الشبكة العصبية تقوم بالشيء نفسه تقريبًا أو تكرر
العمليات الحسابية نفسها مرارًا وتكرارًا. توجد لدينا هنا شبكة عصبية
مكونة من طبقتين وعندما نتناول شبكات عصبية أكثر تعقيدًا
في الفيديوهات القادمة فستلاحظون أنه حتى الشبكات العصبية
الأكثر تعقيدًا تطبق هاتين الخطوتين وتكررهما مرات أكثر مقارنة
بعدد المرات التي لاحظتموها هنا. كانت هذه هي الطريقة التي يمكنكم اتّباعها لاستخدام
موجه لنماذج التدريب في الشبكة العصبية الخاصة بكم. لقد استخدمنا حتى الآن الدالات السينية
خلال الشبكات العصبية الخاصة بنا. وأتضح أنها ليست الخيار الأمثل. لذلك، دعونا في الفيديو التالي نتعمق قليلاً في كيفية استخدام ما يُطلق عليه "دالات تنشيط" مختلفة، والتي تُعتبر
الدالة السينية إحدى خياراتها المحتملة.