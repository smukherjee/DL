為什麼神經網路需要非線性啟動函數呢？ 實際上,為了使您的神經網路能計算有趣的函數 你確實需要選擇一個非線性
啟動函數,讓我們來看看為什麼。 所以,這裡是神經網路的
四個正向傳播的方程式。 我們為什麼不去掉這個? 去掉函數 g? 將 a1 設置為等於 z1。 或者,你可以說 g(z) 等於 z,對嗎? 有時這被稱為
線性啟動函數。 或者我們稱它為恆等啟動函數. 因為它的輸出會是永遠等於輸入 為此,如果 a2 也正好等於 z2? 事實證明,如果你這樣做,
那麼這個模型只是計算 y 或 y-hat 只作為線性函數,對於輸入 x, 拿前面兩個方程式 如果你有那個a1 =z1=w1x + b1,以及 然後 a2 = z 2 = w2a1 + b2 然後,如果您採取此 a1 定義,和 取代它,你會發現 a2 = w2(w1x + b1) 對嗎？所以這是a1,加 b2,所以 這可以簡化成: (w2w1)x + (w2b1 + b2)。 所以這只是 讓我們稱這項為 w', b' 因此,這僅等於 w' x + b'。 如果您使用線性啟動函數 或者我們稱它為恆等啟動函數 那這神經網路只是將輸入用線性函數輸出 稍後我們將討論深層網路,
神經網路具有許多,許多層, 許多隱藏的層。事實證明 如果使用線性啟動函數
或者 如果您沒有啟動功能,
然後,不管你的神經網路有多少層 它只會計算線性啟動函數 所以您有隱藏層跟沒有一樣 提一些簡要的案例
,實際上,如果你有 線性啟動函數在這裡和
sigmoid 函數在這裡,然後此模型是 沒有比任何沒有隱藏層的
標準羅吉斯迴歸分析更有用。 所以,我不會費心證明這一點,但
如果你想的話,你可以嘗試這樣做。 一個線性隱藏層幾乎是沒有用處的 因為兩個線性函數的組合還是線性函數 所以除非你用一個非線性的作用
在那裡,那麼你不會計算更多 更有趣的函數,即便您用了更深層網路. 只有一個地方您或許會用到線性啟動函數 g(z) = z. 這是如果你正在做機器
學習迴歸分析問題。 如果 y 是實數 舉個例子, 如果您試著預估房價 所以y不是0,1,而是一個真正的
實數,任何地方 - 我不知道 - $0, 是房子的價格,然而
昂貴,對,房子,我猜。 也許房子可能是
數百萬美元,所以 不管如何, 有很多房價的資料在您的資料集 如果 y 用這些實數 也許這種情況下可以用線性啟動函數 您的輸出 y-hat 也是實數從負的無窮大到正的無窮大. 但這些隱藏單元,不應該用線性啟動函數 他們可以使用 ReLU 或 tanh 或
Leaky ReLU 或者其他函數。 只有一個地方您或許會用到線性啟動函數 通常在輸出層。 但除此之外,在隱藏層中使用線性的啟動函數, 除了一些非常特殊的情況
與壓縮有關,我們 要談論,使用線性
啟動功能極為罕見。 當然,如果我們
實際預測房價, 正如你看到在第一周的影片,
因為房價都是非負值的, 也許即使這樣,你還是可以使用
ReLU啟動函數,因此 您的輸出 y-hat 會大於,等於 0 所以,我希望這給你一些感覺
為什麼有非線性啟動 啟動函數在神經網路很重要. 接下來,我們將開始
談論梯度下降和 做到這一點,設置
我們關於梯度下降的討論, 在下一個影片中,我想告訴你如何
估計-如何計算-斜率,或 每種啟動函數的導數. 讓我們進入下一段影片