在这节视频中 我们来复习一下逻辑回归 使用这种学习算法会得到的输出标签 y y在监督学习问题中全是0 或者1 因此这是一种针对二分类问题的算法 给定的输入特征向量 x 和一幅图片对应 我们希望识别这是否是一张猫的图片 因此我们想要一种算法能够输出一个预测值 我们称之为 y帽 (yhat) 这代表对真实标签 Y 的估计 形式上讲 yhat 是当给定输入特征x时 预测标签 y 为1的概率 换种说法就是当x是一张图片 就像我们在上一个视频看到的 你想要yhat告诉你这是一张猫图的概率 你想要yhat告诉你这是一张猫图的概率 因此就像我在之前视频中说的 x是一个n_x维的向量 约定逻辑回归的参数是 w w 也是一个n_x维的向量 另外 参数b是一个实数 因此给定了一个输入x 以及参数 w 和 b 那么如何产生输出 yhat 呢？ 有一种方法可以试试 尽管它不怎么奏效 就是让 yhat 等于 w.T*x+b 这是输入x的一个线性函数输出 事实上 如果使用线性回归 就是这样操作的 但是这对于二分类并不是一个好的算法 因为你希望 yhat 能够输出 y 为1的概率 因此 yhat 的值应该在0和1之间 而这种算法很难实现这个要求 因为w.T*x+b可能会比1大很多或者是一个负数 这对于概率就失去了意义 因此 yhat 值要在0和1之间 所以 让逻辑回归中的输出 yhat  等于对这个值应用sigmoid函数的结果 sigmoid函数是这样的 如果水平轴的标签为z 那么函数sigmoid(z)是这样的 它从0平滑地升高到1 让我给轴标上标签 这是0 然后它会在0.5处和竖直轴交叉 因此这就是sigmoid(z)函数 这里用z表示 (w.T*x+b) 因此这就是sigmoid(z)函数 这里用z表示 (w.T*x+b) 这是sigmoid函数的公式 当z是一个实数 sigmoid(z)就等于 1 / ( 1+e^(-z) ) 注意这几件事 如果z非常大 e^(-z)就会接近0 所以sigmoid(z)就大约等于 1 / (1+ 一个接近0的数字) 因为 e^(很大的数的相反数) 会非常接近0 因此这项会接近1 事实上 如果你看左边的图 如果z非常大 那么sigmoid(z)非常接近1 相反地 如果z非常小 或者他是一个非常大的负值 sigmoid(z)会变成(1+ e^(-z) ) 这项会变成很大的数 而这一项 思考一下 1/(1+ 很大的数) 而这一项 思考一下 1/(1+ 很大的数) sigmoid(z) 就会接近0 因此 当z变成一个很大的负数 sigmoid(z)的值就会非常接近0 因此当你实现逻辑回归时 你的目标是尽力学习到参数w和b 因此yhat就能很好地估计y等于1的概率 在下个知识点之前 还有一点需要强调 当进行神经网络编程时 我们通常会将参数w和参数b分开看待 这里的b对应一个偏置量 在其他的一些课程中 可能看到其他符号 用不同的方式处理参数 在一些课程中 定义了一个额外的特征 x0 并让x0等于1 因此x的维度就变成了 n_x+1 之后 定义yhat等于sigmoid(θ.T *x) 在这种符号约定中 有一个向量参数θ θ0 θ1 θ2 一直到θ_nx θ0取代了参数b 这是一个实数 而从θ1到θ_nx是代替了参数w 而事实上 当实现神经网络时 将b和w当做相互独立的参数会更加简单 因此 在这门课程中 我们不会使用红笔写的这种符号表示 如果你没有在其他课程中看到过这种符号表示 不用担心 这部分只是为了向那些看过这种符号的人明确说明 我们不会在这节课中使用这种符号 但如果你之前没有看过这种符号表示 这儿讲的就不重要了 你没必要担心它 因此 你现在已经知道逻辑回归是什么样子了 接下来为了改变参数w和b 需要定义一个代价函数 我们将在下一节课来讲解⏎GTC字幕组翻译