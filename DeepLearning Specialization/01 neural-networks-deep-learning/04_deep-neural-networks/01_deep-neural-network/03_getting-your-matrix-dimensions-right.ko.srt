1
00:00:00,028 --> 00:00:04,605
심층신경망을 도입하는 경우에 디버깅을 
찾기 위한 도구로, 저는 코딩을 correctness를 확인하기 위해

2
00:00:04,605 --> 00:00:08,118
종이를 한 장 꺼내서 

3
00:00:08,118 --> 00:00:11,727
제가 작업하고 있는 매트릭스의 다이멘션을 공부합니다.

4
00:00:11,727 --> 00:00:15,895
어떻게 하는지 보여드리겠습니다.
이 방법이 

5
00:00:15,895 --> 00:00:18,275
여러분이 심층 네트워크를 도입하는데 도움을 줄 수 있을 것이기 때문이죠. 

6
00:00:18,275 --> 00:00:23,174
대문자 L은 5입니다. 
재빨리 계산하면, 입력값 층은 제외하면, 

7
00:00:23,174 --> 00:00:27,390
여기는 5개의 층이 있는데요, 
4개의 숨겨진 레이어와 1개의 결과값 층입니다. 

8
00:00:27,390 --> 00:00:34,878
전 방향전파을 도입하면, 

9
00:00:34,878 --> 00:00:41,408
첫번째 단계는 z1=w1x + b1입니다.

10
00:00:41,408 --> 00:00:48,144
b라는 bias 항은 일단 무시하겠습니다.
그리고 w 매개변수에 집중하겠습니다.

11
00:00:48,144 --> 00:00:54,501
첫번째 숨겨진 레이어는 3개의 숨겨진 유닛이 있는데요, 
이것은 층 0이구요, 

12
00:00:54,501 --> 00:00:59,517
층 1, 층 2, 층 3, 
층 4, 그리고 층 5입니다.

13
00:00:59,517 --> 00:01:05,741
이전 비디오에서의 표기를 그대로 쓰면, n1, 

14
00:01:05,741 --> 00:01:11,265
n1은 층1에 있는 숨겨진 유닛의 개수입니다. 이 값은 3이죠. 

15
00:01:11,265 --> 00:01:16,202
그리고 n2는 그러면 5가 되구요, 

16
00:01:16,202 --> 00:01:23,018
n3는 4이구요, 
n4는 2, n5는 1이 될 것입니다.

17
00:01:23,018 --> 00:01:27,715
현재까지는 오로지 하나의 결과값 유닛만 있는 신경망에 대해서만
다루었는데요, 나중 코스에서는 

18
00:01:27,715 --> 00:01:32,497
복수의 결과값 유닛을 갖는 신경망 네트워크에 대해서도 
이야기하겠습니다. 

19
00:01:32,497 --> 00:01:36,989
마지막으로, 입력값 층으로, 

20
00:01:36,989 --> 00:01:40,443
n0= nx=2 입니다.

21
00:01:40,443 --> 00:01:45,860
그러면 이제, z, w, 그리고 x의 
다이멘션도 한번 생각해보겠습니다.

22
00:01:45,860 --> 00:01:49,120
z는 첫번째 숨겨진 레이어의 

23
00:01:49,120 --> 00:01:54,244
activation 벡터인데요
그러므로, z는 3 x 1일 것입니다.

24
00:01:54,244 --> 00:01:58,675
즉, 3차원의 벡터일 것입니다.

25
00:01:58,675 --> 00:02:03,093
그럼 저는 이것을 n1, 1차원이 벡터로 표기할 것입니다.

26
00:02:03,093 --> 00:02:08,546
n1 x 1차원이 매트릭스인 것이죠. 
이 경우에는 3 x 1 이 것입니다.

27
00:02:08,546 --> 00:02:12,319
입력 특성 x 는 어떨까요? 
2개의 입력 특성이 있습니다.

28
00:02:12,319 --> 00:02:18,622
x는 이 경우, 2 x 1이고, 
더 일반적으로는 n0 x 1일 것입니다.

29
00:02:18,622 --> 00:02:24,082
w1 매트릭스로 필요한 것은, 

30
00:02:24,082 --> 00:02:30,181
n0 x 1 벡터를 곱할때, 
n1 x 1 벡터가 나오는 값이 필요합니다.

31
00:02:30,181 --> 00:02:34,747
그럼 이렇게 3차원의 벡터는

32
00:02:34,747 --> 00:02:38,600
어떤 거 곱하기 2차원 벡터인 경우를 찾습니다.

33
00:02:38,600 --> 00:02:42,993
그러면 매트릭스의 곱하기 규칙에 따라, 

34
00:02:42,993 --> 00:02:46,041
이 값은 3 x 2 매트릭스여야 하겠죠.

35
00:02:46,041 --> 00:02:51,138
3 x 2 매트릭스 곱하기
2 x 1 매트릭스 또는, 

36
00:02:51,138 --> 00:02:56,249
곱하기 2 x 1 벡터는
3 x 1 벡터를 줍니다.

37
00:02:56,249 --> 00:03:02,771
더 일반적인 경우, n1 x n0 차원이 매트릭스가 
될 것입니다.

38
00:03:02,771 --> 00:03:07,167
여기서 알아낸 것은, 

39
00:03:07,167 --> 00:03:12,665
w1 의 다이메션이 n1 x n0 이여야 한다는 것입니다.

40
00:03:12,665 --> 00:03:20,191
일반적인 경우에는, 
wL의 다이멘션은 nL x nL -1 이여야 한다는 것입니다.

41
00:03:20,191 --> 00:03:26,021
예를 들어, w2 의 다이멘션의 경우

42
00:03:26,021 --> 00:03:31,508
이 경우에는 5 x 3일 것입니다.

43
00:03:31,508 --> 00:03:35,119
또는 n2 x n1입니다.

44
00:03:35,119 --> 00:03:40,036
저희는 z2를 w2 곱하기

45
00:03:40,036 --> 00:03:45,132
a1으로 계산할 것이기 때문에, 

46
00:03:45,132 --> 00:03:50,059
그리고 일당은 bias는 무시하겠습니다.

47
00:03:50,059 --> 00:03:54,584
그러면 이것은 3 x 1 이 될 것이고, 

48
00:03:54,584 --> 00:03:59,432
이것은 5 x 1가 되어야 합니다.

49
00:03:59,432 --> 00:04:03,169
이것은 그러면 5 x 3이 되어야 하구요. 

50
00:04:03,169 --> 00:04:10,273
비슷하게 w3는 

51
00:04:10,273 --> 00:04:15,501
다음 층의 다이멘션 컴마 
다름 층의 다이멘션 인데요 

52
00:04:15,501 --> 00:04:19,266
그러면 이것은 4, 5이고, 

53
00:04:22,055 --> 00:04:27,489
w4는 2, 4, 그리고

54
00:04:27,489 --> 00:04:34,405
w5는 1, 2 가 됩니다. 아시겠죠?

55
00:04:34,405 --> 00:04:38,730
그러므로 확인하실 공식은 

56
00:04:38,730 --> 00:04:43,416
L층에 대해 매트릭스를 도입할 때 말이죠, 

57
00:04:43,416 --> 00:04:48,475
그 매트릭스의 다이멘션은 
nL x nL-1입니다.

58
00:04:48,475 --> 00:04:55,362
다음으로 벡터b의 다이멘션을 생객해보겠습니다.

59
00:04:55,362 --> 00:05:01,017
이것은 3 x 1 벡터일 것입니다.
그러므로

60
00:05:01,017 --> 00:05:06,008
3 x 1 벡터를 더해야 
3 x 1 벡터를 결과값으로 가질 수 있는데요, 

61
00:05:06,008 --> 00:05:11,287
이 경우에는 이 값을 더해야 하는데요, 
이 값은 5 x 1입니다.

62
00:05:11,287 --> 00:05:14,823
5 x 1벡터가 하나 더 있는 것인데요, 

63
00:05:14,823 --> 00:05:19,122
여기 박스 안에 있는 것들의 합이

64
00:05:19,122 --> 00:05:22,767
5 x 1벡터이여야 합니다.

65
00:05:22,767 --> 00:05:30,090
더 일반적인 규칙은, 
왼쪽에서 보면, 

66
00:05:30,090 --> 00:05:35,470
b1은 n1 x 1 이죠, 이것은 3 x 1입니다.

67
00:05:35,470 --> 00:05:41,156
2번째는 이것이 n2 x 1입니다.

68
00:05:41,156 --> 00:05:45,891
그러므로 일반적인 케이스에서는 

69
00:05:45,891 --> 00:05:50,637
bL이 nL x 1 차원인 경우입니다.

70
00:05:50,637 --> 00:05:56,402
이런 공식들이 여러분의 w 매트릭스와

71
00:05:56,402 --> 00:06:02,091
벡터 b에 대해서 올바른 다이멘션 값을
갖는지 더블체크할 때 쓰일 수 있을 것입니다.

72
00:06:02,091 --> 00:06:06,206
그리고 당연하지만, 
후 방향전파을 도입하는 경우, 

73
00:06:06,206 --> 00:06:10,657
dw의 다이멘션은 w 의 다이멘션과 
동일해야 합니다.

74
00:06:10,657 --> 00:06:16,373
즉, dw 과 w의 똑같은 다이멘션 이여 야합니다.

75
00:06:16,373 --> 00:06:22,276
그리고 db는 b와 똑같은 다이멘션 이여야 합니다.

76
00:06:22,276 --> 00:06:28,399
z의 다이멘션도 필수적으로 체크해 야합니다.

77
00:06:28,399 --> 00:06:33,658
x 와 a의 l도 마찬가지입니다.
a(l)은 여기서 많이 이야기 하진 않았습니다만, 중요합니다.

78
00:06:33,658 --> 00:06:39,856
z(l)=g(l) a(l)이기 때문에, applied element wise 경우

79
00:06:39,856 --> 00:06:46,914
그러면 z 와 a 는 이런 네트워크 경우
똑같은 다이멘션 일 것입니다.

80
00:06:46,914 --> 00:06:51,582
여러 복수 샘플에 대해 

81
00:06:51,582 --> 00:06:53,258
벡터화 도입을 보는 경우 어떻게 되는지 살펴보겠습니다.

82
00:06:53,258 --> 00:06:56,092
벡터화 도입을 하는 경우에도, 

83
00:06:56,092 --> 00:07:00,687
당연히, wb, dw 그리고 db의 다이멘션
동일할 것입니다. 

84
00:07:00,687 --> 00:07:04,929
벡터화 도입에서 그러나 

85
00:07:04,929 --> 00:07:09,771
z, a, x의 다이멘션은 약간 변할 것입니다.

86
00:07:09,771 --> 00:07:13,420
이전에는 

87
00:07:13,420 --> 00:07:18,372
z1=w1x+b1가 있었습니다.

88
00:07:18,372 --> 00:07:23,845
여기는 n1 x 1이였는데요, 

89
00:07:23,845 --> 00:07:28,276
이것은 n1 x n0 입니다.

90
00:07:28,276 --> 00:07:35,846
x는 n0 x 1, b는 n1 x 1 입니다.

91
00:07:35,846 --> 00:07:40,979
그러면 벡터화 

92
00:07:40,979 --> 00:07:46,398
도입에는 

93
00:07:46,398 --> 00:07:53,536
z1=w1x + b1 가 될 것입니다.

94
00:07:53,536 --> 00:07:58,023
z1는 개인 샘플들에 대한 

95
00:07:58,023 --> 00:08:03,575
값을 통해 어구요
그러면 z11, z12, 

96
00:08:03,575 --> 00:08:10,207
z1m까지 말이죠. 이렇게 쌓도록 합니다.
그러면 이 값이 z1이 됩니다.

97
00:08:10,207 --> 00:08:15,042
z1의 다이멘션은 n1 x 1대신에

98
00:08:15,042 --> 00:08:20,285
n1 x m이 됩니다.
그리고 m은 트레이닝 세트의 크기입니다.

99
00:08:20,285 --> 00:08:26,140
w1의 다이멘션은 똑같습니다.
그러므로 n1 x n0입니다.

100
00:08:26,140 --> 00:08:29,201
x는 n0 x 1대신에 

101
00:08:29,201 --> 00:08:33,431
모든 트레이닝 예시들이 가로로 쌓인 값입니다.

102
00:08:33,431 --> 00:08:38,565
그러므로 이제 n0 x m 입니다. 

103
00:08:38,565 --> 00:08:43,833
n1 x n0 매트릭스으 ㅣ경우
이 값을 n0 x m 매크릭스로 곱합니다.

104
00:08:43,833 --> 00:08:50,160
예상대로 그러면, 이것들이 같이 
n1 x m 차원의 매트릭스 값을 줍니다.

105
00:08:50,160 --> 00:08:55,030
마지막으로 b1은 똑같이
n1 x 1입니다.

106
00:08:55,030 --> 00:09:01,147
하지만 이 값을 갖고 b에 더하면, 
파이썬 broadcasting에서는 

107
00:09:01,147 --> 00:09:08,218
이것이 n1 x m 매트릭스 에 중복될 것입니다.

108
00:09:08,218 --> 00:09:14,977
이전 슬라이드에서는 wb, dw, db의
다이멘션 관련하여 이야기했는데요, 

109
00:09:14,977 --> 00:09:21,143
여기서 보는 것은 zL과 

110
00:09:21,143 --> 00:09:26,922
aL은 nL x 1 의 다이멘션을 갖습니다.

111
00:09:26,922 --> 00:09:34,650
반면에 ZL 과 AL은 nL x m입니다.

112
00:09:34,650 --> 00:09:40,410
특별한 케이스는 
L이 0인 경우인데요, 

113
00:09:40,410 --> 00:09:45,188
이 경우, A0는 

114
00:09:45,188 --> 00:09:49,543
트레이닝세트 입력 특성 x 와 동일합니다.

115
00:09:49,543 --> 00:09:54,616
이 값은 n0 x m 이 됩니다. 

116
00:09:54,616 --> 00:10:01,259
이것을 후 방향전파에 도입하는 경우, 

117
00:10:01,259 --> 00:10:06,749
나중에 보겠지만, 
dZ오 dA를 계산하게 될텐데요, 

118
00:10:06,749 --> 00:10:11,327
이 값들은 

119
00:10:11,327 --> 00:10:15,736
Z 와 A와 그 다이멘션이 동일할 것입니다.

120
00:10:15,736 --> 00:10:19,467
이 연습이 여러분이 작업하는 매트릭스가 

121
00:10:19,467 --> 00:10:21,685
어떤 다이멘션인지 분명하게 해주도록
도움이 됐으면 합니다.

122
00:10:21,685 --> 00:10:25,947
심층신경망에서 후 방향전파을 도입하는 경우, 

123
00:10:25,947 --> 00:10:30,350
여러분의 코딩을 잘하고 
매트릭스의 다이멘션이 일관되게 유지되면 

124
00:10:30,350 --> 00:10:31,825
보통 도움이 됩니다.

125
00:10:31,825 --> 00:10:35,908
일반적으로 생기는 버그의 원인을 
제거해줄 수 있을 것입니다. 

126
00:10:35,908 --> 00:10:40,325
이렇게 여러 매트릭스의 다이멘션을 
알 수 있는 연습이

127
00:10:40,325 --> 00:10:41,979
도움이 됐기를 바랍니다.

128
00:10:41,979 --> 00:10:44,788
심층신경망을 도입하는 경우에, 

129
00:10:44,788 --> 00:10:48,241
작업하는 여러 매트릭스와 벡터 다이멘션을
일직선으로 유지하면 

130
00:10:48,241 --> 00:10:52,162
버그의 원인을 일부 제거할 수 있을 것입니다.

131
00:10:52,162 --> 00:10:54,467
코딩이 올바르도록 확실히 도와주기는 합니다.

132
00:10:54,467 --> 00:10:58,882
전 방향전파을 신경망에서 하는 방법에 대한 

133
00:10:58,882 --> 00:11:01,227
메카닉을 보았는데요, 

134
00:11:01,227 --> 00:11:04,163
왜 심층신경망이 효과적인 것일까요?

135
00:11:04,163 --> 00:11:07,243
또 shallow representations보다 왜 더 잘 작동하는 것일까요?

136
00:11:07,243 --> 00:11:09,939
다음 비디오에서 이와 관련된 내용을 다루겠습니다.