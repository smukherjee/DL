在之前的影片中,您看到了如何計算導數和建立 梯度下降,在僅只一個例子的羅吉斯迴歸分析。 現在,我們要用 m 訓練例子,執行此操作。 首先,讓我們提醒我們成本函數 J 的定義. J, 成本函數 w,b,你關心的是這個平均值, 是 1/m，總和從 i = 1 到 m 當您在 y 的例子上, 相對輸出值 a_i 時的損失, 其中 a_i 是第 i 個訓練例子的預測, 等於 sigma(z_i), 等於 sigma(w轉置 x_i, 加 b)。 因此,我們在上一張投影片中展示的
是任何單一訓練例子, 當您只有一個訓練例子時,如何計算導數。 所以 dw_1, dw_2 和 d_b, 現在加上上標 i 來表示 你得到的對應值，如同你在上一張幻燈片上所做的那樣 但那只是使用一個訓練例子, x_i y_i,對不起, 錯過了那裡一個 i 。 所以,現在你注意到總成本函數
作為一個總和，其實是平均值, 因為 1/ m ,對於每一個損失項目. 實際上這個導數, 相對於 w_1的總體成本函數,也將是 相對於每一個損失項目的 w_1 導數的平均值。 但之前,我們已經演示如何
計算這個項目為 dw_1_i, 在上一張投影片上, 展示如何在單一訓練例子中計算此項。 所以,你真正需要做的是計算 這些導數,正如我們在前面的
訓練例子中展示的,並將它們平均, 這會給你 總體的梯度,您可以用來實現梯度下降. 我知道那是很多細節, 但讓我們把這一切,並包裝一起到 具體的演算法,直到你能够 實現羅吉斯迴歸分析使用梯度下降。 所以,你可以這樣做:讓我們初始化 J 等於零, dw_1 等於零,dw_2 等於零,d_b 等於零。 我們要做的是在訓練集上,使用 for loop, 計算每個訓練例子的導數,然後將它們相加。 所以,這是我們怎麼做的,對 i 等於 1 到 m, 所以m是訓練例子的數量, 我們計算 z_i 等於 w轉置 x_i 加 b。 預測值 a_i 等於 Sigma(z_i), 然後,讓我們把 J 加起來, J += -[(y_i )log(a_i) + (1-y_i)log(1-a_i)] 然後把負號放在整個的前面, 然後我們先前看到, 我們有 dz_i = a_i - y_i, 和 dw_1 += ( x1_i )( dz_i ), dw_2 += ( x2_i )( dz_i ), 我做這個計算,假設你只有兩個特徵, n = 2, 否則, 你要做 dw_1, dw_2, dw_3 ..等等, 然後 db += dz_i, 這是這個 for loop 的終點 最後,對所有的 m 訓練例子都做了這些操作, 您仍然需要除以 m,因為我們的計算平均值。 所以,dw_1 /= m , dw_2 /= m , db /= m , 以計算平均值。 所以,用所有這些計算, 您計算了成本函數 J 的導數,相對於 每個參數 w_1、w_2 和 b。 關於一些我們正在做的幾點細節, 我們使用 dw_1 和 dw_2 和 db 作為累加數, 這樣,經過這個計算之後, dw_1 等於, 總成本函數的導數對於 w_1,
 dw_2 和 db 同樣的。 請注意,dw_1 和 dw_2 沒有上標 i, 因為我們用他們在程式中當作 累加數,以整個訓練集加起來。 相較之下，dz_i 這 dz 只是相對於單一的訓練例子。 所以,這就是為什麼有一個上標 i 
對應到一個訓練的例子, i 是用來計算的 所以,在完成了所有這些計算後, 在實施梯度下降的一個步驟, 您將更新 w_1, 為: w_1 減 (學習率 乘 dw_1) w_2 更新為: w_2 減 (學習率 乘 dw_2) b 更新為: b 減 (學習率 乘 db) 其中 dw_1、dw_2 和 db 是計算結果。 最後,此處的 J 也將是成本函數的正確值。 所以,投影片上的所有內容都僅只
實現了梯度下降的一個步驟而以, 所以你必須重複這張投影片上的所有內容 多次,以便採取梯度下降的多重步驟。 如果這些細節看起來太複雜了,再次, 現在不要太擔心, 希望所有這些會變得比較清楚,當你 在程式設計作業中實現此內容。 但事實證明,有兩個弱點 在我們這裡計算的做法, 也就是說,以這種方式做羅吉斯迴歸分析, 您需要編寫兩個 for loop。 第一個 for loop 是這個對於迴圈的 m 訓練例子, 第二個 for loop 是此處所有特徵的 for loop。 所以,在這個例子中, 只有兩個特徵, n 等於 2, nx 等於 2, 但如果您有更多的特徵, 你寫在這裡dw_1 dw_2, 您有類似的計算對於 dw_3 等等...到 dw_n。 因此,似乎您需要 for loop 對所有特徵, 對 n 個特徵。 當您建置深度學習演算法時, 你會發現,明顯的使用 for loop 在 您的程式,會使演算法運行效率降低。 所以在深度學習時代, 我們將會移到一個越來越大的資料集, 因此,能夠實現您的演算法,
而無需明顯的使用 for loop 是非常重要,它將
幫助您擴展到更大的資料集。 因此,事實證明,有一組技術稱為向量化 允許您在程式中免除這些明顯的 for loop。 我認為在深度學習時代以前, 那是在深度學習興起之前, 向量化是可有可無, 因此,有時您可以這樣做來
加快程式的速度,但有時不會。 但在深度學習時代,向量化, 以去掉 for loop, 像這樣,像這樣, 變得非常重要, 因為我們在非常大的資料集上,
需要越來越多的訓練, 因此,您確實需要非常有效率的程式 所以,在接下來的幾個影片中, 我們將討論向量化以及如何 實現所有這些,甚至不使用一個 for loop。 所以有了這些,我希望您有點概念 去實現羅吉斯迴歸分析或羅吉斯迴歸分析的梯度下降。 當您實現程式設計練習時,
事情會更加清晰。 但在實際做程式設計練習之前, 讓我們先來談談向量化,
這樣你就可以實現這整件事, 實現一個反覆運算的梯度下降,而無需使用任何 for loop。