Ну що ж. Я думає це буде захоплююче відео. В цьому відео ти побачиш як реалізувати Градієнтний спуск для нашої нейронної мережі (НМ) з одним прихованим шаром. В цьому відео я дам рівняння, які необхідні для реалізації зворотного поширення, щоб змусити Градієнтний спуск працювати. А потім, в наступному відео, я дам більш розширене розуміння чому саме ці рівняння є точними або правильними рівняннями для обчислення градієнтів, які необхідні для нашої НМ. Тож наша НМ з поки єдиним прихованим шаром матиме параметри w[1], b[1], w[2] і b[2]. Просто нагадаю, що якщо ми маємо nₓ або n[0] вхідних ознак і n[1] прихованих вузлів, і n[2] вихідних вузлів (а в нашому випадку поки n[2]=1), то матриця w[1] матиме розмір n[1]*n[0]. b[1] буде вектором розміру n[1], який можна записати як матрицю розміру n[1]*1, яка є вектором-стовпцем. Розмір w[2] буде n[2]*n[1]. І розмір b[2] буде n[2]*1. Повторю, що досі ми бачили лише приклади, де n[2]=1, тобто де був один єдиний прихований вузол. Також ми маємо функцію Витрат для НМ. І поки, мушу нагадати, ми реалізовуємо двійкову класифікацію. Тож в цьому випадку Витрати наших параметрів наступні: середнє - 1/m - від оцієї функції Втрати. Тож L тут є втратою коли НМ передбачає ŷ. Насправді це - a[2] коли емпірична позначка [ground truth label] =y. Тож, реалізуючи двійкову класифікацію, функція Втрати може бути точно такою ж, 
яку ми вже використовували для логістичної регресії (ЛР). Отже, щоб натренувати параметри нашого алгоритму, нам потрібно застосувати Градієнтний спуск. Коли ми тренуємо НМ дуже важливо ініціалізувати параметри випадковим чином, а не рівними 0. Ми побачимо пізніше чому це так. Тож після ініціалізації параметрів кожен цикл Градієнтного спуску обчислюватиме передбачення або, по суті, обчислюватиме ŷ⁽ⁱ⁾ для i від 1 до m. А потім нам потрібно буде обчислити похідні. Тож нам треба буде обчислити dw[1] (а ми вже це бачили), тобто похідну функції Витрат по параметру w[1]. Також потрібно обчислити ще одну змінну, що називається db[1] і що є похідною або нахилом функції Витрат по змінній b[1] і т.д. Аналогічно для інших параметрів w[2] і b[2]. І, нарешті, корекцію Градієнтного спуску, тобто скоригувати w[1] як w[1] мінус навчальний темп α помножений на dw[1] і скоригувати b[1] як b[1] мінус навчальний темп помножений на db[1]. І, аналогічно, для w[2] і b[2]. Інколи я використовую тут :=, а інколи просто =. Обидва позначення підходять. І оце буде однією ітерацією Градієнтного спуску і ми будемо її повторювати певну кількість разів допоки наші параметри почнуть збігатись. Отже, в попередніх відео ми говорили як обчислити передбачення, як обчислити вихідні дані. І побачили також як це зробити у векторизований спосіб. Тож основний момент - це знати як обчислити ці частинні похідні dw[1], db[1] і, звичайно, похідні dw[2] і db[2]. Тож я дам тобі рівняння, які потрібні для обчислення цих похідних і відкладу до наступного відео, що буде необов'язковим, більш глибокий огляд того, як ми прийшли до цих формул. Давай згадаємо ще раз рівняння прямого поширення. Тож ми маємо: Z[1]=w[1]X+b[1], A[1]= функції активації відповідного шару, що застосовується поелементно до Z[1], Z[2]=w[2]A[1]+b[2], і, нарешті (це все векторизовано для всього тренувального набору), A[2]=g[2](Z[2]). І ще раз повторю, якщо ми реалізуємо двійкову класифікацію, то ця функція активації, насправді, має бути сигмоїдою. Тож я просто допишу це. Тож це - пряме поширення або пряме обчислення зліва-направо нашої НМ. Тепер давай обчислимо похідні. Тож це - крок зворотного поширення, який обчислює dZ[2]=A[2] мінус емпіричне y. Просто нагадаю, що все це векторизовано для всіх зразків. Тож матриця Y має розмір 1*m і містить по горизонталі всі наші m зразків. 
[y⁽ⁱ⁾ для всіх зразків] dw[2]= оцьому. Фактично, оці перші три рівняння дуже подібні до Градієнтного спуску логістичної регресії. ...axis=1, keepdims=True... Невеличке уточнення: np.sum - команда Python/NumPy, яка обчислює по певній заданій осі матриці. В нашому випадку - горизонтально. А keepdims перешкоджає Python-у вивести один з тих дивних масивів 1-ого порядку, що мають розмір (n,). Тож, присвоюючи keepdims True, ми пересвідчуємось, що Python виведе db[2] як вектор розміру n*1 (технічно, це буде (n[2], 1)). В цьому випадку це - просто число 1*1, тобто просто число, тож, можливо, це не має значення, проте, пізніше, ми побачимо коли це дійсно має значення. Тож до цього моменту ми робили щось дуже схоже до ЛР. Проте, тепер, коли ми продовжуємо обчислювати зворотне поширення, необхідно обчислити dZ[1]=w[2]ᵀdZ[2]*g[1]'(Z[1]). Тож цей вираз g[1]' є похідною будь-якої функції активації, яку ми використовуємо для прихованого шару. А для вихідного шару я нагадую, що ми реалізуємо двійкову класифікацію з сигмоїдою, тож воно вже враховано в формулі для dZ[2]. Ось це множення є поелементним. Оцей вираз буде матрицею n[1]*m. А оцей вираз - поелементна похідна - також буде матрицею n[1]*m. Тож оце множення є поелементним добутком двох матриць. І, нарешті, dw[1]= оцьому, а db[1]= оцьому: ...np.sum( dz[1], axis=1, keepdims =True. Тож якщо в попередньому рівнянні keepdims, можливо, не грав значної ролі (коли n[2] мало розмір 1*1, тобто було просто дійсним числом), то тут db[1] є вектором n[1]*1, тож, ми хочемо, щоб Python команда np.sum вивела щось такого ж розміру, а не масив першого порядку, що має отой розмір [(n,)] і може зіпсувати подальші обчислення. Інший спосіб не вдаватись до параметрів keepdims - це явно викликати reshape, щоб перебудувати результат np.sum у потрібний розмір, який має мати db. Отже, це були пряме поширення, що має 4 рівняння, і зворотне поширення, що має 6 рівнянь. Я знаю, що просто записав ці рівняння, але в наступному необов'язковому відео давай отримаємо певне розуміння того, як ці 6 рівнянь алгоритму зворотного поширення були виведені. Незалежно від того, подивишся ти відео чи ні, якщо ти реалізуєш ці алгоритми, то матимеш правильну реалізацію прямого і зворотного поширень і зможеш обчислити похідні, що потрібні для застосування Градієнтного спуску, щоб навчати параметри своєї НМ. Можна реалізувати ці алгоритми і заставити їх працювати і без глибокого розуміння диф.числення. Багато успішних практиків глибокого навчання так роблять. Проте, якщо хочеш, то можеш подивитись наступне відео просто щоб отримати краще розуміння виведення цих рівнянь.