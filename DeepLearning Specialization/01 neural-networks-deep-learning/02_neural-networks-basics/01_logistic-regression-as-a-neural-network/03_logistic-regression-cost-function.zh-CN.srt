1
00:00:00,000 --> 00:00:01,530
在之前的视频中

2
00:00:01,530 --> 00:00:04,227
我们学习了逻辑回归模型

3
00:00:04,227 --> 00:00:07,526
为了优化逻辑回归模型的参数W和B

4
00:00:07,526 --> 00:00:10,570
需要定义一个代价函数

5
00:00:10,570 --> 00:00:14,430
现在 就来了解一下可以优化逻辑回归的代价函数

6
00:00:14,430 --> 00:00:18,195
先来回顾一下 我们之前的讲义中用过的这个方程式

7
00:00:18,195 --> 00:00:20,792
将输出值y ̂设为S(W*X^t+b)

8
00:00:20,792 --> 00:00:24,690
将此处的S(z)设定为x+b

9
00:00:24,690 --> 00:00:27,600
所以 为了了解你的模型参数

10
00:00:27,600 --> 00:00:31,200
这里有一组用于优化m参数的示例

11
00:00:31,200 --> 00:00:34,060
你会很自然想找出参数W和B

12
00:00:34,060 --> 00:00:37,781
至少在获得优化集合和输出值时

13
00:00:37,781 --> 00:00:40,225
对于优化集合的假定

14
00:00:40,225 --> 00:00:43,260
我们只提出y ̂(i)将接近

15
00:00:43,260 --> 00:00:47,720
从优化集合中获得的实标y_i

16
00:00:47,720 --> 00:00:52,110
为了更详细地描述上诉方程式

17
00:00:52,110 --> 00:00:56,205
我们已经提及y ̂已经在之前

18
00:00:56,205 --> 00:01:00,930
为优化示例x和其他示例进行了设置

19
00:01:00,930 --> 00:01:03,240
我们使用上标

20
00:01:03,240 --> 00:01:07,710
圆括号标注索引 区分示例

21
00:01:07,710 --> 00:01:12,870
对优化示例(i) 也就是y ̂(i) 的假定

22
00:01:12,870 --> 00:01:18,835
将通过S函数应用到WX'

23
00:01:18,835 --> 00:01:25,905
输入值(i) 优化示例+V Z(i)也可以如此设定

24
00:01:25,905 --> 00:01:30,110
Z(i)等于WX'+b

25
00:01:30,110 --> 00:01:31,350
所以本课中

26
00:01:31,350 --> 00:01:33,966
我们就使用这个标记法则

27
00:01:33,966 --> 00:01:41,605
上标括号指的是数据

28
00:01:41,605 --> 00:01:47,615
X Y Z 以及其他字母与i-th优化示例相关联

29
00:01:47,615 --> 00:01:50,885
也与i-th示例相关

30
00:01:50,885 --> 00:01:54,840
这是括弧里上标i的意义

31
00:01:54,840 --> 00:01:57,630
现在 我们来学习损失函数或

32
00:01:57,630 --> 00:02:01,315
误差函数可以用来检测算法运行情况

33
00:02:01,315 --> 00:02:06,015
如在算法输出时定义损失

34
00:02:06,015 --> 00:02:12,320
y ̂和实标Y有可能是一个或半个平方误差

35
00:02:12,320 --> 00:02:14,975
你可以如此操作

36
00:02:14,975 --> 00:02:17,670
但一般在逻辑回归里不进行此操作

37
00:02:17,670 --> 00:02:21,000
因为当研究参数时

38
00:02:21,000 --> 00:02:25,682
我们讨论的优化问题将会变成非凸问题

39
00:02:25,682 --> 00:02:30,105
所以 优化问题会产生多个局部最优解

40
00:02:30,105 --> 00:02:33,285
梯度下降算法也就无法找到全局最优解

41
00:02:33,285 --> 00:02:35,580
如果你暂时不理解以上知识点

42
00:02:35,580 --> 00:02:38,320
不用担心 接下来的视频中会详细讲解

43
00:02:38,320 --> 00:02:40,990
但有一点请记住

44
00:02:40,990 --> 00:02:44,620
函数L被称为损失函数 需要进行设定

45
00:02:44,620 --> 00:02:51,265
才能在实标为y时对输出值y ̂进行检测

46
00:02:51,265 --> 00:02:54,345
平方误差整体是个合理的选择

47
00:02:54,345 --> 00:02:58,160
除了无法让梯度下降算法良好运行

48
00:02:58,160 --> 00:03:00,500
所以 在逻辑回归中 我们会设定

49
00:03:00,500 --> 00:03:05,695
一个不同的损失函数充当平方误差

50
00:03:05,695 --> 00:03:08,910
这样能产生一个凸象最优问题

51
00:03:08,910 --> 00:03:13,530
将使之后的优化变得更容易

52
00:03:13,530 --> 00:03:17,310
所以 实际要使用的逻辑回归

53
00:03:17,310 --> 00:03:21,795
是我写在屏幕右上方的损失函数

54
00:03:21,795 --> 00:03:31,740
[公式]-ly log (y^)+(1-y)log(1-y^)

55
00:03:31,740 --> 00:03:34,600
[公式]

56
00:03:34,600 --> 00:03:38,785
这里说明下为什么这个损失函数有意义

57
00:03:38,785 --> 00:03:41,285
请记住 如果使用平方误差

58
00:03:41,285 --> 00:03:45,820
平方误差需要尽可能的小

59
00:03:45,820 --> 00:03:48,680
在此逻辑回归损失函数里

60
00:03:48,680 --> 00:03:51,495
我们也要让这个数值尽可能小

61
00:03:51,495 --> 00:03:53,508
为了帮助理解

62
00:03:53,508 --> 00:03:55,260
我们举两个例子

63
00:03:55,260 --> 00:03:56,570
第一个

64
00:03:56,570 --> 00:03:59,430
我们假定Y=1

65
00:03:59,430 --> 00:04:05,415
[公式] L (y^, y)

66
00:04:05,415 --> 00:04:08,735
-log(y ̂)

67
00:04:08,735 --> 00:04:10,770
如果y等于1

68
00:04:10,770 --> 00:04:14,070
第二项1-y等于0

69
00:04:14,070 --> 00:04:19,880
所以如果y=1，-logy ̂就必须尽可能大

70
00:04:19,880 --> 00:04:26,040
因此log y ̂数值必须大

71
00:04:26,040 --> 00:04:32,935
这也就是要求y ̂的数值必须大

72
00:04:32,935 --> 00:04:35,170
但是y ̂是S函数

73
00:04:35,170 --> 00:04:38,440
它无法大于1

74
00:04:38,440 --> 00:04:41,850
这也就是说y等于1时

75
00:04:41,850 --> 00:04:44,050
y ̂值要尽可能的大

76
00:04:44,050 --> 00:04:48,220
但却不能大过1 所以y ̂值要无限接近1

77
00:04:48,220 --> 00:04:50,740
另一个例子是 如果y等于0

78
00:04:50,740 --> 00:04:55,375
损失函数中的第一项也等于0

79
00:04:55,375 --> 00:05:01,290
因为y等于0 第二项设定损失函数

80
00:05:01,290 --> 00:05:07,210
所以损失是-log1-y ̂

81
00:05:07,210 --> 00:05:11,480
所以在学习过程中 你需要把损失函数值变小

82
00:05:11,480 --> 00:05:19,450
这意味着log1-y ̂的值要大

83
00:05:19,450 --> 00:05:22,050
又因为有负号

84
00:05:22,050 --> 00:05:24,660
可以总结出类似的原因

85
00:05:24,660 --> 00:05:30,870
就是损失函数会使y ̂尽可能变小

86
00:05:30,870 --> 00:05:34,320
因为y ̂的数值在0~1之间

87
00:05:34,320 --> 00:05:38,155
如果y等于0

88
00:05:38,155 --> 00:05:43,790
损失函数将作用于参数使y ̂无限趋近于0

89
00:05:43,790 --> 00:05:48,305
目前有很多函数有拉斐拉效应 也就是

90
00:05:48,305 --> 00:05:52,950
如果y等于1 y ̂值要变大 如果y等于0 y ̂值要变小

91
00:05:52,950 --> 00:05:55,150
就像这边绿色的内容

92
00:05:55,150 --> 00:05:59,920
关于这个损失函数 有一段非正式的解释之后

93
00:05:59,920 --> 00:06:03,970
另一段视频将更详细地解释

94
00:06:03,970 --> 00:06:08,500
为什么在逻辑回归中 这个特定函数更适用

95
00:06:08,500 --> 00:06:13,630
最后 这个损失函数被单一的优化示例所定义

96
00:06:13,630 --> 00:06:16,760
它将检测单一优化示例的运行情况

97
00:06:16,760 --> 00:06:21,148
接下来，我要设定代价函数

98
00:06:21,148 --> 00:06:24,690
来检测优化组的整体运行情况

99
00:06:24,690 --> 00:06:28,660
所以 运用于参数W和B的代价函数J

100
00:06:28,660 --> 00:06:33,130
是取m平均值

101
00:06:33,130 --> 00:06:43,270
损失函数的总和运用于优化示例

102
00:06:43,270 --> 00:06:45,435
这里的y ̂

103
00:06:45,435 --> 00:06:49,570
你的逻辑回归算法预计的输出值

104
00:06:49,570 --> 00:06:52,430
用一组特定的W和B参数

105
00:06:52,430 --> 00:06:54,480
所以 展开后

106
00:06:54,480 --> 00:06:58,010
这个等于-1/m

107
00:06:58,010 --> 00:07:03,550
在设定的损失函数中i等于1

108
00:07:03,550 --> 00:07:07,530
所以 这就是y(i)logy ̂

109
00:07:07,530 --> 00:07:14,530
(i)+1 y(i) y ̂(i)

110
00:07:14,530 --> 00:07:17,880
我想我可以在这里添加括号

111
00:07:17,880 --> 00:07:20,945
这样减号可以在最外面

112
00:07:20,945 --> 00:07:23,665
所以我要用的术语是

113
00:07:23,665 --> 00:07:29,120
损失函数适用于像这样单一的优化示例

114
00:07:29,120 --> 00:07:33,010
损失函数反映的是你的参数成本

115
00:07:33,010 --> 00:07:36,115
所以 在优化你的逻辑回归模型时

116
00:07:36,115 --> 00:07:38,980
我们要试着去找参数W和B

117
00:07:38,980 --> 00:07:43,475
以此来缩小J的整体成本

118
00:07:43,475 --> 00:07:48,040
所以 你刚看到的就是设置逻辑回归算法

119
00:07:48,040 --> 00:07:50,770
优化示例的损失函数

120
00:07:50,770 --> 00:07:54,190
以及算法参数的总体损失函数

121
00:07:54,190 --> 00:07:59,485
逻辑回归可被视为一个非常小的神经网络

122
00:07:59,485 --> 00:08:01,905
在下一个视频中我们会提及相关内容

123
00:08:01,905 --> 00:08:04,965
大家对神经网络也就会有初步的认识

124
00:08:04,965 --> 00:08:08,230
接下来请大家观看下一个视频

125
00:08:08,230 --> 00:08:11,630
逻辑回归是一个很小的神经网络
GTC字幕组翻译