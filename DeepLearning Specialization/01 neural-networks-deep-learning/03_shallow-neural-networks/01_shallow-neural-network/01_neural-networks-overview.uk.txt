З поверненням! На цьому тижні ти навчишся реалізувати нейронну мережу (НМ). Перед тим як заглибитись в технічні деталі, я хочу ще в цьому відео швиденько розказати що ти побачиш в цьоготижневих відео. Тож якщо ти не зрозумієш всіх деталей у цьому відео, не переймайся. Ми розглянемо технічні деталі в кількох наступних відео. А поки давай швиденько глянемо як ти реалізуватимеш свою НМ. Минулого тижня ми говорили про логістичну регресію (ЛР) і побачили як цю модель обчислювати на Діаграмі обчислень. Ми вводили ознаки x і параметри w та b. Це дозволяло нам обчислити z. Це потім ми використовували для обчислення a. А a ми підставляли замість вихідного ŷ, а вже потім могли обчислити функцію Втрати L. НМ має отакий вигляд. І, як я вже натякав, ми можемо сформувати НМ складаючи разом багато вузлів-сигмоїд. В той же час цей вузол відповідає двом крокам обчислень: у першому 3 змінні дають нам значення z, у другому обчислюється значення a. В цій НМ оцей стек вузлів відповідатиме і обчисленням z, і і обчисленням a. А оцей вузол відповідатиме іншим z та іншим a обчисленням. Тож, згідно позначень, які ми використовуватимемо, це виглядатиме так: спочатку вводяться ознаки x разом з параметрами w і b і це дозволяє нам обчислити z¹. Тож нове позначення, яке ми використовуватимемо, це верхній індекс 1 в квадратних дужках, що відповідатиме номеру стеку вузлів, що називається шаром. А пізніше ми використовуватимемо верхній індекс 2 в квадратних дужках, щоб позначити номер оцього вузла. А це теж називається шаром НМ. Верхні індекси в квадратних дужках, як ось тут, не потрібно плутати з верхніми індексами в звичайних [круглих] дужках, які ми використовуємо, щоб позначати окремі тренувальні зразки. Тож якщо x з верхнім індексом i в звичайних дужках позначає i-ий тренувальний зразок, то верхні індекси 1 і 2 в квадратних дужках позначає ось ці окремі шари: шар 1 і шар 2 цієї НМ. А далі після обчислення z¹, аналогічно як для ЛР, буде обчислюватись a¹ (а це просто сигмоїда від z¹. А потім обчислюється z² з допомогою лінійного рівняння, а потім обчислюється a², що є кінцевим результатом НМ і буде підставлене замість ŷ. Отже, я знаю, що було багато інформації, проте, головне, що треба запам'ятати, це що для ЛР ми маємо ось це z і після цього обчислюємо a, а для НМ ми робимо те ж саме, просто багато разів: обчислюємо z, потім a, потім знову z і знову a. І, врешті-решт, обчислюємо Втрату. І, як ти пам'ятаєш, для ЛР ми також обчислювали зворотне поширення, щоб обчислити похідні (оці хитромудрі da, dz і т.д.). Аналогічно для НМ ми теж закінчуємо зворотнім поширенням. Ось так. Обчислюємо da², dz². Це дозволяє обчислити dw², db² і т.д. От якось так - справа-наліво. Зворотне поширення іде в напрямку цієї червоної стрілки. Отже, це швидкий огляд того, на чому НМ базуються. Беремо ЛР і повторюємо її двічі. Я знаю, що було багато нових позначень, багато інформації. Не переймайся, що не все запам'ятав/ла. Ми ще зануримось глибше в деталі в кількох наступних відео. Тож давай перейдемо до наступного відео і почнемо обговорювати представлення НМ.