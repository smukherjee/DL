1
00:00:00,870 --> 00:00:04,550
當你構建神經網路時,
一個你要選擇的是,使用什麼

2
00:00:04,550 --> 00:00:07,570
啟動函數在隱藏層中,

3
00:00:07,570 --> 00:00:10,950
以及什麼是
神經網路的輸出單位。

4
00:00:10,950 --> 00:00:14,360
到目前為止,我們只使用
S型函數 sigmoid 啟動函數。

5
00:00:14,360 --> 00:00:17,980
但有時其他選擇
可以作得更好。

6
00:00:17,980 --> 00:00:19,970
讓我們來看看一些其他選項。

7
00:00:19,970 --> 00:00:23,840
在神經網路正向傳播的步驟中,

8
00:00:23,840 --> 00:00:28,279
我們有兩個步驟
我們在這裡使用 sigmoid 函數。

9
00:00:28,279 --> 00:00:32,460
因此,sigmoid 被稱為
啟動函數

10
00:00:32,460 --> 00:00:35,871
這是您熟悉的S型
sigmoid 函數

11
00:00:35,871 --> 00:00:39,980
等於 1 / ( 1 + e^-z )

12
00:00:39,980 --> 00:00:43,910
所以在更一般的情況下,
我們可以有不同的函數,

13
00:00:45,050 --> 00:00:50,100
g(z) ,我寫在這裡,

14
00:00:51,300 --> 00:00:57,300
g 可以是一個非線性函數
但未必是 sigmoid 函數。

15
00:00:57,300 --> 00:01:02,530
例如,sigmoid 函數
在0 和 1 之間,

16
00:01:02,530 --> 00:01:07,040
啟動函數,幾乎總是
比 sigmoid 函數效果更好的

17
00:01:07,040 --> 00:01:12,220
的是 tanh 函數
或者稱為雙曲線正切函數.

18
00:01:12,220 --> 00:01:18,909
所以這是 z ,這是 a,
這是一個 a = tanh(z),

19
00:01:18,909 --> 00:01:24,390
這介於 +1 和 -1 之間。

20
00:01:24,390 --> 00:01:29,745
tanh 函數的公式為 e^z

21
00:01:29,745 --> 00:01:35,920
減去 e^-z ,除以他們的總和。

22
00:01:35,920 --> 00:01:42,829
實際在數學上是
sigmoid 函數的移位版本。

23
00:01:42,829 --> 00:01:47,399
所以,作為一個 sigmoid 函數,就像這樣,
但移位,所以

24
00:01:47,399 --> 00:01:54,180
它現在跨越 (0 , 0 ) 和重新調整
,所以它介於 +1 和 -1 之間。

25
00:01:54,180 --> 00:01:59,406
實際上對於隱藏單元
如果您讓函數 g(z)

26
00:01:59,406 --> 00:02:06,775
等於 tanh(z),

27
00:02:06,775 --> 00:02:11,490
這個幾乎總是比 sigmoid 函數更好

28
00:02:11,490 --> 00:02:16,540
因為在 +1, -1 之間,啟動的平均值

29
00:02:16,540 --> 00:02:20,360
從隱藏層出來時,會更趨近于 0 的平均值

30
00:02:20,360 --> 00:02:24,468
就像有時,當訓練一個學習演算法時

31
00:02:24,468 --> 00:02:26,364
您可能選擇資料中間,

32
00:02:26,364 --> 00:02:31,029
使得資料具有 0 均值,使用一個 tanh ,
而不是一個sigmoid函數。

33
00:02:31,029 --> 00:02:34,500
它有集中您的資料的效果,所以

34
00:02:34,500 --> 00:02:39,420
您的資料的平均值是
接近 0 ,而不是 0.5。

35
00:02:39,420 --> 00:02:43,200
實際上,讓下一層的學習更容易一點。

36
00:02:43,200 --> 00:02:46,900
我們將在第二個課程談到這些
當我們談到優化

37
00:02:46,900 --> 00:02:47,990
演算法。

38
00:02:47,990 --> 00:02:50,480
但一個要點是,我

39
00:02:50,480 --> 00:02:54,500
幾乎不再用S型 Sigmoid 啟動函數.

40
00:02:54,500 --> 00:02:58,410
tanh 函數幾乎總是更優越.

41
00:02:58,410 --> 00:03:04,880
一個例外是在輸出層,因為如果 y 是 0 或 1,

42
00:03:04,880 --> 00:03:10,070
那麼它 y-hat 比較合理,
 是一個數目

43
00:03:10,070 --> 00:03:14,830
介於 0 和 1,而不是 -1 和 +1 之間。

44
00:03:14,830 --> 00:03:19,380
因此,一個例外,
我會用 Sigmoid

45
00:03:19,380 --> 00:03:23,610
啟動函數是當您
正在使用二元分類,

46
00:03:23,610 --> 00:03:28,760
在這種情況下,您可以使用 sigmoid
在輸出層的啟動功能。

47
00:03:28,760 --> 00:03:34,828
因此, 這裡 g(z2) 等於 sigmoid of z2。

48
00:03:34,828 --> 00:03:40,236
所以,你在這個例子中看到的
是在那裡,你可能會有一個 tanh

49
00:03:40,236 --> 00:03:47,102
啟動函數在隱藏層,
和 sigmoid 在輸出層。

50
00:03:47,102 --> 00:03:50,982
因此,啟動函數可以不同,
在不同的層。

51
00:03:50,982 --> 00:03:55,112
有時要表示不同的啟動函數,在

52
00:03:55,112 --> 00:03:59,907
不同的層,我們會使用這些
方括弧上標,

53
00:03:59,907 --> 00:04:05,270
以表示 g[1] 或許與 g[2] 不同。

54
00:04:05,270 --> 00:04:09,360
再次,上標方括弧 1 指的是在這一層,並且

55
00:04:09,360 --> 00:04:12,280
上標方括弧 2
指的是輸出層。

56
00:04:13,830 --> 00:04:17,050
現在,一個缺點對於兩者, sigmoid 函數和

57
00:04:17,050 --> 00:04:21,920
tanh 函數是,如果 z 是
要麼很大,要麼很小,

58
00:04:21,920 --> 00:04:26,380
它的梯度或導數或
此函數的斜率變得非常小。

59
00:04:26,380 --> 00:04:29,550
因此,如果 z 是非常大或 z 非常小,

60
00:04:29,550 --> 00:04:33,740
函數的斜率非常接近 0。

61
00:04:33,740 --> 00:04:35,600
所以這會讓梯度下降法慢下來.

62
00:04:36,630 --> 00:04:39,850
所以另一個很受歡迎的選擇在

63
00:04:39,850 --> 00:04:44,650
機器學習,是什麼稱為'線性整流單元'。

64
00:04:44,650 --> 00:04:47,925
因此, ReLU 函數像如下所示。

65
00:04:50,463 --> 00:04:57,020
公式為 a = max(0,z)。

66
00:04:57,020 --> 00:05:01,755
因此,導數是1,只要 z 是正數。

67
00:05:01,755 --> 00:05:05,785
導數值或者說這個函數的斜率為 0,
當 z 為負時。

68
00:05:05,785 --> 00:05:06,855
如果你使用這個,

69
00:05:06,855 --> 00:05:11,425
技術上,導數在 z 正好為 0 時,沒有很好地定義。

70
00:05:11,425 --> 00:05:13,175
但是,當你建置在電腦裡,

71
00:05:13,175 --> 00:05:17,947
你得到的準確答案是
z 等於 00000000000000。

72
00:05:17,947 --> 00:05:22,697
它非常小,所以你不需要去擔心它。

73
00:05:22,697 --> 00:05:26,729
你可以假裝導數,當 z 等於 0 時,

74
00:05:26,729 --> 00:05:31,775
你可以假裝它是1或0,
那你的程式沒問題的.

75
00:05:31,775 --> 00:05:35,640
因此,事實上,它是不能微分的,所以

76
00:05:35,640 --> 00:05:39,670
這裡有一些準則來選擇啟動函數。

77
00:05:39,670 --> 00:05:44,940
如果輸出為 0, 1 值,
如果您使用的是二元分類,

78
00:05:44,940 --> 00:05:49,390
sigmoid 啟動函數是一個
在輸出層非常自然的選擇。

79
00:05:49,390 --> 00:05:54,272
然後,對於所有其他單位, ReLU,

80
00:05:54,272 --> 00:05:58,374
線性整流函數 (Rectified Linear Unit)

81
00:06:02,348 --> 00:06:06,689
日益成為公認的啟動函數選擇。

82
00:06:06,689 --> 00:06:11,000
所以,如果你不確定用什麼
在你的隱藏層

83
00:06:11,000 --> 00:06:14,681
我會直接使用 ReLU 啟動函數.

84
00:06:14,681 --> 00:06:16,820
這是你看到大多數的人使用的。

85
00:06:16,820 --> 00:06:21,460
雖然有時人們也使用 tanh 啟動函數。

86
00:06:21,460 --> 00:06:25,852
ReLU 的一個缺點是
導數等於零,

87
00:06:25,852 --> 00:06:27,152
當 z 為負數時。

88
00:06:27,152 --> 00:06:29,136
在實務上,這沒問題的。

89
00:06:29,136 --> 00:06:33,323
還有另一個版本的 ReLU, 稱為Leaky ReLU。

90
00:06:33,323 --> 00:06:35,174
會在下一張投影片, 給您公式.

91
00:06:35,174 --> 00:06:38,581
但是,當 z 為負時它不是 0,

92
00:06:38,581 --> 00:06:43,890
它只是有一個輕微的斜坡,
因此,這被稱為 leaky ReLU。

93
00:06:45,870 --> 00:06:49,850
它通常比 ReLU 啟動函數好,

94
00:06:49,850 --> 00:06:53,790
在實作上比較少用到.

95
00:06:53,790 --> 00:06:55,130
任何一個都是好的,雖然,

96
00:06:55,130 --> 00:06:59,140
如果你必須挑一個,
我通常只使用 ReLU。

97
00:06:59,140 --> 00:07:03,255
ReLU 和 leaky ReLU 的優勢是,為

98
00:07:03,255 --> 00:07:07,765
在 Z 的很多領域空間,
啟動函數的導數,

99
00:07:07,765 --> 00:07:12,061
啟動函數的斜率
非常不同於 0。

100
00:07:12,061 --> 00:07:15,472
所以在實踐中,
使用 ReLU 啟動函數,

101
00:07:15,472 --> 00:07:19,855
你的神經網路經常學習的快多了,
比使用 tanh 或

102
00:07:19,855 --> 00:07:21,959
sigmoid 啟動函數。

103
00:07:21,959 --> 00:07:26,148
主要原因是
少了這些效果,當函數的斜率

104
00:07:26,148 --> 00:07:30,015
逼近 0,這減慢了學習速度。

105
00:07:30,015 --> 00:07:34,425
我知道,一半 z 的範圍
,ReLU 的斜率為 0,

106
00:07:34,425 --> 00:07:39,585
但在實踐中,足夠的隱藏單位
具有 z 大於 0。

107
00:07:39,585 --> 00:07:43,040
因此,大多數學習示例
學習仍然可以相當快。

108
00:07:43,040 --> 00:07:47,490
因此,讓我們快速回顧一下
不同啟動函數的優點和缺點。

109
00:07:47,490 --> 00:07:49,160
這是 S型 Sigmoid 啟動函數

110
00:07:49,160 --> 00:07:53,360
我會說永遠不要用這個
除了輸出層,

111
00:07:53,360 --> 00:07:56,740
如果您正在做二元分類,
或者幾乎從不用這個.

112
00:07:57,950 --> 00:08:01,860
我幾乎從不使用這是因為, tanh 是

113
00:08:01,860 --> 00:08:04,330
非常嚴整優越

114
00:08:04,330 --> 00:08:06,532
tanh 啟動函數像這樣

115
00:08:11,012 --> 00:08:12,476
然後是公認用的,

116
00:08:12,476 --> 00:08:17,190
最常用的啟動函數是 ReLU,是這樣。

117
00:08:18,400 --> 00:08:22,790
所以,如果你不確定還有什麼用的,
使用這個,並

118
00:08:22,790 --> 00:08:27,668
高興的話也可以嘗試 leaky ReLU.

119
00:08:27,668 --> 00:08:34,540
可能是 (0.01 z, z)。

120
00:08:34,540 --> 00:08:39,140
對？因此, a = max(0.01z, z)

121
00:08:39,140 --> 00:08:43,200
所以給你這些函數中的一些彎曲。

122
00:08:43,200 --> 00:08:48,210
你可能會說為什麼是常數 0.01?

123
00:08:48,210 --> 00:08:53,366
你也可以讓它成為另一個學習演算法的參數

124
00:08:53,366 --> 00:08:55,320
有些人說這樣做會更好,

125
00:08:55,320 --> 00:08:56,790
但我很少看到人們這樣做。

126
00:08:58,380 --> 00:09:01,910
但是,如果你想嘗試在你的
應用程式,請隨時嘗試。

127
00:09:01,910 --> 00:09:05,460
你可以看到它是如何做的,
以及它是如何做的好,

128
00:09:05,460 --> 00:09:07,980
堅持它,如果它
給你一個好的結果。

129
00:09:07,980 --> 00:09:11,828
我希望這給你一種感覺,一些
啟動函數的選擇,你

130
00:09:11,828 --> 00:09:13,389
可在神經網路中使用。

131
00:09:13,389 --> 00:09:17,276
我們會看到在深度學習的話題是
你經常有很多

132
00:09:17,276 --> 00:09:20,299
不同的選擇,在你如何
為您的神經網路編寫程式

133
00:09:20,299 --> 00:09:24,024
從隱藏單位的數量,
到選擇啟動函數,

134
00:09:24,024 --> 00:09:26,752
如何初始化的方式,
我們稍後會看到。

135
00:09:26,752 --> 00:09:28,310
很多這樣的選擇。

136
00:09:28,310 --> 00:09:32,630
事實證明,它有時
很難得到良好的指南,

137
00:09:32,630 --> 00:09:35,280
究竟什麼最適合你的問題。

138
00:09:35,280 --> 00:09:38,410
因此,在這些課程,我
繼續給你一些感覺有關

139
00:09:38,410 --> 00:09:41,890
我在行業中看到的
什麼或多或少比較流行的。

140
00:09:41,890 --> 00:09:45,215
但對於您的應用程式,
與應用程式的特點,

141
00:09:45,215 --> 00:09:49,330
其實很難事先確切知道,
什麼將會最好。

142
00:09:49,330 --> 00:09:52,480
所以一條常見的建議是
如果你不確定這些

143
00:09:52,480 --> 00:09:57,220
啟動函數工作最佳,
嘗試所有的,並評估在

144
00:09:57,220 --> 00:10:02,220
保留驗證集,或開發集,
我們將稍後討論,

145
00:10:02,220 --> 00:10:05,780
哪一種效果最好,然後就用它.

146
00:10:05,780 --> 00:10:10,303
我認為通過測試這些
不同的選擇在您的應用,

147
00:10:10,303 --> 00:10:15,189
你會更好地在面向未來,你的
神經網路架構

148
00:10:15,189 --> 00:10:20,030
你問題的特質,
以及演算法的演變。

149
00:10:20,030 --> 00:10:24,694
而不是如果我告訴你
始終使用 ReLU 啟動和

150
00:10:24,694 --> 00:10:26,720
別用別的了.

151
00:10:26,720 --> 00:10:30,877
那只是可能或可能不適用,
無論你最終解決什麼問題

152
00:10:30,877 --> 00:10:33,925
在不久的將來,或
在遙遠的未來。

153
00:10:33,925 --> 00:10:37,420
好吧,這是有關於
啟動函數的選擇,和

154
00:10:37,420 --> 00:10:39,890
你見過的一些最流行的啟動函數.

155
00:10:39,890 --> 00:10:43,120
還有一個問題
有時候你可以問,

156
00:10:43,120 --> 00:10:46,490
這就是為什麼你甚至需要
使用啟動函數?

157
00:10:46,490 --> 00:10:48,260
為什麼不使用呢?

158
00:10:48,260 --> 00:10:51,510
因此,讓我們來在下一個影片中談談,

159
00:10:51,510 --> 00:10:56,230
為什麼神經網路需要非線性啟動函數.