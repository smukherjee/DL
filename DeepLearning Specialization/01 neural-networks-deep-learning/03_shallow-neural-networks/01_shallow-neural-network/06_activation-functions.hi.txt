जब आप बनाते हैं आपका न्यूरल नेटवर्क, 
एक विकल्प जो आपके पास है कि क्या ऐक्टिवेशन फ़ंक्शन इस्तेमाल में लेना चाहिए हिडन लेयर्स के लिए, और क्या हैं आउट्पुट यूनिट्स आपके न्यूरल नेटवर्क में. अभी तक, हम सिर्फ़ इस्तेमाल करते रहे हैं 
सिग्मोईड ऐक्टिवेशन फ़ंक्शन. लेकिन कभी-कभी अन्य विकल्प बेहतर काम कर सकते हैं. चलिए देखते हैं कुछ अन्य विकल्प. फ़ॉर्वर्ड प्रॉपगेशन सटेप्स में न्यूरल नेटवर्क के, हमारे पास हैं ये तीन स्टेप्स जहाँ हम लेते हैं 
सिग्मोईड फ़ंक्शन यहाँ. तो उस सिग्मोईड फ़ंक्शन को कहते हैं एक ऐक्टिवेशन फ़ंक्शन. और यहाँ है परिचित सिग्मोईड फ़ंक्शन, a बराबर है एक बटा एक जमा e की पावर नेगेटिव z. अधिक सामान्य केस में हमारे पास 
हो सकता है एक भिन्न फ़ंक्शन, g ऑफ़ z, जो मैं लिखूँगा यहाँ, जहाँ g हो सकता है एक नॉन-लिनीअर फ़ंक्शन 
जो शायद न हो सिग्मोईड फ़ंक्शन. तो उदाहरण के लिए, सिग्मोईड फ़ंक्शन होता है 
ज़ीरो और एक के बीच, और ऐक्टिवेशन फ़ंक्शन जो लगभग हमेशा बेहतर 
काम करता हैं तुलना में सिग्मोईड फ़ंक्शन के है tanh फ़ंक्शन या हायपर्बालिक टैंजेंट फ़ंक्शन. तो यह है z, यह है a, यह है a बराबर tanh(z), और यह होता है प्लस 1 और माइनस 1 के बीच. tanh फ़ंक्शन का फ़ॉर्म्युला है e की पावर z माइनस e की पावर नेगेटिव z बटा उनका योग. और यह है वास्तव में गणितीय रूप में एक 
स्थानांतरित वर्ज़न सिग्मोईड फ़ंक्शन का. तो, एक सिग्मोईड फ़ंक्शन जैसे लेकिन स्थानांतरित ताकि यह अब क्रॉस करता है ज़ीरो,ज़ीरो पोईंट और 
री-स्केल होता है, तो जाता है माइनस 1 से प्लस 1 तक. और ऐसा होता है कि हिडन यूनिट्स के लिए, 
यदि आप रखते हैं फ़ंक्शन g ऑफ़ ज बराबर tanh(z) के, यह हमेशा बेहतर 
काम करता है तुलना में सिग्मोईड फ़ंक्शन के क्योंकि माइनस 1 और प्लस 1 के बीच की वैल्यूज़, 
औसत ऐक्टिवेशन्स का जो आता है आपके नेट से, और वे है क़रीब 0 औसत के. और इसलिए कभी-कभी जब आप
 ट्रेन करते हैं एक लर्निंग अल्गोरिद्म, आप डेटा को केंद्रित कर सकते हैं और कर सकते हैं आपका डेटा 0 औसत के क़रीब इस्तेमाल
 करके एक tanh बजाय सिग्मोईड फ़ंक्शन के. इसका एक तरह से प्रभाव है केंद्रित करना आपका डेटा ताकि आपके डेटा का औसत है क़रीब 0 के बजाय शायद 0.5 के. और यह वास्तव में बनाता है लर्निंग 
अगली लेयर के लिए थोड़ा आसान. हम बात करेंगे थोड़ा और इस बारे में दूसरे कोर्स में 
जब हम बात करेंगे ऑप्टिमायज़ेशन अल्गोरिद्म्स की भी. लेकिन एक चीज़ समझने की है कि मैं लगभग कभी भी इस्तेमाल नहीं करता 
सिग्मोईड ऐक्टिवेशन फ़ंक्शन अब. tanh फ़ंक्शन है लगभग हमेशा बेहतर. एक अपवाद है आउट्पुट लेयर के लिए, 
क्योंकि यदि y है 0 या 1 तो बेहतर होगा y हैट के होना एक नम्बर, वह जिसकी आउट्पुट है 0 और 1 के बीच 
बजाय माइनस 1 और 1 के बीच. तो एक अपवाद जहाँ मैं इस्तेमाल करूँगा सिग्मोईड ऐक्टिवेशन फ़ंक्शन है जब आप कर रहे हैं बाइनेरी क्लासिफ़िकेशन, जिस केस में आप शायद लें सिग्मोईड
 ऐक्टिवेशन फ़ंक्शन आउट्पुट लेयर के लिए. तो g ऑफ़ z यहाँ है बराबर सिग्मोईड z 2. और इसलिए जो आप देखते हैं इस उदाहरण में है 
जहां आपके पास है शायद एक tanh ऐक्टिवेशन फ़ंक्शन हिडन लेयर के लिए, 
और सिग्मोईड आउट्पुट लेयर के लिए. तो ऐक्टिवेशन फ़ंक्शन्स भिन्न हो सकते हैं भिन्न लेयर्स के लिए. और कभी-कभी बताने के लिए कि ऐक्टिवेशन फ़ंक्शंज़ हैं भिन्न भिन्न लेयर्स के लिए, हम शायद इस्तेमाल करें ये
 वर्ग कोष्ठक सूपर्स्क्रिप्ट भी इंगित करने के लिए कि g वर्ग कोष्ठक एक 
हो सकता है भिन्न g वर्ग कोष्ठक दो से. और फिर से, वर्ग कोष्ठक एक सूपरस्क्रिप्ट 
संदर्भित करता है इस लेयर को और सुपरस्क्रिप्ट वर्ग कोष्ठक दो संदर्भित करता है आउट्पुट लेयर को. अब एक नकारात्मक पहलू दोनो सिग्मोईड फ़ंक्शन और tanh फ़ंक्शन का है कि यदि z है या बहुत बड़ा या बहुत छोटा, तब ग्रेडीयंट या डेरिवेटिव या स्लोप 
इस फ़ंक्शन की हो जाती है बहुत कम. तो यदि z है बहुत बड़ा या z है बहुत छोटा, इस फ़ंक्शन की स्लोप हो जाती लगभग 0. और इसलिए यह धीमा कर सकता है ग्रेडीयंट डिसेंट. तो एक अन्य विकल्प जो कि बहुत लोकप्रिय है मशीन लर्निंग में है जिसे कहते हैं रेक्टिफ़ाई लिनीअर यूनिट. तब वैल्यू फ़ंक्शन दिखता है ऐसा. और फार्मूला है a = मैक्स (0, z). डेरिवेटिव है 1 जब तक z पॉज़िटिव है. डेरिवेटिव का या स्लोप है 0 जब z नेगेटिव है. यदि आप इम्प्लमेंट कर रहे हैं इसे, तकनीकी रूप से डेरिवेटिव जब Z है पूर्णत: 0, 
सही ढंग से परिभाषित नहीं है लेकिन जब आप इम्प्लमेंट करते हैं इसे कम्प्यूटर में, उत्तर जो आपको पूरा मिलता है वह है z बराबर है 0000000000000. यह बहुत कम है और इसलिए अभ्यास
 में इसकी चिंता करने की जरूरत नहीं है. आप सोच सकते हैं डेरिवेटिव जब z है बराबर 0 के, आप ले सकते हैं इसे या तो 1 या 0 और 
तब एक तरह से आपका काम चल जाता है. तो तथ्य कि इसे ड़िफ़्फ़ेरेंशिएट नहीं कर सकते और तथ्य कि, तो यहाँ हैं कुछ अनुभवसिद्ध नियम 
चुनने के लिए ऐक्टिवेशन फ़ंक्शन्सज़. यदि आपकी आउट्पुट है 0,1 वैल्यू, 
यदि आप कर रहे हैं बाइनेरी क्लासिफ़िकेशन, तब सिग्मोईड ऐक्टिवेशन फ़ंक्शन है 
स्वभाविक विकल्प आउट्पुट लेयर के लिए. और फ़िर अन्य सभी यूनिट्स के लिए ReLU, या रेक्टिफ़ायड लिनीअर यूनिट, है अधिकांश डिफ़ॉल्ट विकल्प ऐक्टिवेशन फ़ंक्शन का. तो यदि आप सुनिश्चित नहीं है 
क्या इस्तेमाल करना है हिडन लेयर के लिए, मैं बस इस्तेमाल करूँगा ReLU ऐक्टिवेशन यह आप ज़्यादातर लोगों को करते हुए देखते हैं आजकल. हालाँकि कभी-कभी लोग इस्तेमाल करते हैं 
tanh ऐक्टिवेशन भी. एक नुक़सान ReLU का है कि डेरिवेटिव है बराबर ज़ीरो के, जब z है नेगेटिव. अधिकतर, यह सही काम करता है. लेकिन एक और वर्ज़न है ReLU का जिसे कहते 
हैं लीकी ReLU. मैं आपको दूँगा फ़ॉर्म्युला अगली स्लाइड पर. लेकिन बजाय इसके होने के ज़ीरो जब z है नेगेटिव, यह लेता है थोड़ा स्लोप ऐसे, तो इसे कहते हैं लीकी ReLU. यह अक्सर बेहतर काम करता है ReLU ऐक्टिवेशन फ़ंक्शन से, हालाँकि उतना इस्तेमाल नहीं होता अभ्यास में. दोनो में से कोई भी ठीक होना चाहिए, हालाँकि. यदि आपको लेना होता एक, मैं अक्सर लेता हूँ ReLU. लाभ दोनो ReLU और लीकी ReLU का है कि z की बहुत सी स्पेस में, डेरिवेटिव ऐक्टिवेशन फ़ंक्शन का, स्लोप ऐक्टिवेशन फ़ंक्शन की है बहुत भिन्न 0 से. तो अभ्यास में, इस्तेमाल करना ReLU ऐक्टिवेशन फ़ंक्शन, आपका न्यूरल नेटवर्क अक्सर लर्न करेगा बहुत जल्दी तुलना में 
इस्तेमाल करने से tanh या सिग्मोईड ऐक्टिवेशन फ़ंक्शन. और मुख्य कारण है कि वहाँ इसका प्रभाव कम है स्लोप का फ़ंक्शन के ज़ीरो होने पर जिससे धीरे हो जाती है लर्निंग. और मैं जानता हूँ कि रेंज के लिए z की, स्लोप ReLU की है 0, लेकिन व्यावहारिक रूप में, 
बहुत सी हिडन यूनिट्स में होगा z बड़ा 0 से. तो लर्निंग अभी भी हो सकती है तेज़ी से 
अधिकांश ट्रेनिंग इग्ज़ाम्पल्ज़ के लिए. तो चलिए जल्दी से दोहराते हैं लाभ और हानियाँ 
विभिन्न ऐक्टिवेशन फ़ंक्शंज़ की. यहाँ है सिग्मोईड ऐक्टिवेशन फ़ंक्शन. मैं कहूँगा कभी इस्तेमाल न करें इसे, सिवाय आउट्पुट लेयर के. यदि आप कर रहे हैं बाइनेरी वर्गीकरण, 
या शायद लगभग कभी न इस्तेमाल करें इसे. और कारण कि मैं यह लगभग कभी इस्तेमाल 
नहीं करता इसका क्योंकि tanh है बहुत अधिक बेहतर. तो tanh ऐक्टिवेशन फ़ंक्शन है यह. और फिर डिफ़ॉल्ट, सबसे अधिक इस्तेमाल किया जाने वाला 
ऐक्टिवेशन फ़ंक्शन है ReLU, जो है यह. तो यदि आप सुनिश्चित नहीं है 
क्या इस्तेमाल करना है इसे करें, और और शायद बेझिझक इस्तेमाल करें leaky ReLU. जहाँ यह शायद है (0.01 z, z). ठीक है? तो a है मैक्स ऑफ़ 0.01 गुणा z और z का, तो वह देता है आपको ये मोड़ फ़ंक्शन में. और आप शायद कहें, क्यों वह कॉन्स्टंट 0.01? ख़ैर, आप बना सकते है उसे एक 
अन्य पेरामिटर लर्निंग अल्गोरिद्म का. कुछ लोगों का कहना है कि वह काम करता है 
और भी बेहतर लेकिन मैं शायद ही लोगों को देखता हूँ 
ऐसा करते हुए. लेकिन यदि आपका परीक्षण करने का मन है उसे 
आपकी ऐप्लिकेशन में, बेझिझक करें वैसा. और आप देख सकते हैं यह कैसे काम करता है,
 कितना बेहतर करता है, और रखें इसे यदि यह देता है आपको एक बढ़िया परिणाम. तो मुझे आशा है कि यह आपको देता है एक ज्ञान 
 विकल्पों का ऐक्टिवेशन फ़ंक्शंज़ के आप इस्तेमाल कर सकते हैं आपके न्यूरल नेटवर्क में. एक विषय जो हम देखेंगे डीप लर्निंग में है कि 
आपके पास अक्सर होते हैं बहुत से विभिन्न विकल्प कि कैसे आप कोड करते हैं 
आपका न्यूरल नेटवर्क. हिडन यूनिट्स से लेकर, ऐक्टिवेशन फ़ंक्शन के विकल्प, कैसे आप प्रारम्भिक वैल्यूज़ देते हैं लेयर्स को 
जो हम देखेंगे बाद में. बहुत से विकल्प उस तरह के. और ऐसा होता है कि कभी-कभी कठिन होता है 
मिल पाना अच्छे दिशा निर्देश कि क्या करेगा श्रेष्ठतम कार्य आपकी समस्या के लिए. तो पूरे इस कोर्स में में देता रहूँगा आपको एक समझ जो मैं देखता हूँ इंडस्ट्री में कि क्या कम या अधिक लोकप्रिय है. लेकिन आपकी ऐप्लिकेशन के लिए, 
आपकी ऐप्लिकेशन की बारीकियों के साथ, यह वास्तव में बहुत कठिन है जान पाना 
पहले से कि क्या श्रेष्ठतम रहेगा. तो एक सामान्य सलाह होगी कि यदि आप 
सुनिश्चित नहीं है कौन सा इन activation functions करेगा श्रेष्ठतम कार्य,
सबको ट्राइ करो, और मूल्यांकन करो एक होल्डआउट वैलिडेशन सेट पर, या एक 
डिवेलप्मेंट सेट पर, जिसकी हम बाद में बात करेंगे, और कौन सा काम करता है बेहतर और फिर चुने उसे. और मुझे लगता हैं परीक्षण करने से भिन्न 
विकल्प आपकी ऐप्लिकेशन के लिए आपको भविष्य अशुद्धि जांच में बेहतर होगा 
आपके न्यूरल नेटवर्क आर्किटेक्चर को आपकी प्रॉब्लम की बारीकियों में, 
तथा अल्गोरिद्म्स के विकास में. बजाय इसके कि मैं आपको बोलूँ कि 
हमेशा इस्तेमाल करें ReLU ऐक्टिवेशन और न इस्तेमाल करें कुछ भी और. वह शायद हो या न हो अप्लाई आपकी समस्या पर
 जिस पर आप काम करें या निकट भविष्य में या आने वाले भविष्य में. ठीक है, तो वे थे विकल्प ऐक्टिवेशन फ़ंक्शन्स के और आपने देखे सबसे लोकप्रिय ऐक्टिवेशन फ़ंक्शन्स. वहां एक अन्य सवाल है जो 
कभी-कभी आप पूछ सकते हैं, जो है, क्यों आपको इस्तेमाल करना ही है 
एक ऐक्टिवेशन फ़ंक्शन? क्यों उसे छोड़ नहीं सकते? तो चलो बात करते हैं उस बारे में अगले वीडियो में, जहाँ आप देखेंगे कि क्यों न्यूरल नेटवर्क्स को चाहिए 
किसी तरह का नॉन-लीनियर ऐक्टिवेशन फ़ंक्शन.