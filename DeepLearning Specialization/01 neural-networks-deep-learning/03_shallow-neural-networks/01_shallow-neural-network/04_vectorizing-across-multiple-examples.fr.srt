1
00:00:00,360 --> 00:00:04,530
Dans la dernière vidéo, vous avez vu comment 
calculer la prédiction d'un réseau de neurones,

2
00:00:04,530 --> 00:00:06,610
à partir d'un seul exemple d'apprentissage.

3
00:00:06,610 --> 00:00:11,520
Dans cette vidéo, vous allez apprendre à vectoriser 
sur plusieurs exemples d'apprentissage.

4
00:00:11,520 --> 00:00:15,350
Et le résultat sera assez semblable à ce que 
vous avez vu pour la régression logistique,

5
00:00:15,350 --> 00:00:19,050
en empilant les différents exemples d'apprentissage
 dans les différentes colonnes de

6
00:00:19,050 --> 00:00:23,630
la matrice, vous pourrez prendre les équations 
que vous aviez de la vidéo précédente

7
00:00:23,630 --> 00:00:27,860
et avec très peu de changements,
 les modifier pour que le réseau de neurones calcule

8
00:00:27,860 --> 00:00:32,340
les sorties sur tous les exemples
 à peu près tous en même temps.

9
00:00:32,340 --> 00:00:35,080
Nous allons donc voir les détails de comment faire cela.

10
00:00:35,080 --> 00:00:40,192
Voici les quatre équations nous avions 
dans la vidéo précédente pour calculer z1,

11
00:00:40,192 --> 00:00:41,348
a1, z2 et a2.

12
00:00:41,348 --> 00:00:46,867
Et elles vous disent comment, 
à partir d'un vecteur de caractéristiques x,

13
00:00:46,867 --> 00:00:53,810
vous pouvez les utiliser pour générer a2 = ŷ 
pour un seul exemple d'apprentissage.

14
00:00:54,920 --> 00:01:00,050
Maintenant, si vous avez m exemples d'apprentissage,
 vous devez répéter cette procédure pour

15
00:01:00,050 --> 00:01:01,870
disons, le premier exemple d'apprentissage

16
00:01:01,870 --> 00:01:06,600
x exposant parenthèses 1, pour calculer

17
00:01:06,600 --> 00:01:11,062
ŷ (1) (y-chapeau(1)), c'est la prédiction sur
 votre premier exemple d'apprentissage.

18
00:01:11,062 --> 00:01:16,537
Puis x (2) qu'on utilise pour générer
 la prédiction ŷ (2).

19
00:01:16,537 --> 00:01:23,050
Et ainsi de suite jusqu'à x(m) pour
 générer une prédiction ŷ (m).

20
00:01:23,050 --> 00:01:28,349
Pour pouvoir écrire ces fonctions d'activation en une seule,

21
00:01:28,349 --> 00:01:31,669
Je vais écrire ceci a[2](1).

22
00:01:31,669 --> 00:01:36,676
Voici a[2](2),

23
00:01:36,676 --> 00:01:40,640
et a[2](m).

24
00:01:40,640 --> 00:01:46,830
Dans cette notation a[2](i),

25
00:01:46,830 --> 00:01:52,520
l'exposant entre parenthèses, i,
 se réfère à l'exemple d'apprentissage i,

26
00:01:52,520 --> 00:01:57,220
l'exposant entre crochets, 2,
 se réfère à la couche 2.

27
00:01:58,530 --> 00:02:02,460
Voilà donc ce que signifient les indices 
entre crochets et entre parenthèses.

28
00:02:04,170 --> 00:02:07,920
Et donc si vous avez
 une implémentation non vectorisée et

29
00:02:07,920 --> 00:02:11,000
que vous voulez calculer les prédictions 
de tous vos exemples d'apprentissage,

30
00:02:11,000 --> 00:02:15,630
vous devez faire 
pour i = 1 à m,

31
00:02:15,630 --> 00:02:18,260
puis, basiquement, implémenter 
ces quatre équations, n'est ce pas ?

32
00:02:18,260 --> 00:02:24,162
Vous devez faire z[1](i)

33
00:02:24,162 --> 00:02:30,064
z[1](i) = W[1] x(i) + b [1],

34
00:02:30,064 --> 00:02:38,253
a[1](i) = sigmoïde de z[1](i).

35
00:02:38,253 --> 00:02:43,683
z[2](i) = W[2] a[1](i) + b [2],

36
00:02:43,683 --> 00:02:50,099
z[2](i) = W[2] a[1](i) + b [2],

37
00:02:50,099 --> 00:02:56,686
et a[2](i) = sigmoïde de z[2](i).

38
00:02:56,686 --> 00:03:03,172
C’est donc essentiellement ces quatre équations
 en haut en ajoutant l'exposant

39
00:03:03,172 --> 00:03:08,788
i entre parenthèses pour toutes les variables 
qui dépendent de l’exemple d'apprentissage.

40
00:03:08,788 --> 00:03:12,612
Donc ajouter cet exposant parenthèses i à x et z et a,

41
00:03:12,612 --> 00:03:18,570
si vous voulez calculer toutes les sorties
 sur vos m exemples d'apprentissage.

42
00:03:18,570 --> 00:03:23,930
Ce que nous voulons faire est vectoriser ce calcul entier, 
afin de se débarrasser de cette boucle for.

43
00:03:23,930 --> 00:03:27,680
Et en passant, dans le cas où il semble que
 j'insiste un peu lourdement sur toute

44
00:03:27,680 --> 00:03:31,170
cette algèbre linéaire, c'est parce que
 savoir implémenter ceci

45
00:03:31,170 --> 00:03:34,580
correctement est important à l’ère
 de l’apprentissage profond.

46
00:03:34,580 --> 00:03:38,160
Et nous avons choisi les notations
 très soigneusement pour ce cours pour

47
00:03:38,160 --> 00:03:41,460
rendre ces étapes de vectorisation
 aussi faciles que possible.

48
00:03:41,460 --> 00:03:46,140
Donc j’espère qu'aller comme ça 
au fond des choses vous aidera à

49
00:03:46,140 --> 00:03:49,750
à avoir plus rapidement des implémentations
 correctes et fonctionnelles de ces algorithmes.

50
00:03:51,060 --> 00:03:56,210
Très bien, donc laissez moi recopier tout
 ce bloc de code sur la diapositive suivante et

51
00:03:56,210 --> 00:03:57,880
nous verrons comment vectoriser cela.

52
00:03:59,130 --> 00:04:02,154
Alors, voici ce que nous avions 
sur la diapositive précédente avec

53
00:04:02,154 --> 00:04:04,324
la boucle for qui parcourt
 nos m exemples d'apprentissage.

54
00:04:04,324 --> 00:04:09,769
Rappelons que nous avons défini
 la matrice X comme égale à

55
00:04:09,769 --> 00:04:16,860
nos exemples d'apprentissage empilés 
en colonnes comme ceci.

56
00:04:16,860 --> 00:04:20,180
Donc, prenez les exemples d'apprentissage
 et empilez-les en colonnes.

57
00:04:20,180 --> 00:04:23,220
Ainsi, cela devient une matrice de taille n,

58
00:04:23,220 --> 00:04:27,860
ou peut-être nx, par m.

59
00:04:29,198 --> 00:04:32,630
Je vais tout de suite vous dire 
ce que vous avez besoin d'implémenter

60
00:04:32,630 --> 00:04:35,760
afin d’avoir une implémentation 
vectorisée de cette boucle for.

61
00:04:35,760 --> 00:04:41,394
Il s’avère que ce que vous devez faire est calculer

62
00:04:41,394 --> 00:04:46,035
grand Z [1] = W [1] X + b [1],

63
00:04:46,035 --> 00:04:50,692
grand A [1] = sigmoïde de Z [1].

64
00:04:50,692 --> 00:04:56,157
Puis grand Z [2] = W[2] A[1] + b [2],

65
00:04:56,157 --> 00:05:01,348
Z [2] = W[2] A[1] + b [2] et

66
00:05:01,348 --> 00:05:10,100
ensuite A [2] = sigmoïde de Z [2].

67
00:05:10,100 --> 00:05:16,440
Donc si vous voulez l’analogie est que 
nous sommes passés des vecteurs petit x

68
00:05:16,440 --> 00:05:23,480
à la matrice X majuscule en empilant 
les petits x dans différentes colonnes.

69
00:05:23,480 --> 00:05:28,494
Si vous faites la même chose pour les petits z,
 donc par exemple,

70
00:05:28,494 --> 00:05:33,509
Si vous prenez z[1](1), z[1](2), etc,

71
00:05:33,509 --> 00:05:40,290
et ce sont tous des vecteurs colonnes,
 jusqu'à z[1](m), ok.

72
00:05:40,290 --> 00:05:46,270
C’est cette première quantité, toutes ces m quantités,
 et que vous les empilez en colonnes,

73
00:05:46,270 --> 00:05:50,045
alors ça vous donne la matrice Z [1].

74
00:05:50,045 --> 00:05:55,299
Et de la même façon, vous prenez cette quantité et

75
00:05:55,299 --> 00:06:00,957
vous prenez a[1](1), a[1](2) et ainsi de suite et

76
00:06:00,957 --> 00:06:06,980
a[1](m) et vous les empilez en colonnes.

77
00:06:06,980 --> 00:06:11,610
Puis, tout comme nous sommes passés
 de x minuscule à X majuscule, et

78
00:06:11,610 --> 00:06:13,280
de z minuscule à Z majuscule,

79
00:06:13,280 --> 00:06:20,920
Cela transforme les a minuscules, 
qui sont des vecteurs, à A [1] majuscule,

80
00:06:20,920 --> 00:06:26,685
qui est là. Et de même pour Z [2] et A [2].

81
00:06:26,685 --> 00:06:30,141
Ils sont aussi obtenus en prenant ces vecteurs et

82
00:06:30,141 --> 00:06:32,016
en les empilant horizontalement.

83
00:06:32,016 --> 00:06:37,326
Et en prenant ces vecteurs et 
en les empilant horizontalement,

84
00:06:37,326 --> 00:06:40,840
afin d’obtenir Z [2] et A [2].

85
00:06:40,840 --> 00:06:44,042
L’une des propriétés de cette notation
 qui peut peut-être vous aider

86
00:06:44,042 --> 00:06:47,391
à vous y retrouver est que ces matrices,
 par exemple, Z et A,

87
00:06:47,391 --> 00:06:51,420
sont indexées horizontalement,
sur les exemples d'apprentissage.

88
00:06:51,420 --> 00:06:55,631
C’est pourquoi l’indice horizontal correspond
 à un exemple d'apprentissage différent,

89
00:06:55,631 --> 00:06:59,730
Lorsque vous balayez de gauche à droite, 
vous parcourez le set d'apprentissage.

90
00:06:59,730 --> 00:07:04,617
Et verticalement, cet indice vertical
 correspond aux différents nœuds dans

91
00:07:04,617 --> 00:07:06,130
le réseau de neurones.

92
00:07:06,130 --> 00:07:11,077
Ainsi, par exemple, ce nœud, cette valeur tout en haut

93
00:07:11,077 --> 00:07:16,554
à gauche de la matrice correspond à l’activation

94
00:07:16,554 --> 00:07:21,633
du premier neurone caché sur
 le premier exemple d'apprentissage.

95
00:07:21,633 --> 00:07:25,812
La valeur juste en dessous correspond à 
l’activation du deuxième neurone caché

96
00:07:25,812 --> 00:07:27,525
sur le premier exemple d'apprentissage,

97
00:07:27,525 --> 00:07:31,505
puis le troisième neurone caché sur
 le premier exemple d'apprentissage,
 et ainsi de suite.

98
00:07:31,505 --> 00:07:37,540
Donc quand vous descendez, l'indexation
 va jusqu'au nombre de neurones cachés.

99
00:07:39,670 --> 00:07:42,564
Alors que si vous vous déplacez horizontalement,
 vous allez du premier neurone caché

100
00:07:42,564 --> 00:07:45,450
sur le premier exemple d'apprentissage 
vers le premier neurone caché

101
00:07:45,450 --> 00:07:48,240
sur le seconde exemple d'apprentissage, 
le troisième exemple d'apprentissage,

102
00:07:48,240 --> 00:07:53,718
et ainsi de suite jusqu'à ce nœud ici
 qui correspond à l’activation du premier

103
00:07:53,718 --> 00:07:59,030
neurone caché sur le dernier 
exemple d'apprentissage, le m-ième.

104
00:08:00,760 --> 00:08:07,663
OK donc horizontalement la matrice A 
parcourt les différents exemples d'apprentissage.

105
00:08:10,150 --> 00:08:14,195
Et verticalement les différents indices
 dans la matrice A

106
00:08:14,195 --> 00:08:17,589
correspondent aux différents neurones cachés.

107
00:08:22,342 --> 00:08:26,870
Et une intuition similaire vaut pour
 la matrice Z également, ainsi que

108
00:08:26,870 --> 00:08:31,840
pour X, horizontalement, on a 
les différents exemples de formation,

109
00:08:31,840 --> 00:08:36,227
et verticalement, cela correspond à 
différentes caractéristiques d'entrée,

110
00:08:36,227 --> 00:08:41,180
qui sont en fait différents nœuds de 
la couche d'entrée du réseau de neurones.

111
00:08:42,750 --> 00:08:46,600
Donc avec ces équations, vous savez maintenant 
comment implémenter votre réseau de neurones

112
00:08:46,600 --> 00:08:51,320
avec de la vectorisation, de la vectorisation 
par rapport à plusieurs exemples.

113
00:08:51,320 --> 00:08:55,130
Dans la prochaine vidéo je veux vous montrer 
une justification un peu plus poussée, pour montrer

114
00:08:55,130 --> 00:08:59,070
que c'est une implémentation correcte
 de ce type de vectorisation.

115
00:08:59,070 --> 00:09:03,468
Il s’avère que la justification est similaire à 
ce que vous avez vu pour la régression logistique.

116
00:09:03,468 --> 00:09:05,300
Continuons sur la prochaine vidéo.