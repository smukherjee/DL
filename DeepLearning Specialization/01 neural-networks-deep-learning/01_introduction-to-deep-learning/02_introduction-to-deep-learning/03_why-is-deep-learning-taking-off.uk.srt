1
00:00:00,320 --> 00:00:04,140
Якщо технічні ідеї, на яких базується глибоке навчання (ГН)

2
00:00:04,140 --> 00:00:07,110
і нейронні мережі (НМ) витають в повітрі останні кілька десятиліть,

3
00:00:07,110 --> 00:00:08,970
то чому вони стали популярні лише тепер?

4
00:00:08,970 --> 00:00:11,310
Давай в цьому відео розглянемо кілька

5
00:00:11,310 --> 00:00:14,690
основних рушіїв розквіту ГН, тому що я думаю, це

6
00:00:14,690 --> 00:00:17,540
допоможе тобі підібрати ті,

7
00:00:17,540 --> 00:00:20,560
які найбільше підходять для твоєї компанії.

8
00:00:20,560 --> 00:00:21,690
За останні кілька років,

9
00:00:21,690 --> 00:00:23,095
багато людей мене питали:

10
00:00:23,095 --> 00:00:25,880
"Андрію, чому ГН так добре працює"?

11
00:00:25,880 --> 00:00:30,460
Коли мене про це питають, я зазвичай малюю таку картинку.

12
00:00:30,460 --> 00:00:33,830
Скажімо, ми малюємо графік, де на горизонтальній осі

13
00:00:33,830 --> 00:00:37,675
ми відображаємо об'єм даних, які дано для завдання.

14
00:00:37,675 --> 00:00:40,040
І, скажімо, на вертикальній осі

15
00:00:40,040 --> 00:00:43,610
відображаємо продуктивність нашого алгоритму навчання.

16
00:00:43,610 --> 00:00:47,660
Наприклад, точність нашого класифікатора спаму або

17
00:00:47,660 --> 00:00:50,870
передбачення клацання по рекламі або точність

18
00:00:50,870 --> 00:00:55,980
нашої НМ при обчисленні позицій інших авто для нашого безпілотного авто.

19
00:00:55,980 --> 00:01:00,680
Виявляється, якщо зобразити продуктивність 
традиційного алгоритму навчання

20
00:01:00,680 --> 00:01:02,090
такого як метод опорних векторів (МОВ) або

21
00:01:02,090 --> 00:01:06,340
логістичної регресії (ЛР), як функцію від об'єму вхідних даних,

22
00:01:06,340 --> 00:01:09,185
ти, ймовірно, ми отримаємо ось таку криву,

23
00:01:09,185 --> 00:01:14,220
на якій продуктивність збільшується певний час, 
якщо збільшувати кількість вхідних даних,

24
00:01:14,220 --> 00:01:17,565
але в якийсь момент переходить в плато.

25
00:01:17,565 --> 00:01:20,100
Вона має бути більш схожа на горизонтальну лінію, 
я просто погано намалював.

26
00:01:20,100 --> 00:01:26,760
Ці алгоритми не знають що робити з величезною кількістю даних.

27
00:01:26,760 --> 00:01:29,270
В нашому суспільстві

28
00:01:29,270 --> 00:01:32,360
за останні, можливо, 20 років для великої кількості задач

29
00:01:32,360 --> 00:01:35,900
ми перейшли від оцих відносно малих об'ємів даних до,

30
00:01:35,900 --> 00:01:39,985
часто, дійсно великих.

31
00:01:39,985 --> 00:01:43,430
Для більшості випадків це відбулось завдяки оцифруванню

32
00:01:43,430 --> 00:01:48,020
нашого суспільства, в якому вже так багато 
людської діяльності - в царстві цифр.

33
00:01:48,020 --> 00:01:50,810
Ми проводимо дуже багато часу 
за нашими комп'ютерами, на веб сайтах,

34
00:01:50,810 --> 00:01:56,475
в мобільних додатках... 
А активність на цифрових пристроях породжує дані.

35
00:01:56,475 --> 00:01:59,720
Завдяки появі недорогих камер,

36
00:01:59,720 --> 00:02:01,970
що вбудовані в наші телефони, акселерометрів,

37
00:02:01,970 --> 00:02:05,645
усіх тих сенсорів інтернету речей,

38
00:02:05,645 --> 00:02:09,350
ми також накопичуємо все більше, більше і більше даних.

39
00:02:09,350 --> 00:02:12,590
Тож за останні 20 років для більшості застосунків

40
00:02:12,590 --> 00:02:15,935
ми назбирали набагато більше даних, ніж

41
00:02:15,935 --> 00:02:20,015
традиційні алгоритми навчання 
могли ефективно опрацювати.

42
00:02:20,015 --> 00:02:23,465
А з допомогою НМ, виявляється, що тренуючи

43
00:02:23,465 --> 00:02:28,865
малу НМ, продуктивність буде десь отакою.

44
00:02:28,865 --> 00:02:31,765
Тренуючи дещо більшу НМ,

45
00:02:31,765 --> 00:02:34,840
назвімо її середнього розміру НМ,

46
00:02:34,840 --> 00:02:40,275
продуктивність часто буде дещо кращою. 
А, тренуючи велику НМ,

47
00:02:40,275 --> 00:02:43,750
продуктивність буде ставати все кращою і кращою.

48
00:02:43,750 --> 00:02:45,965
Отже, кілька зауважень. По-перше,

49
00:02:45,965 --> 00:02:49,150
Якщо ми хочемо досягти дуже високої продуктивності,

50
00:02:49,150 --> 00:02:50,720
то нам потрібні дві речі:

51
00:02:50,720 --> 00:02:53,660
перша - можливість тренувати

52
00:02:53,660 --> 00:02:56,540
досить велику НМ, щоб

53
00:02:56,540 --> 00:02:59,630
скористатись перевагами великих об'ємів даних, а друга -

54
00:02:59,630 --> 00:03:01,760
нам потрібно бути ось тут на осі x,

55
00:03:01,760 --> 00:03:03,850
тобто потрібно мати дійсно багато даних.

56
00:03:03,850 --> 00:03:09,590
Тож, ми часто кажемо, що розмір рухає прогрес ГН. І під розміром

57
00:03:09,590 --> 00:03:12,380
я маю на увазі як розмір НМ,

58
00:03:12,380 --> 00:03:16,090
тобто НМ з великою кількістю прихованих вузлів, параметрів

59
00:03:16,090 --> 00:03:21,055
і зв'язків, так і розмір даних.

60
00:03:21,055 --> 00:03:25,190
Фактично, зараз, один з найнадійніших способів 
отримати вищу продуктивність НМ,

61
00:03:25,190 --> 00:03:29,690
найчастіше, - або натренувати більшу НМ, або дати їй більше даних.

62
00:03:29,690 --> 00:03:32,210
Хоча це працює лише до певного моменту, бо,

63
00:03:32,210 --> 00:03:35,510
врешті, або закінчаться дані або, врешті, НМ стане

64
00:03:35,510 --> 00:03:38,120
настільки великою, що тренуватиметься занадто довго.

65
00:03:38,120 --> 00:03:43,325
Але підбір розмірів вже пройшов достатньо довгий шлях в ГН.

66
00:03:43,325 --> 00:03:46,860
Щоб зробити цю діаграму дещо технічно точнішою,

67
00:03:46,860 --> 00:03:48,815
дозволь додати ще кілька елементів.

68
00:03:48,815 --> 00:03:51,465
Я підписав вісь x "Кількість даних".

69
00:03:51,465 --> 00:03:56,240
Технічно, це - кількість позначених даних,

70
00:03:56,240 --> 00:04:02,795
де під позначеними даними маються на увазі 
тренувальні зразки - і вхідні x, і позначки y.

71
00:04:02,795 --> 00:04:07,500
Також хочу представити позначення, яке будемо 
використовувати трохи пізніше в цьому курсі.

72
00:04:07,500 --> 00:04:12,460
Ми будемо використовувати маленьку m, щоб позначати 
розмір нашого тренувального набору.

73
00:04:12,460 --> 00:04:17,390
Тож кількість тренувальних зразків - маленька m. 
Вона і відображається на горизонтальній осі.

74
00:04:17,390 --> 00:04:20,300
І ще декілька деталей про цей графік.

75
00:04:20,300 --> 00:04:24,080
В оцьому режимі - малих тренувальних наборів -

76
00:04:24,080 --> 00:04:28,970
точно визначити який алгоритм кращий, 
насправді, неможливо.

77
00:04:28,970 --> 00:04:32,690
Тож, за відсутності великої кількості тренувальних даних, часто

78
00:04:32,690 --> 00:04:37,120
підбір інженерних показників, що визначають продуктивність, 
залежить від навичок інженера.

79
00:04:37,120 --> 00:04:41,810
Тож, достатньо ймовірно, що якщо комусь 
більше подобається тренувати за допомогою МОВ

80
00:04:41,810 --> 00:04:46,640
і він/вона там підбирає інженерні показники, 
а хтось в цей час тренує навіть велику НМ,

81
00:04:46,640 --> 00:04:49,260
то може статись, що в режимі малих тренувальних наборів

82
00:04:49,260 --> 00:04:50,815
МОВ буде продуктивнішим.

83
00:04:50,815 --> 00:04:54,140
Тож, скажімо, в цій ділянці, в лівій частині зображення,

84
00:04:54,140 --> 00:04:56,990
визначити більш продуктивний алгоритм

85
00:04:56,990 --> 00:05:00,800
не завжди можна, 
а продуктивність залежить більше від наших навичок

86
00:05:00,800 --> 00:05:03,380
підбирати інженерні показники 
та інших низькорівневих показників

87
00:05:03,380 --> 00:05:07,700
алгоритмів. І лише в цьому режимі великих даних,

88
00:05:07,700 --> 00:05:09,080
дуже великих тренувальних наборів,

89
00:05:09,080 --> 00:05:12,320
в режимі дуже великого m, справа, ми більш

90
00:05:12,320 --> 00:05:16,960
чітко бачимо домінування великих НМ над іншими підходами.

91
00:05:16,960 --> 00:05:21,315
Тож коли хтось з твоїх друзів спитає чому НМ так злетіли,

92
00:05:21,315 --> 00:05:24,820
раджу намалювати і їм цей малюнок.

93
00:05:24,820 --> 00:05:27,695
Отже, я б сказав, що на початку

94
00:05:27,695 --> 00:05:29,450
теперішнього злету ГН

95
00:05:29,450 --> 00:05:33,395
було збільшення кількості даних і обчислювальних можливостей.

96
00:05:33,395 --> 00:05:36,425
Тож коли ми змогли тренувати дуже великі НМ

97
00:05:36,425 --> 00:05:38,420
(чи то на центральному процесорі (ЦП) [CPU], 
чи на графічному (ГП) [GPU])

98
00:05:38,420 --> 00:05:41,555
це дозволило нам здійснити суттєвий прогрес.

99
00:05:41,555 --> 00:05:44,320
Проте, все частіше, особливо останні кілька років, ми

100
00:05:44,320 --> 00:05:47,980
бачимо також колосальні нововведення і в алгоритмах.

101
00:05:47,980 --> 00:05:50,830
Тож я не хотів би недооцінити значення цього.

102
00:05:50,830 --> 00:05:54,995
Цікаво, що багато нововведень в алгоритмах

103
00:05:54,995 --> 00:06:00,285
були намаганнями значимо прискорити нейронні мережі.

104
00:06:00,285 --> 00:06:02,940
Тож, як конкретний приклад,

105
00:06:02,940 --> 00:06:05,210
одним з великих досягнень НМ був

106
00:06:05,210 --> 00:06:08,125
перехід від сигмоїди,

107
00:06:08,125 --> 00:06:09,840
що виглядає ось так,

108
00:06:09,840 --> 00:06:13,535
до функції ВЛВ, про яку ми

109
00:06:13,535 --> 00:06:17,855
згадували в попередніх відео і яка виглядає ось так.

110
00:06:17,855 --> 00:06:22,010
Якщо зрозуміло не все, 
про що я почав говорити, не хвилюйся.

111
00:06:22,010 --> 00:06:25,820
Виявляється, що одна з проблем використання сигмоїди в

112
00:06:25,820 --> 00:06:29,980
машинному навчанні - це ось ці ділянки, 
де кути нахилу дотичних

113
00:06:29,980 --> 00:06:31,905
або Градієнт дуже близький до 0.

114
00:06:31,905 --> 00:06:34,685
Через це навчання стає дуже повільним.

115
00:06:34,685 --> 00:06:37,945
Тому що при реалізації Градієнтного спуску, коли Градієнт =0,

116
00:06:37,945 --> 00:06:41,820
параметри змінюються дуже повільно, 
а, отже, і навчання сповільнюється.

117
00:06:41,820 --> 00:06:47,090
В той час як, змінивши функцію активації НМ,

118
00:06:47,090 --> 00:06:52,835
на оцю, яка називається ВЛВ, 
тобто Випрямленим лінійним вузлом,

119
00:06:52,835 --> 00:06:58,350
В-Л-В, Градієнт стає =1 для всіх додатніх вхідних значень. Вірно?

120
00:06:58,350 --> 00:07:03,130
Тож Градієнт менш схильний збігатись до 0.

121
00:07:03,130 --> 00:07:05,720
А отут зліва Градієнт або дотична до цієї прямої =0,

122
00:07:05,720 --> 00:07:08,960
але, виявляється, що проста заміна

123
00:07:08,960 --> 00:07:12,320
сигмоїди на функцію ВЛВ,

124
00:07:12,320 --> 00:07:16,380
зробило алгоритм Градієнтного спуску набагато швидшим.

125
00:07:16,380 --> 00:07:21,350
Тож, це - приклад, можливо, відносно простого 
алгоритмічного нововведення, проте,

126
00:07:21,350 --> 00:07:26,755
в остаточному підсумку, це алгоритмічне нововведення, 
насправді, дуже допомогло обчисленням.

127
00:07:26,755 --> 00:07:29,765
Насправді, є дуже багато схожих прикладів,

128
00:07:29,765 --> 00:07:33,230
в яких ми змінили алгоритм, 
тому що це значно прискорювало виконання коду

129
00:07:33,230 --> 00:07:37,400
і дозволяло не тренувати більші НМ чи зменшувати

130
00:07:37,400 --> 00:07:42,065
обґрунтовану кількість коду навіть коли в нас є велика НМ або багато даних.

131
00:07:42,065 --> 00:07:47,060
Ще одне, що показує важливість швидких обчислень -

132
00:07:47,060 --> 00:07:52,525
це те, що, виявляється, тренування НМ - дуже циклічне.

133
00:07:52,525 --> 00:07:54,050
Часто в нас є ідея

134
00:07:54,050 --> 00:07:58,285
архітектури НМ і ми починаємо її реалізувати - писати код.

135
00:07:58,285 --> 00:08:01,999
Реалізація ідеї потім дозволяє запустити експеримент,

136
00:08:01,999 --> 00:08:05,880
який показує наскільки добре працює наша НМ, а потім на основі цього

137
00:08:05,880 --> 00:08:08,240
ми вертаємось назад, щоб змінити

138
00:08:08,240 --> 00:08:13,190
налаштування НМ і далі знову і знову по колу.

139
00:08:13,190 --> 00:08:17,089
І коли наша НМ потребує дуже багато часу для тренування,

140
00:08:17,089 --> 00:08:21,140
то і рух по колу буде займати дуже багато часу. Тому

141
00:08:21,140 --> 00:08:27,215
відчувається величезна різниця в продуктивності 
при побудові ефективної НМ, коли

142
00:08:27,215 --> 00:08:32,455
від ідеї до її випробування проходить 10 хвилин

143
00:08:32,455 --> 00:08:38,570
або навіть цілий день і коли доводиться тренувати НМ цілий місяць

144
00:08:38,570 --> 00:08:41,480
(іноді це, все-таки, трапляється), тому що коли ми отримуємо

145
00:08:41,480 --> 00:08:44,630
результат через 10 хвилин чи день,

146
00:08:44,630 --> 00:08:47,750
то можемо випробувати набагато більше ідей і з більшою

147
00:08:47,750 --> 00:08:52,005
імовірністю знайти НМ, що підходитиме для нашого застосунку.

148
00:08:52,005 --> 00:08:57,320
Також, швидші обчислення дійсно допомогли в сенсі

149
00:08:57,320 --> 00:09:02,495
зменшення часу отримання експериментальних результатів. А це

150
00:09:02,495 --> 00:09:06,950
дійсно допомогло як інженерам НМ, так і

151
00:09:06,950 --> 00:09:10,550
дослідникам ГН рухатись циклом

152
00:09:10,550 --> 00:09:14,530
набагато швидше і вдосконалювати ідеї швидше.

153
00:09:14,530 --> 00:09:17,570
І все це вилилось в гучний бум для

154
00:09:17,570 --> 00:09:22,390
всієї дослідницької спільноти ГН, просто неймовірний я б сказав,

155
00:09:22,390 --> 00:09:27,470
в сенсі розробки нових алгоритмів і невпинного прогресу на цьому фронті.

156
00:09:27,470 --> 00:09:33,200
Тож це - деякі з рушіїв, що спричинили злет ГН. І, на щастя,

157
00:09:33,200 --> 00:09:38,825
ці рушійні сили досі потужно діють, щоб зробити ГН ще кращим. 
Візьмімо дані.

158
00:09:38,825 --> 00:09:42,890
Суспільство виробляє все більше і більше цифрових даних. 
Або візьмімо

159
00:09:42,890 --> 00:09:47,630
обчислення. З появою спеціалізованого апаратного забезпечення, 
як ГП [GPU], і збільшення швидкості мереж

160
00:09:47,630 --> 00:09:49,595
та багато іншого апаратного забезпечення,

161
00:09:49,595 --> 00:09:54,260
я впевнений, що наші можливості будувати дуже великі НМ

162
00:09:54,260 --> 00:09:59,300
з точки зору обчислень продовжать збільшуватись. 
І візьмімо алгоритми.

163
00:09:59,300 --> 00:10:02,150
Поки вся дослідницька спільнота ГН

164
00:10:02,150 --> 00:10:06,170
продовжує феноменально нововводити на алгоритмічному фронті.

165
00:10:06,170 --> 00:10:07,550
Тож через це,

166
00:10:07,550 --> 00:10:09,530
думаю, ми можемо бути оптимістами.

167
00:10:09,530 --> 00:10:11,720
Я - оптиміст. Я думаю, що ГН продовжить

168
00:10:11,720 --> 00:10:14,570
покращуватись ще багато років.

169
00:10:14,570 --> 00:10:17,855
Тож давай перейдемо до останнього відео розділу

170
00:10:17,855 --> 00:10:21,200
де поговоримо трохи детальніше про те, що ти вивчиш на цьому курсі.