1
00:00:00,060 --> 00:00:03,750
이전 비디오에서는 1개의 트레이닝 example에 

2
00:00:01,890 --> 00:00:05,819
대해서 로지스틱 회귀분석을 위한 기울기 강하를

3
00:00:03,750 --> 00:00:07,500
도입하기 위해 derivative를

4
00:00:05,819 --> 00:00:09,929
산출하는 방법을 배웠는데요, 

5
00:00:07,500 --> 00:00:12,450
이제는 m개의 트레이닝 example에 대하여

6
00:00:09,929 --> 00:00:14,429
구하고 싶습니다. 시작하기에 앞서, 

7
00:00:12,450 --> 00:00:17,460
j의 비용함수 정의에 사시 한번

8
00:00:14,429 --> 00:00:19,380
짚고 넘어가겠습니다. J(w, b) 비용함수는

9
00:00:17,460 --> 00:00:22,699
여기 이 것의 평균값인데요  

10
00:00:19,380 --> 00:00:25,350
1 나누기 m 의 합, l이 1에서 m 까지의 합이고, 

11
00:00:22,699 --> 00:00:29,519
알고리즘 결과값의 loss가 

12
00:00:25,350 --> 00:00:33,510
a i 이고 y example에 대한 것입니다.

13
00:00:29,519 --> 00:00:36,120
그리고 ai는 i번째의 트레이닝 example의 

14
00:00:33,510 --> 00:00:40,620
예측 수치입니다. 그것은 zi의 시그마인데요

15
00:00:36,120 --> 00:00:46,800
이 값은 또 w transpose 더하기 b의 

16
00:00:40,620 --> 00:00:48,510
시그마입니다.

17
00:00:46,800 --> 00:00:51,600
이전 슬라이드에서는, 1개의 

18
00:00:48,510 --> 00:00:55,620
트레이닝 example에 대해서 d의 

19
00:00:51,600 --> 00:01:00,180
derivative 값을 구하는 

20
00:00:55,620 --> 00:01:03,809
방법에 대해 배웠는데요, 
즉, dw1, dw2, 그리고

21
00:01:00,180 --> 00:01:06,689
db인데요, 이제는 위 첨자 i를 표기해서

22
00:01:03,809 --> 00:01:08,369
이전 슬라이드에서 했던 것을

23
00:01:06,689 --> 00:01:10,799
똑같이 하면 나오는 값을 

24
00:01:08,369 --> 00:01:15,030
표현하겠습니다. 1개의 트레이닝 

25
00:01:10,799 --> 00:01:17,850
example인 xi, yi를 이용했을 때 말이죠. 

26
00:01:15,030 --> 00:01:20,759
죄송합니다. 여기 i가 빠졌네요. 

27
00:01:17,850 --> 00:01:22,530
그러면 여러분도 아시겠지만, 전체 비용 함수는

28
00:01:20,759 --> 00:01:26,220
합, 아니 사실 평균이죠, 여기 1 나누기

29
00:01:22,530 --> 00:01:29,369
m 에 대한 loss값에서 말이죠. 

30
00:01:26,220 --> 00:01:32,810
w1에 대한 derivative는

31
00:01:29,369 --> 00:01:38,600
전체 비용함수에서 

32
00:01:32,810 --> 00:01:42,600
마찬가지로 w1에 대한 derivative의 

33
00:01:38,600 --> 00:01:46,170
개인 loss 항들에 대한 평균값이 

34
00:01:42,600 --> 00:01:48,240
될 것입니다. 하지만 이전에 

35
00:01:46,170 --> 00:01:54,119
이 항을 계산하는 방법을 보여줬듯이, 

36
00:01:48,240 --> 00:01:55,890
dw i라고 했는데요, 맞죠.

37
00:01:54,119 --> 00:01:57,659
이것을 이전 슬라이드에서는 

38
00:01:55,890 --> 00:02:00,450
1개의 트레이닝 example의 경우 계산하는 방법을 알려드렸는데요, 

39
00:01:57,659 --> 00:02:03,119
이제 여러분은 여기 dervative를 

40
00:02:00,450 --> 00:02:04,680
이전의 트레이닝 example에서 보여줬듯이

41
00:02:03,119 --> 00:02:07,350
계산해야 합니다.

42
00:02:04,680 --> 00:02:10,379
그런 다음에 평균치를 구하면, 

43
00:02:07,350 --> 00:02:10,830
기울기 강하를 바로 도입할 수 있는

44
00:02:10,379 --> 00:02:12,870
전체 기울기 값을 

45
00:02:10,830 --> 00:02:15,390
구할 수 있는 것입니다. 지금 다룬 내용이

46
00:02:12,870 --> 00:02:17,730
굉장히 많은 양인데요, 이것들을 모두 모아서

47
00:02:15,390 --> 00:02:19,920
더 구체적인 알고리즘을 

48
00:02:17,730 --> 00:02:21,690
만들어 봅시다. 기울기 강하와

49
00:02:19,920 --> 00:02:24,960
로지스틱 회귀분석이 같이 

50
00:02:21,690 --> 00:02:28,350
도입된 잘 작동하는 알고리즘으로 말이죠. 

51
00:02:24,960 --> 00:02:37,770
이제 여러분이 할 수 있는 것은 이런데요, 
J=0으로 초기화 하고, 

52
00:02:28,350 --> 00:02:40,140
dw1 = 0, dw2 = 0, db = 0으로 

53
00:02:37,770 --> 00:02:43,190
설정하고, 이제 할 것은

54
00:02:40,140 --> 00:02:45,690
이런 트레이닝 세트에 for loop를 사용해서 

55
00:02:43,190 --> 00:02:47,670
각각의 트레이닝 example에 대해서 derivative 값을

56
00:02:45,690 --> 00:02:49,020
구한 뒤에, 그 값들을 더할 것입니다.

57
00:02:47,670 --> 00:02:51,480
이제 이렇게 하는데요, i가 

58
00:02:49,020 --> 00:02:54,360
1에서 m까지, m은 트레이닝 example의

59
00:02:51,480 --> 00:02:57,090
개수입니다. zi 는 w transpose 

60
00:02:54,360 --> 00:03:00,360
xi 더하기 b이고요, 예측 값

61
00:02:57,090 --> 00:03:04,020
ai는 zi 의 시그마 

62
00:03:00,360 --> 00:03:09,120
그리고 j를 더해보도록 하죠. j 플러스는

63
00:03:04,020 --> 00:03:12,360
yi log ai 더하기 1 빼기 yi 의 로그 

64
00:03:09,120 --> 00:03:14,010
1 빼기 ai하고 나서 전체 괄호 바깥으로

65
00:03:12,360 --> 00:03:15,959
마이너스 부호를 앞에 두겠습니다.

66
00:03:14,010 --> 00:03:20,580
그리고 이전에도 봤듯이 dzi는

67
00:03:15,959 --> 00:03:28,500
ai 빼기 yi 이고요

68
00:03:20,580 --> 00:03:33,180
dw 플러스 값은 x1 i 곱하기 d zi 입니다.

69
00:03:28,500 --> 00:03:35,280
dw2 플러스는 xi 2 dzi 입니다. 아 그리고 

70
00:03:33,180 --> 00:03:37,680
저는 이 계산들을 오로지 2개의 

71
00:03:35,280 --> 00:03:41,070
특성만 있다는 가정하에 진행하고 있는 것입니다.
즉, n= 2인 경우이죠, 

72
00:03:37,680 --> 00:03:45,480
안 그러면, 아니면 이것을 dw1, dw2, dw3 

73
00:03:41,070 --> 00:03:47,430
등등 이어서 하겠죠, 그러면 db 플러스는

74
00:03:45,480 --> 00:03:49,350
dzi 인데요, 이렇게 하면 for loop의 끝입니다.

75
00:03:47,430 --> 00:03:51,900
이렇게 m트레이닝 example에 대해서

76
00:03:49,350 --> 00:03:54,959
모든 것을 마치면, 아직도

77
00:03:51,900 --> 00:03:56,880
M으로 나누어야 할 텐데요, 

78
00:03:54,959 --> 00:04:01,920
평균을 구하는 것이기 때문입니다. 그러므로

79
00:03:56,880 --> 00:04:04,260
dw1을 나눈 값이 m이고, dw2를 나누면 m이고, 

80
00:04:01,920 --> 00:04:07,019
db 나누면 m이 되겠죠. 모든 평균에서 말이죠 

81
00:04:04,260 --> 00:04:09,060
그러면 전체 모든 계산에서

82
00:04:07,019 --> 00:04:11,160
3개의 parameter w1, w2, b에 대한 

83
00:04:09,060 --> 00:04:14,250
j 비용함수의 

84
00:04:11,160 --> 00:04:17,010
derivative를 계산했는데요, 

85
00:04:14,250 --> 00:04:22,079
저희가 무엇을 했는지 조금 더 커멘트 해드리자면

86
00:04:17,010 --> 00:04:25,020
저희가 dw1, dw2와 b를 이용하는데요, 

87
00:04:22,079 --> 00:04:28,169
누산기들 (accumulators)로 말이죠, 그렇기 때문에

88
00:04:25,020 --> 00:04:31,500
계산 이후에는 dw1이 

89
00:04:28,169 --> 00:04:33,509
전체 비용함수의 derivative와 일치 하다는 것을

90
00:04:31,500 --> 00:04:36,780
알 수 있습니다. w1에 대해서 말이죠. 

91
00:04:33,509 --> 00:04:39,720
비슷하게 dw2와 db에서도 말이죠. 

92
00:04:36,780 --> 00:04:41,520
보시면 알겠지만 dw1과 dw2는 위 첨자 i가 없습니다.

93
00:04:39,720 --> 00:04:43,379
그 이유는 여기 코드에서 이것을 

94
00:04:41,520 --> 00:04:45,690
누산기들로 이용해서 전체 

95
00:04:43,379 --> 00:04:48,960
트레이닝 세트에 대해 사용하기 때문입니다.

96
00:04:45,690 --> 00:04:51,539
반면에, dzi는 여기서 1개의 트레이닝 샘플에 대한

97
00:04:48,960 --> 00:04:53,490
dz 값이었는데요, 그렇기 때문에 

98
00:04:51,539 --> 00:04:55,740
1개의 트레이닝 샘플임을 나타내기 위해

99
00:04:53,490 --> 00:04:58,379
위 첨자 i가 있었던 것인데요, 

100
00:04:55,740 --> 00:05:00,960
이제 이런 계산을 모두 마쳤는데요, 

101
00:04:58,379 --> 00:05:03,449
기울기 강하의 1개의 단계를 

102
00:05:00,960 --> 00:05:06,360
도입하기 위해서는 w1을 도입하고, 

103
00:05:03,449 --> 00:05:10,710
이것은 w1 빼기 러닝속도 

104
00:05:06,360 --> 00:05:13,740
곱하기 dw1으로 업데이트 되고요. dw2는 

105
00:05:10,710 --> 00:05:17,190
w2 빼기 러닝속도 곱하기 dw2로 업데이트 됩니다. 

106
00:05:13,740 --> 00:05:21,000
그리고 b는 b 빼기 러닝속도 곱하기 db로 업데이트됩니다.

107
00:05:17,190 --> 00:05:23,879
그리고 dw1, dw2, db는 이전에 왼쪽에서 계산했던 대로의 값입니다.

108
00:05:21,000 --> 00:05:27,000
마지막으로 여기 J는

109
00:05:23,879 --> 00:05:28,590
비용함수의 올바른 값일 것입니다.

110
00:05:27,000 --> 00:05:31,050
그러면 이제 슬라이드의 모든 내용은

111
00:05:28,590 --> 00:05:33,060
1가지 단계의 기울기 강하를

112
00:05:31,050 --> 00:05:35,699
도입하는 것인데요, 

113
00:05:33,060 --> 00:05:37,680
여기 슬라이드에 있는 모든 내용을 

114
00:05:35,699 --> 00:05:40,469
복수로 반복해야 복수의 gradient 

115
00:05:37,680 --> 00:05:41,819
descent 단계를 갖출 수 있습니다.

116
00:05:40,469 --> 00:05:43,830
여기 상세 내용이 조금 너무 복잡하다고 생각되면, 

117
00:05:41,819 --> 00:05:45,960
다시 말씀 드리지만 너무 걱정하지 마십시요 

118
00:05:43,830 --> 00:05:48,599
아직 까지는요, 

119
00:05:45,960 --> 00:05:50,520
연습문제에서 이것들을 도입하게 되면

120
00:05:48,599 --> 00:05:54,120
더 명백해질 것입니다. 

121
00:05:50,520 --> 00:05:57,300
여기 도입한 것과 같이 계산부분에 있어서는

122
00:05:54,120 --> 00:05:59,729
2가지의 단점이 나타날 수 있는데요, 

123
00:05:57,300 --> 00:06:01,440
로지스틱 회귀분석을 이렇게

124
00:05:59,729 --> 00:06:03,960
도입하기 위해서는 2개의 for loop을

125
00:06:01,440 --> 00:06:05,490
써야 합니다. 첫 번째 for loop은 

126
00:06:03,960 --> 00:06:07,770
여기 m 트레이닝 example에 대한

127
00:06:05,490 --> 00:06:10,919
for loop 이고요, 두 번째는 

128
00:06:07,770 --> 00:06:13,139
여기 모든 특성에 대한 for loop입니다. 

129
00:06:10,919 --> 00:06:15,930
여기 예제에서는 저희는 단순히

130
00:06:13,139 --> 00:06:17,879
2개의 특성만 있었는데요, 
즉 n=2 그리고 

131
00:06:15,930 --> 00:06:21,000
x=2였죠, 하지만 더 많은 특성이 있을 시, 

132
00:06:17,879 --> 00:06:23,099
dw1 과 dw2를 쓰게 될 텐데요

133
00:06:21,000 --> 00:06:25,979
그리고 dw3로 비슷한 산출 법이

134
00:06:23,099 --> 00:06:29,009
적용됩니다. dw n까지 말이죠. 

135
00:06:25,979 --> 00:06:31,279
그러므로 이렇게 n 개의 특성에

136
00:06:29,009 --> 00:06:33,199
거쳐 for loop을 필요로 합니다.

137
00:06:31,279 --> 00:06:36,049
딥러닝을 알고리즘을 도입하는 경우에

138
00:06:33,199 --> 00:06:38,419
명백한 for loop을 갖는 경우, 

139
00:06:36,049 --> 00:06:41,839
코딩에서 말이죠, 이런 경우, 

140
00:06:38,419 --> 00:06:44,149
알고리즘이 덜 효율적으로 

141
00:06:41,839 --> 00:06:46,669
운영됩니다. 그러므로 딥러닝 

142
00:06:44,149 --> 00:06:48,649
시대에 데이터세트는 더욱 커지고 커지는데요, 

143
00:06:46,669 --> 00:06:50,779
알고리즘을 명백히 for loop을

144
00:06:48,649 --> 00:06:52,969
알고리즘에 도입시키지 않고 

145
00:06:50,779 --> 00:06:55,129
진행하는 것이 굉장히 중요합니다.

146
00:06:52,969 --> 00:06:56,719
그리고 더 많이 큰 데이터세트를

147
00:06:55,129 --> 00:06:58,129
스케일링 할 수 있도록 도와주겠죠. 

148
00:06:56,719 --> 00:07:01,159
vectorization이라고 하는 한가지 

149
00:06:58,129 --> 00:07:03,559
세트의 기술이 있는데요, 이 기술은 

150
00:07:01,159 --> 00:07:06,169
for loop을 코드에서 제거하게 해주는데요, 

151
00:07:03,559 --> 00:07:08,199
딥러닝 전 시대에는, 

152
00:07:06,169 --> 00:07:11,239
즉 딥러닝이 급부상하기 전에 말이죠, 

153
00:07:08,199 --> 00:07:13,159
vectorization을 갖는 것이 좋았습니다.

154
00:07:11,239 --> 00:07:15,589
가끔씩은 빠르게 하기 위해서 쓰였고 

155
00:07:13,159 --> 00:07:17,749
안 그럴 수도 있는데요, 

156
00:07:15,589 --> 00:07:20,029
딥러닝 세대에는, vectorization이 

157
00:07:17,749 --> 00:07:22,699
for loop을 이렇게 제거함으로써

158
00:07:20,029 --> 00:07:25,039
또는 이렇게 말이죠, 아주 중요하게 

159
00:07:22,699 --> 00:07:26,989
되었습니다. 왜냐면 

160
00:07:25,039 --> 00:07:29,239
이제는 더욱 더 큰 데이터세트에서 트레이닝 시키기 때문입니다.

161
00:07:26,989 --> 00:07:31,209
그렇기 때문에 여러분의 코드를 효율적으로 만드는 게 매우 중요해졌습니다.

162
00:07:29,239 --> 00:07:34,219
다음 몇 개의 비디오에서는 

163
00:07:31,209 --> 00:07:37,339
vectorization에 대해 이야기하고 

164
00:07:34,219 --> 00:07:40,879
한 개의 for loop도 없이 이 모든 것을 

165
00:07:37,339 --> 00:07:43,069
도입하는 방법에 대해 배워보겠습니다.

166
00:07:40,879 --> 00:07:44,299
그렇게 해서 로지스틱 회귀분석을

167
00:07:43,069 --> 00:07:46,339
또는 기울기 강하를 도입하는 방법에 대해

168
00:07:44,299 --> 00:07:47,959
이해하셨길 바랍니다.

169
00:07:46,339 --> 00:07:50,299
연습문제를 진행하는 경우에
조금 더 명백히 이해하실 수 있을 것입니다.

170
00:07:47,959 --> 00:07:51,829
하지만 연습문제를 하기에 앞서, 

171
00:07:50,299 --> 00:07:54,079
먼저 vectorization에 대해

172
00:07:51,829 --> 00:07:56,419
이야기해보겠습니다. 그러면, 

173
00:07:54,079 --> 00:07:58,369
여기 모든 내용에 for loop없이 

174
00:07:56,419 --> 00:08:01,479
기울기 강하의 single iteration을 통해

175
00:07:58,369 --> 00:08:01,479
도입할 수 있을 것입니다.