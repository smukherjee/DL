1
00:00:00,620 --> 00:00:03,610
作为deeplearning.ai带来的这门课程的一部分

2
00:00:03,610 --> 00:00:07,590
我希望不仅仅是教大家深度学习上的技术思路，还能给大家

3
00:00:07,590 --> 00:00:11,658
介绍一些对深度学习发展做出贡献的的英雄们

4
00:00:11,658 --> 00:00:13,160
他们开创出了

5
00:00:13,160 --> 00:00:17,700
许多你们会在这门课程或在这个系列中学习的想法

6
00:00:17,700 --> 00:00:21,420
在这些视频中,我希望可以请求这些深度学习的领袖们

7
00:00:21,420 --> 00:00:24,990
提供关于如何进入深度学习领域的职业建议

8
00:00:24,990 --> 00:00:27,805
提供关于如何可以从事深度学习的研究或者找到一份深度学习的工作的建议

9
00:00:27,805 --> 00:00:30,156
作为本次采访系列的第一部分

10
00:00:30,156 --> 00:00:34,228
我非常高兴向大家展示我们对杰弗里·欣顿(Geoffrey·Hinton)的采访

11
00:00:38,427 --> 00:00:44,150
欢迎杰夫,谢谢您接受deeplearning.ai的采访

12
00:00:44,150 --> 00:00:46,550
— 感谢你的邀请
【教育无边界字幕组】 红毛丹 | 贾斯敏 | misssty

13
00:00:46,550 --> 00:00:50,088
我想到目前为止在这个星球上您提出了比任何其他人

14
00:00:50,088 --> 00:00:52,835
都多的基于深度学习的思想

15
00:00:52,835 --> 00:00:57,650
而且很多人已经开始称呼您为深度学习的教父

16
00:00:57,650 --> 00:01:01,529
尽管直到我们聊天的几分钟前

17
00:01:01,529 --> 00:01:05,600
我才意识到你认为我是第一个这样称呼您的人,而我为此感到很高兴

18
00:01:06,780 --> 00:01:11,320
但我想问的是,很多人都知道您是一个传奇

19
00:01:11,320 --> 00:01:15,030
我想要问问您在这个传奇背后的个人故事。

20
00:01:15,030 --> 00:01:19,980
那么您是如何参与并从事多年

21
00:01:19,980 --> 00:01:21,520
您是如何开始涉猎人工智能,机器学习,和神经网络的呢?

22
00:01:22,730 --> 00:01:26,960
在我上中学的时候,我有一个同学

23
00:01:26,960 --> 00:01:31,220
他在任何方面都比我出色,他是一个卓越的数学家

24
00:01:31,220 --> 00:01:37,010
一天他来到学校跟我说,你知道大脑使用全息图吗?

25
00:01:38,190 --> 00:01:44,161
我猜那是在1966年左右,我说我知道一些，然后问什么是全息图?

26
00:01:44,161 --> 00:01:47,390
他解释说道,在一个全息图里,你可以砍掉其中一半

27
00:01:47,390 --> 00:01:49,730
之后你仍然能够看到整张图

28
00:01:49,730 --> 00:01:53,466
而记忆在大脑里可能分布在整个大脑上

29
00:01:53,466 --> 00:01:56,022
所以我猜他读过拉什利的实验

30
00:01:56,022 --> 00:01:57,939
在实验中你砍掉一小部分老鼠的大脑然后

31
00:01:57,939 --> 00:02:01,740
发现想要找到哪一小块大脑储存着哪一部分记忆是非常困难的

32
00:02:04,411 --> 00:02:08,920
这就是第一次让我对大脑如何存储记忆感兴趣的原因

33
00:02:10,180 --> 00:02:12,220
然后等我上了大学之后

34
00:02:12,220 --> 00:02:15,130
我开始学习生理学和物理学

35
00:02:16,400 --> 00:02:17,731
我想当我在剑桥的时候

36
00:02:17,731 --> 00:02:20,260
我是唯一一个做生理学和物理学的本科生

37
00:02:21,888 --> 00:02:25,270
接着我放弃了

38
00:02:25,270 --> 00:02:29,170
然后试着去从事哲学,因为我认为这也许可以给我更多的启示

39
00:02:29,170 --> 00:02:32,780
但实际上在我看来

40
00:02:32,780 --> 00:02:37,130
哲学并不能很好的鉴别出某些人说出的错误的东西

41
00:02:37,130 --> 00:02:39,420
所以之后我转向了心理学

42
00:02:41,988 --> 00:02:45,920
然后在心理学里,他们只有非常非常简单的理论,而这个对我来说似乎

43
00:02:45,920 --> 00:02:49,620
不足以去解释大脑在做什么

44
00:02:49,620 --> 00:02:52,737
于是我抽出了一些时间,成为了一个木匠

45
00:02:52,737 --> 00:02:57,169
然后我决定我想尝试一下人工智能,然后我去了爱丁堡

46
00:02:57,169 --> 00:02:59,580
去跟朗格希金斯(Langer·Higgins)学习人工智能

47
00:02:59,580 --> 00:03:02,662
他已经在神经网络上做出了很好的工作

48
00:03:02,662 --> 00:03:07,830
然后他刚好放弃了神经网络,并且对Winograd的论文印象深刻

49
00:03:07,830 --> 00:03:11,460
所以当我到爱丁堡的时候,他认为我在做那种过时的东西

50
00:03:11,460 --> 00:03:14,210
他认为我应该从符号学的人工智能开始

51
00:03:14,210 --> 00:03:18,210
我们有过很多的争论,但我一直在做着自己觉得对的事情

52
00:03:18,210 --> 00:03:21,138
然后呢?

53
00:03:21,138 --> 00:03:28,033
我最终拿到了我在人工智能上的博士学位,然后我在英国找不到工作

54
00:03:28,033 --> 00:03:30,979
但是我看到了一个很棒的广告

55
00:03:30,979 --> 00:03:36,070
是关于加州的Sloan奖学金,然后我努力拿到了一个

56
00:03:36,070 --> 00:03:40,625
之后我去了加州，在那里一切都与之前不同了

57
00:03:40,625 --> 00:03:46,685
在英国,神经网络被认为是种愚蠢的东西

58
00:03:46,685 --> 00:03:50,272
但是在加州,唐·诺曼和

59
00:03:50,272 --> 00:03:56,640
大卫·鲁梅尔哈特都对神经网络持非常开放的态度

60
00:03:56,640 --> 00:04:00,720
这是我第一次在某个地方开始思考大脑是如何运作的

61
00:04:00,720 --> 00:04:03,290
同时思考这个问题如何与心理学联系起来

62
00:04:03,290 --> 00:04:05,650
这些问题被当作一个非常正面的事情

63
00:04:05,650 --> 00:04:06,936
而且非常有趣

64
00:04:06,936 --> 00:04:09,792
特别是跟大卫·鲁梅尔哈特的合作非常的棒

65
00:04:09,792 --> 00:04:12,968
很好,所以这是当您在UCSD

66
00:04:12,968 --> 00:04:16,177
您和儒大卫在1982年左右

67
00:04:16,177 --> 00:04:20,182
一起撰写关于反向传播的会议论文时候,对吧?

68
00:04:20,182 --> 00:04:23,292
实际情况比这更复杂

69
00:04:23,292 --> 00:04:24,796
— 发生了什么

70
00:04:24,796 --> 00:04:28,214
我想是在1982年初

71
00:04:28,214 --> 00:04:32,900
大卫·鲁梅尔哈特,我和润·威廉

72
00:04:32,900 --> 00:04:37,967
我们一起开发这个逆传算法

73
00:04:37,967 --> 00:04:42,291
这个主要是大卫·鲁梅尔哈特的主意

74
00:04:42,291 --> 00:04:46,390
我们后来才发现很多其他人发明了它

75
00:04:46,390 --> 00:04:52,798
戴维·帕克发明过,也许在我们之后,但是在我们发表之前

76
00:04:52,798 --> 00:04:56,425
保罗·韦伯斯早几年前就已经发表了

77
00:04:56,425 --> 00:04:58,860
但是没有人注意它

78
00:04:58,860 --> 00:05:01,923
其他人研发了非常相似的算法

79
00:05:01,923 --> 00:05:04,340
只是反向传播的定义还很模糊

80
00:05:04,340 --> 00:05:08,055
但使用链式法则来求得导数并不是一个新奇的想法

81
00:05:08,055 --> 00:05:12,484
我明白了,那为什么你认为是你的文章帮助了

82
00:05:12,484 --> 00:05:15,940
学术圈理解反向传播呢?

83
00:05:15,940 --> 00:05:20,540
感觉您的文章就像感染一样传播
并影响大家对这个算法的理解

84
00:05:20,540 --> 00:05:22,934
不论是谁都接受它

85
00:05:22,934 --> 00:05:26,675
所以我们在1986年的时候成功在Nature上面发了一篇文章

86
00:05:26,675 --> 00:05:30,580
我做了大量的政治工作来让我的文章被接受

87
00:05:30,580 --> 00:05:34,622
我发现了其中一个审稿人可能会是斯图尔特·萨德兰

88
00:05:34,622 --> 00:05:36,992
一个英国著名的心理学家

89
00:05:36,992 --> 00:05:38,815
我跟他谈了很久

90
00:05:38,815 --> 00:05:41,480
跟他精确地解释这到底是怎么一回事

91
00:05:41,480 --> 00:05:44,140
而他对这一事实印象深刻

92
00:05:44,140 --> 00:05:48,970
我们演示了逆传算法能够学习文字的表达

93
00:05:48,970 --> 00:05:52,490
你可以看一下这些表达,都是一些小的向量

94
00:05:52,490 --> 00:05:55,950
你能够理解这些个体特征的意义

95
00:05:55,950 --> 00:06:01,600
所以我们实际上用关于家庭族谱的三字组合来训练它

96
00:06:01,600 --> 00:06:06,420
比如玛丽有个妈妈叫维多利亚

97
00:06:06,420 --> 00:06:11,550
然后你给它前两个词,它就能够预测最后一个词

98
00:06:11,550 --> 00:06:12,970
在你训练它之后

99
00:06:12,970 --> 00:06:17,780
你可以在单个词上看到所有的特征表达

100
00:06:17,780 --> 00:06:19,950
比如一个人的国籍

101
00:06:19,950 --> 00:06:25,180
他们是哪一代,属于哪一个分支等等

102
00:06:25,180 --> 00:06:27,680
这个就是让斯图尔特·萨瑟兰印象深刻的东西

103
00:06:27,680 --> 00:06:29,666
我认为这就是这篇文章能够被接收的原因。

104
00:06:29,666 --> 00:06:33,905
非常早期的词嵌入中，您就已经能看到一些特征

105
00:06:33,905 --> 00:06:38,390
一些从训练算法中生成的语义特征

106
00:06:38,390 --> 00:06:44,090
是的，从心理学的观点来看，有一点比较有趣就是

107
00:06:44,090 --> 00:06:49,740
关于知识是什么样有两个完全不同的想法。

108
00:06:49,740 --> 00:06:53,460
有个老心理学家认为一个概念只是

109
00:06:53,460 --> 00:06:56,810
一大推捆绑的特征并由很多证据所组成。

110
00:06:56,810 --> 00:07:02,180
有一个对于时间的AI观点则是一个规范主义的观点。

111
00:07:02,180 --> 00:07:06,190
这是一个概念如何与其他概念进行关联。

112
00:07:06,190 --> 00:07:09,820
为了捕捉一个概念,你必须做一些像图表结构或

113
00:07:09,820 --> 00:07:11,640
是一个语义网。

114
00:07:11,640 --> 00:07:15,875
反向传播的例子中展示的是你可以给它

115
00:07:15,875 --> 00:07:21,070
信息让它进入到图形结构中或例子中的系谱。

116
00:07:22,080 --> 00:07:26,920
它能将信息转换成特征，这种方法它能够

117
00:07:26,920 --> 00:07:33,470
将特征来获得新的一致的信息，即一般化。

118
00:07:33,470 --> 00:07:38,438
但关键是这种来回之间的图形表示或

119
00:07:38,438 --> 00:07:43,000
树形结构的系谱和

120
00:07:43,000 --> 00:07:46,715
用来表征人的一个很大的特征向量。

121
00:07:46,715 --> 00:07:50,873
事实上，你可以从图形表征得到特征向量。

122
00:07:50,873 --> 00:07:51,469
转化为向量

123
00:07:51,469 --> 00:07:54,995
从特征向量,你可以得到更多的图形代表。

124
00:07:54,995 --> 00:07:57,730
所以这是1986年？

125
00:07:57,730 --> 00:08:02,430
在90年代初,Bengio表明你可以利用真实的数据,

126
00:08:02,430 --> 00:08:07,420
你可以采用英语文本,并应用相同的技术和

127
00:08:07,420 --> 00:08:13,980
从英文文本中获取嵌入信息,这给人们留下了深刻印象。

128
00:08:13,980 --> 00:08:18,682
我想最近我们谈论了很多关于计算机像gpu和

129
00:08:18,682 --> 00:08:21,750
超级计算机的快速运行速度正在推动深入学习。

130
00:08:21,750 --> 00:08:26,376
我没有意识到,1986和90年代初,这听起来像

131
00:08:26,376 --> 00:08:29,570
你和Benjio已经开启了这个趋势的开端。

132
00:08:30,600 --> 00:08:32,630
是的,这是一个巨大的进步。

133
00:08:32,630 --> 00:08:41,440
在1986年，我用的是一台每秒浮点运算速度不到10万次的机器

134
00:08:41,440 --> 00:08:47,720
大约1993左右,人们可以见到每秒浮点运算次数在千万解级别的机器。

135
00:08:47,720 --> 00:08:49,600
>>我明白了。>>所以是100倍的差别,

136
00:08:49,600 --> 00:08:51,770
所以它很容易使用,

137
00:08:51,770 --> 00:08:53,580
因为电脑变得越来越快。

138
00:08:53,580 --> 00:08:56,960
在过去的几十年里,你发明了

139
00:08:56,960 --> 00:08:59,970
很多神经网络和深度学习。

140
00:08:59,970 --> 00:09:02,670
我真的很好奇你所发明的东西

141
00:09:02,670 --> 00:09:05,050
哪一个仍使你最为兴奋？

142
00:09:06,940 --> 00:09:09,590
我认为最漂亮的一个是我

143
00:09:09,590 --> 00:09:12,620
和特里谢诺夫斯基共同工作的玻尔兹曼机器。

144
00:09:12,620 --> 00:09:14,500
我们发现这真的是

145
00:09:14,500 --> 00:09:18,830
非常非常简单学习算法，但可以应用在很大的

146
00:09:18,830 --> 00:09:23,550
密集连接的网络中，其中你只可以看到几个节点。

147
00:09:23,550 --> 00:09:27,730
它会学习隐藏代表,而且这是一个非常简单的算法。

148
00:09:27,730 --> 00:09:31,130
像这种事情应该可以在大脑中运行因为

149
00:09:31,130 --> 00:09:34,210
每个突触只需要知道两个直接相连接

150
00:09:34,210 --> 00:09:35,940
的神经元的行为。

151
00:09:37,010 --> 00:09:41,230
通过传播的信息也是一致的。

152
00:09:41,230 --> 00:09:45,160
它有两个不同的阶段,我们称之为唤醒和睡眠。

153
00:09:45,160 --> 00:09:46,820
但在两个不同的阶段中,

154
00:09:46,820 --> 00:09:48,760
你以同样的方式传播信息。

155
00:09:48,760 --> 00:09:52,360
就像反向传播一样，有正向传播和

156
00:09:52,360 --> 00:09:54,820
反向传播，但他们的工作方式不同。

157
00:09:54,820 --> 00:09:56,379
他们发送不同种类的信号。

158
00:09:58,100 --> 00:10:01,190
所以我认为这是最美丽的东西。

159
00:10:01,190 --> 00:10:03,730
多年来,它看起来就像好奇心,

160
00:10:03,730 --> 00:10:05,090
因为看起来太慢了。

161
00:10:06,210 --> 00:10:10,420
但是后来,我去掉了一点点的优雅性,它开始

162
00:10:10,420 --> 00:10:13,730
在一些稍简单的网络中,一次迭代到最优状态。

163
00:10:13,730 --> 00:10:16,570
这给了玻尔兹曼机一些限制，

164
00:10:16,570 --> 00:10:19,430
但其实这种方法在应用中相当有效。

165
00:10:19,430 --> 00:10:21,586
例如,在Netflix的竞争中,

166
00:10:21,586 --> 00:10:26,170
受限的玻尔兹曼机是成功的一个开始。

167
00:10:26,170 --> 00:10:30,210
事实上,约2007开始,很多神经网络和

168
00:10:30,210 --> 00:10:34,790
深度学习的回潮,是受限玻尔兹曼机

169
00:10:34,790 --> 00:10:37,710
和非受限玻尔兹曼机引领的，这都是你和你的实验室的成果。

170
00:10:38,940 --> 00:10:42,130
是的,所以这是另一个我很满意的作品,

171
00:10:42,130 --> 00:10:46,290
你可以训练你的限制性玻尔兹曼机器,它只

172
00:10:46,290 --> 00:10:51,120
有一层隐藏特征,你可以学习一层特征。

173
00:10:51,120 --> 00:10:54,850
然后,你可以把这些特征当做数据,并再做一次,

174
00:10:54,850 --> 00:10:57,953
然后你可以把你学到的新特征当作数据来处理,

175
00:10:57,953 --> 00:10:59,570
次数随你而定。

176
00:10:59,570 --> 00:11:03,060
所以这很好,而且它在实际中能实现。

177
00:11:03,060 --> 00:11:08,709
然后UY泰意识到,整件事情可以被视为单一的模型,

178
00:11:08,709 --> 00:11:11,110
但这是一种怪异的模式。

179
00:11:11,110 --> 00:11:15,946
这是一个在顶部你有一个受限玻尔兹曼机,但

180
00:11:15,946 --> 00:11:20,626
下面你有一个sigmoid置信网络，

181
00:11:20,626 --> 00:11:23,060
而它在很多年前就已经被发明了。

182
00:11:23,060 --> 00:11:24,620
所以它是一个定向模型,

183
00:11:24,620 --> 00:11:28,651
我们发现训练这些受限玻尔兹曼机的

184
00:11:28,651 --> 00:11:32,760
是在Sigmoid置信网络中进行后续推理的非常有效的方式。

185
00:11:33,830 --> 00:11:36,870
所以,在那个时候,

186
00:11:36,870 --> 00:11:41,270
有人做神经网络，他会使用密集连接网络,但

187
00:11:41,270 --> 00:11:45,500
没有任何好的方法来做概率标记。

188
00:11:45,500 --> 00:11:50,050
你要让人做图形模型,不像我的孩子

189
00:11:50,050 --> 00:11:55,603
他们能正确地做推断，但只会在疏用连接网络。

190
00:11:55,603 --> 00:12:01,140
我们设法展示的是学习这些深度置信网络

191
00:12:01,140 --> 00:12:06,280
是个非常快的推理近似形的方法。

192
00:12:06,280 --> 00:12:10,578
它只提交单一的前向路径，这是一种非常漂亮的结果。

193
00:12:10,578 --> 00:12:14,890
你可以保证,每次你学习额外层的特征

194
00:12:16,010 --> 00:12:19,980
每次你学一个新层,你会得到了新的带,

195
00:12:19,980 --> 00:12:22,700
新的带总会比旧的带好。

196
00:12:22,700 --> 00:12:25,810
在你添加添加新网络层时会显示的那种变形带。

197
00:12:25,810 --> 00:12:26,970
是的,我记得那个视频。

198
00:12:26,970 --> 00:12:29,680
所以这是第二件然我非常兴奋的是。

199
00:12:29,680 --> 00:12:35,600
我想第三件事就是我在变分方法上所做的工作。

200
00:12:35,600 --> 00:12:40,750
实际上，搞统计学的人以前就做过类似的工作，

201
00:12:40,750 --> 00:12:43,100
但我们之前并不知道。

202
00:12:44,610 --> 00:12:47,260
我们使得EM（Expectation–maximization）算法的效果得到很大的提升，

203
00:12:47,260 --> 00:12:50,250
却是通过证明你不需要在步骤 E（Expectation）的时候做得很好。

204
00:12:50,250 --> 00:12:52,800
你可以做一个近似的步骤 E（Expectation）。

205
00:12:52,800 --> 00:12:55,320
在统计学里，EM是一个很重要的算法。

206
00:12:55,320 --> 00:12:58,380
我们展示了对EM的一个很大的拓展。

207
00:12:58,380 --> 00:13:02,490
尤其是在大概1993年，和 Van Camp（Drew Van Camp）一起，

208
00:13:02,490 --> 00:13:07,040
我写了一篇论文，我想是第一篇变分贝叶斯论文。

209
00:13:07,040 --> 00:13:12,090
在那篇文章里我们展示了你实际上做一种贝叶斯学习，

210
00:13:12,090 --> 00:13:17,950
通过用 Gaussian 来逼近真实后验概率，从而使它更易于处理。

211
00:13:17,950 --> 00:13:20,320
你可以用神经网络来实现这个方法。

212
00:13:20,320 --> 00:13:22,600
我对此非常兴奋。

213
00:13:22,600 --> 00:13:23,680
好的，我明白了。

214
00:13:23,680 --> 00:13:26,670
是的，我感觉我记得所有这些论文。

215
00:13:26,670 --> 00:13:32,630
"Neal 和 Hinton" 那篇论文，逼近EM那篇，我花了好几个小时来读。

216
00:13:32,630 --> 00:13:36,070
我想你现在使用的一些算法，

217
00:13:36,070 --> 00:13:41,110
很多人几乎每天都使用的一些算法，

218
00:13:41,110 --> 00:13:46,570
像舍弃（Dropout）或 ReLu 激活，这些都是来自你的团队的吗？

219
00:13:46,570 --> 00:13:47,390
说是也是，说不是也不是。

220
00:13:47,390 --> 00:13:51,470
其他人也想过矫正线性单元（ReLu）。

221
00:13:51,470 --> 00:13:56,860
我们做了一些关于与受限玻尔兹曼机工作。

222
00:13:56,860 --> 00:14:02,880
这些工作显示 ReLU 几乎完全等同于一堆逻辑单元。

223
00:14:02,880 --> 00:14:05,190
这也是使得 ReLU 流行起来的原因之一。

224
00:14:05,190 --> 00:14:07,440
我对这个非常好奇。

225
00:14:07,440 --> 00:14:12,570
ReLU 这篇论文有很多的数学公式，从而证明这个（ReLU）函数
可以通过一个复杂的公式来近似。

226
00:14:12,570 --> 00:14:15,530
ReLU 这篇论文有很多的数学公式，从而证明这个（ReLU）函数
可以通过一个复杂的公式来近似。

227
00:14:15,530 --> 00:14:19,140
你写了很多的数学公式是为了能让你的论文在学术会议上被接收
还是因为这些数学上的东西确实影响了 max{0,x} 这个函数的由来。

228
00:14:19,140 --> 00:14:24,840
你写了很多的数学公式是为了能让你的论文在学术会议上被接收
还是因为这些数学上的东西确实影响了 max{0,x} 这个函数的由来。

229
00:14:26,450 --> 00:14:30,440
这其实是一个典型的例子。在这里数学对这个想法的发展起到了很大的作用。

230
00:14:30,440 --> 00:14:32,350
这其实是一个典型的例子。在这里数学对这个想法的发展起到了很大的作用。

231
00:14:32,350 --> 00:14:35,262
我之前就知道修正线性单元这个想法，我也知道逻辑单元。

232
00:14:35,262 --> 00:14:36,821
我也知道逻辑单元。

233
00:14:36,821 --> 00:14:39,250
基于在玻尔兹曼机的研究工作，所有的基本工作都是由逻辑单元完成的。

234
00:14:39,250 --> 00:14:42,720
基于在玻尔兹曼机的研究工作，所有的基本工作都是由逻辑单元完成的。

235
00:14:42,720 --> 00:14:45,120
所以问题是，学习算法是否能在含有修正线性单元（ReLU）的某种东西上工作？

236
00:14:45,120 --> 00:14:49,070
所以问题是，学习算法是否能在含有修正线性单元（ReLU）的某种东西上工作？

237
00:14:49,070 --> 00:14:54,400
通过证明 ReLU 几乎完全等价于一堆逻辑单元，我们展示出其他数学上的公式都成立。

238
00:14:54,400 --> 00:15:00,350
通过证明 ReLU 几乎完全等价于一堆逻辑单元，我们展示出其他数学上的公式都成立。

239
00:15:00,350 --> 00:15:01,508
— 我明白了

240
00:15:01,508 --> 00:15:05,890
这这想法启发了现在的很多人，很多人都在使用 ReLU。

241
00:15:05,890 --> 00:15:08,000
而且它确实管用，尽管人们不需要了解当时提出这个想法的动机。

242
00:15:08,000 --> 00:15:12,130
而且它确实管用，尽管人们不需要了解当时提出这个想法的动机。

243
00:15:13,150 --> 00:15:16,850
是的。后来，当我去谷歌的时候，我后来发现一件事。

244
00:15:16,850 --> 00:15:22,796
在2014年，我给 Google 做了一个演讲，关于如何使用 ReLU 和用单位矩阵初始化。

245
00:15:22,796 --> 00:15:26,660
在2014年，我给 Google 做了一个演讲，关于如何使用 ReLU 和用单位矩阵初始化。

246
00:15:26,660 --> 00:15:30,300
因为 ReLU 的好处是如果你反复地复制隐藏单元，

247
00:15:30,300 --> 00:15:32,667
并且用单位阵来初始化，

248
00:15:32,667 --> 00:15:35,050
每一层就会简单地复制它下面那一层的模式。

249
00:15:36,140 --> 00:15:40,120
那篇文章里我写到，你可以训练一个有300个隐藏层的网络，
并且如果用单位阵来初始化，你可以很快地训练它。

250
00:15:40,120 --> 00:15:44,760
那篇文章里我写到，你可以训练一个有300个隐藏层的网络，
并且如果用单位阵来初始化，你可以很快地训练它。

251
00:15:44,760 --> 00:15:48,065
但我当时没有做跟进的研究，我很后悔没有继续。

252
00:15:48,065 --> 00:15:52,507
我们和 Quoc Le 一起发表了一篇文章，
文章里写到你可以通过像 ReLU 一样的来初始化一个循环神经网络（Recurrent nets）。

253
00:15:52,507 --> 00:15:55,565
我们和 Quoc Le 一起发表了一篇文章，
文章里写到你可以通过像 ReLU 一样的来初始化一个循环神经网络（Recurrent nets）。

254
00:15:55,565 --> 00:16:00,370
但我应该进一步跟进，因为之后发表的那些残差网络其实就是类似的这些东西（ReLU）。

255
00:16:00,370 --> 00:16:03,572
但我应该进一步跟进，因为之后发表的那些残差网络其实就是类似的这些东西（ReLU）。

256
00:16:03,572 --> 00:16:06,660
这些年来我听你讲了很多关于大脑的事。

257
00:16:06,660 --> 00:16:09,447
我听到你讲反向传播和大脑的关系。

258
00:16:09,447 --> 00:16:13,720
你现在对于这个问题的想法是什么？

259
00:16:13,720 --> 00:16:16,910
我现在正在写一个关于这方面的论文。

260
00:16:18,250 --> 00:16:21,160
我主要的想法是这样的。

261
00:16:21,160 --> 00:16:25,570
如果反向传播被证明是一个很好的用来学习的算法，

262
00:16:26,620 --> 00:16:31,610
那么，当然（自然）进化可能已经知道如何实现了。

263
00:16:32,730 --> 00:16:37,270
我的意思是，你有那些可以变成眼球或牙齿的细胞。

264
00:16:37,270 --> 00:16:42,440
如果细胞可以做到这些（变成眼球或牙齿），
那么当然他们可以实现反向传播。

265
00:16:42,440 --> 00:16:45,860
想必做这个选择（眼球还是牙齿）的压力一定很大。

266
00:16:45,860 --> 00:16:50,490
一些神经学家认为这个想法似是而非，我觉得他们的想法才愚蠢。

267
00:16:50,490 --> 00:16:52,890
这里面可能有一些很微妙的方法来实现。

268
00:16:52,890 --> 00:16:56,000
我认为大脑里可能存在一些很接近反向传播而不是反向传播的东西。

269
00:16:56,000 --> 00:16:58,620
我认为大脑里可能存在一些很接近反向传播而不是反向传播的东西。

270
00:16:58,620 --> 00:17:02,566
这些年来，我有许多想法来解释这是如何工作的。

271
00:17:02,566 --> 00:17:06,994
在1987年，当我和 Jay McClelland 合作的时候，

272
00:17:06,994 --> 00:17:11,202
我提出了再循环算法。

273
00:17:11,202 --> 00:17:16,090
主要的想法是这样，你在一个循环里发送信息。

274
00:17:17,470 --> 00:17:18,686
你试着让信息在这个循环里传送的时候不发生改变。

275
00:17:18,686 --> 00:17:22,206
你试着让信息在这个循环里传送的时候不发生改变。

276
00:17:22,206 --> 00:17:26,490
一个最简单的版本是假设你有输入单元和隐藏单元，

277
00:17:26,490 --> 00:17:31,046
你将信息从输入（单元）发送到隐藏（单元），然后（信息）返回到输入，
然后回到隐藏（单元）然后再回到输入，以此类推。

278
00:17:31,046 --> 00:17:34,388
你将信息从输入（单元）发送到隐藏（单元），然后（信息）返回到输入，
然后回到隐藏（单元）然后再回到输入，以此类推。

279
00:17:34,388 --> 00:17:38,001
你想要的是训练一个自动编码器，

280
00:17:38,001 --> 00:17:42,300
但你不想通过反向传播来训练它。

281
00:17:42,300 --> 00:17:47,250
因此，你训练它使得它去除所有活动的变化。

282
00:17:47,250 --> 00:17:51,922
所以突触的学习规则是把权重改成<br />正比于突触前输入并且正比于突触后输入的该变量。

283
00:17:51,922 --> 00:17:57,930
所以突触的学习规则是把权重改成<br />正比于突触前输入并且正比于突触后输入的该变量。

284
00:17:57,930 --> 00:18:01,780
所以突触的学习规则是把权重改成<br />正比于突触前输入并且正比于突触后输入的该变量。

285
00:18:01,780 --> 00:18:04,060
但在循环算法里，你试图使突触后输入变得更好，而突出前输入变得更差

286
00:18:04,060 --> 00:18:08,330
但在循环算法里，你试图使突触后输入变得更好，而突出前输入变得更差

287
00:18:08,330 --> 00:18:09,620
所以你朝着这个方向改变。

288
00:18:11,010 --> 00:18:14,472
在神经学家提出尖峰时间相关的可塑性（spike-timing-dependent plasticity）之前，<br />我们就发明这个算法。

289
00:18:14,472 --> 00:18:16,521
在神经学家提出尖峰时间相关的可塑性（spike-timing-dependent plasticity）之前，<br /> 我们就发明这个算法。

290
00:18:16,521 --> 00:18:20,700
尖峰时间相关的可塑性实际上是相同的算法，只不过颠倒了一下，<br />在学习规则里新的东西是好的而老的东西是不好的。

291
00:18:20,700 --> 00:18:26,220
尖峰时间相关的可塑性实际上是相同的算法，只不过颠倒了一下，<br />在学习规则里新的东西是好的而老的东西是不好的。

292
00:18:26,220 --> 00:18:30,010
所以你会权重比例改为正比于突触前的活动乘以新的突触后活动减去老的。

293
00:18:30,010 --> 00:18:35,690
所以你会权重比例改为正比于突触前的活动乘以新的突触后活动减去老的。

294
00:18:37,060 --> 00:18:42,020
后来，在2007年我意识到，如果你训练一堆受限玻尔兹曼机，

295
00:18:42,020 --> 00:18:47,830
后来，在2007年我意识到，如果你训练一堆受限玻尔兹曼机，

296
00:18:47,830 --> 00:18:52,620
经过训练，你会得到刚好正确的条件来实现反向传播，只需要重建一下（输入）。

297
00:18:52,620 --> 00:18:56,450
经过训练，你会得到刚好正确的条件来实现反向传播，只需要重建一下（输入）。

298
00:18:56,450 --> 00:19:01,124
如果你看一下重建误差，重建误差会告诉你判别结果的梯度。

299
00:19:01,124 --> 00:19:05,728
如果你看一下重建误差，重建误差会告诉你判别结果的梯度。

300
00:19:05,728 --> 00:19:12,079
2007年，在 NIPS 的第一次深度学习研讨会上，我做了一个相关的演讲。

301
00:19:12,079 --> 00:19:16,454
这个演讲几乎完全被大家忽略了。

302
00:19:16,454 --> 00:19:19,799
后来，Yoshua Bengio 把这个想法捡了起来。

303
00:19:19,799 --> 00:19:24,340
在这方面，他又做了相当多的工作。

304
00:19:24,340 --> 00:19:26,490
我自己也做了更多的工作。

305
00:19:26,490 --> 00:19:33,280
有这么一个想法，如果你有一堆自动编码器，
你可以通过把当前的活动反向传播并且观察重建误差来得到梯度。

306
00:19:33,280 --> 00:19:38,440
有这么一个想法，如果你有一堆自动编码器，
你可以通过把当前的活动反向传播并且观察重建误差来得到梯度。

307
00:19:38,440 --> 00:19:42,520
我认为这是一个非常有趣的想法。大脑很可能就是这么做的。

308
00:19:42,520 --> 00:19:47,520
还有一个的题目，我知道你研究了很多而且现在还在研究，<br />就是如何在深度学习中处理多时标（Multiple time-scale）问题。

309
00:19:47,520 --> 00:19:51,930
还有一个的题目，我知道你研究了很多而且现在还在研究，<br />就是如何在深度学习中处理多时标（Multiple time-scale）问题。

310
00:19:51,930 --> 00:19:54,468
你能分享一下你的想法吗？

311
00:19:54,468 --> 00:19:58,910
好的。这可以追溯到我读研究生的前几年。

312
00:19:58,910 --> 00:20:04,040
我第一次讲座是关于所谓的快速权重。

313
00:20:04,040 --> 00:20:07,560
那种能迅速适应，但也会迅速衰减的权重。

314
00:20:07,560 --> 00:20:08,832
因此这些权重可以保持短期记忆。

315
00:20:08,832 --> 00:20:13,496
在1973年我展示了一个非常简单的系统。

316
00:20:13,496 --> 00:20:16,590
（在那个系统里）你可以用这些权重做真递归（True recursion）。

317
00:20:16,590 --> 00:20:23,010
真递归的意思是那些被用来表示事物的神经元会被重新利用来表示递归核里的事物。

318
00:20:23,010 --> 00:20:28,470
真递归的意思是那些被用来表示事物的神经元会被重新利用来表示递归核里的事物。

319
00:20:30,210 --> 00:20:31,750
那些用来表示知识的权重被重新在递归核里利用。

320
00:20:31,750 --> 00:20:34,388
那些用来表示知识的权重被重新在递归核里利用。

321
00:20:34,388 --> 00:20:39,170
这就引出了一个问题：当你输出递归核的时候，你如何能记得在这个过程中它们是什么样子的？

322
00:20:39,170 --> 00:20:41,600
这就引出了一个问题：当你输出递归核的时候，你如何能记得在这个过程中它们是什么样子的？

323
00:20:41,600 --> 00:20:42,970
这些记忆存在哪里？

324
00:20:42,970 --> 00:20:45,015
因为你把神经元用于递归核。

325
00:20:46,080 --> 00:20:49,240
答案是你可以把记忆放进快速权重里，然后通过它们还原神经元的活动状态。

326
00:20:49,240 --> 00:20:53,940
答案是你可以把记忆放进快速权重里，然后通过它们还原神经元的活动状态。

327
00:20:53,940 --> 00:20:56,151
更近的一段时间，我和 Jimmy Ba 在合作，

328
00:20:56,151 --> 00:21:00,141
我们写了一篇 NIPS 的文章，在文章里我们把快速权重用在了递归上。

329
00:21:00,141 --> 00:21:00,898
— 我明白了

330
00:21:00,898 --> 00:21:04,145
这中间隔了很长时间。

331
00:21:04,145 --> 00:21:08,746
第一个模型在 1973 年未发布（完成但未公开发表），

332
00:21:08,746 --> 00:21:14,966
Jimmy Ba 的模型是在 2015 年或 2016 年（提出的），

333
00:21:14,966 --> 00:21:16,469
这大概是 40 年之后了。

334
00:21:16,469 --> 00:21:22,840
而且，还有一个我听你说了好几年的一个想法，

335
00:21:22,840 --> 00:21:29,350
大概有五年了，那就是“胶囊（网络）”这个想法。这个项目进展得怎么样了？

336
00:21:29,350 --> 00:21:34,150
我又回到了我之前的一个阶段，那就是我坚信我的一个想法，而其他人都不相信。

337
00:21:34,150 --> 00:21:39,320
我又回到了我之前的一个阶段，那就是我坚信我的一个想法，而其他人都不相信。

338
00:21:39,320 --> 00:21:42,120
我投过相关的论文，（但）它们都被拒收了。

339
00:21:42,120 --> 00:21:45,938
但我非常相信这个想法，我会继续做下去。

340
00:21:45,938 --> 00:21:53,880
它（胶囊网络）有几个主要的想法。

341
00:21:53,880 --> 00:22:00,000
一个是如何表示多维的实体。

342
00:22:00,000 --> 00:22:05,070
你可以通过一些 *** 活动来表示多维实体，只要你知道它们中的任何一个（活动）。

343
00:22:05,070 --> 00:22:07,630
你可以通过一些 *** 活动来表示多维实体，只要你知道它们中的任何一个（活动）。

344
00:22:07,630 --> 00:22:12,150
这个想法是，在图像的每个区域里，你假设某一种特征只存在一个。

345
00:22:12,150 --> 00:22:14,000
这个想法是，在图像的每个区域里，你假设某一种特征只存在一个。

346
00:22:15,200 --> 00:22:18,020
然后你用一堆神经元和他们的活动来表示这个特征的不同方面。

347
00:22:18,020 --> 00:22:23,190
然后你用一堆神经元和他们的活动来表示这个特征的不同方面。

348
00:22:24,230 --> 00:22:27,270
比如在一个区域里，那个特征的 x 和 y 坐标是什么，

349
00:22:27,270 --> 00:22:28,780
它的方向是什么样？

350
00:22:28,780 --> 00:22:29,930
它移动的速度有多快？

351
00:22:29,930 --> 00:22:30,630
它是什么颜色的？

352
00:22:30,630 --> 00:22:31,270
它有多亮？

353
00:22:31,270 --> 00:22:32,590
诸如此类。

354
00:22:32,590 --> 00:22:36,350
所以你可以用一堆神经元来代表同一个事物不同维度（属性）。

355
00:22:36,350 --> 00:22:37,710
是一样的。

356
00:22:37,710 --> 00:22:39,410
但前提是，特征的某一个属性只（在这个区域里）存在一个。

357
00:22:40,490 --> 00:22:46,110
同平时我们用神经网络的用法不同，这是一个非常不同的来研究表征的方法。

358
00:22:46,110 --> 00:22:48,155
同平时我们用神经网络的用法不同，这是一个非常不同的来研究表征的方法。

359
00:22:48,155 --> 00:22:49,820
通常在神经网络里，我们只有一个很大的层，每个神经元做着他们自己的事情。

360
00:22:49,820 --> 00:22:52,080
通常在神经网络里，我们只有一个很大的层，每个神经元做着他们自己的事情。

361
00:22:52,080 --> 00:22:55,770
而不是把他们捆绑在一起，形成小的团队，来表示同一个事物的不同方面。

362
00:22:55,770 --> 00:22:57,310
而不是把他们捆绑在一起，形成小的团队，来表示同一个事物的不同方面。

363
00:22:58,660 --> 00:23:02,080
所以我认为这样一个额外的结构应该是存在的。

364
00:23:02,080 --> 00:23:05,020
还有一些从这个引申出的其他的想法。

365
00:23:05,020 --> 00:23:07,410
所以这意味着在分布表征里，你把这个表征拆分成更小的部分？

366
00:23:07,410 --> 00:23:09,280
所以这意味着在分布表征里，你把这个表征拆分成更小的部分？

367
00:23:09,280 --> 00:23:11,270
所以这意味着在分布表征里，你把这个表征拆分成更小的部分？

368
00:23:11,270 --> 00:23:13,900
是的。

369
00:23:13,900 --> 00:23:15,600
我把每个小部分称为胶囊。

370
00:23:15,600 --> 00:23:16,180
— 我明白了

371
00:23:16,180 --> 00:23:21,078
这个想法就是一个胶囊代表一个特征的，而且只有一个。

372
00:23:21,078 --> 00:23:21,794
这个想法就是一个胶囊代表一个特征的，而且只有一个。

373
00:23:21,794 --> 00:23:27,130
这个胶囊可以表示这个特征不同的性质。

374
00:23:27,130 --> 00:23:29,880
这是一个有很多属性的特征，不同意一个普通的神经网络里的神经元，只有一个数值属性。

375
00:23:29,880 --> 00:23:34,530
这是一个有很多属性的特征，不同意一个普通的神经网络里的神经元，只有一个数值属性。

376
00:23:34,530 --> 00:23:36,240
对，我明白了。

377
00:23:36,240 --> 00:23:41,423
如果你有一个这样的神经网络，你可以做一些普通的神经网络不擅长的事情。

378
00:23:41,423 --> 00:23:48,980
如果你有一个这样的神经网络，你可以做一些普通的神经网络不擅长的事情。<br />你可以做一个我叫作 routing-by-agreement 的事情。

379
00:23:48,980 --> 00:23:52,960
假设你想做分割。你有一个可能是嘴巴的东西，还有一个可能是鼻子的东西。

380
00:23:52,960 --> 00:23:56,660
假设你想做分割。你有一个可能是嘴巴的东西，还有一个可能是鼻子的东西。

381
00:23:57,910 --> 00:24:02,179
你想知道你是否应该把他们放在一起组成一个事物。

382
00:24:02,179 --> 00:24:03,879
这个想法是有一个针对嘴巴的胶囊，那个胶囊有针对嘴巴的参数。

383
00:24:03,879 --> 00:24:06,040
这个想法是有一个针对嘴巴的胶囊，那个胶囊有针对嘴巴的参数。

384
00:24:06,040 --> 00:24:10,582
并且你还有一个针对鼻子的胶囊，那个胶囊有针对鼻子的参数。

385
00:24:10,582 --> 00:24:13,797
为了决定是否把它们（嘴巴和鼻子）放在一起，<br />你会让它们（胶囊）各自投票来决定针对一张脸的参数应该是什么。

386
00:24:13,797 --> 00:24:18,670
为了决定是否把它们（嘴巴和鼻子）放在一起，<br />你会让它们（胶囊）各自投票来决定针对一张脸的参数应该是什么。

387
00:24:19,930 --> 00:24:23,718
如果鼻子和嘴巴（的胶囊）有着正确的空间关系，它们的选择会是一样。

388
00:24:23,718 --> 00:24:24,725
如果鼻子和嘴巴（的胶囊）有着正确的空间关系，它们的选择会是一样。

389
00:24:24,725 --> 00:24:28,888
当你有两个在同一层的胶囊都选择了同一批参数来往上一层传输，<br />那么你可以假设它们很可能是对的。

390
00:24:28,888 --> 00:24:32,106
当你有两个在同一层的胶囊都选择了同一批参数来往上一层传输，<br />那么你可以假设它们很可能是对的。

391
00:24:32,106 --> 00:24:35,350
这是因为在高维空间里能出现一致（的选择）是很少见的。

392
00:24:36,950 --> 00:24:42,109
这与我们在普通的神经网络里做滤波是很不同的。

393
00:24:42,109 --> 00:24:46,130
这与我们在普通的神经网络里做滤波是很不同的。

394
00:24:46,130 --> 00:24:50,708
因此，我认为这个 routing-by-agreement <br />对于提高神经网络对有限数据的泛化能力来说是很重要的。

395
00:24:50,708 --> 00:24:56,700
因此，我认为这个 routing-by-agreement <br />对于提高神经网络对有限数据的泛化能力来说是很重要的。

396
00:24:56,700 --> 00:24:59,797
我认为它（胶囊）会很擅长视角变化和分割。

397
00:24:59,797 --> 00:25:01,500
我认为它（胶囊）会很擅长视角变化和分割。

398
00:25:01,500 --> 00:25:04,794
我觉得它比我们现在在神经网络上的工作要更高效。

399
00:25:04,794 --> 00:25:06,147
我觉得它比我们现在在神经网络上的工作要更高效。

400
00:25:06,147 --> 00:25:08,575
现在你如果想处理视角变化的问题，你做的不过是人为地改变视角，<br />然后让神经网络在各个视角上训练。

401
00:25:08,575 --> 00:25:12,000
现在你如果想处理视角变化的问题，你做的不过是人为地改变视角，<br />然后让神经网络在个个视角上训练。

402
00:25:12,000 --> 00:25:16,460
我明白了。与前向学习（feed-forward learning）和监督学习不同，你可以同过这种不一样的方式来学习。

403
00:25:16,460 --> 00:25:19,120
我明白了。与前向学习（feed-forward learning）和监督学习不同，你可以同过这种不一样的方式来学习。

404
00:25:20,220 --> 00:25:24,120
其实我还是打算用监督学习的方式来做，但是前向传播（forward pass）的方式大有不同。

405
00:25:24,120 --> 00:25:27,720
其实我还是打算用监督学习的方式来做，但是前向传播（forward pass）的方式大有不同。

406
00:25:27,720 --> 00:25:32,010
这不是一个纯粹的前向传播，因为这里面涉及到一些循环。

407
00:25:32,010 --> 00:25:36,550
你认为你发现了一个嘴巴和一个鼻子，然后你做一些迭代来决定它们是否应该合起来组成一张脸。

408
00:25:36,550 --> 00:25:39,127
你认为你发现了一个嘴巴和一个鼻子，然后你做一些迭代来决定它们是否应该合起来组成一张脸。

409
00:25:39,127 --> 00:25:42,530
你认为你发现了一个嘴巴和一个鼻子，然后你做一些迭代来决定它们是否应该合起来组成一张脸。

410
00:25:42,530 --> 00:25:46,352
你可以从那个迭代中做反向传播。

411
00:25:46,352 --> 00:25:50,286
所以你可以尝试着把它用在判别上。

412
00:25:50,286 --> 00:25:54,417
我们在多伦多的组就在研究这个问题。

413
00:25:54,417 --> 00:26:00,260
我在多伦多有一个小的谷歌团队，是大脑团队（Google Brain）的一部分。

414
00:26:00,260 --> 00:26:02,127
这就是我现在所兴奋的。

415
00:26:02,127 --> 00:26:02,891
我明白了，很好。

416
00:26:02,891 --> 00:26:05,366
期待那篇论文。

417
00:26:05,366 --> 00:26:10,750
是的，如果它能发表。

418
00:26:10,750 --> 00:26:13,040
你在深度学习这个领域工作了几十年。

419
00:26:13,040 --> 00:26:15,330
我很好奇你对于人工智能的想法和理解在这些年中是如何改变的？

420
00:26:15,330 --> 00:26:18,760
我很好奇你对于人工智能的想法和理解在这些年中是如何改变的？

421
00:26:20,380 --> 00:26:27,678
我想我过去很多工作都是围绕反向传播的，如何利用反向传播。

422
00:26:27,678 --> 00:26:33,531
我想我过去很多工作都是围绕反向传播的，如何利用反向传播。

423
00:26:33,531 --> 00:26:36,966
首先，在80年代中期，我们用它来做判别学习，并且效果很好。

424
00:26:36,966 --> 00:26:40,203
首先，在80年代中期，我们用它来做判别学习，并且效果很好。

425
00:26:40,203 --> 00:26:42,405
然后我在90年代初觉得实际上多数的人类学习是无监督学习

426
00:26:42,405 --> 00:26:46,749
然后我在90年代初觉得实际上多数的人类学习是无监督学习

427
00:26:46,749 --> 00:26:50,138
后来我就对无监督学习产生了更大的兴趣，也就是那个时候我在研究 wake-sleep 算法。

428
00:26:50,138 --> 00:26:54,300
后来我就对无监督学习产生了更大的兴趣，也就是那个时候我在研究 wake-sleep 算法。

429
00:26:54,300 --> 00:26:58,306
你当时的评论也影响了我的想法。

430
00:26:58,306 --> 00:27:03,010
当我领导谷歌大脑，我们的第一个项目花了很多功夫在无监督学习上，正式因为你的影响。

431
00:27:03,010 --> 00:27:07,900
当我领导谷歌大脑，我们的第一个项目花了很多功夫在无监督学习上，正式因为你的影响。

432
00:27:07,900 --> 00:27:09,740
是的，但我也可能误导了你。

433
00:27:09,740 --> 00:27:11,470
但从长远来看，我认为无监督学习将是非常重要的。

434
00:27:11,470 --> 00:27:13,840
但从长远来看，我认为无监督学习将是非常重要的。

435
00:27:15,160 --> 00:27:19,376
但你必须要面对现实。

436
00:27:19,376 --> 00:27:24,107
过去十年里，好用的是监督学习。

437
00:27:24,107 --> 00:27:27,179
像判别训练，其中你有标签，或者你想预测一系列事件中的下一件，那件事也可以当做标签。

438
00:27:27,179 --> 00:27:31,810
像判别训练，其中你有标签，或者你想预测一系列事件中的下一件，那件事也可以当做标签。

439
00:27:31,810 --> 00:27:33,769
这个方向很成功。

440
00:27:37,528 --> 00:27:42,266
我仍然相信无监督学习将是至关重要的。

441
00:27:42,266 --> 00:27:47,145
当我们使得无监督学习运行合理的时候，无监督学习的效果会非常好。

442
00:27:47,145 --> 00:27:48,200
但我们还没有做到这点。

443
00:27:49,990 --> 00:27:53,225
是的。我觉得深度学习这个领域很多有资历的人，包括我自己，<br />仍然对它（非监督学习）很感兴趣。

444
00:27:53,225 --> 00:27:56,074
是的。我觉得深度学习这个领域很多有资历的人，包括我自己，<br />仍然对它（非监督学习）很感兴趣。

445
00:27:56,074 --> 00:28:01,513
只是我们谁也不知道怎么做。

446
00:28:01,513 --> 00:28:04,983
也许你知道，我不觉得我知道。

447
00:28:04,983 --> 00:28:08,160
变分自编码器用了重新参数化这个技巧，在我看来是一个不错的想法。

448
00:28:08,160 --> 00:28:10,120
变分自编码器用了重新参数化这个技巧，在我看来是一个不错的想法。

449
00:28:10,120 --> 00:28:15,260
生成对抗网络在我看来也是一个不错的想法。

450
00:28:15,260 --> 00:28:18,645
我认为生成对抗网络是深度学习领域里面很新的很大的想法之一。

451
00:28:18,645 --> 00:28:23,430
我认为生成对抗网络是深度学习领域里面很新的很大的想法之一。

452
00:28:23,430 --> 00:28:26,363
我希望我能使胶囊（网络）成功，但是目前生成对抗网络是一个很大的突破。

453
00:28:26,363 --> 00:28:31,740
我希望我能使胶囊（网络）成功，但是目前生成对抗网络是一个很大的突破。

454
00:28:31,740 --> 00:28:34,439
稀疏性和满特征是另外两个无监督学习的方法，它们现在怎么样了？

455
00:28:34,439 --> 00:28:38,806
稀疏性和满特征是另外两个无监督学习的方法，它们现在怎么样了？

456
00:28:41,556 --> 00:28:47,788
我从来没有像你这么看重稀疏性。

457
00:28:47,788 --> 00:28:52,672
缓慢特性，我认为是一个错误，你不应该叫它”缓慢”。

458
00:28:52,672 --> 00:28:53,660
缓慢特性，我认为是一个错误，你不应该叫它”缓慢”。

459
00:28:53,660 --> 00:28:57,880
基本的想法是对的，但你不应该去找那些不发生变化的特征，你应该去找那些有规律变化的特征。

460
00:28:57,880 --> 00:29:00,660
基本的想法是对的，但你不应该去找那些不发生变化的特征，你应该去找那些有规律变化的特征。

461
00:29:01,680 --> 00:29:07,060
无论你把任何事物进行建模，这里都有一个基本原则。

462
00:29:08,620 --> 00:29:13,391
你把你的观测量进行非线性的变换，一直变到你得到一个状态矢量作为表征，它实际上就是线性的。

463
00:29:13,391 --> 00:29:17,612
你把你的观测量进行非线性的变换，一直变到你得到一个状态矢量作为表征，它实际上就是线性的。

464
00:29:17,612 --> 00:29:22,672
你把你的观测量进行非线性的变换，一直变到你得到一个状态矢量作为表征，它实际上就是线性的。

465
00:29:22,672 --> 00:29:26,103
所以你不只是假装它是线性的，就像你用 Kalman 滤波器一样。

466
00:29:26,103 --> 00:29:29,625
而且你去寻找一种变换，从测量值到隐藏的变量。从那之后，你可以做线性运算，比如矩阵乘法。

467
00:29:29,625 --> 00:29:32,616
而且你去寻找一种变换，从测量值到隐藏的变量。从那之后，你可以做线性运算，比如矩阵乘法。

468
00:29:32,616 --> 00:29:37,480
而且你去寻找一种变换，从测量值到隐藏的变量。从那之后，你可以做线性运算，比如矩阵乘法。

469
00:29:37,480 --> 00:29:39,700
比如你想改变视角。

470
00:29:39,700 --> 00:29:42,890
如果你要从另一个视角生成图片，你需要做的是从像素变到空间坐标。

471
00:29:42,890 --> 00:29:46,900
如果你要从另一个视角生成图片，你需要做的是从像素变到空间坐标。

472
00:29:47,950 --> 00:29:50,686
当你得到了坐标表征（这也是我希望胶囊网络能实现的功能），你就可以通过矩阵乘法来改变视角。

473
00:29:50,686 --> 00:29:54,120
当你得到了坐标表征（这也是我希望胶囊网络能实现的功能），你就可以通过矩阵乘法来改变视角。

474
00:29:54,120 --> 00:29:57,350
当你得到了坐标表征（这也是我希望胶囊网络能实现的功能），你就可以通过矩阵乘法来改变视角。

475
00:29:57,350 --> 00:29:59,210
然后你可以把坐标表征变回到像素。

476
00:29:59,210 --> 00:29:59,893
这就是你为什么这么做的原因。

477
00:29:59,893 --> 00:30:02,170
我认为这是一个非常非常普遍的原则。

478
00:30:02,170 --> 00:30:04,773
这就是为什么你做了那些人脸合成的工作，对吗？

479
00:30:04,773 --> 00:30:09,355
你把人脸压缩到一个低维的向量，这样你可以处理它，然后得到其他人脸。

480
00:30:09,355 --> 00:30:12,450
你把人脸压缩到一个低维的向量，这样你可以处理它，然后得到其他人脸。

481
00:30:12,450 --> 00:30:15,950
我之前有一个学生做过那方面的工作，我自己没有做太多。

482
00:30:17,100 --> 00:30:19,180
你一定经常被问到，如果一个人想开始研究深度学习，他应该怎么做。

483
00:30:19,180 --> 00:30:23,920
你一定经常被问到，如果一个人想开始研究深度学习，他应该怎么做。

484
00:30:23,920 --> 00:30:25,040
那你有什么建议？

485
00:30:25,040 --> 00:30:28,938
我相信你一定当面给了很多人建议。

486
00:30:28,938 --> 00:30:31,550
面对全世界正在看这个视频的观众，你对他们中想做深度学习的人有什么建议？

487
00:30:31,550 --> 00:30:35,999
面对全世界正在看这个视频的观众，你对他们中想做深度学习的人有什么建议？

488
00:30:35,999 --> 00:30:42,171
所以我的建议是要读文献，但不要读得太多。

489
00:30:42,171 --> 00:30:48,030
这是我从导师那里得到的建议，这与大多数人所说的不同。

490
00:30:48,030 --> 00:30:52,474
大多数人都说，你应该花几年阅读文献，然后开始做你自己的工作。

491
00:30:52,474 --> 00:30:55,421
大多数人都说，你应该花几年阅读文献，然后开始做你自己的工作。

492
00:30:55,421 --> 00:31:00,295
对于一些研究人员这可能是真的,但对于创造研究人员，

493
00:31:00,295 --> 00:31:03,803
你需要做的是读一些文献，然后注意到一些大家都做得不对的地方。

494
00:31:03,803 --> 00:31:07,792
你需要做的是读一些文献，然后注意到一些大家都做得不对的地方。

495
00:31:07,792 --> 00:31:10,340
这样说来，我是比较不合群的。

496
00:31:10,340 --> 00:31:13,568
当你看着这个地方的时候，你就是会觉得这个地方有什么不对。

497
00:31:13,568 --> 00:31:15,660
然后想办法用正确的方法去做。

498
00:31:16,890 --> 00:31:22,476
当然有人告诉你说这个方法不对的时候，继续做下去。

499
00:31:22,476 --> 00:31:26,339
我有一个很好的原则，来帮助人们坚持他们的想法。

500
00:31:26,339 --> 00:31:29,996
那就是，要么你的直觉是好的，要么不是。

501
00:31:29,996 --> 00:31:32,030
如果你的直觉是对的，你应该继续做下去，最终你会成功的。

502
00:31:32,030 --> 00:31:34,060
如果你的直觉是对的，你应该继续做下去，最终你会成功的。

503
00:31:34,060 --> 00:31:36,478
如果你的直觉不够好，那么你做什么都无所谓了。

504
00:31:36,478 --> 00:31:40,329
好的。

505
00:31:40,329 --> 00:31:43,420
非常鼓舞人心的建议，我也不妨这么去做。

506
00:31:43,420 --> 00:31:45,410
你也应该相信你的直觉。没有理由不信任你的直觉。

507
00:31:45,410 --> 00:31:47,847
你也应该相信你的直觉。没有理由不信任你的直觉。

508
00:31:47,847 --> 00:31:49,420
好的。

509
00:31:49,420 --> 00:31:55,193
我通常建议人们不只是读文献，而要重复已经发表的工作。

510
00:31:55,193 --> 00:31:58,161
也许这会让你知道你的极限在哪，因为复制别人的工作是很费时间的。

511
00:31:58,161 --> 00:32:00,800
也许这会让你知道你的极限在哪，因为复制别人的工作是很费时间的。

512
00:32:01,910 --> 00:32:05,312
是的。当你试图复制别人的工作，你可以发现那些有用的小窍门。

513
00:32:05,312 --> 00:32:08,100
是的。当你试图复制别人的工作，你可以发现那些有用的小窍门。

514
00:32:08,100 --> 00:32:11,938
我的另外一个建议是，永远不要停止编程。

515
00:32:11,938 --> 00:32:15,577
因为如果你给学生一些事情做，如果他们水平差，他们会回来说，这行不通。

516
00:32:15,577 --> 00:32:18,550
因为如果你给学生一些事情做，如果他们水平差，他们会回来说，这行不通。

517
00:32:18,550 --> 00:32:22,030
而这行不通的原因则是那个学生在一些很小的地方的失误，但他们并没有意识到这些小地方的重要性。

518
00:32:22,030 --> 00:32:25,100
而这行不通的原因则是那个学生在一些很小的地方的失误，但他们并没有意识到这些小地方的重要性。

519
00:32:25,100 --> 00:32:28,850
如果你把它给一个水平很高的学生，比如 A。

520
00:32:28,850 --> 00:32:31,120
你可以让他做任何东西，而他会回来说，这行得通。

521
00:32:32,670 --> 00:32:36,420
我记得这发生过一次。我当时说，慢着，A，自从上次我们讨论之后，<br />我意识到这个想法是行不通的，理由是等等等。

522
00:32:36,420 --> 00:32:37,330
我记得这发生过一次。我当时说，慢着，A，自从上次我们讨论之后，<br />我意识到这个想法是行不通的，理由是等等等。

523
00:32:37,330 --> 00:32:40,380
我记得这发生过一次。我当时说，慢着，A，自从上次我们讨论之后，<br />我意识到这个想法是行不通的，理由是等等等。

524
00:32:40,380 --> 00:32:43,586
A 马上就说，哦，是的，我也想到了，所以我以为你说的不是这件事儿。

525
00:32:43,586 --> 00:32:47,627
好的。

526
00:32:47,627 --> 00:32:51,575
对其那些想研究深度学习的人来说，还有其他的建议吗？

527
00:32:51,575 --> 00:32:57,782
对其那些想研究深度学习的人来说，还有其他的建议吗？

528
00:32:57,782 --> 00:33:02,000
大体上说就是，读足够的文献直到你开始产生自己的直觉。

529
00:33:02,000 --> 00:33:05,811
然后，相信你的直觉，然后做下去。

530
00:33:05,811 --> 00:33:10,783
即使别人都说这是无稽之谈，也不要担心。

531
00:33:10,783 --> 00:33:14,352
我想，当别人说这是无稽之谈的时候，你也不会知道这是对知错，直到你继续做下去，从而发现它（是对是错）。

532
00:33:14,352 --> 00:33:19,950
我想，当别人说这是无稽之谈的时候，你也不会知道这是对知错，直到你继续做下去，从而发现它（是对是错）。

533
00:33:19,950 --> 00:33:24,350
对，有一件事，如果你认为这是一个绝妙的主意，而其他人告诉你这彻彻底底是一个无稽之谈，

534
00:33:24,350 --> 00:33:27,201
对，有一件事，如果你认为这是一个绝妙的主意，而其他人告诉你这彻彻底底是一个无稽之谈，

535
00:33:27,201 --> 00:33:29,761
那你就明白其实你在做一件很大的事情。

536
00:33:29,761 --> 00:33:33,960
其中一个例子 Radford Neal 和我第一次提出了变分法（应用在神经网络上）。

537
00:33:35,420 --> 00:33:40,690
我写了一个邮件解释给我的一个名叫 Peter Brown的学生，他很懂EM（Expectation-Maximization）。

538
00:33:40,690 --> 00:33:42,560
我写了一个邮件解释给我的一个名叫 Peter Brown的学生，他很懂EM（Expectation-Maximization）。

539
00:33:43,570 --> 00:33:46,967
他给那些和他一起工作的人看（这封邮件），他们叫做 *** 兄弟。

540
00:33:46,967 --> 00:33:51,253
他给那些和他一起工作的人看（这封邮件），他们叫做 *** 兄弟。

541
00:33:51,253 --> 00:33:55,914
后来他告诉我他们说了什么，他们说，要么这家伙喝醉了，要么就是他很笨

542
00:33:55,914 --> 00:34:00,277
后来他告诉我他们说了什么，他们说，要么这家伙喝醉了，要么就是他很笨

543
00:34:00,277 --> 00:34:04,260
他们真的认为这是无稽之谈。

544
00:34:04,260 --> 00:34:06,460
这可能有一部分的原因是我的描述的方式（不对），因为我用了很直观的表述。

545
00:34:06,460 --> 00:34:08,043
这可能有一部分的原因是我的描述的方式（不对），因为我用了很直观的表述。

546
00:34:09,150 --> 00:34:13,100
当你有一个你认为很好但是其他人认为是无用的想法的时候，那就是一个很好的想法的前兆。

547
00:34:13,100 --> 00:34:16,810
当你有一个你认为很好但是其他人认为是无用的想法的时候，那就是一个很好的想法的前兆。

548
00:34:18,026 --> 00:34:21,555
好的，至于研究课题，新的研究生应该去研究胶囊和无监督学习，还有其他的吗？

549
00:34:21,555 --> 00:34:26,183
好的，至于研究课题，新的研究生应该去研究胶囊和无监督学习，还有其他的吗？

550
00:34:26,183 --> 00:34:30,707
好的，至于研究课题，新的研究生应该去研究胶囊和无监督学习，还有其他的吗？

551
00:34:30,707 --> 00:34:34,078
对新的研究说的建议就是，尝试去找一个和你有着相似的想法的导师。

552
00:34:34,078 --> 00:34:38,344
对新的研究说的建议就是，尝试去找一个和你有着相似的想法的导师。

553
00:34:38,344 --> 00:34:42,637
因为如果你的工作你的导师很认可的话，你会从你的导师那里得到很多好的建议，他们会花时间在你身上。

554
00:34:42,637 --> 00:34:47,170
因为如果你的工作你的导师很认可的话，你会从你的导师那里得到很多好的建议，他们会花时间在你身上。

555
00:34:47,170 --> 00:34:50,590
如果你的工作你的导师不感兴趣，你能得到一些建议，但并不会有多大帮助。

556
00:34:50,590 --> 00:34:55,262
如果你的工作你的导师不感兴趣，你能得到一些建议，但并不会有多大帮助。

557
00:34:55,262 --> 00:34:58,386
好的。最后一个是给学生的建议，

558
00:34:58,386 --> 00:35:02,440
你对于读博，或在顶级公司或顶级的研究团队工作有什么想法？

559
00:35:02,440 --> 00:35:09,687
你对于读博，或在顶级公司或顶级的研究团队工作有什么想法？

560
00:35:09,687 --> 00:35:13,890
好的，这很复杂。

561
00:35:13,890 --> 00:35:18,727
现在没有足够的有深度学习经验的老师去教授那些需要学习（深度学习）的大学生。

562
00:35:18,727 --> 00:35:21,125
现在没有足够的有深度学习经验的老师去教授那些需要学习（深度学习）的大学生。

563
00:35:21,125 --> 00:35:25,011
老师的数量不够多。

564
00:35:25,011 --> 00:35:27,780
我想这只是暂时的。

565
00:35:27,780 --> 00:35:32,410
大多数学校对现在所经历的变革了解很少。

566
00:35:32,410 --> 00:35:34,890
大多数学校对现在所经历的变革了解很少。

567
00:35:34,890 --> 00:35:38,720
我同意你的看法，这谈不上是第二次工业革命，但也有差不多的规模。

568
00:35:38,720 --> 00:35:41,000
我同意你的看法，这谈不上是第二次工业革命，但也有差不多的规模。

569
00:35:41,000 --> 00:35:43,691
我们在经历巨大的变化，基本上是因为我们与电脑的关系发生了变化。

570
00:35:43,691 --> 00:35:47,980
我们在经历巨大的变化，基本上是因为我们与电脑的关系发生了变化。

571
00:35:47,980 --> 00:35:53,920
我们现在不是编程让他们去做，而是给它们展示（数据），它们会自己去理解。

572
00:35:53,920 --> 00:35:56,570
这是一个完全不同的使用计算机的方式。

573
00:35:56,570 --> 00:36:01,210
计算机科学系是围绕计算机编程的思想而建立的。

574
00:36:01,210 --> 00:36:03,480
他们不理解，给计算机展示和给计算机输入指令是一样重要的。

575
00:36:05,000 --> 00:36:09,330
他们不理解，给计算机展示和给计算机输入指令是一样重要的。

576
00:36:09,330 --> 00:36:13,940
他们不明白，其实系里面一半的人应该去做展示给计算机这件事。

577
00:36:13,940 --> 00:36:16,510
他们不明白，其实系里面一半的人应该去做展示给计算机这件事。

578
00:36:16,510 --> 00:36:22,183
我所在的系就拒绝承认它应该有很多的教授来做这件事

579
00:36:22,183 --> 00:36:24,790
我所在的系就拒绝承认它应该有很多的教授来做这件事

580
00:36:24,790 --> 00:36:28,730
他们认为他们需要有一些，也许稍微多一点，但不要太多。

581
00:36:31,260 --> 00:36:32,452
在这种情况下，你必须提醒那些大公司去做更多的培训。

582
00:36:32,452 --> 00:36:36,510
在这种情况下，你必须提醒那些大公司去做更多的培训。

583
00:36:36,510 --> 00:36:40,335
所以 Google 现在正在训练人们，我们称之为"大脑居民"。

584
00:36:40,335 --> 00:36:43,792
我猜想大学最终会迎头赶上。

585
00:36:43,792 --> 00:36:48,360
好的，事实上很多学生都已经想通了。

586
00:36:48,360 --> 00:36:53,131
很多前顶级的博士项目，超过半数的申请者实际上是在研究“展示”而不是“编程”

587
00:36:53,131 --> 00:36:57,079
很多前顶级的博士项目，超过半数的申请者实际上是在研究“展示”而不是“编程”

588
00:36:57,079 --> 00:37:00,720
是的。值得提到的是，deeplearning.ai 正在计划一个深度学习的专栏。

589
00:37:00,720 --> 00:37:04,930
是的。值得提到的是，deeplearning.ai 正在计划一个深度学习的专栏。

590
00:37:04,930 --> 00:37:09,239
据我所知，他们的第一次深度学习 MOOC 实际上是你教的 （http://mooc.org/ 另一个网络课堂）

591
00:37:09,239 --> 00:37:11,752
在 Coursera，在2012年，也是你教的。

592
00:37:12,828 --> 00:37:14,430
有些奇怪（巧合）的是，那也是你发布的RMSprop算法的时候，虽然也很粗糙。

593
00:37:14,430 --> 00:37:18,900
有些奇怪（巧合）的是，那也是你发布的RMSprop算法的时候，虽然也很粗糙。

594
00:37:20,240 --> 00:37:25,910
对的。你也知道，那是因为你邀请我做 MOOC。

595
00:37:25,910 --> 00:37:30,239
当时我对非常怀疑MOOC，而你不停地催我。

596
00:37:30,239 --> 00:37:34,340
我最终做得很好，虽然它的工作量很大。

597
00:37:34,340 --> 00:37:37,409
谢谢你能做那些工作。我记得当时你向我抱怨有那么多的工作要做。你经常做到深夜。

598
00:37:37,409 --> 00:37:38,351
谢谢你能做那些工作。我记得当时你向我抱怨有那么多的工作要做。你经常做到深夜。

599
00:37:38,351 --> 00:37:42,413
但我想很多学生都从你的第一个 MOOC 课上收获了很多。所以我对此非常感激。

600
00:37:42,413 --> 00:37:47,330
但我想很多学生都从你的第一个 MOOC 课上收获了很多。所以我对此非常感激。

601
00:37:47,330 --> 00:37:49,260
这很好。

602
00:37:49,260 --> 00:37:53,290
这些年里，我见过你卷入对于人工智能范式的辩论中。

603
00:37:53,290 --> 00:37:57,030
对于人工智能的范式是否在发生变化，

604
00:37:57,030 --> 00:37:59,984
那么，你能分享你对这个问题的想法吗？

605
00:37:59,984 --> 00:38:05,157
当然。我认为在早期，50年代，

606
00:38:05,157 --> 00:38:10,335
像 Von Neumann 等一些人并不相信符号（Symbolic）人工智能

607
00:38:10,335 --> 00:38:14,220
他们更受大脑的启发。

608
00:38:14,220 --> 00:38:20,127
不幸的是，他们都英年早逝，所以人们并不知道他们的想法。

609
00:38:20,127 --> 00:38:21,806
在人工智能的早期阶段，

610
00:38:21,806 --> 00:38:26,259
人们完全相信实现智能所需要的表达方式，

611
00:38:26,259 --> 00:38:30,500
无非是某种形式的符号表达式。

612
00:38:30,500 --> 00:38:35,509
一种被梳理好的逻辑。通过这些逻辑你可以做非单调的事情。

613
00:38:35,509 --> 00:38:41,143
这些事情不是逻辑，但像逻辑。而智能的本质则是推理。

614
00:38:41,143 --> 00:38:45,662
如今，有一个完全不同的看法，

615
00:38:45,662 --> 00:38:50,984
这就是，思想其实是一个表示神经活动的很大的向量。

616
00:38:50,984 --> 00:38:55,200
这与一个思想是一个符号表达这个认识是不同的。

617
00:38:55,200 --> 00:38:59,087
我认为那些认为思想是符号表达的人犯了一个大错误。

618
00:38:59,087 --> 00:39:00,140
我认为那些认为思想是符号表达的人犯了一个大错误。

619
00:39:01,210 --> 00:39:07,030
输入是一连串的单词，而输出也是一连串的单词。

620
00:39:08,140 --> 00:39:12,580
因此，字符串显然是代表事物的方式。

621
00:39:12,580 --> 00:39:15,710
所以他们认为在输入和输出之间也一定是一个字符串，或是像字符串的东西。

622
00:39:15,710 --> 00:39:18,360
所以他们认为在输入和输出之间也一定是一个字符串，或是像字符串的东西。

623
00:39:18,360 --> 00:39:21,310
我认为之间的东西和字符串一点都不像。

624
00:39:21,310 --> 00:39:26,060
我认为这种“思想一定需要某种语言来表达“的想法
跟那些“对于空间的理解一定需要像素来表达”的想法一样愚蠢。

625
00:39:26,060 --> 00:39:30,980
我认为这种“思想一定需要某种语言来表达“的想法
跟那些“对于空间的理解一定需要像素来表达”的想法一样愚蠢。

626
00:39:30,980 --> 00:39:34,280
必须以像素为单位,像素进入。

627
00:39:34,280 --> 00:39:37,930
如果我们有一个点阵式打印机和人脑连接起来，

628
00:39:37,930 --> 00:39:41,929
然后像素就会被打印出来，但介于两者之间的并不是像素。

629
00:39:43,210 --> 00:39:46,620
所以我认为思想只是一些很大的向量。

630
00:39:46,620 --> 00:39:48,460
这些向量有一些导致其他结果的能量

631
00:39:48,460 --> 00:39:50,490
它们能诱导其他的向量。

632
00:39:50,490 --> 00:39:56,100
这完全不同于标准的人工智能观点，那就是思想是符号表达。

633
00:39:56,100 --> 00:39:56,700
我明白了。好的。

634
00:39:57,740 --> 00:40:01,560
我想，人工智能一定是在往这个新的观点发展。

635
00:40:01,560 --> 00:40:02,660
其中一些。

636
00:40:02,660 --> 00:40:08,230
我想很多在人工智能领域的人仍然认为思想一定是符号表达。

637
00:40:08,230 --> 00:40:09,780
非常感谢你来做这次采访。

638
00:40:09,780 --> 00:40:12,970
能了解到深度学习这些年的演变让人非常着迷。

639
00:40:12,970 --> 00:40:17,680
也了解到了你现在是如何帮助深度学习往前推进的。谢谢你，杰夫。

640
00:40:17,680 --> 00:40:19,038
谢谢你给我这个机会。

641
00:40:19,038 --> 00:40:20,147
— 谢谢你