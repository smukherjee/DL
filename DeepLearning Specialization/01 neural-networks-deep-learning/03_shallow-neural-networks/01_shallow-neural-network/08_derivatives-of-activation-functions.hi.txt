जब आप इम्प्लमेंट करते हैं वह बैक प्रॉपगेशन आपके 
न्यूरल नेटवर्क के लिए, आपको कर पाने चाहिए कम्प्यूट स्लोप या डेरिवेटिव्स ऐक्टिवेशन फ़ंक्शन्स के. तो चलो एक नज़र डालते हैं हमारे विकल्पों पर
 ऐक्टिवेशन फ़ंक्शन्स के, और कैसे आप कम्प्यूट कर सकते हैं इन फ़ंक्शनज़ की स्लोप. यहाँ है परिचित सिग्मोईड ऐक्टिवेशन फ़ंक्शन. और तो z की किसी वैल्यू के लिए, शायद z की 
इस वैल्यू के लिए, इस फ़ंक्शन की होगी कुछ स्लोप, या कुछ डेरिवेटिव, उसके अनुसार, 
यदि आप बनाते हैं एक छोटी लाइन यहाँ, ऊँचाई बटा 
चौड़ाई इस छोटे त्रिकोण की यहाँ. तो यदि g(z) है सिग्मोईड फ़ंक्शन, 
तब स्लोप फ़ंक्शन की है d/dz g(z). तो हम जानते हैं कैल्क्युलस से कि यह है स्लोप g(x) की z पर. और यदि आप परिचित हैं कैल्क्युलस से और 
जानते हैं कैसे लेने हैं डेरिवेटिव्स, यदि आप लेते हैं डेरिवेटिव सिग्मोईड फ़ंक्शन का, यह सम्भव है दिखा पाना कि यह बराबर है इस फ़ॉर्म्युला के. और फिर से, मैं नहीं कर रहा हूँ 
कैल्क्युलस साबित करने लिए इसे. लेकिन यदि आप परिचित है कैल्क्युलस से, बेझिझक रोकें इस वीडियो को और साबित करें इसे स्वयं. और इसलिए यह बराबर है सिर्फ़ g(z) गुणा (1-g(z)) के. तो चलो पहले देखें कि यह इक्स्प्रेशन समझ आता है. सबसे पहले, अगर z बहुत बड़ा है, मान लो z = 10. तब g (z) 1 के करीब होगा, और इसलिए फ़ॉर्म्युला जो हमारे पास है बाईं तरफ़ बताता है हमें 
कि d/dz g(z) होगा क़रीब g(z) के, जो बराबर है 1 गुणा (1-1), जो इसलिए 0 के बहुत करीब है. और यह वास्तव में सही है, क्योंकि जब z 
बहुत बड़ा है स्लोप है करीब 0. इसके विपरीत, यदि z =-10, तो मान लो, वहां उतनी दूर, 
तो g (z) 0 के करीब है. तो बाईं तरफ का फार्मूला हमें बताता है, d/dz g (z) करीब होगा g (z) के , जो 0 गुणा (1-0). इसलिए यह भी 0 के बहुत करीब है, जो सही है. अंत में यदि z = 0, तो g (z) = एक-आधा. वह है सिग्मोईड फ़ंक्शन ठीक यहाँ. और इसलिए डेरिवेटिव है बराबर स्लोप के एक आधा गुणा (1- एक आधा), जो एक-चौथाई के बराबर है. और वह वास्तव में है सही वैल्यू डेरिवेटिव की, या इस फ़ंक्शन की स्लोप की, जब z= 0. अंत में, सिरफ बताने के लिए थोड़ी और नोटेशन. कभी-कभार, इस चीज़ को लिखने के बजाय, संक्षिप्त रूप में, डेरिवेटिव है g प्राइम z. तो g प्राइम z का कैल्क्युलस में, 
लोअर डैश ऊपर कहलाती है प्राइम. तो g प्राइम z है एक संक्षिप्त रूप कैल्क्युलस में, फ़ंक्शन g के डेरिवेटिव का,
 विद रेस्पेक्ट टू इनपुट वेरीयेबल z. एक न्यूरल नेटवर्क में, हमारे पास है a=g(z) के. ठीक है, बराबर इसके. तब यह भी सरलीकृत होगा a गुणा (1-a) पर. तो कभी-कभी इम्प्लमेंटेशन्स में, आप शायद देखें जैसे g प्राइम z = a गुणा (1-a). और वह संदर्भित करता है अवलोकन कि g प्राइम, जिसका सिर्फ़ मतलब है डेरिवेटिव, है बराबर इसके यहाँ पर. और औसत इस फ़ॉर्म्युला का है कि यदि आपके 
पास है पहले से कम्प्यूट की हुई वैल्यू a की, तब इस इक्स्प्रेशन से, आप जल्दी से 
कम्प्यूट कर सकते हैं वैल्यू स्लोप की, g प्राइम की भी. ठीक है, तो वह था सिग्मोईड ऐक्टिवेशन फ़ंक्शन. चलिए अब देखते हैं tanh ऐक्टिवेशन फ़ंक्शन. जो हमने पहले किया था उसके समान ही, परिभाषा d/dz g(z) है स्लोप g(z) की किसी विशेष पोईंट z पर. और यदि आप देखते हैं फ़ॉर्म्युला 
हायपर्बॉलिक टैंजेंट फ़ंक्शन का, और यदि आप जानते हैं कैल्क्युलस, 
आप ले सकते हैं डेरिवेटिव्स और दिखा सकते हैं कि यह सरलीकृत होता है इस फ़ॉर्म्युला में. और संक्षिप्त रूप इस्तेमाल करते हुए जो हमारे पास था पहले, हम कहेंगे इसे g प्राइम z का फिर से. तो यदि आप चाहते हैं चलो पहले देखें 
कि यह इक्स्प्रेशन समझ आता है. तो उदाहरण के लिए यदि z = 10, 
tanh (z) होगा 1 के बहुत करीब. यह 1 से -1 तक जाता है. और फिर g प्राइम z, इस फ़ॉर्म्युला के अनुसार, होगा लगभग 1 माइनस 1 स्क्वेर, इसलिए 0. तो वह था जब z बहुत बड़ा है स्लोप है करीब 0. इसके विपरीत, यदि z है बहुत छोटा, मान लो z = -10, तब tanh (z) होगा -1 के करीब. और इसलिए g प्राइम z होगा लगभग 
1 माइनस नेगेटिव 1 स्क्वेर, तो यह है लगभग 1 माइनस 1, जो है लगभग 0. और अंत में, यदि z = 0, तो tanh (z) = 0. और स्लोप वास्तव में है बराबर 1 के, जो है वास्तव में स्लोप जब z 0 के बराबर है. तो सिर्फ संक्षेप में प्रस्तुत करने के लिए, अगर a = g (z), 
तो यदि a बराबर है इस tanh (z) के, तब डेरिवटिव g प्राइम z होगा 1-a स्क्वेर्ड. तो एक बार फ़िर, यदि आपके 
पास है पहले से कम्प्यूट की हुई वैल्यू a की, आप इस्तेमाल कर सकते हैं इस फ़ॉर्म्युला का 
जल्दी से कम्प्यूट करने के लिए डेरिवेटिव भी. और अंत में, यहाँ है कि कैसे आप कम्प्यूट करते हैं 
डेरिवेटिव्स ReLU और leaky ReLU ऐक्टिवेशन फ़ंक्शन्स के. ReLU के लिए, g(z) है मैक्स (0,z). तो डेरिवेटिव है बराबर, बराबर 0 के. यदि z कम है 0 और 1 से, यदि z बड़ा है 0 से. और यह वास्तव में तकनीकी रूप से अपरिभाषित है, 
जब z पूरा 0 के बराबर है. और यदि आप इम्प्लमेंट कर रहे हैं इसे सॉफ़्ट्वेर में, यह गणितीय रूप से 100% सही नहीं हो सकता. लेकिन वह सिर्फ ठीक काम करेंगे अगर z वास्तव में पूरा 0 है, यदि आप सेट करते हैं डेरिवेटिव को बराबर 0 के, 
या इसे 0 होना ही चाहिए. उससे एक तरह से कोई फर्क नहीं पड़ता. यदि आप परिचित हैं ऑप्टिमायज़ेशन से, तकनीकी तौर पर g प्राइम तब हो जाता है 
जिसे कहते हैं एक सबग्रेडीयंट ऐक्टिवेशन फ़ंक्शन g(z) का, जिस वजह से 
ग्रेडीयंट डिसेंट अभी भी काम करता है. लेकिन आप सोच सकते हैं इसे कि सम्भावना
 कि z होगा पूरा 0.000000, वह है इतनी कम कि लगभग अंतर नहीं पड़ता कि आप 
आप किसके बराबर सेट करते हैं डेरिवेटिव को जब z है बराबर 0 के. तो अभ्यास में, यह है जो लोग इम्प्लमेंट करते हैं
 z के डेरिवेटिव के लिए. और अंत में, जब ट्रेन कर रहे हैं एक न्यूरल नेटवर्क को leaky ReLU ऐक्टिवेशन फ़ंक्शन्स से, तब g(z) होगा मैक्स, मान लो (0.01z, z 1 z) का. और इसलिए g प्राइम z होगा बराबर 0.01 के, यदि z कम है 0 और 1 से, यदि z बड़ा है 0 से. और एक बार फिर, ग्रेडिएंट तकनीकी रूप से निर्धारित नहीं है 
जब z पूरा 0 के बराबर है. लेकिन अगर इम्प्लमेंट करते हैं एक कोड का 
हिस्सा जो सेट करता है डेरिवेटिव, या जो सेट करता है g प्राइम को या 0.01 या 1, कैसे भी, 
उससे वास्तव में कोई अंतर नहीं पड़ता. जब z है पूरा 0 आपका कोड काम करेगा. तो इन फ़ॉर्म्युलॉस के साथ, आप कर सकते हैं 
कम्प्यूट स्लोप, या डेरिवेटिव्स, आपके ऐक्टिवेशन फ़ंक्शन्स की. अब, आपके पास यह बिल्डिंग ब्लाक है, तैयार हो जाएँ देखने के लिए कि कैसे इम्प्लमेंट करना है 
ग्रेडीयंट डिसेंट आपके न्यूरल नेटवर्क के लिए. तो चलिए चलते हैं अगले वीडियो पर उसे देखने के लिए.