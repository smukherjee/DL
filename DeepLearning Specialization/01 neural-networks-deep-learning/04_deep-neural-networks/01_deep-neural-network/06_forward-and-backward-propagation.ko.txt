이전 강의에서 여러분에게 심층신경망을 도입하는 것의 기본 구성요소를 보았는데요, 각각 층에 대한 전 방향전파 스텝과 그에 상응하는 후 방향전파 스텝을 살펴봤습니다. 이제 이런 단계를 어떻게 도입하는지 보겠습니다. 어떻게 도입하는지 보겠습니다. 어떻게 도입하는지 보겠습니다. 결과값 al 이고, cache는 ZL입니다. 이행 시점에서 보면, WL과 BL도 cache로 사용될 수 있습니다. 함수가 조금더 쉽게 되고록 연습문제에서 말이죠. 여기 공식은 이미 익숙해 보이시겠지만, forward function을 도입하기 
위한 방법은 이거는 w[l] * a[l-1] + b[l] 그리고 a[l]은 z에 적용된 activation function입니다. 이 도입에 대한 부분을 벡터화시키고 싶으시면 이거 곱하기 a[l-1] + b 인데요 b를 더하는 것은 하이퍼 브로드캐스팅 인데요 a[l] = g applied element-wise to z 입니다. 기억하시겠지만, forward step의 다이어그램에서 이러한 박스로 된 체인이 앞으로 진행되었습니다. 이것을 a[0] 값을 삽입하면서 초기화를 시키죠. a[0]는 x값과 동일합니다. 초기화를 시켰습니다. 입력값의 첫번째는 무엇인가를 확인합니다. a[0]는 하나의 트레이닝 샘플에 대한 입력 특성일 수 있고, 한번에 한 예시만 쓰는 경우 말이죠. 아니면 대문자 A[0] 일 수도 있습니다. 전체 트레이닝 세트를 진행할 때 말이죠. 이것이 forward function에 대한 첫번때 입력값인데요, 이것을 반복해서 전 방향전파를 계산할 수 있게 해줍니다. 왼쪽에서 오른쪽으로 진행해서 말이죠. 다음으로는 후 방향전파 step에 대해 이야기해보겠습니다. 여기서는 입력값은 da[l]을 갖고 결과값은da[l-1], dw[l] 그리고 db[l]입니다. 이제 이 값들을 계산하기 위한 단계를 적어보겠습니다: dz[l]은 da[l] 곱하기 element wise product with g[l] z[l]입니다. 그리고 derivatives를 계산합니다. dW[l] = dz[l] * a[l - 1]. 이 부분은 제가 cache에 멸백히 두지 않았는데요, 이것도 필요합니다. 그리고 db[l] 은 dz[l] 입니다. 마지막으로 da[l-1]은 w[l] transpose 곱하기 dz[l]입니다. 이것이 어떻게 산출됐는지 정확히 다루지는 앖겠습니다. 하지만 여기 이 da 의 값을 가지고 여기에다가 대입시키면 이전에 여기 있었던 똑같은 공식이 되는데요. dz[l] 을 이전 dz[l] 함수로 말이죠. 만약 이 값을 여기에다가 대입시키면 dz[l] = w[l+1]_transpose dz[l+1] * g[l]` z[l]. 굉장히 많은 양의 산수인데요. 이 값은 여러분이 직접 다시 한번 확인할 수 있습니다. 이 공식은 제가 저번주에 back propagation을 위해 사용했던 공식입니다. 한개의 hidden layer인 신경망에 대해 다룰 때 말이죠. 기억하시겠지만, 여기 곱하기는 
element-wise product인데요, 그러므로 backward function을 도입하기 위해 
필요한 것은 여기 위의 4개 공식이 전부입니다. 마지막으로, 벡터화된 버전을 적을텐데요, 첫번째 줄은 dz[l] = dA[l] element-wise product with g[l]` of z[l] 입니다. 놀랄 부분은 없겠죠. dW[l] 은 1/m, dz[l] * a[l-1]_transpose 입니다. 그리고 db[l] 은 1/m np.sum dz[l] 이고, axis = 1, keepdims = true 입니다. 저희는 np.sum의 대한 사용을 지난주에 
이야기했었는데요, db를 계산하기 위한 방법 말이죠. 그리고 마지막으로 그리고 마지막으로 
dA (l-1)은 W[l]_transpose * dz[l] 입니다. 이것은 이 값을 입력하게 해주고, 여기 da에 말이죠. 그리고 여기 결과값 dW[l], db[l] 은 필요한 derivatives입니다. 그리고 dA[l-1] 도 말이죠, 이렇게 해서 backward function을 도입합니다. 요약해보자면, 입력값 x를 갖고, 여기 이렇게 첫번째 층이 있을 수 있는데요, ReLu activation 함수로 되어 있을 수 있습니다. 다음으로 2번째 층으로 넘어가서 또 하나의 ReLu activation 함수로 되어 있을 수 있습니다. 세번째로 넘어가면 시그모이드 activation 함수로 되어 있을 수 있겠죠. 
이진분류법을 사용하게 되겠죠. 이것이 ŷ의 결과값으로 표출하구요. 그 다음으로, ŷ을 이용해서 loss를 산출할 수 있습니다. 그리고 이제는 이런 값들을 사용해서 
backward iteration을 시작할 수 있게 됩니다. 먼저 화살표를 그리겠습니다. 펜을 자주 바꾸고 싶지 않습니다. 그러면 이제 후 방향전파가 derivatives를 계산할텐데요, dW[3], db[3], dW[2], db[2], dW[1], db[1], 그리고 진행 중에는 역시 cache를 산출하는데 이것은 
z[1], z[2], z[3], 이동시킬 것입니다. 여기서는 da[2] da[1] 이동시킬 것입니다. 이것은 da[0]을 산출할 수 있는데요. 이것은 사용하지 않을 것이기 때문에 지워도 되겠습니다. 자 그럼 이렇게 전 방향전파와 후 방향전파를 3층으로 이루어진 신경망을 도입할 때 사용하면 되겠는데요. 제가 1가지 부분에 대해 이야기 하지 않았는데요, forward recursion 에서는 입력데이터 x로 초기화할 것입니다. 그러면 backward recursion일 때는 어떨까요? da[l], 여러분이 로지스틱 회귀분석을 사용할 때, da[l], 여러분이 로지스틱 회귀분석을 사용할 때, 이진분류법을 이용할 때, da[l]은 y/a + 1-y/1-a 입니다. 결과값에 대한 loss 함수의 derivative는 ŷ 이 이 값과 일치한다고 보여질 수 있고 여러분이 미적분학이 익숙하시면, l loss 함수를 ŷ에 대해 아니면 a에 대한 derivative를 취하면 여기 이 공식처럼 나오는 것을 알 수 있습니다. 이 공식을 최종 층인 대문자 l 인 
경우에 da 에 적용시키면 됩니다. 여러분이 만약에 벡터화된 도입을 구하는 경우, backward recursion을 초기화 합니다. 여기 이 값으로 말고, 여기와 같이 dA (l)층으로 말입니다. 이것은 똑같을 것입니다. 다른 예시 나누기 처럼, 트레이닝 예시인 a + 1-y, 처음 트레이닝 예시에 대해서 말이죠. 이 값 나누기 1-a 의 처음 트레이닝 예시에 대해서 여기는 1-a[m] 되겠죠. 이렇게해서 벡터화된 버전을 도입하는 것입니다. 이런 방법을 통해서 후 방향전파의 
벡터화된 버전을 초기화 시킬 수 있습니다. 여러분은 이제 전 방향전파와 후 
방향전파의 기본구성요소를 보았는데요, 여러분은 이제 전 방향전파와 후 
방향전파의 기본구성요소를 보았는데요, 여러분이 이제 이 공식을 도입하면 필요한 derivative를 얻기 위한 board prop 과 후 방향전파 의 
올바른 도입을 할 수 있습니다. 이 과정에서 여러분은 공식이 너무 많아서 약간 헷갈리시고, 어떤 원리로 
작동하는지 감이 잘 안 올 수 있는데요, 이렇게 느껴지시면 이렇게 느껴지시면, 여러분이 이번주 프로그래밍 과제 부분에 도달하시면 여기서 배운 내용을 직접 도입할 수 있게 되면서 저금 더 구체적으로 알게 될 것입니다. 저도 공식이 많다는 것은 아는데요. 또 일부는 이해가 안가셨을 수도 있습니다. 미적분학과 선형수학을 바탕으로 확인을 할 수 는 있습니다. 
쉽지는 않지만 언제든지 해보셔도 좋습니다. 사실 이부분이 사실 이부분이 머신러닝에서 
끌고오는 derivation중 어려운 부분이기도 합니다. 여기서 사용한 제가 적었던 공식들은 사실 대부분 derivative을 산출하기 위한
 공식이였습니다. 특히, backdrop에서 말이죠. 다시 한번 말씀드리지만, 
이것이 만약 추상적이고 약간 미스테리하면. 제가 조언드리고 싶은 것은 연습문제를 해보면, 해당 내용이 조금 더 
구체적으로 이해가 될 수 있습니다. 하지만 저도 말할 수 있는 부분이 오늘 날, 제가 러닝 알고리즘을 도입할 때, 
저도 제 러닝 알고리즘이 작동하면 놀랍니다. 그 이유는 머신 러닝의 복잡성은 
사실 상당부분 데이터에서 옵니다. 코드 라인 말고 말이죠. 
가끔식은 몇 줄의 코드를 도입해서 정확히 무엇을 했는지 코딩을 어떻게 했는지 잘 모르는 경우에도, 바로 잘 작동하는 경우가 있습니다. 이런 경우는 코딩의 마법 효과가 아니라, 이런 경우는 코딩의 마법 효과가 아니라, 이런 코드가 흔히 너무 길지는 않고, 물론 굉장히 간단한 것은 아니지만 10,000중 또는 100,000줄의 코드로 이루어 진 것이 아닙니다. 그렇지만 데이터를 굉장히 많이 삽입하기 때문에, 머신러닝을 오래 접했음에도 불구하고 가끔씩은 저도 놀랩니다. 저의 러닝 알고리즘이 잘 작동할 때 말이죠. 
그 이유는 러닝 알고리즘의 복작성은 상당부분 데이터에서 시작되기 때문입니다. 아주 길게 1000줄 또는 수천줄의 
코드를 사용해서 온다기 보다 말이죠. 자 이렇게해서 심층신경망을 
어떻게 도입하는지 배웠는데요, 다시 말씀드리지만, 연습문제를 진행하면 
조금 더 구체적으로 이해하실 수 있을 것입니다. 넘어가기 앞서, 다음 비디오에서는 하이퍼 파라미터와 파라미터에 
대해 이야기를 나누고 싶습니다. 여러분이 깊은 네트워크를 트레이닝 하는 경우 하이퍼 파라미터를 조직화 시키는 능력이 네트워크를 개발하는데 효윻적으로 
작업할 수 있도록 도롸줄 것입니다. 다름 비디오에서는 이것이 정확히 
무슨 뜻인지 이야기해보겠습니다.