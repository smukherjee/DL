1
00:00:00,000 --> 00:00:02,389
このコースの４週目に
ようこそ

2
00:00:02,389 --> 00:00:06,354
これまでに ニューラルネットワークでの
隠れ層が１つの時の

3
00:00:06,354 --> 00:00:10,807
順誤差伝播法と逆誤差伝播法と
ロジステック回帰を見てきました

4
00:00:10,807 --> 00:00:13,064
ベクトル化についてや
重みを乱数的に初期化することが

5
00:00:13,064 --> 00:00:15,936
重要な時について学びました

6
00:00:15,936 --> 00:00:19,417
これまでの数週の宿題を
行って実装も行っていたら

7
00:00:19,417 --> 00:00:21,378
自分自身でこれらの考えが
動作することを見てきました

8
00:00:21,378 --> 00:00:21,977
今では

9
00:00:21,977 --> 00:00:26,718
深層ニューラルネットワークを実装するのに必要な
考えはほとんど見てきました

10
00:00:26,718 --> 00:00:30,453
今週行うのは これらの考えを
つなげてみて

11
00:00:30,453 --> 00:00:33,669
自分自身で深層ニューラルネットワークを
実装できるようにすることです

12
00:00:33,669 --> 00:00:36,406
今週のコーディング演習は
長なっているので

13
00:00:36,406 --> 00:00:39,457
作業が多くなっています
このため今週のビデオは

14
00:00:39,457 --> 00:00:43,784
短くしておきました
ビデオは素早く見終わって

15
00:00:43,784 --> 00:00:48,107
より多くの時間をプログラム演習に
避けるようにしたいです

16
00:00:48,107 --> 00:00:52,537
これによって誇りに思えるような
深層ニューラルネットワークを作った経験を残したいです

17
00:00:52,537 --> 00:00:55,576
深層ニューラルネットワークとは
何でしょうか

18
00:00:55,576 --> 00:00:59,225
ロジステック回帰のこの絵は
見てきましたね

19
00:00:59,225 --> 00:01:03,439
隠れ層１つのニューラルネットワークも
見てきました

20
00:01:03,439 --> 00:01:07,925
これは 隠れ層２つの
ニューラルネットワークの例と

21
00:01:07,925 --> 00:01:10,661
隠れ層５つのニューラルネットワークの例です

22
00:01:10,661 --> 00:01:15,744
これを見ると ロジステック回帰は
非常に浅いモデルで

23
00:01:15,744 --> 00:01:19,947
一方で ここのモデルは
それよりかなり深いモデルです

24
00:01:19,947 --> 00:01:23,585
浅いか深いかは
程度の問題です

25
00:01:23,585 --> 00:01:26,952
右上は隠れ層１つの
ニューラルネットワークで

26
00:01:26,952 --> 00:01:30,052
２層のニューラルネットワークです

27
00:01:30,052 --> 00:01:34,880
層の数え方を覚えていますか
入力層は数えずに

28
00:01:34,880 --> 00:01:38,076
隠れ層と出力層だけを
数えます

29
00:01:38,076 --> 00:01:42,849
これは２層ニューラルネットワークですが
まだ浅いものですが

30
00:01:42,849 --> 00:01:45,961
ロジステック回帰ほどは
浅くありません

31
00:01:45,961 --> 00:01:50,086
技術的に言えば ロジステック回帰は
層１つのニューラルネットワークです

32
00:01:50,086 --> 00:01:53,536
ここ数年にわたって
AIまたは

33
00:01:53,536 --> 00:01:58,561
機械学習のコミュニティでは
非常に深い深層ニューラルネットワークでは学習できて

34
00:01:58,561 --> 00:02:03,590
浅いようなモデルでは
学習できないことが多いと実感してきています

35
00:02:03,590 --> 00:02:08,119
ただどんな問題であっても
事前にどの程度の深さのネットワークが良いかを予測するのは

36
00:02:08,119 --> 00:02:10,163
難しいでしょう

37
00:02:10,163 --> 00:02:14,305
このため ロジステック回帰を試して
隠れ層１つを試し

38
00:02:14,305 --> 00:02:19,200
２つを試した後に
隠れ層の数を様々な値を試せるような

39
00:02:19,200 --> 00:02:22,739
ハイパーパラメーターとして
見なしても良いでしょう

40
00:02:22,739 --> 00:02:27,515
ホールドアウト交差検証を行ったり
Dev セットの評価に使ってもよいでしょう

41
00:02:27,515 --> 00:02:29,447
後ほど
詳しく見ていきましょう

42
00:02:29,447 --> 00:02:33,998
今は 深層ニューラルネットワークを表すための
表記方法を見ていきましょう

43
00:02:33,998 --> 00:02:39,147
ここには１、２、３、４層の
ニューラルネットワークがあります

44
00:02:40,974 --> 00:02:45,729
３つの隠れ層で
隠れ層のユニットの数は

45
00:02:45,729 --> 00:02:50,842
５、５、３で
１つの出力ユニットがあります

46
00:02:50,842 --> 00:02:52,731
使っていく表記方法は

47
00:02:52,731 --> 00:02:56,591
大文字のLで
ネットワークの層の数を表します

48
00:02:56,591 --> 00:03:03,881
この場合にはL=4で
これが層の数です

49
00:03:03,881 --> 00:03:11,880
n[l]を使って
小文字のl で その層での

50
00:03:11,880 --> 00:03:17,101
ノードやユニットの数を表します

51
00:03:17,101 --> 00:03:22,501
これに索引を付ければ
入力層を０とし

52
00:03:22,501 --> 00:03:28,950
これを１ これを２
これを３ これを４層目とします

53
00:03:28,950 --> 00:03:33,822
こうすると
n[1]はこれです

54
00:03:33,822 --> 00:03:39,529
最初の隠れ層でn[1]=5です
５つの隠れユニットがあるためです

55
00:03:39,529 --> 00:03:43,623
これについては n[2]=5で

56
00:03:43,623 --> 00:03:48,810
２つ目の隠れ層のユニットの数です

57
00:03:48,810 --> 00:03:53,315
n[3]=3 です

58
00:03:53,315 --> 00:03:59,459
n[4]=n[L]で
出力ユニットの数は１です

59
00:03:59,459 --> 00:04:04,101
大文字のLは４だからです

60
00:04:04,101 --> 00:04:08,878
この入力層の部分では

61
00:04:08,878 --> 00:04:13,003
n[0]=nx=3です

62
00:04:13,003 --> 00:04:17,879
これがそれぞれの層にあるノードの数を
表わす表記方法

63
00:04:17,879 --> 00:04:18,463
です

64
00:04:18,463 --> 00:04:23,913
それぞれの層Lに対して

65
00:04:23,913 --> 00:04:30,196
a[l]と書いて
l層の活性化を表します

66
00:04:30,196 --> 00:04:34,669
後に順誤差伝播法で
これを見ることになります

67
00:04:34,669 --> 00:04:40,791
a[l]=活性化のg[l](Z[l])で

68
00:04:40,791 --> 00:04:46,440
恐らく活性化は
層lで索引を付けます

69
00:04:46,440 --> 00:04:51,736
W[l]を使って重みを表します

70
00:04:51,736 --> 00:04:55,973
重みでz[l]として計算します

71
00:04:55,973 --> 00:05:00,714
b[l]もz[l]を計算するのに使われます

72
00:05:00,714 --> 00:05:07,114
最後に表記方法を纏めると
入力特徴量はX です

73
00:05:07,114 --> 00:05:12,215
層０の活性化は
a[0]=Xです

74
00:05:12,215 --> 00:05:17,133
最後の層の活性化は
a[L]=yハットです

75
00:05:17,133 --> 00:05:25,275
a[L]はニューラルネットワークの
予測値のｙハットと出力予測と同じです

76
00:05:25,275 --> 00:05:28,200
これで深層ニューラルネットワークが
どんなものか分かりましたし

77
00:05:28,200 --> 00:05:32,427
深層ネットワークで記述し計算する
今後使っていく表記も学びました

78
00:05:32,427 --> 00:05:36,457
このビデオで多くの表記を
紹介しましたが

79
00:05:36,457 --> 00:05:40,916
もし単純な表記について忘れてしまったら
コースのサイトにも表記のシートを載せましたので見てください

80
00:05:40,916 --> 00:05:45,089
探して様々な表記を探すこともできます
表記方法のシートやガイドもあります

81
00:05:45,089 --> 00:05:48,958
次回は 順誤差伝播法では
この種のネットワークを記述するとどうなるのかお見せしようと

82
00:05:48,958 --> 00:05:49,620
思っています

83
00:05:49,620 --> 00:05:51,019
次のビデオを見てみましょう