1
00:00:00,000 --> 00:00:04,740
上一段影片您看到了一個單一
隱藏層神經網路的樣子

2
00:00:04,740 --> 00:00:07,020
這段影片讓我們更進一步
談論細節

3
00:00:07,020 --> 00:00:10,140
這個神經網路是如何計算這些輸出的。

4
00:00:10,140 --> 00:00:13,530
你看到的是如同羅吉斯迴歸分析

5
00:00:13,530 --> 00:00:15,585
重複很多次。

6
00:00:15,585 --> 00:00:19,725
讓我們來看看。這是一個雙層神經網路。

7
00:00:19,725 --> 00:00:23,850
讓我們更進一步看
這個神經網路怎麼做計算的.

8
00:00:23,850 --> 00:00:26,985
我們之前已經說過,羅吉斯迴歸分析

9
00:00:26,985 --> 00:00:28,980
羅吉斯迴歸分析中的圓圈,

10
00:00:28,980 --> 00:00:31,515
實際上代表兩個步驟的計算。

11
00:00:31,515 --> 00:00:33,825
首先您計算 z 像這樣,
再來

12
00:00:33,825 --> 00:00:37,290
您計算這個 z 的
S型啟動函數.

13
00:00:37,290 --> 00:00:40,715
神經網路只是
重複做這樣很多次.

14
00:00:40,715 --> 00:00:44,460
讓我們開始專注於隱藏層的
一個節點.

15
00:00:44,460 --> 00:00:46,385
讓我們看在隱藏層的
第一個節點

16
00:00:46,385 --> 00:00:48,095
我先將其他節點掩藏灰掉,

17
00:00:48,095 --> 00:00:50,810
類似于左邊的羅吉斯迴歸分析

18
00:00:50,810 --> 00:00:54,470
這個在隱藏層的節點
做這兩個步驟的計算

19
00:00:54,470 --> 00:00:57,905
第一步, 您想成是
這個節點的左半部,

20
00:00:57,905 --> 00:01:02,570
它計算 z 等於 w轉置, x, 加 b,

21
00:01:02,570 --> 00:01:05,060
我們用的標記法是,

22
00:01:05,060 --> 00:01:08,300
這些都是與第一個隱藏層關聯的數值。

23
00:01:08,300 --> 00:01:10,720
所以,這就是為什麼我們有一堆方括弧。

24
00:01:10,720 --> 00:01:13,610
這是隱藏層中的第一個節點。

25
00:01:13,610 --> 00:01:16,700
所以這是為什麼在這裡
我們有下標 1

26
00:01:16,700 --> 00:01:18,005
所以首先,它這樣做,

27
00:01:18,005 --> 00:01:19,130
然後第二步

28
00:01:19,130 --> 00:01:24,605
是它計算 a[1]_1 等於 z[1]_1 的 sigmoid,如下所示。

29
00:01:24,605 --> 00:01:26,030
所以,對於 z 和 a,

30
00:01:26,030 --> 00:01:29,690
符號慣例是 a, [l] , i,

31
00:01:29,690 --> 00:01:32,270
在這裡的上標方括弧 [l],

32
00:01:32,270 --> 00:01:33,635
指的是層號數值,

33
00:01:33,635 --> 00:01:35,060
在這裡下標的 i ,

34
00:01:35,060 --> 00:01:37,830
是該層中的節點。

35
00:01:37,830 --> 00:01:40,115
所以,我們查看的節點是在第一層,

36
00:01:40,115 --> 00:01:42,035
是隱藏的層第一個節點。

37
00:01:42,035 --> 00:01:45,965
這是為什麼上標跟
下標都是 1, 1

38
00:01:45,965 --> 00:01:47,375
那麼,那個小圓圈,

39
00:01:47,375 --> 00:01:49,385
神經網路中的第一個節點,

40
00:01:49,385 --> 00:01:52,640
表示執行這兩個計算步驟。

41
00:01:52,640 --> 00:01:55,190
現在，讓我們看在這個神經網路
的第二個節點,

42
00:01:55,190 --> 00:01:58,340
在這個神經網路的
隱藏層的第二個節點.

43
00:01:58,340 --> 00:02:01,370
類似於在左邊的
羅吉斯迴歸分析

44
00:02:01,370 --> 00:02:04,850
這個小圓圈代表兩個計算步驟。

45
00:02:04,850 --> 00:02:07,550
第一步是計算 z。

46
00:02:07,550 --> 00:02:08,735
這仍然是第一層

47
00:02:08,735 --> 00:02:12,395
但現在為第二個節點,等於 w 轉置, x,

48
00:02:12,395 --> 00:02:19,190
加上 b[1]_2,然後 a[1]_2,等於 z[1]_2 的 sigmoid。

49
00:02:19,190 --> 00:02:21,200
再次,請隨時暫停影片,如果你想,

50
00:02:21,200 --> 00:02:23,840
但您可以仔細檢查上標和

51
00:02:23,840 --> 00:02:28,430
下標符號是跟
上面用紫色寫的公式一致的

52
00:02:28,430 --> 00:02:32,845
因此,我們已經討論了神經網路中的
前兩個隱藏單元,

53
00:02:32,845 --> 00:02:36,940
隱藏單元 3 跟 4 
代表類似的計算.

54
00:02:36,940 --> 00:02:40,205
現在
讓我拿著這對方程式

55
00:02:40,205 --> 00:02:42,320
和這兩對方程式,

56
00:02:42,320 --> 00:02:44,210
並把它們複製到下一張投影片。

57
00:02:44,210 --> 00:02:45,380
所以這是我們的神經網路,

58
00:02:45,380 --> 00:02:46,550
這是第一個,

59
00:02:46,550 --> 00:02:49,550
這裡是我們計算出的第二個方程式

60
00:02:49,550 --> 00:02:54,155
以前用於第一個和第二個隱藏單位。

61
00:02:54,155 --> 00:02:57,650
如果您用同樣方式寫下
對應方程式

62
00:02:57,650 --> 00:03:01,475
對於第三和第四個隱藏單位,您將獲得以下內容。

63
00:03:01,475 --> 00:03:03,635
所以,讓我弄清一下這個記號是正確的,

64
00:03:03,635 --> 00:03:06,680
這是向量 w[1]_1,

65
00:03:06,680 --> 00:03:09,335
這是一個向量轉置,乘 x, 好吧

66
00:03:09,335 --> 00:03:12,050
這就是上標 T 所代表的。

67
00:03:12,050 --> 00:03:13,535
這是一個向量轉置。

68
00:03:13,535 --> 00:03:15,085
你可能已經猜到了,

69
00:03:15,085 --> 00:03:17,195
當您建置神經網路時,

70
00:03:17,195 --> 00:03:20,285
用 for loop 來做這個,真的效率不高。

71
00:03:20,285 --> 00:03:21,530
那麼,我們要做的是,

72
00:03:21,530 --> 00:03:25,220
就是向量化這四個方程式。

73
00:03:25,220 --> 00:03:29,050
因此,我們將首先展示
如何用向量的方式計算 z

74
00:03:29,050 --> 00:03:30,935
您可以這樣做.

75
00:03:30,935 --> 00:03:34,910
讓我們拿這些 w 
將他們疊成一個矩陣

76
00:03:34,910 --> 00:03:37,980
然後你有 w[1]_1 轉置,

77
00:03:37,980 --> 00:03:39,450
這是一個行向量,

78
00:03:39,450 --> 00:03:42,500
或此列向量轉置為行向量,然後 w[1]_2

79
00:03:42,500 --> 00:03:48,980
轉置, w[1]_3 轉置, w[1]_4 轉置。

80
00:03:48,980 --> 00:03:53,150
所以,通過把這四個w向量堆疊在一起,

81
00:03:53,150 --> 00:03:54,575
你最終有一個矩陣。

82
00:03:54,575 --> 00:03:58,985
所以,另一個可以思考的方法是,
我們有四個羅吉斯迴歸分析單位

83
00:03:58,985 --> 00:04:01,190
以及每個羅吉斯迴歸分析單位,

84
00:04:01,190 --> 00:04:03,290
具有相應的參數向量,

85
00:04:03,290 --> 00:04:06,080
w. 通過把這四個向量堆疊在一起,

86
00:04:06,080 --> 00:04:08,930
您會得到這個 (4, 3) 矩陣.

87
00:04:08,930 --> 00:04:14,180
如果您拿這個矩陣,
乘上您的輸入特徵 x1,

88
00:04:14,180 --> 00:04:18,650
x2, x3 您會得到,
使用矩陣乘法.

89
00:04:18,650 --> 00:04:20,870
您會得到 w[1]_1轉置, x;

90
00:04:20,870 --> 00:04:28,775
w[1]_2 轉置 x; w[1]_3 轉置, x; w[1]_4 轉置, x.

91
00:04:28,775 --> 00:04:31,010
然後,我們不要忘記 b。

92
00:04:31,010 --> 00:04:39,450
因此,我們現在添加一個向量 b[1]_1 1, b[1]_2, b[1]_3, b[1]_4。

93
00:04:39,450 --> 00:04:40,860
基本上就是這個,

94
00:04:40,860 --> 00:04:46,500
然後這是 b[1]_1 1, b[1]_2, b[1]_3, b[1]_4。

95
00:04:46,500 --> 00:04:48,950
所以,你看到這四行

96
00:04:48,950 --> 00:04:53,195
結果與這四行中的每一行完全一致,

97
00:04:53,195 --> 00:04:55,610
這四行每一個
都相配我們上面的方程式

98
00:04:55,610 --> 00:04:59,870
換句話說, 我們剛剛展示了
這個東西等於 z[1],

99
00:04:59,870 --> 00:05:05,695
z[1]_1 , z[1]_2 , z[1]_3 , z[1]_4 
像這裡的定義.

100
00:05:05,695 --> 00:05:09,830
也許並不奇怪,我們要稱這
整個東西為,向量z[1],

101
00:05:09,830 --> 00:05:15,095
也就是把這些每一個 z 
疊起來變成一個列向量.

102
00:05:15,095 --> 00:05:20,300
當我們進行向量化時,一個經驗法則
來幫助您導引這些,

103
00:05:20,300 --> 00:05:22,450
是當您有不同的節點
在這一層時,

104
00:05:22,450 --> 00:05:23,965
我們將它們垂直疊起來.

105
00:05:23,965 --> 00:05:27,100
這就是為什麼我們有z[1]_1 到 z[1]_4,

106
00:05:27,100 --> 00:05:30,430
那些對應在隱藏層中的四個不同的節點,

107
00:05:30,430 --> 00:05:35,540
因此,我們垂直堆疊這四個數字,以形成向量 z[1]。

108
00:05:35,540 --> 00:05:37,410
最後, 再介紹一個符號,

109
00:05:37,410 --> 00:05:44,210
這裡四乘三矩陣,我們通過堆疊小寫 w[1]_1,

110
00:05:44,210 --> 00:05:48,260
w[1]_2 ,等等,我們將稱為矩陣,大寫 W[1]。

111
00:05:48,260 --> 00:05:52,625
同樣,這個向量,我們將稱為 b上標[1]方括弧。

112
00:05:52,625 --> 00:05:54,590
所以,這是一個 4 乘 1 向量。

113
00:05:54,590 --> 00:05:59,600
現在,我們使用向量矩陣標記法計算 z,

114
00:05:59,600 --> 00:06:03,300
最後一件事是我們也需要
計算這些 a 的值

115
00:06:03,300 --> 00:06:08,120
或許不意外您看到
我們將定義 a[1] 為

116
00:06:08,120 --> 00:06:09,710
一樣堆在一起,

117
00:06:09,710 --> 00:06:11,330
這些啟動值, a[1]_1,

118
00:06:11,330 --> 00:06:13,175
到 a[1]_4。

119
00:06:13,175 --> 00:06:18,260
就是拿這四個值
疊起來成為一個向量稱為 a[1]

120
00:06:18,260 --> 00:06:21,050
這將會是  sigmoid z[1],

121
00:06:21,050 --> 00:06:23,360
而現在這建置是

122
00:06:23,360 --> 00:06:26,900
S型 sigmoid 函數,拿 z 的四個元素,

123
00:06:26,900 --> 00:06:30,460
然後逐元素的應用S型函數作用在它們.

124
00:06:30,460 --> 00:06:31,950
總結一下,

125
00:06:31,950 --> 00:06:38,820
我們發現 z[1] 等於 W[1] 乘 x 加上向量 b[1],

126
00:06:38,820 --> 00:06:42,000
和 a[1] 是 sigmoid z[1]。

127
00:06:42,000 --> 00:06:44,055
讓我們將其複製到下一張投影片。

128
00:06:44,055 --> 00:06:48,050
我們看到的是,對於給定輸入 x 
的神經網路的第一層,

129
00:06:48,050 --> 00:06:52,070
我們有 z[1] 等於 W[1] 乘 x 加上 b[1],

130
00:06:52,070 --> 00:06:55,740
和 a[1] 是 sigmoid z[1]。

131
00:06:55,740 --> 00:06:59,060
這個維度是 (4,1),

132
00:06:59,060 --> 00:07:05,255
這是一個 (4,3) 矩陣,乘以 (3,1) 向量,加 (4,1) 向量 b,

133
00:07:05,255 --> 00:07:08,480
這是 (4,1) 個相同的維度作為結束。

134
00:07:08,480 --> 00:07:12,890
記得我們說 x = a[0],

135
00:07:12,890 --> 00:07:16,535
且 y-hat = a[2]

136
00:07:16,535 --> 00:07:21,060
如果您要的話, 您可以將這個
a[0] 取代 x , 因為 a[0]

137
00:07:21,060 --> 00:07:25,460
因為 a[0] 是,如果您要的話,作為輸入特徵向量 x 的別名。

138
00:07:25,460 --> 00:07:27,230
現在,通過類似的推導,

139
00:07:27,230 --> 00:07:31,580
你可以斷定出下一層的表現形式也可以

140
00:07:31,580 --> 00:07:36,125
寫成類似的, 而輸出層做的是

141
00:07:36,125 --> 00:07:37,945
與它相關的值。

142
00:07:37,945 --> 00:07:40,890
因此參數 W[2] 和 b[2]。

143
00:07:40,890 --> 00:07:44,600
所以,在這種情況下,W[2] 將是一個 (1,4) 矩陣,

144
00:07:44,600 --> 00:07:47,240
b[2] 就只是一個實數 (1,1)

145
00:07:47,240 --> 00:07:52,130
z[2] 會是一個實數
寫成 (1,1) 矩陣

146
00:07:52,130 --> 00:07:56,090
這是 (1,4) 矩陣, 乘 a 是 (4,1)

147
00:07:56,090 --> 00:07:57,770
加上b[2] 是 (1,1),

148
00:07:57,770 --> 00:07:59,805
就只是一個實數

149
00:07:59,805 --> 00:08:02,900
如果你想象這最後一個輸出單位只是

150
00:08:02,900 --> 00:08:07,295
類似于具有參數 w 和 b 的羅吉斯迴歸分析

151
00:08:07,295 --> 00:08:12,560
w 確實扮演了與 w[2] 轉置,類似的角色,

152
00:08:12,560 --> 00:08:16,790
或 w[2] 實際上是 W 轉置,b 等於 b[2]。

153
00:08:16,790 --> 00:08:21,840
如果您將左邊這個網路
蓋起來, 先忽略他們

154
00:08:21,840 --> 00:08:26,300
那麼最後一個輸出單位
就像羅吉斯迴歸分析一樣,

155
00:08:26,300 --> 00:08:29,375
除了與其寫成
參數 w 跟 b

156
00:08:29,375 --> 00:08:32,700
我們將它們寫成 w[2] 和 b[2],

157
00:08:32,700 --> 00:08:35,535
有 (1,4) 和 (1,1) 的維度。

158
00:08:35,535 --> 00:08:37,365
總結一下,

159
00:08:37,365 --> 00:08:41,690
對於羅吉斯迴歸分析,建置輸出或建置預測,

160
00:08:41,690 --> 00:08:44,945
您計算 z 等於 w 轉置, x 加 b,

161
00:08:44,945 --> 00:08:48,200
a 或 y-hat 等於 a,

162
00:08:48,200 --> 00:08:50,705
等於 sigmoid z。

163
00:08:50,705 --> 00:08:54,330
當你有一個神經網路與一個隱藏的層,

164
00:08:54,330 --> 00:08:55,530
您需要建置的

165
00:08:55,530 --> 00:08:59,085
就只是這四個方程式,去計算這個輸出。

166
00:08:59,085 --> 00:09:03,020
您可以將其視為向量化建置的計算

167
00:09:03,020 --> 00:09:08,015
首先,這四個隱藏層中羅吉斯迴歸分析單位的輸出.

168
00:09:08,015 --> 00:09:09,350
這就是它所做的,

169
00:09:09,350 --> 00:09:13,730
然後輸出層中的這種羅吉斯迴歸分析,這就是它所做的。

170
00:09:13,730 --> 00:09:15,680
我希望這個描述有道理,

171
00:09:15,680 --> 00:09:19,240
但要點是,計算這個神經網路的輸出,

172
00:09:19,240 --> 00:09:21,785
您需要的就是這四行程式.

173
00:09:21,785 --> 00:09:25,280
現在您看到,給予一個輸入特徵,

174
00:09:25,280 --> 00:09:27,755
向量 a ,你可以用四行代碼,

175
00:09:27,755 --> 00:09:30,185
計算此神經網路的輸出。

176
00:09:30,185 --> 00:09:32,600
類似于我們在羅吉斯迴歸分析所做的,

177
00:09:32,600 --> 00:09:36,635
我們還需要向量化多個訓練例子。

178
00:09:36,635 --> 00:09:41,690
我們看到,通過堆疊訓練示例在矩陣中的不同的列,

179
00:09:41,690 --> 00:09:44,615
稍作修改,你也

180
00:09:44,615 --> 00:09:46,610
類似于你在此迴歸分析中所看到的,

181
00:09:46,610 --> 00:09:50,170
能夠計算這個神經網路的輸出,

182
00:09:50,170 --> 00:09:52,030
不只是一次一個例子,

183
00:09:52,030 --> 00:09:55,010
一次用到你的整個訓練集.

184
00:09:55,010 --> 00:09:57,940
讓我們在下一段影片看這些細節