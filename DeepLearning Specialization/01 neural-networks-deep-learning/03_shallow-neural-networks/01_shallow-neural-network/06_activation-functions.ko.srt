1
00:00:00,390 --> 00:00:04,350
여러분이 신경망을 부팅하는 경우에, 

2
00:00:02,580 --> 00:00:06,720
선택할 수 있는 것중 한가지는 

3
00:00:04,350 --> 00:00:09,599
숨겨진 레이어에서 어떤 activation 함수를 

4
00:00:06,720 --> 00:00:11,490
쓸지와 또한, 신경망의 결과값 유닛을

5
00:00:09,599 --> 00:00:13,139
어떻게 할지에 대한 선택입니다. 현재까지는

6
00:00:11,490 --> 00:00:16,080
시그모이드 activation 함수를 써왔는데요, 

7
00:00:13,139 --> 00:00:18,720
하지만 다른 선택이 더 잘 작동하는 경우가 간혹 

8
00:00:16,080 --> 00:00:20,939
있습니다. 그 옵션들을 한번 보겠습니다. 

9
00:00:18,720 --> 00:00:23,279
신경망 네트워크 

10
00:00:20,939 --> 00:00:26,099
전 방향전파 스텝 에서 

11
00:00:23,279 --> 00:00:28,710
여기 2가지 단계가 있는데요, 

12
00:00:26,099 --> 00:00:32,610
여기서는 시그모이드 함수를 씁니다.

13
00:00:28,710 --> 00:00:37,590
이것은 activation 함수라고 하는데요

14
00:00:32,610 --> 00:00:40,680
여기는 낯이 익은 시그모이드 함수인데요 a=

15
00:00:37,590 --> 00:00:42,600
1 나누기 1 더하기 e의 -2승입니다.

16
00:00:40,680 --> 00:00:49,739
더 일반적인 케이스 같은 경우엔, 

17
00:00:42,600 --> 00:00:53,309
g의 z인 함수를 가질 수 있는데요, 

18
00:00:49,739 --> 00:00:56,010
여기 적을 텐데요, G는 비선형 

19
00:00:53,309 --> 00:00:59,250
함수로 시그모이드 함수가 아닐 수 있습니다.

20
00:00:56,010 --> 00:01:01,879
예를 들어, 시그모이드 함수는

21
00:00:59,250 --> 00:01:04,290
0 과 1사이 값을 갖는데요, 

22
00:01:01,879 --> 00:01:06,900
시그모이드 함수보다 거의 

23
00:01:04,290 --> 00:01:10,320
항상 잘 작동하는 activation 함수는

24
00:01:06,900 --> 00:01:14,189
tanh h 함수입니다. 또는 쌍곡선의

25
00:01:10,320 --> 00:01:19,979
탄젠트 함수인데요, 이것은 z이고, 이 건은

26
00:01:14,189 --> 00:01:25,710
a, 그리고 이것은 a=tanh 의 z입니다. 

27
00:01:19,979 --> 00:01:31,079
그리고 이것은 -1과 1사이으 값을 갖습니다.

28
00:01:25,710 --> 00:01:37,799
tanh h의 공식은 e의 z 승 빼기 

29
00:01:31,079 --> 00:01:40,140
e의 -v승 나누기 그 둘의 합인데요, 

30
00:01:37,799 --> 00:01:43,890
수학적으로 이동한 시그모이드 

31
00:01:40,140 --> 00:01:46,350
함수 버전입니다. 

32
00:01:43,890 --> 00:01:49,860
이것과 같은 시그모이드 함수인데요. 

33
00:01:46,350 --> 00:01:52,079
이것이 이렇게 이동된 것입니다. 

34
00:01:49,860 --> 00:01:54,570
그러므로 이제는 0, 0을 지나도록 rescale되는 것인데요, 

35
00:01:52,079 --> 00:01:58,530
그러므로 이제 -1에서 1의 값을 갖게 됩니다. 

36
00:01:54,570 --> 00:02:05,340
그리고 이제 여러분이 숨겨진 유닛에 대해 

37
00:01:58,530 --> 00:02:09,910
g(z) 함수를 

38
00:02:05,340 --> 00:02:12,490
tan h (z)와 일치하게 해주면, 이것이 

39
00:02:09,910 --> 00:02:14,020
시그모이드 함수보다 거의 항상 더 잘 작동합니다.

40
00:02:12,490 --> 00:02:16,930
그 이유는 숨겨진 레이어에서 나오는 

41
00:02:14,020 --> 00:02:19,000
activation의 평균값이, -1에서 1사이의 범위 경우, 

42
00:02:16,930 --> 00:02:21,550
0 이라는 평균값을 갖는데 

43
00:02:19,000 --> 00:02:23,020
더 가깝기 때문에, 

44
00:02:21,550 --> 00:02:23,590
러닝 알고리즘을 간혹

45
00:02:23,020 --> 00:02:25,690
트레이닝 하는 경우, 

46
00:02:23,590 --> 00:02:29,709
데이터를 중앙화시켜

47
00:02:25,690 --> 00:02:31,510
데이터가 0의 평균값을 갖도록 할 것입니다. 

48
00:02:29,709 --> 00:02:34,750
시그모이드 함수대한 tan h 함수를 이용해서 말이죠. 

49
00:02:31,510 --> 00:02:36,880
데이터를 중앙화시키는 효과가 있다고 보시면 됩니다.

50
00:02:34,750 --> 00:02:39,610
그 평균값이 0.5와 같은 값이 아닌

51
00:02:36,880 --> 00:02:41,410
0과 근접한 값이 될 수 있게 말이죠. 

52
00:02:39,610 --> 00:02:43,510
이렇게 진행하면, 다음 

53
00:02:41,410 --> 00:02:45,820
층에서의 러닝을 조금 더 수월하게 만들어줍니다.

54
00:02:43,510 --> 00:02:47,380
이것과 관련된 내용은 두번째 코스에서 

55
00:02:45,820 --> 00:02:50,739
최적화 알고리즘을 이야기하면서 더 다루도록 하겠습니다.

56
00:02:47,380 --> 00:02:52,480
한가지 기억하실 것은, 

57
00:02:50,739 --> 00:02:54,250
저는 이제 시그모이드 함수는 거의

58
00:02:52,480 --> 00:02:56,410
쓰지 않습니다.

59
00:02:54,250 --> 00:02:59,560
tan h 함수가 거의 항상

60
00:02:56,410 --> 00:03:03,550
더 우월합니다. 한가지 예외의 경우는

61
00:02:59,560 --> 00:03:07,420
결과값 층의 경우입니다. 

62
00:03:03,550 --> 00:03:10,570
만약 Y가 0이거나 1인 경우, 

63
00:03:07,420 --> 00:03:13,989
y hat 같은 경우, 출력하고 싶은 값인데요, 

64
00:03:10,570 --> 00:03:16,570
이 값이 0에서 1입니다.

65
00:03:13,989 --> 00:03:19,360
-1과 1 사이값 대신에 말이죠. 

66
00:03:16,570 --> 00:03:21,430
그렇기 때문에 한가지 예외 경우는, 

67
00:03:19,360 --> 00:03:24,670
시그모이드 함수를 이진분류법을

68
00:03:21,430 --> 00:03:26,350
쓰는 경우에 사용할 것입니다.

69
00:03:24,670 --> 00:03:29,709
이 경우, 시그모이드 함수는

70
00:03:26,350 --> 00:03:35,170
결과값 층에 대해 쓸 것입니다.

71
00:03:29,709 --> 00:03:37,180
그러므로 여기 g(z2)는 시그마의 z2입니다.

72
00:03:35,170 --> 00:03:40,299
여기서 example을 볼수 있는 것은, 

73
00:03:37,180 --> 00:03:43,920
tan h activation 함수가 hidden 

74
00:03:40,299 --> 00:03:47,769
layer에 대해 있을 수 있고, 그리고

75
00:03:43,920 --> 00:03:49,299
결과값 층에 대해서 시그모이드 함수가 있을 수 있습니다.

76
00:03:47,769 --> 00:03:51,670
그렇기 때문에, 다른 층들에 대해서, 

77
00:03:49,299 --> 00:03:53,709
activation 함수가 다를 수 있습니다. 

78
00:03:51,670 --> 00:03:55,690
그리고 가끔씩 다른 층에 대해서

79
00:03:53,709 --> 00:03:58,510
activation 함수가 다르다는 점을

80
00:03:55,690 --> 00:04:02,230
여기 대괄호 위첨자를 사용해 나타낼 수 있습니다.

81
00:03:58,510 --> 00:04:04,540
G의 대괄호 1이 

82
00:04:02,230 --> 00:04:06,940
G의 대괄호 2와 다르다는 것을

83
00:04:04,540 --> 00:04:09,340
그리고, 대괄호 1 위첨자는 

84
00:04:06,940 --> 00:04:11,470
여기 층을 나타내며, 

85
00:04:09,340 --> 00:04:12,879
위첨자 대괄호 2는 

86
00:04:11,470 --> 00:04:15,680
결과값 층을 뜻합니다.

87
00:04:12,879 --> 00:04:18,109
자 그러면 시그모이드 함수와 

88
00:04:15,680 --> 00:04:20,780
tan h 함수의 단점 중에 하나는

89
00:04:18,109 --> 00:04:22,910
만약 z가 매우 크거나 

90
00:04:20,780 --> 00:04:24,460
매우 작은 값인 경우, 

91
00:04:22,910 --> 00:04:27,560
derivative의 기울기나, 여기 이 함수의 기울기가

92
00:04:24,460 --> 00:04:30,139
매우 작아집니다. 그렇기 때문에 z는 매우 크거나

93
00:04:27,560 --> 00:04:33,169
매우 작습니다.

94
00:04:30,139 --> 00:04:35,270
기울기는 그러면 거의 0에 가까워 질 수 있는데요, 

95
00:04:33,169 --> 00:04:38,360
이렇게 되면 기울기 강하가 늦춰질 수 있습니다.

96
00:04:35,270 --> 00:04:41,810
그렇기 때문에 머신 러닝에서 

97
00:04:38,360 --> 00:04:44,900
인기있는 또 다른 방법은

98
00:04:41,810 --> 00:04:50,720
rectified linear unit라고 하는 것인데요.

99
00:04:44,900 --> 00:04:57,110
ReLu 함수는 이렇게 생겼습니다.

100
00:04:50,720 --> 00:05:00,500
그리고 그 공식은 a=max (0, 1) 입니다.

101
00:04:57,110 --> 00:05:03,530
derivative는 1인데요, Z가 

102
00:05:00,500 --> 00:05:05,990
양수이고, z 값이 음수인 경우엔, 

103
00:05:03,530 --> 00:05:07,580
기울기는 0이 됩니다.

104
00:05:05,990 --> 00:05:10,190
이것을 기술적인 접근을 하는 경우, 

105
00:05:07,580 --> 00:05:12,349
z가 0인 경우의 derivative는

106
00:05:10,190 --> 00:05:14,210
잘 정의되어 있지 않습니다. 

107
00:05:12,349 --> 00:05:18,770
여러분이 컨퓨터에서 도입하는 경우, 

108
00:05:14,210 --> 00:05:21,229
자주 000000000 값을 갖는데요, 

109
00:05:18,770 --> 00:05:22,940
값이 매우 작기 때문에, 

110
00:05:21,229 --> 00:05:25,610
실제로는 걱정하지는 않을 것입니다.

111
00:05:22,940 --> 00:05:29,659
만약 z의 값이 0인 경우에 

112
00:05:25,610 --> 00:05:32,270
derivative가 0 도는 1이라고 가정해도 되는데요, 

113
00:05:29,659 --> 00:05:35,479
differentiate 할 수 없다고 

114
00:05:32,270 --> 00:05:37,430
가정하면 됩니다.

115
00:05:35,479 --> 00:05:40,010
activation 함수를 고르는데 있어서의 

116
00:05:37,430 --> 00:05:43,280
몇개의 경험에 의거한 규칙인데요, 

117
00:05:40,010 --> 00:05:45,620
만약 이진분류법을 이용해서 그 결과값이

118
00:05:43,280 --> 00:05:47,539
0 과 1의 값을 갖을 때, 

119
00:05:45,620 --> 00:05:50,479
시그모이드 함수의 경우, 

120
00:05:47,539 --> 00:05:59,419
결과값에 대해서 자연스러운데요, 

121
00:05:50,479 --> 00:06:04,460
ReLu 또는 rectified linear

122
00:05:59,419 --> 00:06:07,190
유닛과 같은 경우, activation 함수의 

123
00:06:04,460 --> 00:06:10,280
기본값인 경우가 많은데요, 만약 여러분이

124
00:06:07,190 --> 00:06:13,849
숨겨진 레이어로 무엇을 쓸지 모르시는 경우엔, 

125
00:06:10,280 --> 00:06:15,289
그냥 ReLu 함수를 쓸 것입니다.

126
00:06:13,849 --> 00:06:17,570
요즘 대부분의 사람들이 사용하는

127
00:06:15,289 --> 00:06:20,120
함수인데요, 물론 가끔씩 

128
00:06:17,570 --> 00:06:21,350
사람들이 tanh 함수를 사용하긴 

129
00:06:20,120 --> 00:06:23,150
합니다.

130
00:06:21,350 --> 00:06:26,270
ReLu의 단점중 하나는, 

131
00:06:23,150 --> 00:06:28,640
derivative의 값이 0이라는 것입니다. 

132
00:06:26,270 --> 00:06:31,700
z의 값이 마이너스 인 경우에 말이죠, 

133
00:06:28,640 --> 00:06:33,890
실제로는 괜찮습니다만, 또 다른

134
00:06:31,700 --> 00:06:35,420
ReLu 함수가 있는데요 Leaky ReLu하고 하는
것이 있습니다.

135
00:06:33,890 --> 00:06:38,690
공식은 다음 슬라이드에서 드리겠습니다.

136
00:06:35,420 --> 00:06:40,520
z가 마이너스인 경우에 derivative가 

137
00:06:38,690 --> 00:06:42,940
0 대신에, 이렇게 생긴 슬로프 값을

138
00:06:40,520 --> 00:06:47,900
갖습니다. 이것은 아까 얘기했던 것처럼

139
00:06:42,940 --> 00:06:51,170
Leaky ReLu라고 하는데요, 이 함수가

140
00:06:47,900 --> 00:06:53,900
보통 ReLu activation 함수보다는 잘

141
00:06:51,170 --> 00:06:54,860
작동하는데요, 실제로는 잘 사용되니는
않습니다.

142
00:06:53,900 --> 00:06:56,770
둘다 사용해도 괜찮은데요, 

143
00:06:54,860 --> 00:06:59,330
만약 여러분이 줄중에 한개를

144
00:06:56,770 --> 00:07:01,460
골라야 한다면 저 같은 경우엔 그냥 ReLu를 고르겠습니다.

145
00:06:59,330 --> 00:07:04,460
그리고 두개 모두, ReLu와 Leaky ReLu의 장점은, 

146
00:07:01,460 --> 00:07:06,500
많은 z값에 대해서, 

147
00:07:04,460 --> 00:07:08,150
activation 함수의 derivative가 

148
00:07:06,500 --> 00:07:11,870
즉, activation 함수의 기울기가 

149
00:07:08,150 --> 00:07:13,970
0가 매우 다릅니다. 실제로는

150
00:07:11,870 --> 00:07:15,920
그러므로 ReLu activation 함수를

151
00:07:13,970 --> 00:07:18,590
신경망에서 이용하는 것이 

152
00:07:15,920 --> 00:07:20,810
tan h 함수를 이용하는 것보다 훨씬

153
00:07:18,590 --> 00:07:23,840
더 빠를 것입니다. 또는 시그모이드

154
00:07:20,810 --> 00:07:26,420
activation 함수보다도 말이죠. 주된 이유는, 

155
00:07:23,840 --> 00:07:28,700
이렇게 함수의 기울기가 0으로

156
00:07:26,420 --> 00:07:31,580
근접하는 효과가 덜하기 때문입니다.

157
00:07:28,700 --> 00:07:33,950
이런 효과는 러닝의 속도를 느리게 하는데요, 

158
00:07:31,580 --> 00:07:36,710
저는 아는데요, 여기 z의 반 정도 되는 범위는

159
00:07:33,950 --> 00:07:39,050
ReLu 의 기울기가 0인 것을 말이죠. 

160
00:07:36,710 --> 00:07:41,120
하지만 실제로는 숨겨진 유닛이 충분하게

161
00:07:39,050 --> 00:07:43,700
Z가 0보다 더 큰 값을 갖는 경우인데요, 

162
00:07:41,120 --> 00:07:45,800
그렇기 때문에 대부분의 트레이닝 예시에 대해서
러닝의 속도가 계속 빠를 것입니다.

163
00:07:43,700 --> 00:07:47,600
그러면 이제 activation 함수들의

164
00:07:45,800 --> 00:07:50,030
장점과 단점에 대해서 복습해보겠습니다.

165
00:07:47,600 --> 00:07:52,790
여기는 시그모이드 activation 함수인데요, 

166
00:07:50,030 --> 00:07:54,410
이것은 이진분류법에서 결과값 층 관련한

167
00:07:52,790 --> 00:07:56,330
것 외에는 절대로 사용하지 말라고 

168
00:07:54,410 --> 00:07:59,540
말씀드리겠습니다. 거의 전혀 사용하지 마십시요. 

169
00:07:56,330 --> 00:08:02,720
제가 사용하지 않는 이유는, 

170
00:07:59,540 --> 00:08:05,060
tan h 함수가 거의 항상 

171
00:08:02,720 --> 00:08:12,080
우월하기 때문입니다. 이것이 

172
00:08:05,060 --> 00:08:13,430
tan h 함수인데요, 그리고 

173
00:08:12,080 --> 00:08:15,490
기본값 함수, 사장 흔하게 사용되는

174
00:08:13,430 --> 00:08:19,100
activation 함수는 ReLu 함수인데요, 

175
00:08:15,490 --> 00:08:23,660
여기 보이는 것인데요, 

176
00:08:19,100 --> 00:08:26,600
여러분이 어떤 것을 써야할지 잘 모르겠는 경우, 
이것을 쓰십시요, 

177
00:08:23,660 --> 00:08:31,930
그리고 또 Leaky ReLu를 언제든지

178
00:08:26,600 --> 00:08:36,659
사용하세요, 여기가 

179
00:08:31,930 --> 00:08:40,390
0.01 z, z 가 될수도 있겠죠. 그러면

180
00:08:36,659 --> 00:08:43,810
a 는 0.01 곱하기 z 와 z에서의 최대값입니다.

181
00:08:40,390 --> 00:08:46,200
그런 값은 여기 함수에서 구부러지게 하는데요, 

182
00:08:43,810 --> 00:08:51,670
그러면 여러분은 여기 상수의 값이 왜

183
00:08:46,200 --> 00:08:53,380
0.01인지 의문을 가질 수 있는데요, 

184
00:08:51,670 --> 00:08:54,670
이런 경우, 여러분은 또 하나의 러닝 알고리즘 파라미터를

185
00:08:53,380 --> 00:08:58,480
만들 수도 있는데요, 어떤 사람들은 그렇게 하는 것이 더 잘 작동한다고도 
이야기 하는데요, 

186
00:08:54,670 --> 00:08:59,649
하지만 대부분의 사람들은 새로 파라미터를 만들지 않습니다.

187
00:08:58,480 --> 00:09:01,360
여러분의 어플에서 만약

188
00:08:59,649 --> 00:09:03,430
여러분이 직접 시도해보시겠다고 하면

189
00:09:01,360 --> 00:09:05,800
얼마든지 그렇게 하셔도 좋습니다.

190
00:09:03,430 --> 00:09:08,290
어떻게 작동하는 한번 확인해보려면 말이죠 

191
00:09:05,800 --> 00:09:09,880
좋은 결과를 주면, 계속 사용할 수도 있겠죠. 

192
00:09:08,290 --> 00:09:11,620
이런 내용들이 바라건대, 여러분이 신경망에서 사용할 수 있는 activation 함수의

193
00:09:09,880 --> 00:09:13,870
초이스들에 대한 이해를 돕는데 

194
00:09:11,620 --> 00:09:15,940
일조했으면 좋겠습니다. 여러분이 딥러닝에서 볼 수 있는 테마 중 한가지는

195
00:09:13,870 --> 00:09:18,130
바로 많은 선택권이 있다는

196
00:09:15,940 --> 00:09:20,110
것인데요, 신경망을 어떻게

197
00:09:18,130 --> 00:09:22,089
만드는지에 대해서 말이죠, 

198
00:09:20,110 --> 00:09:24,430
숨겨진 유닛의 개수를 어떻게 설정할지에서부터, 

199
00:09:22,089 --> 00:09:25,839
어떤 activation 함수를 기용할지와, 

200
00:09:24,430 --> 00:09:28,480
어떻게 초기화를 시킬지 등의

201
00:09:25,839 --> 00:09:30,880
여러가지 결정을 필요로 합니다.

202
00:09:28,480 --> 00:09:33,279
그리고 이런 선택권들이 있기 때문에

203
00:09:30,880 --> 00:09:35,649
또 동시에 여러분의 문제에서 어떤 것들이 가장 잘 작동할지에

204
00:09:33,279 --> 00:09:37,270
대한 좋은 가이드라인을 찾기가 자주 어렵습니다.

205
00:09:35,649 --> 00:09:39,070
그러므로 해당 코스를 통해서

206
00:09:37,270 --> 00:09:40,839
저는 여러분에게 제가 이 분야에서 보는

207
00:09:39,070 --> 00:09:43,450
것들과 어떤 것이 더 수요가 있고, 

208
00:09:40,839 --> 00:09:45,520
또는 인기가 없는지 어플리케이션에 따라 

209
00:09:43,450 --> 00:09:46,930
확인하고, 사실 

210
00:09:45,520 --> 00:09:49,450
미리 어떤 것이 가장 잘 작동할지 

211
00:09:46,930 --> 00:09:51,400
여부를 아는 것은 굉장히 어려운 

212
00:09:49,450 --> 00:09:52,930
일이죠. 그러면 가장 흔하지만서도 좋은

213
00:09:51,400 --> 00:09:54,940
방법은 이러한 activation 함수중에서

214
00:09:52,930 --> 00:09:57,700
어떤 것이 가장 좋은지 확실하지 않으면, 
바로 다 시도해 보는 것입니다.

215
00:09:54,940 --> 00:10:00,010
그러고나서 holdout validation set를 

216
00:09:57,700 --> 00:10:02,529
기준으로 평가하거나, devlopment set를

217
00:10:00,010 --> 00:10:04,480
기준으로 평가합니다. 이 부분은 나중에 다루겠습니다.

218
00:10:02,529 --> 00:10:08,350
어떤 것이 더 잘 작동하는지 알아내고, 

219
00:10:04,480 --> 00:10:10,180
직접 시도하는 것입니다. 이런 것들을 

220
00:10:08,350 --> 00:10:13,510
본인 어플에서 직접 시도해보면서 

221
00:10:10,180 --> 00:10:16,240
신경망의 구조를 증명하는 것이

222
00:10:13,510 --> 00:10:18,130
미래에도 좋을 것입니다. 

223
00:10:16,240 --> 00:10:20,550
특이한 성향도 문제고, 

224
00:10:18,130 --> 00:10:23,440
알고리즘의 진화에 맞게 적용하는 것이 중요합니다

225
00:10:20,550 --> 00:10:25,630
제가 만약에 여러분에게 ReLu activation 함수를 

226
00:10:23,440 --> 00:10:27,339
무조건 쓰고 다른 것들은 절대

227
00:10:25,630 --> 00:10:29,440
사용하지 말라고 하면 여러분이 

228
00:10:27,339 --> 00:10:30,790
작업하고 있는 문제에 적용할 수도, 

229
00:10:29,440 --> 00:10:32,410
또는 전혀 적용이 안될 수 있습니다.

230
00:10:30,790 --> 00:10:36,220
근접한 미래에서는 먼 미래에서든 말이죠, 

231
00:10:32,410 --> 00:10:37,870
그럼 이 내용이 activation 함수의 선택에

232
00:10:36,220 --> 00:10:39,310
대한 내용이였는데요, 여러분은 

233
00:10:37,870 --> 00:10:41,459
가장 인기가 있는 activation 함수들을

234
00:10:39,310 --> 00:10:44,260
봤는데요, 또 다른 질문을 할 수 있는 부분이 있는데요

235
00:10:41,459 --> 00:10:45,160
왜 애초에 activation 함수를 

236
00:10:44,260 --> 00:10:46,959
사용해야 하는 

237
00:10:45,160 --> 00:10:49,240
것 일까요? 이런 내용에 

238
00:10:46,959 --> 00:10:49,779
대해서 다음 비디오에서 이야기 하겠습니다.

239
00:10:49,240 --> 00:10:52,240
그럼

240
00:10:49,779 --> 00:10:54,430
다음 비디오에서는 여러분이 

241
00:10:52,240 --> 00:10:58,259
왜 신경망에서는 어떤 유형의 비선형

242
00:10:54,430 --> 00:10:58,259
activation 함수가 필요한지 보게될 것입니다.