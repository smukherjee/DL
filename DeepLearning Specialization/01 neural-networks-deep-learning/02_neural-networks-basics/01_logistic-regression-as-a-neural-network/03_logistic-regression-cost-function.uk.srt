1
00:00:00,000 --> 00:00:01,530
В попередньому відео

2
00:00:01,530 --> 00:00:04,227
ти бачив/ла модель логістичної регресії (ЛР).

3
00:00:04,227 --> 00:00:07,526
Щоб натренувати параметри w і b моделі ЛР,

4
00:00:07,526 --> 00:00:10,570
нам потрібно визначити функцію Витрат.

5
00:00:10,570 --> 00:00:14,430
Давай розглянемо функцію Витрат, яку ми можемо 
використовувати для тренування ЛР.

6
00:00:14,430 --> 00:00:18,195
Згадаємо з попереднього слайду що нам треба було знайти.

7
00:00:18,195 --> 00:00:20,792
Тож, наша вихідна ŷ= сигмоїді від

8
00:00:20,792 --> 00:00:24,690
wᵀx+b, а сигмоїда від z визначається ось так.

9
00:00:24,690 --> 00:00:27,600
Тож, щоб навчити параметри нашої моделі, нам дано

10
00:00:27,600 --> 00:00:31,200
тренувальний набір з m тренувальних зразків і,

11
00:00:31,200 --> 00:00:34,060
природно, що ми хочемо знайти параметри w

12
00:00:34,060 --> 00:00:37,781
і b, такі, щоб хоча б для тренувального набору, вивід або

13
00:00:37,781 --> 00:00:40,225
передбачення, які ми маємо для тренувального набору,

14
00:00:40,225 --> 00:00:43,260
які ми записуємо справа як ŷ⁽ⁱ⁾, були наближеними до

15
00:00:43,260 --> 00:00:47,720
емпіричних позначок y⁽ⁱ⁾, які ми маємо в тренувальному наборі.

16
00:00:47,720 --> 00:00:52,110
Тож, щоб більш детально описати рівняння вгорі,

17
00:00:52,110 --> 00:00:56,205
ми мусимо сказати, що ŷ=, 
як визначено вгорі для

18
00:00:56,205 --> 00:01:00,930
тренувального зразка x і, очевидно, 
для кожного тренувального зразка,

19
00:01:00,930 --> 00:01:03,240
ми використовуємо ось такі верхні індекси в

20
00:01:03,240 --> 00:01:07,710
дужках, щоб позначати і відрізняти тренувальні зразки.

21
00:01:07,710 --> 00:01:12,870
Наше передбачення для тренувального зразка ⁽ⁱ⁾, 
тобто ŷ, буде

22
00:01:12,870 --> 00:01:18,835
отримане застосуванням сигмоїди до wᵀx⁽ⁱ⁾

23
00:01:18,835 --> 00:01:25,905
(вхідні дані тренувального зразка) +b. 
z⁽ⁱ⁾ можна також визначити наступним чином:

24
00:01:25,905 --> 00:01:30,110
z⁽ⁱ⁾=wᵀx⁽ⁱ⁾+b.

25
00:01:30,110 --> 00:01:31,350
Тож, протягом цього курсу

26
00:01:31,350 --> 00:01:33,966
ми будемо використовувати оцю нотативну конвенцію,

27
00:01:33,966 --> 00:01:41,605
згідно якої, верхній індекс ⁽ⁱ⁾ відсилає до даних

28
00:01:41,605 --> 00:01:47,615
x або y, або z, чи інших, пов'язаних з i-тим тренувальним зразком

29
00:01:47,615 --> 00:01:50,885
або i-тим зразком.

30
00:01:50,885 --> 00:01:54,840
Ось що означає верхній індекс в дужках.

31
00:01:54,840 --> 00:01:57,630
Тепер давай розглянемо функцію Втрати або функцію Похибки,

32
00:01:57,630 --> 00:02:01,315
яку ми можемо використовувати щоб оцінити 
наскільки добре працює наш алгоритм.

33
00:02:01,315 --> 00:02:06,015
Одне з того, що можна зробити, 
це обчислити втрату між тим, що видає алгоритм ŷ

34
00:02:06,015 --> 00:02:12,320
і справжньою міткою як, скажімо, 
квадратичну похибку або половину квадратичної похибки.

35
00:02:12,320 --> 00:02:14,975
Виявляється, що зробити це можна,

36
00:02:14,975 --> 00:02:17,670
але для ЛР, це, як правило, не використовується.

37
00:02:17,670 --> 00:02:21,000
Тому, що перед навчанням параметрів,

38
00:02:21,000 --> 00:02:25,682
виявляється, що проблема оптимізації 
(про яку ми поговоримо пізніше) стає неопуклою.

39
00:02:25,682 --> 00:02:30,105
Тож, в кінцевому підсумку, проблема оптимізації 
буде мати багато локальних оптимумів.

40
00:02:30,105 --> 00:02:33,285
А Градієнтний спуск може не знайти глобального оптимуму.

41
00:02:33,285 --> 00:02:35,580
Якщо ти не зрозумів/ла кілька останніх коментарів,

42
00:02:35,580 --> 00:02:38,320
не переймайся, ми повернемось до цього пізніше.

43
00:02:38,320 --> 00:02:40,990
Все, що необхідно запам'ятати, - це,

44
00:02:40,990 --> 00:02:44,620
що ця функція L, що називається функцією Втрати, - це функція, яка буде

45
00:02:44,620 --> 00:02:51,265
необхідна для оцінки наскільки точною є наша вихідна ŷ 
відносно справжньої мітки y.

46
00:02:51,265 --> 00:02:54,345
І що квадратична похибка, схоже, - виправданий вибір,

47
00:02:54,345 --> 00:02:58,160
якщо вона не впливає негативно на роботу Градієнтного спуску.

48
00:02:58,160 --> 00:03:00,500
Тож, в ЛР ми, насправді, будемо визначати

49
00:03:00,500 --> 00:03:05,695
дещо іншу функцію Втрати, що виконує роль схожу до квадратичної функції,

50
00:03:05,695 --> 00:03:08,910
але яка дасть нам опуклий результат оптимізації

51
00:03:08,910 --> 00:03:13,530
і, як ми побачимо пізніше, яку буде набагато легше оптимізувати.

52
00:03:13,530 --> 00:03:17,310
Тож, те, що ми використовуємо в ЛР, є, насправді,

53
00:03:17,310 --> 00:03:21,795
функція Втрати, яку я зараз тут запишу:

54
00:03:21,795 --> 00:03:31,740
-y*log(ŷ)+(1-y)*

55
00:03:31,740 --> 00:03:34,600
log(1-ŷ)

56
00:03:34,600 --> 00:03:38,785
Пролиємо трохи світла на зміст цієї функції Втрати.

57
00:03:38,785 --> 00:03:41,285
Пам'ятай, що коли ми використовуємо

58
00:03:41,285 --> 00:03:45,820
квадратичну похибку, то хочемо, 
щоб вона була настільки малою, наскільки це можливо.

59
00:03:45,820 --> 00:03:48,680
І з цією функцією Втрати ЛР

60
00:03:48,680 --> 00:03:51,495
те саме - ми теж хочемо, 
щоб вона була настільки малою, наскільки це можливо.

61
00:03:51,495 --> 00:03:53,508
Щоб зрозуміти чому це важливо,

62
00:03:53,508 --> 00:03:55,260
давай розглянемо такі 2 випадки.

63
00:03:55,260 --> 00:03:56,570
В першому випадку

64
00:03:56,570 --> 00:03:59,430
скажімо y=1, тоді функція Втрати

65
00:03:59,430 --> 00:04:05,415
L(ŷ, y) = першому доданку.

66
00:04:05,415 --> 00:04:08,735
тобто =-log(ŷ),

67
00:04:08,735 --> 00:04:10,770
а y=1. Тому що, якщо y=1,

68
00:04:10,770 --> 00:04:14,070
то другий доданок, де ми маємо (1-y), =0.

69
00:04:14,070 --> 00:04:19,880
Тож звідси випливає, що якщо y=1, 
нам треба, щоб log(ŷ) був якомога більший.

70
00:04:19,880 --> 00:04:26,040
Значить, потрібно, щоб log(ŷ) був великим,

71
00:04:26,040 --> 00:04:32,935
якомога більшим, що значить, 
що потрібно щоб ŷ був великим.

72
00:04:32,935 --> 00:04:35,170
Але через те, що ŷ, як відомо,

73
00:04:35,170 --> 00:04:38,440
це - сигмоїда, яка ніколи не може буде >1.

74
00:04:38,440 --> 00:04:41,850
Скажімо так: коли y=1

75
00:04:41,850 --> 00:04:44,050
потрібно щоб ŷ був настільки великим, 
наскільки це можливо.

76
00:04:44,050 --> 00:04:48,220
Проте, він ніколи не може бути >1, 
то потрібно щоб він був якомога ближчим до 1.

77
00:04:48,220 --> 00:04:50,740
Інший приклад - коли y=0.

78
00:04:50,740 --> 00:04:55,375
Якщо y=0, тоді перший доданок функції Втрати =0, тому що

79
00:04:55,375 --> 00:05:01,290
y=0. І тоді другий доданок визначає функцію Втрати.

80
00:05:01,290 --> 00:05:07,210
Тож Втрата буде =-log(1-ŷ).

81
00:05:07,210 --> 00:05:11,480
Тож якщо, застосовуючи навчання, 
ми намагаємося зробити функцію Втрати маленькою,

82
00:05:11,480 --> 00:05:19,450
це значить, що ми хочемо, щоб log(1-ŷ) був великим.

83
00:05:19,450 --> 00:05:22,050
А через цей знак мінус

84
00:05:22,050 --> 00:05:24,660
і з тої ж причини ми можемо дійти висновку,

85
00:05:24,660 --> 00:05:30,870
що ця функція Втрати намагається зробити ŷ якомога меншим.

86
00:05:30,870 --> 00:05:34,320
І знову ж, через те, що ŷ мусить бути між 0 і 1,

87
00:05:34,320 --> 00:05:38,155
ми маємо, що, якщо y=0, то

88
00:05:38,155 --> 00:05:43,790
наша функція Втрати підбиратиме параметри так, 
щоб зробити ŷ якомога ближче до 0.

89
00:05:43,790 --> 00:05:48,305
Тепер. Існує багато функцій з подібним ефектом - якщо y=1,

90
00:05:48,305 --> 00:05:52,950
то ми намагаємось зробити ŷ великим, 
а якщо y=0, то ми намагаємось зробити ŷ малим.

91
00:05:52,950 --> 00:05:55,150
Тут ми даємо лише (те, що зеленим)

92
00:05:55,150 --> 00:05:59,920
поверхневе обґрунтування цієї конкретної функції Втрати. Ми надамо

93
00:05:59,920 --> 00:06:03,970
пізніше необов'язкове відео з більш повним обґрунтуванням

94
00:06:03,970 --> 00:06:08,500
чому ми хочемо використовувати в ЛР 
саме цю конкретну формулу.

95
00:06:08,500 --> 00:06:13,630
Отож, функція Втрати була визначена для одного тренувального зразка.

96
00:06:13,630 --> 00:06:16,760
Вона вимірює наскільки добре ми справляємось 
з одним тренувальним зразком.

97
00:06:16,760 --> 00:06:21,148
Зараз я збираюсь визначити те, що називається функцією Витрат,

98
00:06:21,148 --> 00:06:24,690
яка вимірює наскільки добре ми справляємось 
з усім тренувальним набором.

99
00:06:24,690 --> 00:06:28,660
Тож функція Витрат J, що застосовується до

100
00:06:28,660 --> 00:06:33,130
наших параметрів w і b, буде середнім, себто 1/m,

101
00:06:33,130 --> 00:06:43,270
від суми функцій Втрати 
застосованих до кожного тренувального зразка по черзі.

102
00:06:43,270 --> 00:06:45,435
Де ŷ - звичайно,

103
00:06:45,435 --> 00:06:49,570
передбачення виведене нашим алгоритмом ЛР, 
що використовує, як відомо,

104
00:06:49,570 --> 00:06:52,430
певний набір параметрів w і b.

105
00:06:52,430 --> 00:06:54,480
Тепер просто розпишемо цю формулу.

106
00:06:54,480 --> 00:06:58,010
=-1/m,

107
00:06:58,010 --> 00:07:03,550
сума від 1 до m визначених функцій Втрати.

108
00:07:03,550 --> 00:07:07,530
Тобто, y(i)*log(ŷ(i))

109
00:07:07,530 --> 00:07:14,530
+(1-y(i))*log(1-ŷ(i)).

110
00:07:14,530 --> 00:07:17,880
Думаю, я можу дописати тут квадратні дужки.

111
00:07:17,880 --> 00:07:20,945
І винести за них мінус.

112
00:07:20,945 --> 00:07:23,665
Тож, термінологія, яку я використовуватиму наступна:

113
00:07:23,665 --> 00:07:29,120
функція Втрати застосовується до одного тренувального зразка (ось так),

114
00:07:29,120 --> 00:07:33,010
а функція Витрат - це витрати параметрів.

115
00:07:33,010 --> 00:07:36,115
Тож, тренуючи свою модель ЛР,

116
00:07:36,115 --> 00:07:38,980
ми будемо намагатись знайти такі параметри w і b, що

117
00:07:38,980 --> 00:07:43,475
мінімізують загальну функцію Витрат J, записану внизу.

118
00:07:43,475 --> 00:07:48,040
Отже, ти щойно побачив/ла побудову алгоритму ЛР,

119
00:07:48,040 --> 00:07:50,770
функцію Втрати для тренувального зразка і

120
00:07:50,770 --> 00:07:54,190
загальну функцію Витрат для параметрів твого алгоритму.

121
00:07:54,190 --> 00:07:59,485
Виявляється, що ЛР може розглядатись 
як дуже маленька нейронна мережа (НМ).

122
00:07:59,485 --> 00:08:01,905
В наступному відео ми про це поговоримо, тож ти почнеш

123
00:08:01,905 --> 00:08:04,965
розуміти що роблять НМ.

124
00:08:04,965 --> 00:08:08,230
Тож давай перейдемо до наступного відео про те,

125
00:08:08,230 --> 00:08:11,630
як розглянути ЛР як дуже маленьку НМ.