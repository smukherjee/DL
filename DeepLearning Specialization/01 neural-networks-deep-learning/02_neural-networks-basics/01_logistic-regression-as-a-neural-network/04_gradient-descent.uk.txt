Ти бачив/ла модель логістичної регресії (ЛР). Ти бачив/ла функцію Втрати, 
що оцінює наскільки добре ти справляєшся з одним тренувальним зразком. Ти також бачив/ла функцію Витрат, 
що оцінює наскільки добре наші параметри w і b справляються з усім тренувальним набором. Тепер давай поговоримо про те, 
як можна використати алгоритм Градієнтного спуску для тренування чи то навчання параметрів w і b на нашому тренувальному наборі. Згадаємо. Це - знайомий вже алгоритм ЛР. В другому рядку ми маємо функцію Витрат J, що є функцією від наших параметрів w і b, і яка визначається як середнє, себто 1/m помножене на суму функцій Втрати. Тож функція Втрати оцінює наскільки добре наш алгоритм обчислює ŷ⁽ⁱ⁾ для кожного тренувального зразка в порівнянні з базовою справжньою міткою y⁽ⁱ⁾ 
для кожного тренувального зразка. Повна формула розписана справа. Функція Витрат оцінює наскільки добре параметри w і b підходять для усього тренувального набору. Тож для того, щоб навчити набір параметрів w і b, 
природно, що ми захочемо знайти такі w і b, які зроблять функцію Витрат J(w, b) якомога меншою. Ось ілюстрація Градієнтного спуску. На цій діаграмі горизонтальні осі відображають значення параметрів w і b. На практиці, w має більші значення, але для відображення на діаграмі, давай покажемо w і b 
приблизно рівними реальними числами. Функція Витрат J(w, b) в цьому випадку буде площиною між цими двома 
горизонтальними осями w і b. А висота площини буде відображати значення J(w, b) 
в конкретній точці. Тож те, що ми хочемо знайти, це значення w і b, що відповідають мінімуму функції Витрат J. Як бачимо, дана функція Витрат J є опуклою. Вона має єдину велику "чашу", через це є опуклою функцією і цим відрізняється від отаких функцій, які є неопуклими і мають багато мінімумів. Тож те, що наша функція Витрат J(w, b), за означенням, є опуклою, є вагомою причиною для вибору саме її функцією Витрат J для ЛР. Щоб знайти потрібні значення для параметрів ми спочатку ініціалізуємо w і b деякими первинними значеннями, хай вони будуть = вказаним нам червоною точкою. Для ЛР підходять практично будь-які методи ініціалізації. зазвичай, при ініціалізації присвоюють 0. Випадкові числа також підходять, але зазвичай ніхто цього не робить для ЛР. Це через те, що функція опукла. Тому немає значення де ініціалізувати 
(мається на увазі точка на графіку), все одно ми дістанемось одної точки або близько до неї. Градієнтний спуск стартує з ініціалізованої точки і далі робить крок в напрямку найнижчої точки. Тож після першого кроку Градієнтний спуск може опинитись ось тут, бо він намагається крокувати якомога стрімкіше вниз і якомога швидше. Це одна ітерація Градієнтного спуску. Після двох ітерацій Градієнтного спуск може опинитись тут, після трьох - тут і т.д. (Схоже, зараз наступні кроки будуть прикриті передньою стороною графіка). 
І так поки, сподіваємось, збіжиться з цим глобальним оптимумом 
чи з точкою близькою до нього. Тож це зображення ілюструє алгоритм Градієнтного спуску. Давай опишемо це більш детально. Задля ілюстрації, давай скажемо, що деяка функція J(w), яку ми хочемо мінімізувати, виглядає приблизно так. Щоб було легше зобразити, я поки ігноруватиму b. Просто щоб отримати двовимірний графік замість багатовимірного. Тож Градієнтний спуск робить наступне. Ми будемо повторно виконувати наступну корекцію. Ми братимемо значення w і коригувати його. Позначати корекцію будемо ":=". Коригуємо w до w-α помножене на (це похідна) dJ(w)/dw. І будемо повторювати це поки алгоритм не збіжиться. Дещо про позначення. α - це навчальний темп, він контролює величину кроку в кожній ітерації Градієнтного спуску. Пізніше ми обговоримо як вибирати навчальний темп α. Друга величина, ось ця похідна, це фактично корекція або величина зміни параметру w, 
яку ми хочемо застосувати. Коли ми почнемо писати код для застосування Градієнтного спуску, ми будемо використовувати конвенцію, згідно якої назву змінної в коді будемо записувати як dw, щоб позначити цю похідну. Тож при написанні коду будемо писати щось типу w:=w-α*dw. Тож dw ми будемо використовувати як назву змінної 
для позначення цієї похідної. Тепер давай пересвідчимось, 
що дана корекція Градієнтним спуском має зміст. Скажімо, w знаходиться ось тут. Тож ми знаходимось в цій точці функції Витрат J(w). Запам'ятай, що згідно визначення, похідна - це нахил [кутовий коефіцієнт дотичної] функції в заданій точці. Тобто, нахил - це відношення висоти до ширини 
[протилежного катета до прилеглого], ось цього маленького трикутника, що і є дотичною до J(w) в цій точці. Тут бачимо, що похідна має позитивне значення, w коригується на величину 
w мінус навчальний темп помножений на похідну. Похідна має позитивне значення, тож ми зменшуємо w і просуваємось вліво, здійснюючи цей крок. Тож Градієнтний спуск змушує алгоритм повільно зменшувати параметр, якщо ми почали з оцього великого значення w. Інший приклад. Якби w було отут, то в цій точці нахил dJ(w)/dw 
матиме негативне значення, тож корекція Градієнтним спуском буде віднімати добуток 
α і негативного числа. А значить, буде повільно збільшувати w, тож ми будемо збільшувати і збільшувати w з кожною успішною ітерацією Градієнтного спуску. Тож, сподіваємось, що незалежно від ініціалізації (зліва чи справа) Градієнтний спуск буде рухатись 
назустріч оцьому глобальному мінімуму. Якщо ти незнайомий/а з похідними чи 
диференціальним численням і не розумієш, що означає dJ(w)/dw, не переймайся дуже через це. Ми поговоримо дещо детальніше про похідні в наступному відео. Якщо ти добре ознайомлений/а з диф.численням, то, можливо, будеш глибше розуміти роботу нейронних мереж (НМ). Проте, навіть якщо ти зовсім незнайомий/а з диф.численням, в кількох наступних відео ми дамо тобі розуміння похідних і диф.числення достатнє для ефективного використання НМ. Зараз достатньо розуміти, що цей вираз відображає нахил функції і що нам потрібно знати нахил функції і 
актуальні значення параметрів, щоб ми могли крокувати якомога стрімкіше вниз, тобто могли знати напрямок, який поведе нас вниз по функції Витрат J. Тож ми записали Градієнтний спуск для J лише для параметра w. А в ЛР наша функція Витрат залежить і від w і від b. Тож в цьому випадку, вкладений цикл Градієнтного спуску, 
ось цей вираз, те, що потрібно повторювати, стає наступним. Робимо корекцію w: w мінус навчальний темп помножений на похідну від J(w, b) по w. І робимо корекцію b: b мінус навчальний темп помножений на похідну функції Витрат по b. Тож ці 2 вирази внизу - це фактична корекція, яку ми впроваджуємо. Також хочу просто згадати одну конвенцію позначень 
в диф.численні, яка може декого заплутати. Я не вважаю, що розуміти диф.числення це дуже важливо, 
але якщо ти побачиш отаке, то я хочу запевнити, 
що не варто надто цим перейматись. Я маю на увазі, що в диф.численні оцей вираз ми записуємо ось так, отаким черв'ячком. Цей символ, це насправді маленьке d, записане вигадливим стилізованим шрифтом. 
Коли ти побачиш його, то знай, що це позначення похідної J(w, b), 
тобто нахил функції J(w, b), тобто величина нахилу функції в напрямку до w. І, згідно правил позначень в диф.численні, 
яке, на мою думку, не надто логічне, проте, згідно цих правил позначень в диф.численні, 
які, на мою думку, набагато складніші ніж необхідно, якщо J є функцією від двох чи більше змінних, то замість використання маленького d 
використовується цей вигадливий знак. Він називається знаком частинної похідної. Але не переймайся цим, бо якщо J є функцією від однієї змінної, 
то використовується маленька d. Тож, єдина причина для використанням цього вигадливого знаку частинної похідної замість маленької d (як вгорі) - це чи є J функцією від двох чи більше змінних. В цьому випадку, використовується знак частинної похідної. А якщо J є функцією від однієї змінної, то використовується маленька d. Це одне з вигадливих правил позначень диф.числення, які, на мою думку, лише ускладнюють те, що цього не вимагає. Тож, якщо ти бачиш знак частинної похідної, 
то це значить лише, що обчислюється значення нахилу функції від однієї зі змінних. Аналогічно, щоб дотримуватись, скажімо, 
формально правильних математичних позначень в диф.численні, 
так як J тут має два вхідних параметри, а не один, цей вираз внизу повинен бути записаний зі знаком частинної похідної. Але це насправді те ж саме, майже те саме, що й маленька d. Врешті, при реалізації цього в коді, ми використовуватимемо конвенцію, згідно якої 
оце значення - число, на яке ми коригуємо w - в коді позначається dw. А оце значення - число, на яке ми хочемо скоригувати b - в коді позначається як змінна db. Добре. Отож, так реалізується Градієнтний спуск. Тепер, якщо ти не стикався з диф.численням багато років, 
я розумію, що, можливо, до цього часу ти звик/ла використовувати 
багато інших похідних диф.числення. Якщо це так, не переймайся. В наступному відео ми дано краще розуміння похідних. І, навіть, без глибокого математичного розуміння диф.числення, з просто інтуїтивним розумінням диф.числення ти будеш здатен/на заставляти НМ ефективно працювати. Тож з цим, давай перейдемо до наступного відео, 
в якому ми детальніше обговоримо похідні.