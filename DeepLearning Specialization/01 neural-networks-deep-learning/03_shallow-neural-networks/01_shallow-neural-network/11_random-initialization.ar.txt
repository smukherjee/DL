عندما تقوم بتغيير شبكتك العصبية، يكون من المهم تهيئة
الأثقال بشكل عشوائي. بالنسبة للانحدار اللوجستي، كان من
المقبول تهيئة الأثقال إلى صفر. ولكن بالنسبة للشبكة العصبية، إذا قمت
بتهيئة الأثقال للمتغيرات إلى صفر ثم تطبيق الانحدار التدريجي،
فإنها لن تعمل. دعونا نرى السبب. إذًا، لديك هنا خاصيتا إدخال، إذًا n0=2، ووحدتان مخفيتان، إذًا n1=2. وبالتالي فإن المصفوفة المرتبطة
بالطبقة المخفية، w1، ستكون 2 في 2. لنفترض أنك قمت بتهيئته للكل إلى 0،
إذًا مصفوفة 0 0 0 0، 2 في 2. ولنفترض أن B1 يساوي أيضًا 0 0. يتضح أن تهيئة مصطلحات
التحيز b إلى 0 مقبولة في الواقع، ولكن تهيئة w لجميع 0 تمثل مشكلة. إذًا، فالمشكلة في هذه
الصياغة هي أنه بالنسبة لأي مثال تقوم بتقديمه،
فسوف يكون لديك a1,1 وa1,2، سيكونان متساويين، تمام؟ إذًا، فهذا التنشيط وذاك
التنشيط سيكونان متشابهين، لأن كلتا الوحدتين المخفيتين تقومان
بحساب نفس الدالة بالضبط. وبعد ذلك، عندما تقوم
بحساب الانتشار الخلفي، يتبين أن dz11 وdz12 سيكونان أيضًا بنفس
اللون بالتناظر، تمام؟ كلا هاتين الوحدتين المخفيتين
سيقومان بالتهيئة بالطريقة نفسها. من الناحية الفنية، بالنسبة لما أقول، أفترض أن الأثقال الصادرة
متطابقة أيضًا. إذًا، w2 هذا يساوي 0 0. لكن إذا قمت بتهيئة الشبكة
العصبية بهذه الطريقة، فإن هذه الوحدة المخفية وتلك
الوحدة المخفية تتطابقان تمامًا. في بعض الأحيان،
تكونان متماثلتين تمامًا، مما يعني أنهما تكملان
نفس الدالة بالضبط. وعن طريق نوع من الإثبات بالاستقراء، يتبين أنه بعد كل تكرار فردي
للتدريب، لا تزال الوحدتان المخفيتان تقومان بحساب
الدالة نفسها بالضبط. بما أن [كلام غير مسموع] يبين أن dw
سوف يكون مصفوفة تبدو هكذا. حيث يأخذ كل صف نفس القيمة. إذًا، نقوم بتنفيذ تحديث للثقل. إذًا، عند تنفيذ تحديث للثقل،
يتم تحديث w1 ليكون w1 - ألفا×dw تجد أن w1، بعد كل تكرار، سيكون صفها الأول
مساويًا للصف الثاني. لذلك من الممكن إنشاء إثبات
بالاستقراء أنه إذا قمت بتهيئة جميع الطرق،
فإن كل قيم w إلى 0، ثم لأن كلتا الوحدتين المخفيتين
تبدأ بحساب الدالة نفسها. وكلا الوحدتين المخفيتين لهما التأثير
نفسه على وحدة الإخراج، ثم بعد تكرار واحد،
يظل نفس المنطوق صحيحًا، والوحدتان المخفيتان تظلان متماثلتان. وبالتالي، عن طريق الاستقراء،
بعد تكرارين، وثلاثة تكرارات وهكذا، بغض النظر عن مدة تدريب
شبكتك العصبية، تظل كلتا الوحدتين المخفيتين
تحسبان الدالة نفسها بالضبط. وبالتالي في هذه الحالة، لا توجد في الواقع
نقطة تمتلك أكثر من وحدة مخفية. لأنها جميعًا تحسب
الشيء نفسه. وبالطبع، بالنسبة للشبكات العصبية الأكبر،
مثل التي تتكون من ثلاث خصائص وربما عدد كبير جدًا من الوحدات المخفية، فإن وسيطة مشابهة تعمل على إظهار ذلك
مع شبكة عصبية مثل هذه. [كلام غير مسموع] نرسم جميع الحواف،
إذا قمت بتهيئة الأثقال إلى صفر، ثم كانت كل الوحدات
المخفية لديك متماثلة. وبغض النظر عن المدة التي
تقوم فيها بترقية المركز، تستمر جميعها في حساب
الدالة نفسها بالضبط. إذًا، هذا ليس مفيدًا،
لأنك تريد أن تقوم الوحدات المخفية المختلفة
بحساب دوال مختلفة. الحل لذلك هو تهيئة
المتغيرات بشكل عشوائي. إذًا، إليك ما تفعله. يمكنك تعيين w1 = np.random.randn. هذا يولّد متغير
غاوسي عشوائيًا (2,2). ثم عادةً، تضرب هذا العدد
الصغير جدًا، مثل 0,01. إذًا، قم بتهيئته إلى قيم
عشوائية صغيرة جدًا. ومن ثم b، يتبين أن b لا
يعاني من مشكلة التناظر، ما تسمى مشكلة
كسر التناظر. إذًا، لا بأس من تهيئة b
إلى أصفار فقط. لأنه طالما تمت
تهيئة w بشكل عشوائي، فستبدأ بالوحدات المخفية المختلفة
التي تحسب أشياء مختلفة. وهكذا تتخلص من مشكلة
كسر التناظر هذه. وبالمثل، بالنسبة إلى w2، ستقوم
بتهيئة ذلك بشكل عشوائي. وكذلك b2، يمكنك تهيئة ذلك إلى 0. لذا قد تتساءل، من أين جاء
هذا الثابت ولماذا يساوي 0,01؟ لماذا لا نضع الرقم 100 أو 1000؟ تبين أننا عادة
نفضل تهيئة الأثقال إلى قيم عشوائية صغيرة جدًا. لأنه إذا كنت تستخدم دالة تنشيط
سينية أو ظل زائدي، أو دالة سينية أخرى،
حتى عند طبقة الإخراج. إذا كانت الأثقال كبيرة جدًا، فعند حساب
قيم التنشيط، تذكر أن z[1]=w1 x + b. ثم a1 هي دالة التنشيط
المطبقة على z1. إذًا، إذا كان w كبيرًا جدًا، فسيكون z كبيرًا،
أو على الأقل ستكون بعض قيم z كبيرة جدًا أو
صغيرة جدًا. وهكذا في هذه الحالة، من المرجح أن ينتهي
بك الأمر في هذه الأجزاء المكتظة من دالة الظل الزائدي أو الدالة السينية، حيث
يكون المنحدر أو التدرج صغيرًا جدًا. مما يعني أن الانحدار التدريجي
سيكون بطيئًا جدًا. لذلك كان التعلم بطيئًا جدًا. إذًا، باختصار، إذا كان حجم w كبيرًا جدًا،
فمن المرجح أن ينتهي بك المطاف، حتى في بداية التدريب،
مع قيم كبيرة جدًا من z. مما يؤدي إلى تشبع دالة الظل الزائدي
أو دالة التنشيط السينية، وبالتالي إبطاء التعلم. إذا لم تكن لديك أي دوال تنشيط سينية أو ظل زائدي في جميع أنحاء
الشبكة العصبية، فهذه مشكلة ثانوية. ولكن إذا كنت تقوم بتصنيف ثنائي،
ووحدة الإخراج لديك دالة سينية، فأنت لا تريد أن تكون المتغيرات
الأولية كبيرة جدًا. ولهذا السبب، فإن الضرب في 0.01
سيكون أمرًا يستحق المحاولة، أو أي رقم صغير آخر. ونفس الشيء بالنسبة إلى w2، تمام؟ وهذا يمكن أن يكون random.random. أعتقد أن هذا سيكون 1 في 2
في هذا المثال، مضروبًا في 0.01. هناك s مفقودة. وأخيرًا، تبين أنه في بعض الأحيان يمكن أن
تكون هناك ثوابت أفضل من 0.01. عندما تقوم بتدريب شبكة عصبية
ذات طبقة مخفية واحدة فقط، فهي عبارة عن شبكة عصبية سطحية نسبيًا،
بدون طبقات مخفية كثيرة. بتعيينها إلى 0.01 ستعمل على ما يرام على الأرجح. ولكن عندما تقوم بتدريب شبكة
عصبية عميقة للغاية، فقد تحتاج إلى اختيار ثابت
مختلف عن 0.01. وفي مادة الأسبوع المقبل،
سنتحدث قليلاً عن كيفية وتوقيت وجوب اختيار
ثابت مختلف عن 0.01. لكن في كلتا الحالتين، عادة ما ينتهي الأمر
به إلى أن يكون عددًا صغيرًا نسبيًا. إذًا، سنتناول هذا في مقاطع فيديو هذا الأسبوع. أنت الآن تعرف كيفية إعداد شبكة
عصبية ذات طبقة مخفية، وتهيئة المتغيرات وإجراء التنبؤات
باستخدام [كلام غير مسموع]. وكذلك حساب المشتقات
وتنفيذ الانحدار التدريجي، وذلك باستخدام الانتشار الخلفي. إذًا، يجب أن تكون قادرًا
على إجراء الاختبارات الموجزة، بالإضافة إلى تمارين البرمجة
لهذا الأسبوع. حظًا موفقًا مع ذلك. آمل أن تستمتع
بالتدريب على المسائل، وأتطلع إلى رؤيتك في
مواد الأسبوع الرابع.