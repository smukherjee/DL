在上节课中 我们介绍了 L层的深度神经网络 以及用于描述深度神经网络所使用的 各种标记符号 在本次视频中 我们会讲 怎样在深度神经网络中进行前向传播和反向传播 和以往一样 我们先来看一下 对于单独的训练样本X 如何实现前向传播 之后我们会讨论 向量化的情况 这种情况下我们会学到如何同时对 整个训练集进行前向传播 假设给定一个单一的训练样本x 我们会这样来计算第一个隐藏层的激活函数 对这里的第一个隐藏层 先计算z[1]=w[1]*x+b[1] 在这里划出来的w[1]和b[1]是 影响第一层激活函数的参数 这是神经网络中的第一个 隐藏层 然后来计算 这一层的激活函数 等于 g(z[1]) 激活函数g 的上标取决于你现在在哪一层 我们现在正在计算第一个隐藏层的 激活函数 所以上标为1 现在我们已经得出了 第一个隐藏层的计算公式 第二个隐藏层怎么处理呢 我把它框起来 这里就是z[2]=w[2]a[1]+b[2] 所以第二层的激活函数 就是权重参数矩阵w乘以 第一层的输出函数 也就是我划出的这个值 加上第二层的偏差向量 然后a[2]等于z[2]代入激活函数 这就是第二层的计算公式 后面几层以此类推 直到输出层 这个例子中输出层是第四层 在第四层的计算中 z[4]等于权重参数矩阵w[4] 乘以上一层的激活函数 再加上这个偏差向量 类似的 将z[4]代入激活函数就是a[4] 这样你就推导出了 输出值y帽 另外要注意一下 这里的X也等于a[0] 因为我们也可以将输入的特征向量x 看作是第0层的激活函数输出 所以我们这里就用a[0]来取代x 然后你就会发现 所有屏幕上的等式看起来形式都相同 我们来总结一下通用规则 z[l]=w[l]*a[l-1]+b[l] 其次是l层的激活函数 a[l]就是将z[l]代入到激活函数g中 这两个就是通用的前向传播等式 我们已经完成了对单一训练样本 的前向传播通用公式地推导 下面我们来看看怎样将其 向量化后应用在整个训练集中 公式看起来和之前很类似 我们先来看第一层 使用大写字母 Z[1]=W[1]X+b[1] 而A[1]=g[1](Z[1]) 要记住X=A[0] 其实这些大写字母只是意味着把 训练样本放入了矩阵的不同列中 这里我们一样可以 用A[0]来代替X 下一层的公式看起来 很相似 Z[2]=W[2]A[1]+b[2] A[2]=g[2](Z[2]) 我们将这些向量 例如z和a等 把它们堆叠起来变成矩阵 这是第一个训练样本的z向量 这是第二个训练样本的z向量 以此类推直到第m个训练样本 把这些都堆叠在列中形成矩阵 将Z大写 我们称之为Z矩阵 对矩阵A做类似的操作 将所有的训练样本 从左到右堆叠起来形成矩阵 然后在整体向量化完成之后 我们能得到Yhat=g(Z[4]) 这也等于A[4] 这就是将所有训练样本经过深度神经网络 生成的预测值水平堆叠起来 下面来总结一下我们用过的的标记符号 我在这里修改一下 把小写z和a替换成大写的Z和A 噢这个小写的z看上去 已经像是大写的Z了 这样我们就在整个训练集上 得到了向量化的前向传播公式 这里的A[0]=X 如果你仔细观察 这个向量化的过程 就会发现看起来很像for循环 当i取值范围是1到4时 当l取值范围是1到总层数大写L时 去计算第一层的激活函数 接着是第二层 然后类推至第三层和第四层 看起来在这里就有个for循环 我们之前讲过 在使用神经网络时 要尽可能避免使用for循环 但这是唯一的一处 我觉得除了使用for循环以外 并没有更好的实现方法 所以当你在实现前向传播的时候 使用一个for循环是可以接受的 它被用来计算第一层的激活函数 然后是第二层和第三层 至今为止没有人提出 而且我也不觉得有别的方法 可以让我们不通过for循环来完成 从1到L层的计算 也就是整个 深度神经网络的计算工作 所以在这里使用for循环 是没有问题的 到这里 深度神经网络的标记符号 和如何在该网络中进行 前向传播已经讲完了 如果你觉得有些内容 看起来很眼熟 那是因为其实这些内容 和我们之前提到的只有 一个隐藏神经层的神经网络很相似 只是将它重复了几次而已 下期预告 当我们 在构建自己的神经网络时 如果试图尽量减少 发生问题的概率 就必须要很系统很仔细地 处理相关矩阵的维度 当我自己在写代码的时候 通常会拿一张纸 仔细地考虑我正在使用的矩阵的维度 我们会在下一个视频教程中 学习如何使用这种方法 GTC字幕组翻译