Bon retour parmi nous.Dans cette vidéo, Nous allons voir comment
 calculer les dérivées pour implémenter la descente de gradient 
pour la régression logistique. Ce qu'il faut vraiment retenir,
 c'est ce que vous devrez implémenter. c'est à dire, les équations clés, 
dont vous avez besoin pour implémenter la descente de gradient 
pour la régression logistique. Dans cette vidéo, je veux faire le calcul
 en utilisant le graphe de calculs. Je dois l'admettre, utiliser le graphe de calculs
 est un peu exagéré pour le calcul de la descente de gradient
 pour la régression logistique, mais je veux commencer à expliquer
 les choses comme cela pour vous familiariser avec ces idées pour que, j'espère, ce soit un peu plus facile quand on
 parlera de réseaux de neurones à part entière. Maintenant, nous allons plonger dans la descente
 de gradient pour la régression logistique. Pour rappel, nous avions mis en place
 la régression logistique comme ceci, votre prédiction, y chapeau, 
est définie comme ceci, où z est cela et si nous nous concentrons sur un seul exemple
 pour l’instant, alors la perte, par rapport à cet exemple, est définie comme ceci, où a est le résultat de 
la régression logistique, et y est l'étiquette qui contient la vérité. Nous allons écrire cela sous forme de
 graphe de calculs et pour cet exemple, disons que nous avons seulement 
deux caractéristiques, x1 et x2. Pour calculer z, nous aurons besoin en entrée de w1, w2 et b, en plus des valeurs 
des caractéristiques x1, x2. Ces choses, dans un graphe de calculs, sont utilisées pour calculer z, qui vaut w1*x1 + w2*x2 + b, je vais dessiner une boîte
 rectangulaire autour de cela. Ensuite, nous calculons y chapeau, ou a = sigma(z), C’est l'étape suivante dans
 le graphe de calculs, puis, enfin, nous calculons L(a, y) et je ne vais pas recopier la formule. Dans la régression logistique, ce que 
nous voulons faire est modifier les paramètres, w et b, afin de réduire cette perte. Nous avons décrit la propagation vers l'avant, 
la façon dont vous calculez la perte sur 
un seul exemple d'entrainement, maintenant nous allons parler de
 comment vous pouvez revenir en arrière 
pour calculer les dérivées. Voici une version nettoyée du diagramme. Parce que ce que nous voulons faire est
 calculer les dérivées par rapport à cette perte, la première chose que nous allons faire 
en allant vers l’arrière est de calculer la dérivée de
 cette perte par rapport à cette variable a. Ainsi, dans le code, vous utilisez juste da
 pour désigner cette variable. Il s’avère que si vous êtes familier 
avec le calcul infinitésimal, vous pourriez montrer que cela vaut
 -y/a + (1-y)/(1-a). Et la façon de le faire, c'est de 
prendre la formule pour la perte et, si vous connaissez le calcul infinitésimal, vous pouvez dériver
 par rapport à la variable petit a et vous obtenez cette formule. Mais si vous ne connaissez pas bien 
l'analyse, ne vous inquiétez pas. Nous vous fournirons la forme dérivée, et tout ce dont vous avez besoin,
 tout au long de ce cours. Si vous êtes un expert en analyse, Je vous encourage à aller chercher 
la formule pour la perte sur la diapositive précédente et à essayer de 
dériver vous même la perte par rapport à a, mais si vous ne connaissez pas assez l'analyse
 pour le faire, ne vous inquiétez pas. Maintenant, après avoir calculé
 cette quantité da, la dérivée de votre variable 
de sortie finale par rapport à a, vous pouvez aller ensuite vers l’arrière. Il s’avère que vous pouvez montrer que dz, c'est le nom de la variable dans le code python, dz va être la dérivée de la perte par rapport à z. Pour L, vous pouvez écrire explicitement
 la perte L(a,y) avec les paramètres 
ou simplement L, ok ? Les deux types de notation sont acceptables. On peut montrer que cela est égal à a-y. Juste quelques commentaires 
seulement pour ceux d'entre vous qui 
sont experts en analyse, Si vous n’êtes pas expert en analyse,
 ne vous inquiétez pas pour ça. Mais il se trouve que cela, dL/dz peut être exprimé comme dL/da * da/dz, et il s’avère que da/dz, cela vaut a*(1-a), et dL/da nous l'avons fait ici, Donc vous prenez ces deux quantités, DL/da, qui est ce terme, ainsi que da/dz, qui est ce terme, vous prenez 
ces deux termes et vous les multipliez. Vous pouvez montrer que l’équation se simplifie en a-y. Voilà comment vous le dérivez et ça vient vraiment de la règle de composition des dérivées, 
que j'ai éludée en donnant la formule. N’hésitez pas à faire ce calcul vous-même 
si vous savez faire des dérivées, mais si vous ne l’êtes pas, tout ce que 
vous devez savoir est que vous pouvez calculer dz comme a-y et nous avons déjà 
fait ce calcul pour vous. Enfin, la dernière étape
 de ce calcul est de revenir au calcul de combien vous devez
 changer W et b. En particulier, vous pouvez montrer que
 la dérivée par rapport à W1, et dans le code nous l'appellerons dW1, 
que ceci est égale à x1*dz. Puis, de la même façon, dW2, c'est-à-dire 
de combien vous voulez changer W2, dW2 = x2*dz, et b, excusez-moi, db est égal à dz. Si vous voulez faire la descente de gradient
 par rapport à un seul exemple, ce que vous faites est ceci : vous utilisez cette formule pour calculer dz, puis vous utilisez ces formules
 pour calculer dW1, dW2, et db et puis vous effectuez ces mises à jour. W1 est remplacé par W1 moins le taux d'apprentissage
 alpha fois dW1 W2 est mis à jour de la même façon, et b est remplacé par
 b - taux d'apprentissage * db Et alors, ce sera une étape de descente 
de gradient par rapport à un seul exemple. Vous avez vu comment calculer
 les dérivées et implémenter la descente de gradient pour la régression 
logistique pour un seul exemple d'apprentissage. Mais pour entrainer un modèle
 de régression logistique, vous n'avez pas un unique 
exemple d'apprentissage, mais un set entier
 de m exemples d'apprentissage. Dans la prochaine vidéo, nous allons voir comment vous pouvez prendre
 ces idées et les appliquer à l’apprentissage, à partir non plus d’un seul exemple, mais de l’ensemble du set d'apprentissage.