1
00:00:00,320 --> 00:00:04,140
如果在深度學習背後的基本技術思維,

2
00:00:04,140 --> 00:00:07,110
在新興網絡背後的基本技術思維已經存在好幾十年

3
00:00:07,110 --> 00:00:08,970
為什麼它們只剛開始起飛

4
00:00:08,970 --> 00:00:11,310
在這段影片中, 讓我們看一些

5
00:00:11,310 --> 00:00:14,690
深度學習發展背後的主要驅動力，因為我認為

6
00:00:14,690 --> 00:00:17,540
這能幫助你找到最好的機會

7
00:00:17,540 --> 00:00:20,560
在你所在的組織來運用這些驅動力

8
00:00:20,560 --> 00:00:21,690
在過去幾年

9
00:00:21,690 --> 00:00:23,095
很多人問過我

10
00:00:23,095 --> 00:00:25,880
"Andrew, 為什麼深度學習突然作用得這麼好"

11
00:00:25,880 --> 00:00:30,460
當我被問到這個問題時，我通常會畫這張圖給他們

12
00:00:30,460 --> 00:00:33,830
我們畫的這張圖，在橫軸

13
00:00:33,830 --> 00:00:37,675
我們畫出一件任務的資料量

14
00:00:37,675 --> 00:00:40,040
在縱軸

15
00:00:40,040 --> 00:00:43,610
我們畫出我們學習表現的演算

16
00:00:43,610 --> 00:00:47,660
像是我們垃圾資訊分類的準確度，或是我們

17
00:00:47,660 --> 00:00:50,870
廣告點選預測，或是

18
00:00:50,870 --> 00:00:55,980
我們神經系統對於找出其他車子與我們正在開的車子的相對位置的準確度

19
00:00:55,980 --> 00:01:00,680
結果發現，如果你畫的是傳統的學習表現演算，像是

20
00:01:00,680 --> 00:01:02,090
支援向量機器，或是

21
00:01:02,090 --> 00:01:06,340
羅吉斯迴歸分析當成是一個資料量的函數

22
00:01:06,340 --> 00:01:09,185
你可能會得出像是這樣的一條曲線

23
00:01:09,185 --> 00:01:14,220
一開始當你增加數據，學習表現會增加，但是過一陣子

24
00:01:14,220 --> 00:01:17,565
學習表現會到達高原期

25
00:01:17,565 --> 00:01:20,100
這應該是一條水平線，我沒畫好

26
00:01:20,100 --> 00:01:26,760
這就像是他們不知道怎麼處理大量的數據

27
00:01:26,760 --> 00:01:29,270
我們的社群在

28
00:01:29,270 --> 00:01:32,360
過去20年間所遇到的問題是

29
00:01:32,360 --> 00:01:35,900
我們從擁有相對少量的數據

30
00:01:35,900 --> 00:01:39,985
到擁有非常大量的數據

31
00:01:39,985 --> 00:01:43,430
而這是歸功於社會的數位化

32
00:01:43,430 --> 00:01:48,020
因為很多人類活動都發生在數位領域

33
00:01:48,020 --> 00:01:50,810
我們花很多的時間在我們的電腦, 在網站, 

34
00:01:50,810 --> 00:01:56,475
在行動裝置, 在電子裝置上創建資料

35
00:01:56,475 --> 00:01:59,720
多虧有了很多便宜的相機

36
00:01:59,720 --> 00:02:01,970
與我們的手機，加速度計

37
00:02:01,970 --> 00:02:05,645
以及所有在網路裡的感測器

38
00:02:05,645 --> 00:02:09,350
我們一直在蒐集更多更多的資料

39
00:02:09,350 --> 00:02:12,590
所以，在過去的20年間，因為有很多應用程式

40
00:02:12,590 --> 00:02:15,935
我們收集了非常多的數據，數量超過

41
00:02:15,935 --> 00:02:20,015
傳統的學習演算法能夠有效的利用

42
00:02:20,015 --> 00:02:23,465
有了神經網絡，如果你訓練

43
00:02:23,465 --> 00:02:28,865
一個小的神經網絡，他的表現可能會是這樣

44
00:02:28,865 --> 00:02:31,765
如果你訓練一個大一點的神經網絡

45
00:02:31,765 --> 00:02:34,840
我們稱之為中型神經網絡

46
00:02:34,840 --> 00:02:40,275
表現通常會比較好，如果你訓練一個非常大的神經網絡

47
00:02:40,275 --> 00:02:43,750
他的表現就會不斷地變好

48
00:02:43,750 --> 00:02:45,965
所以，有幾個觀察發現，其中一個是

49
00:02:45,965 --> 00:02:49,150
如果你想要達到非常高的表現

50
00:02:49,150 --> 00:02:50,720
你需要兩件事

51
00:02:50,720 --> 00:02:53,660
首先, 你往往需要能夠訓練

52
00:02:53,660 --> 00:02:56,540
一個夠大的神經網絡，以至於能夠

53
00:02:56,540 --> 00:02:59,630
有效利用大量的數據，第二

54
00:02:59,630 --> 00:03:01,760
你必須要在x軸的外邊

55
00:03:01,760 --> 00:03:03,850
你需要很多數據

56
00:03:03,850 --> 00:03:09,590
所以，我們說推動深度學習進步的是規模。當我說規模，

57
00:03:09,590 --> 00:03:12,380
我一是指是神經網路的大小，

58
00:03:12,380 --> 00:03:16,090
意思是神經網路有很多的隱藏單元，很多參數，

59
00:03:16,090 --> 00:03:21,055
很多的連結,二是資料的規模

60
00:03:21,055 --> 00:03:25,190
事實上，現在獲得神經網路更好性能最可靠的方法之一

61
00:03:25,190 --> 00:03:29,690
是訓練更大的神經網路，或是往裡面丟更多數據

62
00:03:29,690 --> 00:03:32,210
而這只可行到一定程度。因為

63
00:03:32,210 --> 00:03:35,510
最終你會用盡所有數據或神經網路會

64
00:03:35,510 --> 00:03:38,120
大到需要花太長的時間來訓練，但是

65
00:03:38,120 --> 00:03:43,325
僅僅提高規模已經幫我們在深度學習的世界裡達成了許多。

66
00:03:43,325 --> 00:03:46,860
為了要讓這個圖增加一點技術上的精確度

67
00:03:46,860 --> 00:03:48,815
讓我再加一些東西

68
00:03:48,815 --> 00:03:51,465
我在Ｘ軸上寫下數據量

69
00:03:51,465 --> 00:03:56,240
技術上來說，這是有標籤的數據量

70
00:03:56,240 --> 00:04:02,795
所謂「標籤」過的資料，我指的是訓練資料有輸入 x， 以及其相對應的標籤輸出 y 。

71
00:04:02,795 --> 00:04:07,500
接下來引入往後課程中會使用的符號：

72
00:04:07,500 --> 00:04:12,460
我們會用小寫的 m 描述訓練資料集的大小。

73
00:04:12,460 --> 00:04:17,390
也就是說，這裡我們的橫軸「訓練資料量」就是 m。

74
00:04:17,390 --> 00:04:20,300
一些其他有關這個圖的細節

75
00:04:20,300 --> 00:04:24,080
在這個區域是小的訓練集

76
00:04:24,080 --> 00:04:28,970
這相對的演算法排名實際上不是那麼好定義

77
00:04:28,970 --> 00:04:32,690
所以, 如果你沒有很多的訓練資料, 它往往是

78
00:04:32,690 --> 00:04:37,120
依賴您的技術來手工打造特徵決定表現

79
00:04:37,120 --> 00:04:41,810
所以, 很有可能, 如果有人利用'支援向量機'(SVM:Support Vector Machine) 訓練會

80
00:04:41,810 --> 00:04:46,640
更有促動力去用手工設計特徵，相比於使用更大的神經網路訓練,

81
00:04:46,640 --> 00:04:49,260
有可能在小訓練集小的時候

82
00:04:49,260 --> 00:04:50,815
支援向量機(SVM)可以做得更好。

83
00:04:50,815 --> 00:04:54,140
所以在這個圖形的靠左邊區域

84
00:04:54,140 --> 00:04:56,990
演算法的效果排序

85
00:04:56,990 --> 00:05:00,800
並不是那麼確定，更多取決於你在

86
00:05:00,800 --> 00:05:03,380
選取特徵的技巧、或是其他細節上。

87
00:05:03,380 --> 00:05:07,700
只是在大量資料的區域

88
00:05:07,700 --> 00:05:09,080
很大的訓練集

89
00:05:09,080 --> 00:05:12,320
也就是圖上右邊，m 很大的區域

90
00:05:12,320 --> 00:05:16,960
我們一般會看到大型神經網路會有比較好的表現。

91
00:05:16,960 --> 00:05:21,315
因此，如果有朋友問你「為甚麼神經網路正在興起」

92
00:05:21,315 --> 00:05:24,820
我會鼓勵您畫這個圖讓他們了解

93
00:05:24,820 --> 00:05:27,695
我會說在早期

94
00:05:27,695 --> 00:05:29,450
現代深度學習的興起

95
00:05:29,450 --> 00:05:33,395
是因為資料規模跟計算規模

96
00:05:33,395 --> 00:05:36,425
反映在我們訓練大型神經網路的能力上

97
00:05:36,425 --> 00:05:38,420
無論是在 cpu 還是在 gpu 上,

98
00:05:38,420 --> 00:05:41,555
使我們取得了很大的進步。

99
00:05:41,555 --> 00:05:44,320
但是逐漸地，尤其是最近幾年

100
00:05:44,320 --> 00:05:47,980
我們也看到很多演算法上的巨大革新

101
00:05:47,980 --> 00:05:50,830
所以, 我也不想低估這一點。

102
00:05:50,830 --> 00:05:54,995
有趣的是，許多演算法的革新

103
00:05:54,995 --> 00:06:00,285
一直在嘗試讓神經網路運行得更快。

104
00:06:00,285 --> 00:06:02,940
舉一個具體的例子

105
00:06:02,940 --> 00:06:05,210
一個重大的突破在神經網路是從

106
00:06:05,210 --> 00:06:08,125
從 S型函數(Sigmoid),

107
00:06:08,125 --> 00:06:09,840
看起來是這樣的, 換到

108
00:06:09,840 --> 00:06:13,535
一個線性整流(ReLU)數學函數, 我們在先前的影片中談到

109
00:06:13,535 --> 00:06:17,855
, 並進行了簡要介紹, 如下所示。

110
00:06:17,855 --> 00:06:22,010
如果您不知道這些我將談到的細節, 不用擔心

111
00:06:22,010 --> 00:06:25,820
但實際上使用S型函數(Sigmoid function)的問題之一在

112
00:06:25,820 --> 00:06:29,980
機器學習是有些區域像這裡這個函數的斜率

113
00:06:29,980 --> 00:06:31,905
其中梯度接近零,

114
00:06:31,905 --> 00:06:34,685
所以學習變得非常緩慢

115
00:06:34,685 --> 00:06:37,945
因為當你執行梯度下降,而梯度是零,

116
00:06:37,945 --> 00:06:41,820
參數會改變得很慢學習也會很慢

117
00:06:41,820 --> 00:06:47,090
而通過改變神經網路的啟動函數,

118
00:06:47,090 --> 00:06:52,835
使用此函數稱為 "值函數"或 "線性整流單位",

119
00:06:52,835 --> 00:06:58,350
ReLU, 對於輸入的所有正值, 漸變梯度等於 1, 對吧。

120
00:06:58,350 --> 00:07:03,130
因此, 梯度逐漸縮小到零的可能性要低得多。

121
00:07:03,130 --> 00:07:05,720
這裡的坡度, 這條線的斜率是零

122
00:07:05,720 --> 00:07:08,960
在左邊, 但事實證明, 只是通過換

123
00:07:08,960 --> 00:07:12,320
S型函數(Sigmoid)到ReLU函數(ReLU)

124
00:07:12,320 --> 00:07:16,380
會使梯度下降演算法工作得快得多.

125
00:07:16,380 --> 00:07:21,350
所以, 這是一個例子, 也許一個相對簡單的演算法創新, 但

126
00:07:21,350 --> 00:07:26,755
最終的影響, 這種演算法的創新是它真對計算非常有幫助。

127
00:07:26,755 --> 00:07:29,765
所以, 實際上已經有相當多像這樣的例子,

128
00:07:29,765 --> 00:07:33,230
在那裡, 我們改變演算法, 因為它允許我們的程式運行的

129
00:07:33,230 --> 00:07:37,400
更快, 這讓我們訓練更大的神經網路或推論,在

130
00:07:37,400 --> 00:07:42,065
合理的時間內, 即使我們有一個大的網路或大量的資料。

131
00:07:42,065 --> 00:07:47,060
另一個理由是快速的計算對於

132
00:07:47,060 --> 00:07:52,525
常反覆訓練的神經網路過程是很重要的

133
00:07:52,525 --> 00:07:54,050
通常您有一個想法在

134
00:07:54,050 --> 00:07:58,285
神經網路架構而您建立您的想法在程式裡邊

135
00:07:58,285 --> 00:08:01,999
實行您的想法, 然後讓您進行實驗

136
00:08:01,999 --> 00:08:05,880
它告訴你你的神經網路設計有多好, 然後通過看它,

137
00:08:05,880 --> 00:08:08,240
你回去更改

138
00:08:08,240 --> 00:08:13,190
你的神經網路細節, 然後你繞著這個循環步驟一遍又一遍。

139
00:08:13,190 --> 00:08:17,089
當你的新網路需要很長時間來訓練時,

140
00:08:17,089 --> 00:08:21,140
它是需要很長的時間去經過這個循環週期, 有

141
00:08:21,140 --> 00:08:27,215
在你建立有效的神經網路時有一個很大的效率差異, , 每當你

142
00:08:27,215 --> 00:08:32,455
有一個想法, 並嘗試實驗它, 在10分鐘內看看它是否有效

143
00:08:32,455 --> 00:08:38,570
或者最多用一天; 相對於如你用一個月的時間訓練神經網路。

144
00:08:38,570 --> 00:08:41,480
好吧, 有時它確實發生的, 因為當你得到

145
00:08:41,480 --> 00:08:44,630
結果在10分鐘或是一天內,

146
00:08:44,630 --> 00:08:47,750
您可以試很多想法而更能夠

147
00:08:47,750 --> 00:08:52,005
發現非常適合您應用的一個神經網路

148
00:08:52,005 --> 00:08:57,320
所以快速的計算能力確實已經幫助

149
00:08:57,320 --> 00:09:02,495
加快了你可以得到實驗結果回來的速度,

150
00:09:02,495 --> 00:09:06,950
而這非常幫助於神經網路的從業者, 以及

151
00:09:06,950 --> 00:09:10,550
在深度學習領域工作的研究人員,循環得

152
00:09:10,550 --> 00:09:14,530
更快, 改進想法更快。

153
00:09:14,530 --> 00:09:17,570
所以, 這一切是一個巨大的利多對於

154
00:09:17,570 --> 00:09:22,390
整個深度學習研究社區, 我認為這是令人難以置信的,

155
00:09:22,390 --> 00:09:27,470
發明新的演算法, 並在這方面取得不斷的進展。

156
00:09:27,470 --> 00:09:33,200
這一些力量增強了深度學習的興起, 但好消息是

157
00:09:33,200 --> 00:09:38,825
這些力量仍然在有力地工作, 使深度學習更好。用資料作例子,

158
00:09:38,825 --> 00:09:42,890
社會仍在創作越來越多的數位資料。或在

159
00:09:42,890 --> 00:09:47,630
計算能力的例子, 像GPU等特殊硬體的興起和更快的網路,

160
00:09:47,630 --> 00:09:49,595
很多種硬體

161
00:09:49,595 --> 00:09:54,260
我實際上很有信心, 我們的能力來建立非常大的神經網路, 從一個純粹的

162
00:09:54,260 --> 00:09:59,300
計算的角度來看, 將不斷變得更好。在演算法方面,

163
00:09:59,300 --> 00:10:02,150
整個深度學習研究社區

164
00:10:02,150 --> 00:10:06,170
在演算法方面的創新仍然是驚人的。

165
00:10:06,170 --> 00:10:07,550
所以, 正因為如此,

166
00:10:07,550 --> 00:10:09,530
我認為我們可以樂觀。

167
00:10:09,530 --> 00:10:11,720
我當然樂觀地認為, 深度學習將保持

168
00:10:11,720 --> 00:10:14,570
在未來的許多年裡變得越來越好。

169
00:10:14,570 --> 00:10:17,855
所以, 有了這個, 讓我們繼續到這一節的最後一段影片

170
00:10:17,855 --> 00:10:21,200
在這裡我們將更多地談談你從這門課程中學到的東西。