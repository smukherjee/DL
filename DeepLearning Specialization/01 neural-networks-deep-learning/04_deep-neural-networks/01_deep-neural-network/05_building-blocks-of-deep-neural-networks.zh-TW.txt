在這個禮拜前面的影片及 前面幾個禮拜的影片 您已經看過基本建構礎石 的正向傳播及反向傳播 建置深度神經網路時您需要的關鍵元件 我們來看您如何將這些元件
放在一起來建立深度網路 這是一個網路有幾個層 讓我們選一層只集中注意
到這一層的計算 對於 l 層您有一些參數 wl 跟 bl 對於正向傳播 您會輸入啟動值 a[l-1] 從前面一層來的, 然後輸出 a[l] 我們之前做的方式是您計算 z[l] = w[l] 乘 a[l-1] + b[l] 然後 a[l] = g of z[l], 對吧? 這是您如何從輸入 a[l-1] 到輸出 a[l] 實際上為了以後用到 將 z[l] 值快取(cache)起來會很有用 讓我一起將這個快取寫下來因為 儲存 z[l] 的值對反向有用 對於等一下的反向傳播步驟, 然後 反向傳播步驟 對於反向傳播步驟, 再一次 注意力集中在計算這個 l 層 您將建置這個函數輸入 da[l] 輸出 da[l-1] 來充實一下細節 輸入實際上是 da of l 跟這個 快取 裡面存放您計算過的 z[l] 值 然後除了輸出 da[l-1] 您可以輸出梯度 為了建置梯度下降法來學習 這是基本的架構您如何建置這個正向步驟 我們稱為正向函數以及反向步驟 我們應該稱它為反向函數 總結一下, 在 l 層 您有正向步驟或者說正向傳播
或者說正向函數 輸入 a[l-1] 輸出 a[l] 為了做這個計算 您會用到 w[l] 跟 b[l] 也 輸出快取包含了 z[l], 然後反向函數 使用反向傳播步驟 會是另一個函數輸入 da of l 輸出 da[l-1] 這個告訴您, 給予相對於這個啟動值的導數 也就是 da of l, 導數是什麼? 我希望 a[l-1] 改變多少? 所以來計算相對於前面一層的啟動值的導數 在這個箱子裡您會需要 w[l] 跟 b[l] 實際上一路下來 您也會計算 dz[l] 然後這個箱子 這個反向函數也會輸出 dw[l] 跟 db[l] 但有時候我會用紅色箭頭
來表示反向評測 如果您要的話, 您也可以用紅色表示這些箭頭 如果您可以建置這兩個函數 那基本的神經網路計算如下 您拿這個輸入特徵 a[0] 餵這個進去, 然後計算第一層的啟動值 我們稱之為 a[1] 為了做這個, 您需要 w[1] 跟 b[1] 我們同時將 z[1] 快取起來 做完這個 您將它餵到第二層 然後使用 w[2] 跟 b[2] 您將計算出下一層啟動值 a[2] 等等直到 您最終輸出 a 大寫 L 也就等於 y-hat 一路下來我們快取了這些 z 值 所以這是正向傳播步驟 現在反向傳播步驟 我們要做的是反向一系列的 迭代, 我們反向的計算梯度像這樣 我們從這裡餵 da of l 而這個箱子會給我們 da of l - 1 等等，直到我們到 da[2], da[1] 您實際上可以得到額外一個的輸出 da[0] 但這是相對於您的輸入特徵的導數是沒用的 至少對於訓練這些
監督式神經網路的權重而言 所以您可以停在這裡 一路下來, 反向傳播也輸出了 dw[l], db[l] 用到了 w[l] 跟 b[l] 參數 這個會輸出 dw[3] db[3] 等等 您最終會計算到所有您所需要的導數 也許多填一點東西到這個架構上 這些箱子也會用到這些參數 w[l], b[l], 實際上我們以後會看到在這個箱子裡 我們也會計算 dz 一次迭代的神經網路訓練包含了從 a[0] 也就是 x 開始 經歷正向傳播像這樣 計算 y-hat, 然後用它來計算這個 然後反向傳播這樣做 這樣您會得到所有導數 所以 w 會更新為 w 減學習率乘上 dw 對於每一層 b 也一樣 計算反向傳播來得到這些所有導數 這是一個迭代的梯度下降
對於您的神經網路 在進入下一步之前, 有一項資訊細節 概念上，可以將快取想像成儲存 z 的值用來反向函數時用到 但當您建置這個 您看過之前的練習，我們建置這個時 您發現快取也是一個方便的方式來 儲存這些參數 w1, b1 的值用在反向函數上 在前面的練習中 您實際上在快取上儲存了 z 跟 w, b 所以儲存了 z2, w2, b2 從建置的角度看 我只是發現到這是一個方便的方式
將這些參數複製 等您以後在計算反向傳播時會用到它們 這只是一個建置上的細節
您會在您的程式練習中看到 您已經看到了一個基本的建構基石
在建置深度神經網路上 每一層, 有一個正向傳播步驟 跟一個相對的反向傳播步驟 然後用快取來傳送資訊從一個箱子到另一個箱子 在下一個影片中 我們會談到您如何實際上來建置這些建構基石 讓我們進入下一段影片