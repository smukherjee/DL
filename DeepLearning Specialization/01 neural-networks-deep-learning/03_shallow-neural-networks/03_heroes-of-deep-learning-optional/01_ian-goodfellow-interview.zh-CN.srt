1
00:00:02,550 --> 00:00:05,830
您好 Ian 感谢您接受采访

2
00:00:05,830 --> 00:00:06,860
您好 Andrew 感谢您的邀请

3
00:00:06,860 --> 00:00:08,775
我很高兴能来到这里

4
00:00:08,775 --> 00:00:11,920
现在你是世界上最炙手可热的深度学习研究者之一

5
00:00:11,920 --> 00:00:14,450
请和我们一起来分享一下你的个人故事

6
00:00:14,450 --> 00:00:16,810
你是怎样决定进入这个领域的呢

7
00:00:16,810 --> 00:00:19,150
好的

8
00:00:19,150 --> 00:00:24,287
我想 我最开始对机器学习感兴趣 是在认识你以前

9
00:00:24,287 --> 00:00:29,705
之前我一直致力于研究神经系统科学

10
00:00:29,705 --> 00:00:34,600
而我斯坦福的本科导师Jerry Cain鼓励我选修你的AI入门课程

11
00:00:34,600 --> 00:00:35,790
哈哈 我现在刚知道这事

12
00:00:35,790 --> 00:00:39,885
所以在此之前我一直都认为AI是一个很好的概念

13
00:00:39,885 --> 00:00:42,590
但谈到在实际生活中的应用 我理解为

14
00:00:42,590 --> 00:00:44,483
主要体现在游戏中

15
00:00:44,483 --> 00:00:47,375
有很多事先编码的规则

16
00:00:47,375 --> 00:00:49,700
让一些非玩家控制的角色

17
00:00:49,700 --> 00:00:52,085
在不同的时间点按脚本说出不同的话

18
00:00:52,085 --> 00:00:56,750
然后在你的AI入门的课程上

19
00:00:56,750 --> 00:01:02,815
你讲解了诸如线性回归和线性回归误差的偏差和方差分解

20
00:01:02,815 --> 00:01:06,665
我才开始意识到这是真正的科学

21
00:01:06,665 --> 00:01:10,970
和神经科学相比，不如在人工智能方面开启一段科学职业生涯

22
00:01:10,970 --> 00:01:12,730
我明白了 很棒 那之后呢

23
00:01:12,730 --> 00:01:15,290
之后我回到学校成为你的课程的助教

24
00:01:15,290 --> 00:01:17,815
噢 是的 想一个助教

25
00:01:17,815 --> 00:01:22,595
其实对我来说 成为这门课的助教 是我生命中一个非常大的转折点

26
00:01:22,595 --> 00:01:23,720
其中的一个学生

27
00:01:23,720 --> 00:01:25,310
我的朋友Ethan Dreifuss

28
00:01:25,310 --> 00:01:28,689
对Geoff Hinton的深度信念网络论文很感兴趣

29
00:01:28,689 --> 00:01:29,022
我懂了我懂了

30
00:01:29,022 --> 00:01:35,660
最后在斯坦福 我们两个利用寒假的课余时间一起搭建了第一台

31
00:01:35,660 --> 00:01:43,280
以GPU CUDA为基础的机器 并用它来运行玻尔兹曼机

32
00:01:43,280 --> 00:01:43,817
我懂了我懂了

33
00:01:43,817 --> 00:01:46,295
在那时 我的直觉告诉我

34
00:01:46,295 --> 00:01:50,720
深度学习是未来的主要发展方向

35
00:01:50,720 --> 00:01:53,660
我也曾使用过许多其它的算法

36
00:01:53,660 --> 00:01:56,285
比如支持向量机

37
00:01:56,285 --> 00:01:58,845
但结果并不理想

38
00:01:58,845 --> 00:02:01,400
训练集越多 算法计算得越慢

39
00:02:01,400 --> 00:02:03,476
或者对于相同数量的训练集

40
00:02:03,476 --> 00:02:08,240
很难在改变其他参数的情况下获得很大的性能提升

41
00:02:08,240 --> 00:02:13,065
从那时开始 我就全力以赴投入深度学习中了

42
00:02:13,065 --> 00:02:18,595
我记得Richard Reyna有一篇很古老的GPU论文

43
00:02:18,595 --> 00:02:21,585
他在论文中的致谢部分 感谢你做了大量的前期工作

44
00:02:21,585 --> 00:02:25,850
是的 那篇论文用到了我们搭建的一些机器

45
00:02:25,850 --> 00:02:26,656
嗯

46
00:02:26,656 --> 00:02:30,755
我搭建的第一台机器 是Ethan和我用我们自己的钱

47
00:02:30,755 --> 00:02:35,120
在他妈妈家里组装的

48
00:02:35,120 --> 00:02:39,835
在那之后 我们拿实验经费为斯坦福搭建了第二和第三台机器

49
00:02:39,835 --> 00:02:42,965
太棒了 我从未听说这个故事 实在是太棒了

50
00:02:42,965 --> 00:02:45,830
如今 基于之前你发明的

51
00:02:45,830 --> 00:02:48,365
生成对抗网络(Gans)

52
00:02:48,365 --> 00:02:51,645
产生了一场席卷深度学习领域的风暴

53
00:02:51,645 --> 00:02:54,085
你是怎么产生这个想法的呢

54
00:02:54,085 --> 00:02:56,885
我研究生成模型有很长一段时间了

55
00:02:56,885 --> 00:02:59,000
生成对抗网络是一种

56
00:02:59,000 --> 00:03:02,570
生成模型 当你有很多训练数据

57
00:03:02,570 --> 00:03:08,420
并且你希望生成更多的虚构数据来加入训练集

58
00:03:08,420 --> 00:03:13,265
在过去并没有出现过这种形式

59
00:03:13,265 --> 00:03:16,220
在我产生生成对抗网络这个想法之前

60
00:03:16,220 --> 00:03:19,780
曾有一些著名的方法用来生成模型

61
00:03:19,780 --> 00:03:24,860
在我攻读博士期间 我尝试过其它所有的方法

62
00:03:24,860 --> 00:03:29,000
比如玻尔兹曼机和稀疏编码

63
00:03:29,000 --> 00:03:32,630
还有其它著名方法

64
00:03:32,630 --> 00:03:35,955
我非常了解这些方法的优势和劣势

65
00:03:35,955 --> 00:03:40,265
同时我也在寻找能避免所有劣势的机制

66
00:03:40,265 --> 00:03:44,110
最后 当我在酒吧和我的朋友争论生成模型的时候

67
00:03:44,110 --> 00:03:45,845
灵光一现

68
00:03:45,845 --> 00:03:47,540
我告诉朋友们 你们需要做这个 这个 这个

69
00:03:47,540 --> 00:03:49,510
和这个 并且我用人格担保这一定成功

70
00:03:49,510 --> 00:03:52,890
但是他们没有相信我

71
00:03:52,890 --> 00:03:55,410
我本来准备写一本关于深度学习的教科书

72
00:03:55,410 --> 00:03:55,790
我懂了我懂了

73
00:03:55,790 --> 00:03:57,620
但是我有强烈的自信 一定能成功

74
00:03:57,620 --> 00:03:59,870
我赶紧回家 写出了算法模型 并且真的成功了

75
00:03:59,870 --> 00:04:02,920
所以你只用了一晚上就完成了生成对抗网络的第一个版本？

76
00:04:02,920 --> 00:04:06,050
我离开朋友在酒吧举办的欢送会回到家后

77
00:04:06,050 --> 00:04:09,530
差不多在凌晨 这个算法模型诞生了

78
00:04:09,530 --> 00:04:10,086
我懂了我懂了

79
00:04:10,086 --> 00:04:11,784
第一版就能完美运行

80
00:04:11,784 --> 00:04:13,275
非常非常幸运

81
00:04:13,275 --> 00:04:15,825
我不用再去查找超参数或者其他东西

82
00:04:15,825 --> 00:04:17,840
我之前在某个地方读到一个故事

83
00:04:17,840 --> 00:04:21,851
你曾有一次濒临死亡 而这让你更为执着地从事人工智能研究

84
00:04:21,851 --> 00:04:24,160
能不能和我们分享一下

85
00:04:24,160 --> 00:04:30,215
好 其实当时我并没有垂死 但以为自己命不久矣

86
00:04:30,215 --> 00:04:33,170
我当时头痛欲裂 感觉要炸了

87
00:04:33,170 --> 00:04:37,571
医生说我可能有脑出血

88
00:04:37,571 --> 00:04:39,740
当我等待MRI结果

89
00:04:39,740 --> 00:04:43,180
想看看情况到底有多糟糕的时候

90
00:04:43,180 --> 00:04:47,810
我突然意识到 当时我主要考虑的是

91
00:04:47,810 --> 00:04:49,910
要确保能够有人能够

92
00:04:49,910 --> 00:04:52,750
接手我的研究方向并继续下去

93
00:04:52,750 --> 00:04:53,224
我懂了我懂了 我懂了我懂了

94
00:04:53,224 --> 00:04:55,820
然而回想起来 那是一些非常可笑的研究方向

95
00:04:55,820 --> 00:04:56,553
我懂了我懂了

96
00:04:56,553 --> 00:04:58,700
但这个经历让我意识到

97
00:04:58,700 --> 00:05:02,325
实际上我人生中最重要的事情之一

98
00:05:02,325 --> 00:05:05,780
是从事机器学习研究的工作

99
00:05:05,780 --> 00:05:07,910
我了解了 非常好

100
00:05:07,910 --> 00:05:10,055
当你以为自己快要死去的时候

101
00:05:10,055 --> 00:05:12,265
你只是关心不要让研究半途而废

102
00:05:12,265 --> 00:05:12,649
嗯

103
00:05:12,649 --> 00:05:15,690
看来你已经找到了生命的真谛

104
00:05:15,690 --> 00:05:17,850
嗯

105
00:05:17,850 --> 00:05:21,808
现在 你仍然活跃在很多

106
00:05:21,808 --> 00:05:24,560
关于生成对抗网络研究的活动中

107
00:05:24,560 --> 00:05:27,710
能不能给我们讲讲你怎么看待生成对抗网络的未来

108
00:05:27,710 --> 00:05:32,930
现在 生成对抗网络被用在很多不同的领域 比如半监督学习

109
00:05:32,930 --> 00:05:39,185
为其它模型生成训练数据 甚至是模拟科学实验

110
00:05:39,185 --> 00:05:43,850
理论上 这些事情都可以用其他的生成模型来完成

111
00:05:43,850 --> 00:05:47,695
因此我认为生成对抗网络正处于一个很重要的抉择关头

112
00:05:47,695 --> 00:05:50,210
现在 它们在很多时候能取得不错的效果

113
00:05:50,210 --> 00:05:55,890
但是如果要真正发挥出它们的性能 更象是一门艺术而非科学

114
00:05:55,890 --> 00:05:59,870
这有点象10年前人们对深度学习的感受

115
00:05:59,870 --> 00:06:01,430
当时 我们使用

116
00:06:01,430 --> 00:06:05,330
以玻尔兹曼机为基础的深度信念网络

117
00:06:05,330 --> 00:06:07,420
但它们非常非常挑剔

118
00:06:07,420 --> 00:06:11,945
我们逐渐开始使用Relu函数和批标准化

119
00:06:11,945 --> 00:06:14,635
这使深度学习变得更加可靠了

120
00:06:14,635 --> 00:06:18,470
如果我们能够将生成对抗网络变得和深度学习一样可靠

121
00:06:18,470 --> 00:06:20,840
我想我们就能持续发现生成对抗网络在今天这些应用领域

122
00:06:20,840 --> 00:06:24,110
拥有比现在更成功的应用表现

123
00:06:24,110 --> 00:06:29,060
如果无法找出让生成对抗网络更为稳定的方法

124
00:06:29,060 --> 00:06:32,960
那么我想它对深度学习的主要贡献

125
00:06:32,960 --> 00:06:35,060
就是展示给人们

126
00:06:35,060 --> 00:06:37,590
怎样使用生成模型来完成这些任务

127
00:06:37,590 --> 00:06:41,505
最后 我们会用其它形式的生成模型来取代生成对抗模型

128
00:06:41,505 --> 00:06:47,870
所以现在我大约用了40%的时间来让生成对抗网络变得更稳定

129
00:06:47,870 --> 00:06:50,780
我明白了 很赞 那么就像10年前

130
00:06:50,780 --> 00:06:53,765
有很多人投身于深度学习领域一样 比如你

131
00:06:53,765 --> 00:06:54,963
最终会成为行业先锋

132
00:06:54,963 --> 00:06:57,360
也许如今加入研究或使用生成对抗网络的人

133
00:06:57,360 --> 00:07:00,120
如果能成功的话 最终也会开拓一片新天地

134
00:07:00,120 --> 00:07:04,220
是的 很多人已经成为生成对抗网络的前期领航人

135
00:07:04,220 --> 00:07:09,105
我觉得 假如你想给生成对抗网络勾勒出一些历史故事的话

136
00:07:09,105 --> 00:07:12,740
不得不提到一些团队 比如Indico

137
00:07:12,740 --> 00:07:17,280
和Facebook以及伯克利实验室做出的各种贡献

138
00:07:17,280 --> 00:07:19,735
那么除了你的研究

139
00:07:19,735 --> 00:07:24,300
你还合著了一本关于深度学习的书 能不能和我们分享一下?

140
00:07:24,300 --> 00:07:26,897
是的 和Yoshua Bengio以及Aaron Courville一起写的

141
00:07:26,897 --> 00:07:29,900
他们是我的博导

142
00:07:29,900 --> 00:07:35,465
我们编写了第一本关于现代深度学习的教科书

143
00:07:35,465 --> 00:07:38,615
有英文和中文两个版本

144
00:07:38,615 --> 00:07:42,920
这本书获得了大家的肯定

145
00:07:42,920 --> 00:07:48,915
两个版本加起来一共售出了大约有7万本

146
00:07:48,915 --> 00:07:54,730
很多学生反馈 这本书让他们获益匪浅

147
00:07:54,730 --> 00:07:58,940
在编写这本书时 与其他书有一点不一样的是 我们最开始强调

148
00:07:58,940 --> 00:08:03,905
在涉足深度学习领域时所需要用到的数学基础知识

149
00:08:03,905 --> 00:08:07,670
你在斯坦福的课程中提到 线性代数和概率论

150
00:08:07,670 --> 00:08:11,570
是非常重要的数学基础 这让我记忆深刻

151
00:08:11,570 --> 00:08:15,230
大家听到机器学习的算法 都会非常兴奋

152
00:08:15,230 --> 00:08:18,500
但是如果真的想成为一个杰出的从业者

153
00:08:18,500 --> 00:08:26,055
必须精通这些基础数学 否则无异于建造空中楼阁

154
00:08:26,055 --> 00:08:27,290
所以我们在这本书的开始部分

155
00:08:27,290 --> 00:08:31,345
就集中列出了所需的数学基础知识

156
00:08:31,345 --> 00:08:34,153
这样你就不用去学习所有的线性代数的知识

157
00:08:34,153 --> 00:08:35,900
而是可以迅速了解

158
00:08:35,900 --> 00:08:37,770
那些对于深度学习来说非常重要的

159
00:08:37,770 --> 00:08:40,540
学习算法所需要用到的线性代数方面的基础

160
00:08:40,540 --> 00:08:44,660
因此对于一些数学并不是很好或者很多年都没有接触过数学的人来说

161
00:08:44,660 --> 00:08:47,000
这个部分可以让他们无需其他教材 从这本书中直接学习所需要的知识

162
00:08:47,000 --> 00:08:49,790
再开始深度学习的学习 是这样吗?

163
00:08:49,790 --> 00:08:52,175
所有你需要知道的知识都在那里

164
00:08:52,175 --> 00:08:59,520
这必然也需要花一些精力去学习和实践

165
00:08:59,520 --> 00:08:59,684
嗯 嗯 好很好

166
00:08:59,684 --> 00:09:01,370
如果有的人确实很害怕数学

167
00:09:01,370 --> 00:09:03,700
这可能会有一点痛苦

168
00:09:03,700 --> 00:09:08,323
但是如果你已经下定决心去学一些东西并且坚信可以掌握它

169
00:09:08,323 --> 00:09:11,360
我想所有你需要的东西都在书里了

170
00:09:11,360 --> 00:09:15,470
对于像你这样 在深度学习领域沉浸了很长时间的人来说

171
00:09:15,470 --> 00:09:18,710
我比较好奇 如果你现在回首过去

172
00:09:18,710 --> 00:09:21,050
能不能和我们分享一下 你觉得人工智能和深度学习

173
00:09:21,050 --> 00:09:24,650
这些年都发生了怎样的演变

174
00:09:24,650 --> 00:09:28,595
十年前 我觉得机器学习领域像是一个社区

175
00:09:28,595 --> 00:09:31,580
在机器学习里最大的挑战是如何

176
00:09:31,580 --> 00:09:34,715
利用深度学习来完成人工智能相关的任务

177
00:09:34,715 --> 00:09:39,440
在一些很简单的任务上 其实我们有一些非常好的工具

178
00:09:39,440 --> 00:09:44,555
比如识别提取特征的规律

179
00:09:44,555 --> 00:09:47,000
人类的设计师可以做许多工作

180
00:09:47,000 --> 00:09:51,965
来创造一些特征然后提交给计算机

181
00:09:51,965 --> 00:09:54,170
而现在 这同样适用于很多不同的领域

182
00:09:54,170 --> 00:09:56,750
比如预测某一个用户可能会点击广告

183
00:09:56,750 --> 00:10:01,895
比如做一些基础的科学研究分析

184
00:10:01,895 --> 00:10:07,505
但真正困难的是分析一个图像里上百万的像素

185
00:10:07,505 --> 00:10:10,150
或者一段音频波形文件

186
00:10:10,150 --> 00:10:13,950
这需要系统从无到有建立学习体系

187
00:10:13,950 --> 00:10:18,880
大约5年前 我们解决了这个障碍

188
00:10:18,880 --> 00:10:22,180
现在 我们所处的时代

189
00:10:22,180 --> 00:10:26,268
愿意加入人工智能领域的研究者 可以选择许多不同的方向

190
00:10:26,268 --> 00:10:31,060
或许现在最难的部分是让他们决定该走哪一个方向

191
00:10:31,060 --> 00:10:35,500
你想让强化学习和监督学习一样成功吗?

192
00:10:35,500 --> 00:10:40,410
你想让无监督学习和监督学习一样成功吗?

193
00:10:40,410 --> 00:10:44,333
你想要确保机器学习的算法很均衡

194
00:10:44,333 --> 00:10:48,460
避免我们想要杜绝的偏差吗?

195
00:10:48,460 --> 00:10:54,565
如果能确保人工智能在社会上不会引发问题

196
00:10:54,565 --> 00:10:58,535
例如引起社会动荡和造成大量人类失业

197
00:10:58,535 --> 00:11:03,440
那我们就能保证每个人都可以受惠于人工智能

198
00:11:03,440 --> 00:11:04,600
我想现在

199
00:11:04,600 --> 00:11:08,025
的确有很多非比寻常的事情我们可以做

200
00:11:08,025 --> 00:11:11,380
在预防人工智能带来负面影响的同时

201
00:11:11,380 --> 00:11:14,965
确保我们能利用它带给我们的好处

202
00:11:14,965 --> 00:11:19,800
现在 有很多人想参与AI工作

203
00:11:19,800 --> 00:11:23,285
你能给他们分享下你的建议吗？

204
00:11:23,285 --> 00:11:26,950
我想 很多想从事AI工作的人开始会想

205
00:11:26,950 --> 00:11:32,200
他们绝对需要获得博士学位 或者其它类似的证书

206
00:11:32,200 --> 00:11:35,155
我觉得 这实际上不是一个硬性要求

207
00:11:35,155 --> 00:11:40,285
有一种获得大量关注的方式是写点好代码 然后放到GitHub上

208
00:11:40,285 --> 00:11:43,380
如果你有一个有趣的项目

209
00:11:43,380 --> 00:11:47,320
解决了在顶级水平工作的人员想要解决的问题

210
00:11:47,320 --> 00:11:49,840
一旦他们发现你的GitHub库

211
00:11:49,840 --> 00:11:53,450
他们会去找你 邀请你一起工作

212
00:11:53,450 --> 00:11:56,140
去年我在OpenAI以及

213
00:11:56,140 --> 00:12:00,010
今年我在谷歌招聘的很多人员 都是这种类型

214
00:12:00,010 --> 00:12:02,755
我第一次对与他们共事感兴趣是因为

215
00:12:02,755 --> 00:12:06,895
我看到了他们在互联网上的开源论坛上发布的一些代码

216
00:12:06,895 --> 00:12:11,275
另外 通过写论文 并且把论文发布到arxiv.org也很好

217
00:12:11,275 --> 00:12:12,745
有时候 在反复锤炼某个理念之后

218
00:12:12,745 --> 00:12:16,750
足以真正在学术界做出新的学术贡献

219
00:12:16,750 --> 00:12:20,860
但很多时候 达到这一点比较难

220
00:12:20,860 --> 00:12:27,885
相比之下 通常创造出有用的软件产品 到达这一高度的时间要短的多

221
00:12:27,885 --> 00:12:30,022
所以 认真读书

222
00:12:30,022 --> 00:12:33,930
练习相关材料 把它放到GitHub 或者arxiv.org上

223
00:12:33,930 --> 00:12:36,100
我觉得 如果你通过看书学习

224
00:12:36,100 --> 00:12:39,454
那么同时做项目也真的很重要

225
00:12:39,454 --> 00:12:42,730
或者 选择一种合适的方式

226
00:12:42,730 --> 00:12:46,555
把机器学习的知识应用到你已经感兴趣的领域中

227
00:12:46,555 --> 00:12:50,500
比如 如果你是一个野外生物学家 你希望进入深度学习领域

228
00:12:50,500 --> 00:12:53,255
或许你可以使用深度学习来鉴别鸟类

229
00:12:53,255 --> 00:12:56,905
或者 如果你不知道在你的生活中如何应用机器学习

230
00:12:56,905 --> 00:13:01,600
你可以挑选一些东西来实践 比如创造一个街景门牌号分类器

231
00:13:01,600 --> 00:13:05,580
其中所有的数据集已经有了 对你来说相当简单

232
00:13:05,580 --> 00:13:07,330
采用这种方式 你可以

233
00:13:07,330 --> 00:13:09,700
在阅读课本或

234
00:13:09,700 --> 00:13:14,105
观看Coursera讲解概念的视频时练习所有的基本技能

235
00:13:14,105 --> 00:13:15,670
那么 最近几年

236
00:13:15,670 --> 00:13:20,045
我看到你在对抗样本方面做了许多工作

237
00:13:20,045 --> 00:13:21,535
能跟我们谈一谈吗？

238
00:13:21,535 --> 00:13:24,490
当然可以 我认为对抗样本

239
00:13:24,490 --> 00:13:29,835
开启了一个新的领域 我把它称为机器学习安全

240
00:13:29,835 --> 00:13:33,250
过去 我们见过一些计算机安全问题

241
00:13:33,250 --> 00:13:38,275
其中攻击者可以愚弄计算机 让它执行错误的代码

242
00:13:38,275 --> 00:13:40,890
这被称为应用级安全

243
00:13:40,890 --> 00:13:46,300
还有一种假冒身份的攻击 人们可以愚弄计算机 让它相信

244
00:13:46,300 --> 00:13:52,545
网络中的消息来源于某个人 但实际上并不是它们说的那个人

245
00:13:52,545 --> 00:13:55,025
这被称为网络级安全

246
00:13:55,025 --> 00:13:57,230
现在 我们可以看到 你也可以愚弄

247
00:13:57,230 --> 00:13:59,920
机器学习算法 让它们做它们不应该做的事情

248
00:13:59,920 --> 00:14:06,010
即使运行机器学习算法的程序执行了正确的代码

249
00:14:06,010 --> 00:14:07,960
即使运行机器学习算法

250
00:14:07,960 --> 00:14:10,025
的程序知道

251
00:14:10,025 --> 00:14:13,605
所有这些信息是来自于谁

252
00:14:13,605 --> 00:14:17,050
所以 我觉得 在开发一项新技术的起步阶段

253
00:14:17,050 --> 00:14:20,830
就要把构筑安全性放在重要位置

254
00:14:20,830 --> 00:14:27,065
我们发现 先构建一个可以运行的系统 再添加安全 这是非常困难的

255
00:14:27,065 --> 00:14:30,640
所以 当我听到如果我们现在深入进去 开始预测机器学习面临的安全问题这一理念时

256
00:14:30,640 --> 00:14:34,705
真的非常激动

257
00:14:34,705 --> 00:14:37,600
我们可以确保 这些算法一开始就很安全

258
00:14:37,600 --> 00:14:41,650
而不是若干年后想方设法从头开始打补丁

259
00:14:41,650 --> 00:14:43,111
谢谢 太棒了

260
00:14:43,111 --> 00:14:46,090
我觉得 你的故事里面有很多迷人的地方

261
00:14:46,090 --> 00:14:47,470
虽然我们已经认识了很多年

262
00:14:47,470 --> 00:14:49,935
但是我实际上并没有真正了解你 所以谢谢您今天的分享

263
00:14:49,935 --> 00:14:53,090
偶 不客气 谢谢您邀请我 这是个很好的机会

264
00:14:53,090 --> 00:14:53,630
好
谢谢大家

265
00:14:53,630 --> 00:14:55,010
谢谢<br />翻译 | 审阅：Cousera Global Translator Community