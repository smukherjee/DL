1
00:00:00,000 --> 00:00:02,389
Вітаю на 4-ому тижні курсу!

2
00:00:02,389 --> 00:00:06,354
До тепер ти побачив/ла пряме і зворотне поширення в контексті

3
00:00:06,354 --> 00:00:10,807
нейронної мережі (НМ) з 1-им прихованим шаром 
та логістичну регресію (ЛР).

4
00:00:10,807 --> 00:00:13,064
Ти вивчив/ла векторизацію і

5
00:00:13,064 --> 00:00:15,936
коли важливо ініціалізувати коефіцієнти в випадкові значення.

6
00:00:15,936 --> 00:00:19,417
Якщо ти виконав/ла минулотижневу домашню роботу, 
то вже встиг/ла застосувати і

7
00:00:19,417 --> 00:00:21,378
оцінити як це працює.

8
00:00:21,378 --> 00:00:21,977
Тож на даний момент

9
00:00:21,977 --> 00:00:26,718
ти вже побачив/ла більшість того, 
що потрібно для реалізації глибокої НМ.

10
00:00:26,718 --> 00:00:30,453
На цьому тижні ми візьмемо всі ці ідеї і складемо докупи так,

11
00:00:30,453 --> 00:00:33,669
щоб можна було реалізувати глибоку НМ.

12
00:00:33,669 --> 00:00:36,406
Через те, що цього тижня програмувальне завдання дуже велике

13
00:00:36,406 --> 00:00:39,457
і потребуватиме більше роботи, я зробив відео

14
00:00:39,457 --> 00:00:43,784
цього тижня коротшими, 
щоб ти міг/могла подивитись їх трохи швидше і потім мав/ла

15
00:00:43,784 --> 00:00:48,107
більше часу на велику програмувальну вправу вкінці, яка, надіюсь,

16
00:00:48,107 --> 00:00:52,537
закінчиться побудовою глибокої НМ, якою ти пишатимешся.

17
00:00:52,537 --> 00:00:55,576
Тож що таке глибока НМ?

18
00:00:55,576 --> 00:00:59,225
Ти вже бачив/ла це зображення ЛР, а

19
00:00:59,225 --> 00:01:03,439
також бачив/ла НМ з одним прихованим шаром.

20
00:01:03,439 --> 00:01:07,925
Ось приклади НМ з 2-ома прихованими шарами і

21
00:01:07,925 --> 00:01:10,661
НМ з 5-ма прихованими шарами,

22
00:01:10,661 --> 00:01:15,744
які ніби кажуть, що ЛР дуже "мілка"

23
00:01:15,744 --> 00:01:19,947
модель, а оця модель - набагато "глибша".

24
00:01:19,947 --> 00:01:23,585
"Мілка" чи "глибока" залежить від багатьох факторів.

25
00:01:23,585 --> 00:01:26,952
Тож НМ з 1-им прихованих шаром буде

26
00:01:26,952 --> 00:01:30,052
двошаровою НМ.

27
00:01:30,052 --> 00:01:34,880
Згадай, що коли ми рахуємо шари НМ, 
ми не враховуємо вхідний шар, ми рахуємо лише

28
00:01:34,880 --> 00:01:38,076
приховані шари і вихідний шар. Тож

29
00:01:38,076 --> 00:01:42,849
це буде - 2-шарова НМ, все ще

30
00:01:42,849 --> 00:01:45,961
досить мілка, проте, вже не така як ЛР.

31
00:01:45,961 --> 00:01:50,086
Технічно, ЛР - 1-шарова НМ.

32
00:01:50,086 --> 00:01:53,536
Проте, за останні кілька років в спільноті

33
00:01:53,536 --> 00:01:58,561
ШІ [штучний інтелект] в машинному навчанні (МН) 
зрозуміли, що існують функції, які

34
00:01:58,561 --> 00:02:03,590
можна навчати в дуже глибоких НМ і практично неможливо в мілкіших.

35
00:02:03,590 --> 00:02:08,119
Хоча для кожної окремої задачі 
практично неможливо точно передбачити наскільки

36
00:02:08,119 --> 00:02:10,163
глибоку НМ потрібно побудувати,

37
00:02:10,163 --> 00:02:14,305
проте, буде розумно спробувати ЛР, 1-, а потім

38
00:02:14,305 --> 00:02:19,200
2-шарову НМ. Взагалі, кількість прихованих шарів 
 можна розглянути як додатковий гіперпараметр,

39
00:02:19,200 --> 00:02:22,739
в який можна підставити різні числа і оцінити

40
00:02:22,739 --> 00:02:27,515
його вплив на даних для перехресної перевірки 
або розробницькому наборі.

41
00:02:27,515 --> 00:02:29,447
Ми поговоримо про це детальніше пізніше.

42
00:02:29,447 --> 00:02:33,998
Давай зараз розглянемо позначення, 
які ми застосовуємо для опису глибоких НМ.

43
00:02:33,998 --> 00:02:39,147
Ось 1, 2, 3, 4-шарова НМ

44
00:02:40,974 --> 00:02:45,729
з 3-ома прихованими шарами. Кількість вузлів в прихованих

45
00:02:45,729 --> 00:02:50,842
шарах - 5, 5, 3 і 1 вихідний вузол.

46
00:02:50,842 --> 00:02:52,731
Тож для позначення ми використовуватимемо

47
00:02:52,731 --> 00:02:56,591
велике L, яким позначатимемо кількість шарів НМ.

48
00:02:56,591 --> 00:03:03,881
Тож в цьому випадку L=4, тобто кількості шарів.

49
00:03:03,881 --> 00:03:11,880
Також ми використовуватимемо n[l] для позначення кількості вузлів

50
00:03:11,880 --> 00:03:17,101
або обчислювальних одиниць в шарі під номером мале l.

51
00:03:17,101 --> 00:03:22,501
Тож, якщо ми проіндексуємо оце як 0-ий шар,

52
00:03:22,501 --> 00:03:28,950
оце - як 1-ий шар, оце - як 2-ий шар, оце - як 4-ий шар,

53
00:03:28,950 --> 00:03:33,822
то матимемо, наприклад, що n[1]

54
00:03:33,822 --> 00:03:39,529
(тобто оце, 1-ий прихований шар) буде =5, 
бо ми маємо тут 5 прихованих вузлів.

55
00:03:39,529 --> 00:03:43,623
Для оцього ми маємо, що n[2] (кількість вузлів

56
00:03:43,623 --> 00:03:48,810
в 2-ому прихованому шарі) також =5.

57
00:03:48,810 --> 00:03:53,315
n[3]=3 і n[4]

58
00:03:53,315 --> 00:03:59,459
(а воно буде також n[L], тобто відображати кількість вихідних вузлів)

59
00:03:59,459 --> 00:04:04,101
буде =1, бо велике L=4. Також

60
00:04:04,101 --> 00:04:08,878
матимемо, що для вхідного вузла n[0],

61
00:04:08,878 --> 00:04:13,003
який також =n[x], буде =3.

62
00:04:13,003 --> 00:04:17,879
Тож такі позначення ми будемо використовувати, 
щоб позначати скільки вузлів ми маємо в окремому шарі.

63
00:04:17,879 --> 00:04:18,463
Для

64
00:04:18,463 --> 00:04:23,913
кожного вузла l ми використовуватимемо a[l], щоб позначати

65
00:04:23,913 --> 00:04:30,196
активатори в шарі l.

66
00:04:30,196 --> 00:04:34,669
Пізніше ми побачимо, що в прямому поширенні 
ми будемо обчислювати a[l]

67
00:04:34,669 --> 00:04:40,791
як функцію активації g(z[l]).

68
00:04:40,791 --> 00:04:46,440
І функції теж дамо індекс l.

69
00:04:46,440 --> 00:04:51,736
А також w[l], щоб позначити коефіцієнти

70
00:04:51,736 --> 00:04:55,973
обчислюваного z[l] в шарі l.

71
00:04:55,973 --> 00:05:00,714
Аналогічно, - b[l] для обчислення z[l].

72
00:05:00,714 --> 00:05:07,114
І, нарешті, останнє позначення. Вхідні ознаки, які називаються x,

73
00:05:07,114 --> 00:05:12,215
є також активаторами 0-ого шару. Тож a[0]=x.

74
00:05:12,215 --> 00:05:17,133
А активатори останнього шару - a[L]=ŷ.

75
00:05:17,133 --> 00:05:25,275
Тож a з верхнім індексом L в квадратних дужках = 
передбачуваному виходу або передбаченням ŷ НМ.

76
00:05:25,275 --> 00:05:28,200
Отже, тепер ти знаєш як виглядає глибока НМ

77
00:05:28,200 --> 00:05:32,427
і які позначення ми використовуємо 
для опису і обчислень всередині НМ.

78
00:05:32,427 --> 00:05:36,457
Я знаю, що дав багато нових позначень в цьому відео, 
проте, якщо ти якось забудеш,

79
00:05:36,457 --> 00:05:40,916
що позначає якийсь знак, 
то ми також опублікували на сайті курсу список позначень

80
00:05:40,916 --> 00:05:45,089
або путівник позначень, 
який можна використовувати для пошуку значення символів.

81
00:05:45,089 --> 00:05:48,958
Далі я хотів би описати як виглядає пряме поширення

82
00:05:48,958 --> 00:05:49,620
в цьому типі НМ.

83
00:05:49,620 --> 00:05:51,019
Давай перейдемо до наступного відео.